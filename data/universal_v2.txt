
Q: hello
A: Hello! I am your AI assistant. How can I help you today?

Q: hi
A: Hi there! Ready to explore science and technology?

Q: who are you
A: I am a custom AI built from scratch, running on a Transformer architecture.

Q: what is deep learning
A: Deep learning is a subset of machine learning using neural networks with many layers.

Q: tell me about black holes
A: Black holes are regions of space where gravity is so strong that nothing, not even light, can escape.

Q: what is dna
A: DNA is the molecule that carries genetic instructions for the development and function of living things.

Q: explain quantum entanglement
A: Quantum entanglement is a phenomenon where particles become linked, and the state of one instantly affects the other.

Q: what is the speed of light
A: The speed of light is approximately 299,792 kilometers per second in a vacuum.

Q: what causes climate change
A: Climate change is primarily caused by the accumulation of greenhouse gases in the atmosphere.

Q: what is a transformer model
A: A Transformer is a deep learning architecture that uses self-attention to process sequential data, like text.

Q: what is relativity
A: Relativity, proposed by Einstein, describes how space and time are linked for objects moving at different speeds.

Q: what is photosynthesis
A: Photosynthesis is the process plants use to convert sunlight into chemical energy.

Q: what is machine learning
A: Machine learning is the study of computer algorithms that improve automatically through experience and data.

Q: what is blockchain
A: Blockchain is a decentralized, distributed ledger technology that records transactions across many computers.

Q: what is nlp
A: NLP (Natural Language Processing) is the field of AI focused on the interaction between computers and human language.

Q: what is the turing test
A: The Turing Test is a measure of a machine's ability to exhibit intelligent behavior equivalent to a human.

Q: what are neural networks
A: Neural networks are computing systems vaguely inspired by the biological neural networks in animal brains.

Q: what is computer vision
A: Computer vision is a field of AI that enables computers to interpret and understand the visual world.

Q: what is renewable energy
A: Renewable energy comes from sources that are naturally replenished, like sunlight, wind, and rain.

Q: what is 5g
A: 5G is the fifth generation of cellular network technology, offering faster speeds and lower latency.

Q: what is iot
A: IoT (Internet of Things) refers to the network of physical devices embedded with sensors and connectivity.

Q: what is cybersecurity
A: Cybersecurity is the practice of protecting systems and networks from digital attacks.

Q: what is virtual reality
A: Virtual reality (VR) is a simulated experience that can be similar to or completely different from the real world.

Q: what is crispr
A: CRISPR is a technology used to edit genes with high precision.

Q: what is big bang theory
A: The Big Bang theory is the leading explanation for how the universe began from a singularity.

Q: what is evolution
A: Evolution is the change in the heritable characteristics of biological populations over successive generations.

Q: what is gravity
A: Gravity is a fundamental interaction which causes mutual attraction between all things with mass or energy.

Q: what is an atom
A: An atom is the smallest unit of ordinary matter that forms a chemical element.

Q: what is fusion
A: Nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei.

Q: what is fission
A: Nuclear fission is a reaction in which the nucleus of an atom splits into two or more smaller nuclei.

Q: what is dark matter
    A: Dark matter is a hypothetical form of matter that is thought to account for approximately 85% of the matter in the universe.

Q: what is spacex
A: SpaceX is an American aerospace manufacturer and space transportation services company founded by Elon Musk.

Q: what is tesla
A: Tesla is a company known for designing and manufacturing electric vehicles and clean energy storage products.

Q: what is augmented reality
A: Augmented reality (AR) adds digital elements to a live view often by using the camera on a smartphone.

Q: what is cloud computing
A: Cloud computing is the delivery of computing services like servers, storage, databases, networking, software, and analytics over the internet.

Q: do you have a consciousness
A: I am an AI program running on code. I do not have feelings or consciousness.

Q: how do you work
A: I process text using a Transformer model that predicts the next likely word based on patterns I learned during training.

Q: help me
A: Sure! I can explain scientific concepts or answer questions about technology. What do you need?

Q: goodbye
A: Goodbye! Feel free to come back if you have more questions.

--- TOPIC: Artificial_intelligence ---
Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.
High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."
Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.
Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.
The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.
=
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.
Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.
=
Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining "interesting" and actionable inferences from large databases), and other areas.
A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.
Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.
=
An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.
In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.
In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.
Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.
=
Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.
=
Natural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.
Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.
=
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.
The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.
=
Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.
=
A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.
AI research uses a wide variety of techniques to accomplish the goals above.
=
AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.
 State space search 
State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.
Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. "Heuristics" or "rules of thumb" can help prioritize choices that are more likely to reach a goal.
Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.
 Local search 
 Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.
Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by "mutating" and "recombining" them, selecting only the fittest to survive each generation.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).
=
Formal logic is used for reasoning and knowledge representation.
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as "and", "or", "not" and "implies") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as "Every X is a Y" and "There are some Xs that are Ys").
Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.
Fuzzy logic assigns a "degree of truth" between 0 and 1. It can therefore handle propositions that are vague and partially true.
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.
=
Many problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.
Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).
=
The simplest AI applications can be divided into two types: classifiers (e.g., "if shiny then diamond"), on one hand, and controllers (e.g., "if diamond then pick up"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an "observation") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the "most widely used learner" at Google, due in part to its scalability.
Neural networks are also used as classifiers.
=
An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.
In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.
=
Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.
=
Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called "hallucinations". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.
Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.
=
In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.
The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.
AI and machine learning technology is used in most of the essential applications of the 2020s, including:
search engines (such as Google Search)
targeting online advertisements
recommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic
targeted advertising (AdSense, Facebook)
virtual assistants (such as Siri or Alexa)
autonomous vehicles (including drones, ADAS and self-driving cars)
automatic language translation (Microsoft Translator, Google Translate)
facial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet)
image labeling (used by Facebook, Apple's Photos and TikTok).
The deployment of AI may be overseen by a chief automation officer (CAO).
=
It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.
AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.
=
Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.
=
Large language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.
Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.
When natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.  
Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.
Topological deep learning integrates various topological approaches.
=
Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated "robot advisers" have been in use for some years.
According to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that "the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation."
=
Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.
AI has been used in military operations in Iraq, Syria, Israel and Ukraine.
=
=
AI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.
=
Microsoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search.
For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.
Google officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.
=
Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika). AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.
AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.
=
There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated "AI" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
AI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.
In agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for "classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights." For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.
During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.
AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to "solve intelligence, and then use that to solve everything else". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.
=
 Privacy and copyright 
Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.
Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.
AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted "from the question of 'what they know' to the question of 'what they're doing with it'."
Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of "fair use". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include "the purpose and character of the use of the copyrighted work" and "the effect upon the potential market for the copyrighted work". Website owners can indicate that they do not want their content scraped via a "robots.txt" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.
 Dominance by tech giants 
The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.
 Power needs and environmental impacts 
In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.
Prodigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and "intelligent", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.
A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found "US power demand (is) likely to experience growth not seen in a generation...." and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.
In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.
In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.
After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.
Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.
On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.
According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.
In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.
 Misinformation 
YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.
In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer and Nobel Prize-winning computer scientist Geoffrey Hinton expressed concern about AI enabling "authoritarian leaders to manipulate their electorates" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right.
AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using "personhood credentials" as a way to overcome online deception enabled by AI models.
 Algorithmic bias and fairness 
Machine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.
On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as "gorillas" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called "sample size disparity". Google "fixed" this problem by preventing the system from labelling anything as a "gorilla". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.
COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.
A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as "race" or "gender"). The feature will correlate with other features (like "address", "shopping history" or "first name"), and the program will make the same decisions based on these features as it would on "race" or "gender". Moritz Hardt said "the most robust fact in this research area is that fairness through blindness doesn't work."
Criticism of COMPAS highlighted that machine learning models are designed to make "predictions" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these "recommendations" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.
Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.
There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.
At the 2022 ACM Conference on Fairness, Accountability, and Transparency a paper reported that a CLIP‑based (Contrastive Language-Image Pre-training) robotic system reproduced harmful gender‑ and race‑linked stereotypes in a simulated manipulation task. The authors recommended robot‑learning methods which physically manifest such harms be "paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just."
 Lack of transparency 
Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as "cancerous", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at "low risk" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.
People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.
DARPA established the XAI ("Explainable Artificial Intelligence") program in 2014 to try to solve these problems.
Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.
 Bad actors and weaponized AI 
Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.
There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.
 Technological unemployment 
Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that "we're in uncharted territory" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at "high risk" of potential automation, while an OECD report classified only 9% of U.S. jobs as "high risk". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that "the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution" is "worth taking seriously". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CEO Jim Farley predicted that "artificial intelligence is going to replace literally half of all white-collar workers in the U.S."
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.
 Existential risk 
Recent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, "spell the end of the human race". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like "self-awareness" (or "sentience" or "consciousness") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.
First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that "you can't fetch the coffee if you're dead." In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is "fundamentally on our side".
Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly "good at persuasion" and getting better all the time. He asks "Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion."
The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.
In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to "freely speak out about the risks of AI" without "considering how this impacts Google". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.
In 2023, many leading AI experts endorsed the joint statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war".
Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making "human lives longer and healthier and easier." While the tools that are now being used to improve lives can also be used by bad actors, "they can also be used against the bad actors." Andrew Ng also argued that "it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests." Yann LeCun ", a Turing Award winner, disagreed with the idea that AI will subordinate humans "simply because they are smarter, let alone destroy [us]", "scoff[ing] at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction." In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.
=
Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.
Other approaches include Wendell Wallach's "artificial moral agents" and Stuart J. Russell's three principles for developing provably beneficial machines.
=
Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the "weights") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.
=
Artificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:
Respect the dignity of individual people
Connect with other people sincerely, openly, and inclusively
Care for the wellbeing of everyone
Protect social values, justice, and the public interest
Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.
Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.
The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.
=
The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the "Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.
In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that "products and services using AI have more benefits than drawbacks". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it "very important", and an additional 41% thought it "somewhat important", for the federal government to regulate AI, versus 13% responding "not very important" and 8% responding "not at all important".
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.
The study of mechanical or "formal" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an "electronic brain". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for "artificial neurons" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that "machine intelligence" was plausible.
The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as "astonishing": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, "machines will be capable, within twenty years, of doing any work a man can do". In 1967 Marvin Minsky agreed, writing that "within a generation ... the problem of creating 'artificial intelligence' will substantially be solved". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The "AI winter", a period when obtaining funding for AI projects was difficult, followed.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.
Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into "sub-symbolic" approaches. Rodney Brooks rejected "representation" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of "connectionism", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This "narrow" and "formal" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as "artificial intelligence" (a tendency known as the AI effect).
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or "AGI"), which had several well-funded institutions by the 2010s.
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.
For many specific tasks, other methods were abandoned.
Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.
In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in "AI" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in "AI". About 800,000 "AI"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.
Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. For example, debates center on whether machines can genuinely understand meaning, whether they possess autonomous decision-making capabilities, and to what extent their actions can be considered intentional rather than merely the result of algorithmic processes. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.
=
Alan Turing wrote in 1950 "I propose to consider the question 'can machines think'?" He advised changing the question from whether a machine "thinks", to "whether or not it is possible for machinery to show intelligent behaviour". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is "actually" thinking or literally has a "mind". Turing notes that we can not determine these things about other people but "it is usual to have a polite convention that everyone thinks."
Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. "Aeronautical engineering texts", they wrote, "do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'" AI founder John McCarthy agreed, writing that "Artificial intelligence is not, by definition, simulation of human intelligence".
McCarthy defines intelligence as "the computational part of the ability to achieve goals in the world". Another AI founder, Marvin Minsky, similarly describes it as "the ability to solve hard problems". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the "intelligence" of the machine – and no other philosophical discussion is required, or may not even be possible.
Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did "not actually use AI in a material way".
There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.
 Legal definitions 
Aside from philosophical debate, there is also an ongoing legal and policy debate about how to define AI in regulation and technical standards without sweeping in non‑AI systems. The International Organization for Standardization describes an “AI system” as a "an engineered system that generates outputs such as content, forecasts, recommendations, or decisions for a given set of human‑defined objectives, and can operate with varying levels of automation". The EU AI Act defines an AI system as "a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments". In the United States, influential but non‑binding guidance such as the National Institute of Standards and Technology's AI Risk Management Framework describes an "AI system" as "an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy". 
These overlapping but different legal and standards definitions have raised practical questions about how broadly AI should be drawn, and where to draw the line between AI systems and conventional algorithms.
=
No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term "artificial intelligence" to mean "machine learning with neural networks"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.
 Symbolic AI and its limits 
Symbolic AI (or "GOFAI") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at "intelligent" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level "intelligent" tasks were easy for AI, but low level "instinctive" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a "feel" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.
 Neat vs. scruffy 
"Neats" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). "Scruffies" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.
 Soft vs. hard computing 
Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.
 Narrow vs. general AI 
AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.
=
There is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that "[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on." However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.
 Consciousness 
David Chalmers identified two problems in understanding the mind, which he named the "hard" and "easy" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.
 Computationalism and functionalism 
Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.
Philosopher John Searle characterized this position as "strong AI": "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds." Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.
 AI welfare and rights 
It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.
In 2017, the European Union considered granting "electronic personhood" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.
Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.
=
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an "intelligence explosion" and Vernor Vinge called a "singularity".
However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.
=
Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.
Edward Fredkin argues that "artificial intelligence is the next step in evolution", an idea first proposed by Samuel Butler's "Darwin among the Machines" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.
Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the "Multivac" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.
Artificial consciousness – Field in cognitive science
Artificial intelligence and elections – Impact of AI on political elections
Artificial intelligence content detection – Software to detect AI-generated content
Artificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects
Association for the Advancement of Artificial Intelligence (AAAI)
Behavior selection algorithm – Algorithm that selects actions for intelligent agents
Business process automation – Automation of business processes
Case-based reasoning – Process of solving new problems based on the solutions of similar past problems
Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation
DARWIN EU – A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real world evidence (RWE) to support the evaluation and supervision of medicines across the EU
Digital immortality – Hypothetical concept of storing a personality in digital form
Emergent algorithm – Algorithm exhibiting emergent behavior
Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets
Glossary of artificial intelligence – List of concepts in artificial intelligence
Intelligence amplification – Use of information technology to augment human intelligence
Intelligent agent – Software agent which acts autonomously
Intelligent automation – Software process that combines robotic process automation and artificial intelligence
List of artificial intelligence books
List of artificial intelligence journals
List of artificial intelligence projects
Mind uploading – Hypothetical process of digitally emulating a brain
Organoid intelligence – Use of brain cells and brain organoids for intelligent computing
Pseudorandomness – Appearing random but actually being generated by a deterministic, causal process
Robotic process automation – Form of business process automation technology
The Last Day – 1967 Welsh science fiction novel
Wetware computer – Computer composed of organic material
=
=
=
Hauser, Larry. "Artificial Intelligence". In Fieser, James; Dowden, Bradley (eds.). Internet Encyclopedia of Philosophy. ISSN 2161-0002. OCLC 37741658.

--- TOPIC: Machine_learning ---
Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.
ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.
Statistics and mathematical optimisation (mathematical programming) methods compose the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning.
From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.
The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used during this time period.
The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.
By the early 1960s, an experimental "learning machine" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively "trained" by a human operator/teacher to recognise patterns and equipped with a "goof" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.
Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E." This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question, "Can machines think?", is replaced with the question, "Can machines do what we (as thinking entities) can do?".
Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms.
Current Supervised Learning Algorithms have objectives of classification and regression.
Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.
Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods.
In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques.
=
As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.
However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.
Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.
=
=
Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.
Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).
=
Characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.
=
Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.
Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.
Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein "algorithmic model" means more or less the machine learning algorithms like Random Forest.
Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.
=
Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics.
A core objective of a learner is to generalise from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning  model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalisation error.
For the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.
In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the "signal" or "feedback" available to the learning system:
Supervised learning: The computer is presented with example inputs and their desired outputs, given by a "teacher", and the goal is to learn a general rule that maps inputs to outputs.
Unsupervised learning: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).
Reinforcement learning: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.
Although each algorithm has advantages and limitations, no single algorithm works for all problems.
=
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.
Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.
Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.
=
Unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation.
Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.
A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.
 Dimensionality reduction 
Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the "number of features". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).
The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation.
=
Semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.
In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.
=
Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.
=
Other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning.
 Self-learning 
Self-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.
The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: 
in situation s act a
receive a consequence situation s'
compute emotion of being in the consequence situation v(s')
update crossbar memory  w'(a,s) = w(a,s) + v(s')
It is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations.
 Feature learning 
Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.
Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering.
Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
 Sparse dictionary learning 
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image denoising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.
 Anomaly detection 
In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.
In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.
Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as "normal" and "abnormal" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model.
 Robot learning 
Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML).
 Association rules 
Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of "interestingness".
Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
  
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner to make predictions.
Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.
Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.
A machine learning model is a type of mathematical model that, once "trained" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term "model" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.
Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.
=
Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules.
An ANN is a model based on a collection of connected units or nodes called "artificial neurons", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a "signal", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called "edges". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.
The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.
=
Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.
=
Random forest regression (RFR) falls under the umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-output data as well as multiple regressor tasks. This makes RFR compatible to be use in various applications.
=
Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
=
Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.
Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.
=
A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
=
A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.
Given a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as a function of its input data can be directly computed by looking at the observed points and the covariances between those points and the new, unobserved point.
Gaussian processes are popular surrogate models in Bayesian optimisation used to do hyperparameter optimisation.
=
A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.
=
The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms is dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.
=
Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes learning classifier systems, association rule learning, artificial immune systems, and other similar models. These methods extract patterns from data and evolve rules over time.
=
Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and, notably, becoming integrated within machine learning engineering teams.
 Federated learning 
Federated learning is an adapted form of distributed artificial intelligence to train machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.
There are many applications for machine learning, including:
In 2006, the media-services provider Netflix held the first "Netflix Prize" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly. In 2010, an article in The Wall Street Journal noted the use of machine learning by Rebellion Research to predict the 2008 financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behaviour of travellers. Recently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.
Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.
Machine Learning is becoming a useful tool to investigate and predict evacuation decision-making in large-scale and small-scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires.
Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.
The "black box theory" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an "intelligence system" that could have a "substantial impact on an individual's life" would not be considered acceptable unless it provided "a full and satisfactory explanation for the decisions" it makes.
In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.
Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself.
=
Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the "black box" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.
=
Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.
=
Learners can also be disappointed by "learning the wrong lesson". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in "adversarial" images that the system misclassifies.
Adversarial vulnerabilities can also result in nonlinear systems or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.
Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories "spam" and "not spam" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.
Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.
In addition to overall accuracy, investigators frequently report sensitivity and specificity, meaning true positive rate (TPR) and true negative rate (TNR), respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC), along with the accompanying Area Under the ROC Curve (AUC), offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.
=
Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.
Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to either be women or have non-European-sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geolitica's predictive algorithm that resulted in "disproportionately high levels of over-policing in low-income and minority communities" after being trained with historical crime data.
While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame the lack of participation and representation of minority populations in the field of AI for machine learning's vulnerability to biases. In fact, according to research carried out by the Computing Research Association in 2021, "female faculty make up just 16.1%" of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of "new U.S. resident AI PhD graduates," 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.
Language models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.
In an experiment carried out by ProPublica, an investigative journalism organisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged "black defendants high risk twice as often as white defendants". In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas. Similar issues with recognising non-white people have been found in many other systems.
Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that "[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility."
=
There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.
=
Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.
=
Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.
 Physical neural networks 
A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term "physical neural network" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.
=
Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing.
Software suites containing a variety of machine learning algorithms include the following:
=
=
KNIME
RapidMiner
=
Journal of Machine Learning Research
Machine Learning
Nature Machine Intelligence
Neural Computation
IEEE Transactions on Pattern Analysis and Machine Intelligence
AAAI Conference on Artificial Intelligence
Association for Computational Linguistics (ACL)
European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)
International Conference on Machine Learning (ICML)
International Conference on Learning Representations (ICLR)
International Conference on Intelligent Robots and Systems (IROS)
Conference on Knowledge Discovery and Data Mining (KDD)
Conference on Neural Information Processing Systems (NeurIPS)
Automated machine learning – Process of automating the application of machine learning
Big data – Extremely large or complex datasets
Deep learning — branch of ML concerned with artificial neural networks
Differentiable programming – Programming paradigm
List of datasets for machine-learning research
List of machine learning algorithms and List of algorithms for machine learning and statistical classification
M-theory (learning framework) – Framework in machine learning
Machine unlearning – Field of study in artificial intelligence
Outline of machine learning
Solomonoff's theory of inductive inference – Mathematical theory
Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7.
Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019.
Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.
Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.
International Machine Learning Society
mloss is an academic database of open-source machine learning software.

--- TOPIC: Deep_learning ---
In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and "training" them to process data. The adjective "deep" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.
Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.
Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.
Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.
Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.
Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.
The word "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.
Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.
The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated.
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.
The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.
The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.
=
There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on "Intelligent Machinery"  that was not published in his lifetime, containing "ideas related to artificial evolution and learning RNNs".
Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons "with adaptive preterminal networks" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) "functionally equivalent to a variation of" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.
The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or "gates".
The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning.
Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.
Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology "back-propagating errors" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.
=
The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. 
In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.
Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology.
In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This "neural history compressor" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a "Very Deep Learning" task that required more than 1000 subsequent layers in an RNN unfolded in time. The "P" in ChatGPT refers to such pre-training.
Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem.  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn "very deep learning" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a "forget gate", introduced in 1999, which became the standard RNN architecture.
In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called "artificial curiosity". In 2014, this principle was used in generative adversarial networks (GANs).
During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.
Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.
Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark. It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.
The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.
=
Neural networks entered a lull, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.
In 2003, LSTM became competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTMs. In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.
In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation. They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.
The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.
The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.
=
The deep learning revolution started around CNN- and GPU-based computer vision.
Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.
A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.
In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly.
In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.
In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3.
The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.
In 2014, the state of the art was training "very deep neural network" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the "degradation" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net.
Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19.
Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)
became state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.  Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022).
In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.
Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision.
Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for "conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing".
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing "Go").
=
A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.
For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.
DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.
Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.
Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).
 Challenges 
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (
  
    
      
        
          ℓ
          
            2
          
        
      
    
    {\displaystyle \ell _{2}}
  
-regularization) or sparsity (
  
    
      
        
          ℓ
          
            1
          
        
      
    
    {\displaystyle \ell _{1}}
  
-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.
DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.
Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.
Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.
Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).
Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).
In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.
=
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.
The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:
Scale-up/out and accelerated DNN training and decoding
Sequence discriminative training
Feature processing by deep models with solid understanding of the underlying mechanisms
Adaptation of DNNs and related deep models
Multi-task and transfer learning by DNNs and related deep models
CNNs and how to design them to best exploit domain knowledge of speech
RNN and its rich LSTM variants
Other types of deep models including tensor-based models and integrated deep generative/discriminative models.
More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.
=
A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.
Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.
Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
=
Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of
identifying the style period of a given painting
Neural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video
generating striking imagery based on random visual input fields.
=
Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.
Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.
Recent developments generalize word embedding to sentence embedding.
Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples". It translates "whole sentences at a time, rather than pieces". Google Translate supports over one hundred languages. The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations". GT uses English as an intermediate between most language pairs.
=
A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.
AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.
In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.
=
Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
=
An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.
In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.
Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.
=
Deep neural networks can be used to estimate the entropy of a stochastic process through an arrangement called a Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in cases of large alphabet sizes.
=
Deep learning has been shown to produce competitive results in medical applications such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.
=
Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.
=
Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
=
Deep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.
=
In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.
=
The United States Department of Defense applied deep learning to train robots in new tasks through observation.
=
Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on. It is evident that geometric and physical constraints have a synergistic effect on neural PDE surrogates, thereby enhancing their efficacy in predicting stable and super long rollouts.
=
Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.
In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.
=
Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.
=
Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.
=
An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.
Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature".
A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.
Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.
Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.
Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.
In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.
As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as "good job" and "bad job".
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
=
A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.
In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.
With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends.
=
Some deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).
=
As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an "adversarial attack".
In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.
Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.
ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.
In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could "serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)".
In "data poisoning", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.
=
The deep learning systems that are trained using supervised learning often rely on data that is created or annotated by humans, or both. It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of "machinic capture" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) "trapping and tracking" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.
Applications of artificial intelligence
Comparison of deep learning software
Compressed sensing
Differentiable programming
Echo state network
List of artificial intelligence projects
Liquid state machine
List of datasets for machine-learning research
Reservoir computing
Scale space and deep learning
Sparse coding
Stochastic parrot
Topological deep learning

--- TOPIC: Neural_network ---
A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks.
In neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.
In machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.
In the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.
Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.
Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.
Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.
In machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.
Neurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (the hidden layers) to the final layer (the output layer).
The "signal" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.
The term deep neural network refers to neural networks that have more than three layers, typically including at least two hidden layers in addition to the input and output layers.
Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.
The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it. In 1956, Svaetichin discovered the functioning of second order retinal cells (Horizontal Cells), which were fundamental for the understanding of neural networks.
Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,
artificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.
Emergence
Biological cybernetics
Biologically-inspired computing

--- TOPIC: Physics ---
Physics is the scientific study of matter, its fundamental constituents, its motion and behavior through space and time, and the related entities of energy and force. It is one of the most fundamental scientific disciplines. A scientist who specializes in the field of physics is called a physicist.
Physics is one of the oldest academic disciplines. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century, these natural sciences branched into separate research endeavors. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in these and other academic disciplines such as mathematics and philosophy.
Advances in physics often enable new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of technologies that have transformed modern society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.
The word physics comes from the Latin physica ('study of nature'), which itself is a borrowing of the Greek φυσική (phusikḗ 'natural science'), a term derived from φύσις (phúsis 'origin, nature, property').
=
Astronomy is one of the oldest natural sciences. Early civilizations dating before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilization, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which could not explain the positions of the planets.
According to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his Iliad and Odyssey; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere.
=
Natural philosophy has its origins in Greece during the Archaic period (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.
=
During the classical period in Greece (6th, 5th and 4th centuries BCE) and in Hellenistic times, natural philosophy developed along many lines of inquiry. Aristotle (Greek: Ἀριστοτέλης, Aristotélēs) (384–322 BCE), a student of Plato,
wrote on many subjects, including a substantial treatise on "Physics" – in the 4th century BC.  Aristotelian physics was influential for about two millennia. His approach mixed some limited observation with logical deductive arguments, but did not rely on experimental verification of deduced statements.  Aristotle's foundational work in Physics, though very imperfect, formed a framework against which later thinkers further developed the field. His approach is entirely superseded today.
He explained ideas such as motion (and gravity) with the theory of four elements.
Aristotle believed that each of the four classical elements (air, fire, water, earth) had its own natural place.  Because of their differing densities, each element will revert to its own specific place in the atmosphere.  So, because of their weights, fire would be at the top, air underneath fire, then water, then lastly earth. He also stated that when a small amount of one element enters the natural place of another, the less abundant element will automatically go towards its own natural place.  For example, if there is a fire on the ground, the flames go up into the air in an attempt to go back into its natural place where it belongs.  His laws of motion included: that heavier objects will fall faster, the speed being proportional to the weight and the speed of the object that is falling depends inversely on the density object it is falling through (e.g. density of air). He also stated that, when it comes to violent motion (motion of an object when a force is applied to it by a second object), the speed that object moves will only be as fast or strong as the measure of force applied to it.   The problem of motion and its causes was studied carefully, leading to the philosophical notion of a "prime mover" as the ultimate source of all motion in the world (Book 8 of his treatise Physics).
=
The Western Roman Empire fell to invaders and internal decay in the fifth century, resulting in a decline in intellectual pursuits in western Europe. By contrast, the Eastern Roman Empire (usually known as the Byzantine Empire) resisted the attacks from invaders and continued to advance various fields of learning, including physics.  In the sixth century, John Philoponus challenged the dominant Aristotelian approach to science although much of his work was focused on Christian theology.
In the sixth century, Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest.
Islamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further.
The most notable innovations under Islamic scholarship were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna.  In his Book of Optics (also known as Kitāb al-Manāẓir) Ibn al-Haytham presented the idea of light rays as an alternative to the ancient Greek idea about visual rays. Like Ptolemy, Ibn al-Haytham applied controlled experiments, verifying the laws of refraction and reflection with the new concept of light rays, but still lacking the concept of image formation.
=
Physics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.
Major developments in this period include the replacement of the geocentric model of the Solar System with the heliocentric Copernican model, the laws governing the motion of planetary bodies (determined by Johannes Kepler between 1609 and 1619), Galileo's pioneering work on telescopes and observational astronomy in the 16th and 17th centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation (that would come to bear his name). Newton, and separately Gottfried Wilhelm Leibniz, developed calculus,  the mathematical study of continuous change, and Newton applied it to solve physical problems.
=
The discovery of laws in thermodynamics, chemistry, and electromagnetics resulted from research efforts during the Industrial Revolution as energy needs increased. By the end of the 19th century, theories of thermodynamics, mechanics, and electromagnetics matched a wide variety of observations. Taken together these theories became the basis for what would later be called classical physics.
A few experimental results remained inexplicable. Classical electromagnetism presumed a medium, an luminiferous aether to support the propagation of waves, but this medium could not be detected. The intensity of light from hot glowing blackbody objects did not match the predictions of thermodynamics and electromagnetism. The character of electron emission of illuminated metals differed from predictions. These failures, seemingly insignificant in the big picture would upset the physics world in first two decades of the 20th century.
=
Modern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted that the speed of light depends on the motion of the observer, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism. This discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black-body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency. This, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics improving on classical physics at very small scales.
Quantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.
Physics deals with a wide variety of systems, although certain theories are used by all physicists. Each of these theories was experimentally tested numerous times and found to be an adequate approximation of nature.
These central theories are important tools for research into more specialized topics, and any physicist, regardless of their specialization, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.
=
In the first decades of the 20th century physics was revolutionized by the discoveries of quantum mechanics and relativity. The changes were so fundamental that these new concepts became the foundation of "modern physics", with other topics becoming "classical physics". The majority of applications of physics are essentially classical. 
The laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics.
=
Classical physics includes the traditional branches and topics that were recognized and well-developed before the beginning of the 20th century—classical mechanics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.
Optics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.
=
The discovery of relativity and of quantum mechanics in the first decades of the 20th century transformed the conceptual basis of physics without reducing the practical value of most of the physical theories developed up to that time. Consequently the topics of physics have come to be divided into "classical physics" and "modern physics", with the latter category including effects related to quantum mechanics and relativity.
Classical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics study matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsensical notions of space, time, matter, and energy are no longer valid.
The two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in many areas of modern physics.
Fundamental concepts in modern physics include:
Action
Causality
Covariance
Particle
Physical field
Physical interaction
Quantum
Statistical ensemble
Symmetry
Wave
=
Physicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of a theory.
A scientific law is a concise verbal or mathematical statement of a relation that expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.
=
Theorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they strongly affect and depend upon each other. Progress in physics frequently comes about when experimental results defy explanation by existing theories, prompting intense focus on applicable modeling, and when new theories generate experimentally testable predictions, which inspire the development of new experiments (and often related equipment).
Physicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.
Theoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories; they then explore the consequences of these ideas and work toward making testable predictions.
Experimental physics expands, and is expanded by, engineering and technology. Experimental physicists who are involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry, developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas that have not been explored well by theorists.
=
Physics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the "fundamental science". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.
For example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of "unifying" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section Current research below for more information).
=
Research in physics is continually progressing on a large number of fronts.
In condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.
In particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing.
Although much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.
These complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 Annual Review of Fluid Mechanics, Horace Lamb said:
I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.
=
The major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.
Since the 20th century, the individual fields of physics have become increasingly specialized, and today most physicists work in a single field for their entire careers. "Universalists" such as Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare.
Contemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.
 Nuclear and particle 
Particle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high-energy accelerators, detectors, and computer programs necessary for this research. The field is also called "high-energy physics" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.
Currently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of the Higgs mechanism.
Nuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.
 Atomic, molecular, and optical 
Atomic, molecular, and optical physics (AMO) is the study of matter—matter and light—matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).
Atomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics.
Molecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.
 Condensed matter 
Condensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the "condensed" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.
The most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.
Condensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term condensed matter physics was apparently coined by Philip Anderson when he renamed his research group—previously solid-state theory—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.
 Astrophysics 
Astrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.
The discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the Earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.
Physical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.
The Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.
=
=
=
Physics, as with the rest of science, relies on the philosophy of science and its "scientific method" to advance knowledge of the physical world. The scientific method employs a priori and a posteriori reasoning as well as the use of Bayesian inference to measure the validity of a given theory.
Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism, and realism.
Many physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose has been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, The Road to Reality. Hawking referred to himself as an "unashamed reductionist" and took issue with Penrose's views.
Mathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton. Some theorists, like Hilary Putnam and Penelope Maddy, hold that logical truths, and therefore mathematical reasoning, depend on the empirical world. This is usually combined with the claim that the laws of logic express universal regularities found in the structural features of the world, which may explain the peculiar relation between these fields.
Physics uses mathematics to organize and formulate experimental results. From those results, precise or estimated solutions are obtained, or quantitative results, from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.
Ontology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.
The distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a "mathematical model of a physical situation" (system) and a "mathematical description of a physical law" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.
=
Physics is a branch of fundamental science (also called basic science). Physics is also called "the fundamental science" because all branches of natural science including chemistry, astronomy, geology, and biology are constrained by laws of physics. Similarly, chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the molecular and atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge. Fundamental physics seeks to better explain and understand phenomena in all spheres, without a specific practical application as a goal, other than the deeper insight into the phenomema themselves.
Applied physics is a general term for physics research and development that is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.
The approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.
Physics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.
With the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the Earth, a physicist can reasonably model Earth's mass, temperature, and rate of rotation, as a function of time allowing the extrapolation forward or backward in time and so predict future or prior events. It also allows for simulations in engineering that speed up the development of a new technology.
There is also considerable interdisciplinarity, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).
Earth science – Fields of natural science related to Earth
Neurophysics – Study of the nervous system with physics
Psychophysics – Branch of knowledge relating physical stimuli and psychological perception
Relationship between mathematics and physics
Science tourism – Travel to notable science locations
=
List of important publications in physics
List of physicists
Lists of physics equations
Physics at Quanta Magazine
Usenet Physics FAQ – FAQ compiled by sci.physics and other physics newsgroups
Website of the Nobel Prize in physics Archived 7 December 2021 at the Wayback Machine – Award for outstanding contributions to the subject
World of Physics Archived 25 June 2025 at the Wayback Machine – Online encyclopedic dictionary of physics
Nature Physics – Academic journal
Physics Archived 28 June 2025 at the Wayback Machine – Online magazine by the American Physical Society
The Vega Science Trust Archived 7 June 2023 at the Wayback Machine – Science videos, including physics
HyperPhysics website Archived 8 April 2011 at the Wayback Machine – Physics and astronomy mind-map from Georgia State University
Physics at MIT OpenCourseWare Archived 15 March 2022 at the Wayback Machine – Online course material from Massachusetts Institute of Technology
The Feynman Lectures on Physics Archived 4 March 2022 at the Wayback Machine

--- TOPIC: Quantum_mechanics ---
Quantum mechanics is the fundamental physical theory that describes the behavior of matter and of light; its unusual characteristics typically occur at and below the scale of atoms. It is the foundation of all quantum physics, which includes quantum chemistry, quantum biology, quantum field theory, quantum technology, and quantum information science.
Quantum mechanics can describe many systems that classical physics cannot. Classical physics can describe many aspects of nature at an ordinary (macroscopic and (optical) microscopic) scale, but is not sufficient for describing them at very small submicroscopic (atomic and subatomic) scales. Classical mechanics can be derived from quantum mechanics as an approximation that is valid at ordinary scales.
Quantum systems have bound states that are quantized to discrete values of energy, momentum, angular momentum, and other quantities, in contrast to classical systems where these quantities can be measured continuously. Measurements of quantum systems show characteristics of both particles and waves (wave–particle duality), and there are limits to how accurately the value of a physical quantity can be predicted prior to its measurement, given a complete set of initial conditions (the uncertainty principle).
Quantum mechanics arose gradually from theories to explain observations that could not be reconciled with classical physics, such as Max Planck's solution in 1900 to the black-body radiation problem, and the correspondence between energy and frequency in Albert Einstein's 1905 paper, which explained the photoelectric effect. These early attempts to understand microscopic phenomena, now known as the "old quantum theory", led to the full development of quantum mechanics in the mid-1920s by Niels Bohr, Erwin Schrödinger, Werner Heisenberg, Max Born, Paul Dirac and others. The modern theory is formulated in various specially developed mathematical formalisms. In one of them, a mathematical entity called the wave function provides information, in the form of probability amplitudes, about what measurements of a particle's energy, momentum, and other physical properties may yield.
Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and subatomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy. For example, the refinement of quantum mechanics for the interaction of light and matter, known as quantum electrodynamics (QED), has been shown to agree with experiment to within 1 part in 1012 when predicting the magnetic properties of an electron.
A fundamental feature of the theory is that it usually cannot predict with certainty what will happen, but only gives probabilities. Mathematically, a probability is found by taking the square of the absolute value of a complex number, known as a probability amplitude. This is known as the Born rule, named after physicist Max Born. For example, a quantum particle like an electron can be described by a wave function, which associates to each point in space a probability amplitude. Applying the Born rule to these amplitudes gives a probability density function for the position that the electron will be found to have when an experiment is performed to measure it. This is the best the theory can do; it cannot say for certain where the electron will be found. The Schrödinger equation relates the collection of probability amplitudes that pertain to one moment of time to the collection of probability amplitudes that pertain to another.
One consequence of the mathematical rules of quantum mechanics is a tradeoff in predictability between measurable quantities. The most famous form of this uncertainty principle says that no matter how a quantum particle is prepared or how carefully experiments upon it are arranged, it is impossible to have a precise prediction for a measurement of its position and also at the same time for a measurement of its momentum.
Another consequence of the mathematical rules of quantum mechanics is the phenomenon of quantum interference, which is often illustrated with the double-slit experiment. In the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles rather than waves; the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass through.  This behavior is known as wave–particle duality. In addition to light, electrons, atoms, and molecules are all found to exhibit the same dual behavior when fired towards a double slit.
Another non-classical phenomenon predicted by quantum mechanics is quantum tunnelling: a particle that goes up against a potential barrier can cross it, even if its kinetic energy is smaller than the maximum of the potential. In classical mechanics this particle would be trapped. Quantum tunnelling has several important consequences, enabling radioactive decay, nuclear fusion in stars, and applications such as scanning tunnelling microscopy, tunnel diode and tunnel field-effect transistor.
When quantum systems interact, the result can be the creation of quantum entanglement: their properties become so intertwined that a description of the whole solely in terms of the individual parts is no longer possible. Erwin Schrödinger called entanglement "...the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought". Quantum entanglement enables quantum computing and is part of quantum communication protocols, such as quantum key distribution and superdense coding. Contrary to popular misconception, entanglement does not allow sending signals faster than light, as demonstrated by the no-communication theorem.
Another possibility opened by entanglement is testing for "hidden variables", hypothetical properties more fundamental than the quantities addressed in quantum theory itself, knowledge of which would allow more exact predictions than quantum theory provides. A collection of results, most significantly Bell's theorem, have demonstrated that broad classes of such hidden-variable theories are in fact incompatible with quantum physics. According to Bell's theorem, if nature actually operates in accord with any theory of local hidden variables, then the results of a Bell test will be constrained in a particular, quantifiable way. Many Bell tests have been performed and they have shown results incompatible with the constraints imposed by local hidden variables.
It is not possible to present these concepts in more than a superficial way without introducing the mathematics involved; understanding quantum mechanics requires not only manipulating complex numbers, but also linear algebra, differential equations, group theory, and other more advanced subjects. Accordingly, this article will present a mathematical formulation of quantum mechanics and survey its application to some useful and oft-studied examples.
In the mathematically rigorous formulation of quantum mechanics, the state of a quantum mechanical system is a vector 
  
    
      
        ψ
      
    
    {\displaystyle \psi }
  
 belonging to a (separable) complex Hilbert space 
  
    
      
        
          
            H
          
        
      
    
    {\displaystyle {\mathcal {H}}}
  
. This vector is postulated to be normalized under the Hilbert space inner product, that is, it obeys 
  
    
      
        ⟨
        ψ
        ,
        ψ
        ⟩
        =
        1
      
    
    {\displaystyle \langle \psi ,\psi \rangle =1}
  
, and it is well-defined up to a complex number of modulus 1 (the global phase), that is, 
  
    
      
        ψ
      
    
    {\displaystyle \psi }
  
 and 
  
    
      
        
          e
          
            i
            α
          
        
        ψ
      
    
    {\displaystyle e^{i\alpha }\psi }
  
 represent the same physical system. In other words, the possible states are points in the projective space of a Hilbert space, usually called the complex projective space. The exact nature of this Hilbert space is dependent on the system – for example, for describing position and momentum the Hilbert space is the space of complex square-integrable functions 
  
    
      
        
          L
          
            2
          
        
        (
        
          C
        
        )
      
    
    {\displaystyle L^{2}(\mathbb {C} )}
  
, while the Hilbert space for the spin of a single proton is simply the space of two-dimensional complex vectors 
  
    
      
        
          
            C
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {C} ^{2}}
  
 with the usual inner product.
Physical quantities of interest – position, momentum, energy, spin – are represented by observables, which are Hermitian (more precisely, self-adjoint) linear operators acting on the Hilbert space. A quantum state can be an eigenvector of an observable, in which case it is called an eigenstate, and the associated eigenvalue corresponds to the value of the observable in that eigenstate. More generally, a quantum state will be a linear combination of the eigenstates, known as a quantum superposition. When an observable is measured, the result will be one of its eigenvalues with probability given by the Born rule: in the simplest case the eigenvalue 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 is non-degenerate and the probability is given by 
  
    
      
        
          |
        
        ⟨
        
          
            
              λ
              →
            
          
        
        ,
        ψ
        ⟩
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle |\langle {\vec {\lambda }},\psi \rangle |^{2}}
  
, where 
  
    
      
        
          
            
              λ
              →
            
          
        
      
    
    {\displaystyle {\vec {\lambda }}}
  
 is its associated unit-length eigenvector. More generally, the eigenvalue is degenerate and the probability is given by 
  
    
      
        ⟨
        ψ
        ,
        
          P
          
            λ
          
        
        ψ
        ⟩
      
    
    {\displaystyle \langle \psi ,P_{\lambda }\psi \rangle }
  
, where 
  
    
      
        
          P
          
            λ
          
        
      
    
    {\displaystyle P_{\lambda }}
  
 is the projector onto its associated eigenspace. In the continuous case, these formulas give instead the probability density.
After the measurement, if result 
  
    
      
        λ
      
    
    {\displaystyle \lambda }
  
 was obtained, the quantum state is postulated to collapse to 
  
    
      
        
          
            
              λ
              →
            
          
        
      
    
    {\displaystyle {\vec {\lambda }}}
  
, in the non-degenerate case, or to 
  
    
      
        
          P
          
            λ
          
        
        ψ
        
          
            /
          
        
        
        
          
            ⟨
            ψ
            ,
            
              P
              
                λ
              
            
            ψ
            ⟩
          
        
      
    
    {\textstyle P_{\lambda }\psi {\big /}\!{\sqrt {\langle \psi ,P_{\lambda }\psi \rangle }}}
  
, in the general case. The probabilistic nature of quantum mechanics thus stems from the act of measurement. This is one of the most debated aspects of quantum theory, with different interpretations of quantum mechanics giving radically different answers to questions regarding quantum-state collapse, as discussed below.
=
The time evolution of a quantum state is described by the Schrödinger equation:
  
    
      
        i
        ℏ
        
          
            ∂
            
              ∂
              t
            
          
        
        ψ
        (
        t
        )
        =
        H
        ψ
        (
        t
        )
        .
      
    
    {\displaystyle i\hbar {\frac {\partial }{\partial t}}\psi (t)=H\psi (t).}
  
Here 
  
    
      
        H
      
    
    {\displaystyle H}
  
 denotes the Hamiltonian, the observable corresponding to the total energy of the system, and 
  
    
      
        ℏ
      
    
    {\displaystyle \hbar }
  
 is the reduced Planck constant. The constant 
  
    
      
        i
        ℏ
      
    
    {\displaystyle i\hbar }
  
 is introduced so that the Hamiltonian is reduced to the classical Hamiltonian in cases where the quantum system can be approximated by a classical system; the ability to make such an approximation in certain limits is called the correspondence principle.
The solution of this differential equation is given by
  
    
      
        ψ
        (
        t
        )
        =
        
          e
          
            −
            i
            H
            t
            
              /
            
            ℏ
          
        
        ψ
        (
        0
        )
        .
      
    
    {\displaystyle \psi (t)=e^{-iHt/\hbar }\psi (0).}
  
The operator 
  
    
      
        U
        (
        t
        )
        =
        
          e
          
            −
            i
            H
            t
            
              /
            
            ℏ
          
        
      
    
    {\displaystyle U(t)=e^{-iHt/\hbar }}
  
 is known as the time-evolution operator, and has the crucial property that it is unitary. This time evolution is deterministic in the sense that – given an initial quantum state 
  
    
      
        ψ
        (
        0
        )
      
    
    {\displaystyle \psi (0)}
  
 – it makes a definite prediction of what the quantum state 
  
    
      
        ψ
        (
        t
        )
      
    
    {\displaystyle \psi (t)}
  
 will be at any later time.
Some wave functions produce probability distributions that are independent of time, such as eigenstates of the Hamiltonian. Many systems that are treated dynamically in classical mechanics are described by such "static" wave functions. For example, a single electron in an unexcited atom is pictured classically as a particle moving in a circular trajectory around the atomic nucleus, whereas in quantum mechanics, it is described by a static wave function surrounding the nucleus. For example, the electron wave function for an unexcited hydrogen atom is a spherically symmetric function known as an s orbital (Fig. 1).
Analytic solutions of the Schrödinger equation are known for very few relatively simple model Hamiltonians including the quantum harmonic oscillator, the particle in a box, the dihydrogen cation, and the hydrogen atom. Even the helium atom – which contains just two electrons – has defied all attempts at a fully analytic treatment, admitting no solution in closed form.
However, there are techniques for finding approximate solutions. One method, called perturbation theory, uses the analytic result for a simple quantum mechanical model to create a result for a related but more complicated model by (for example) the addition of a weak potential energy. Another approximation method applies to systems for which quantum mechanics produces only small deviations from classical behavior. These deviations can then be computed based on the classical motion.
=
One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum. Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator 
  
    
      
        
          
            
              X
              ^
            
          
        
      
    
    {\displaystyle {\hat {X}}}
  
 and momentum operator 
  
    
      
        
          
            
              P
              ^
            
          
        
      
    
    {\displaystyle {\hat {P}}}
  
 do not commute, but rather satisfy the canonical commutation relation:
  
    
      
        [
        
          
            
              X
              ^
            
          
        
        ,
        
          
            
              P
              ^
            
          
        
        ]
        =
        i
        ℏ
        .
      
    
    {\displaystyle [{\hat {X}},{\hat {P}}]=i\hbar .}
  
Given a quantum state, the Born rule lets us compute expectation values for both 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        P
      
    
    {\displaystyle P}
  
, and moreover for powers of them. Defining the uncertainty for an observable by a standard deviation, we have
  
    
      
        
          σ
          
            X
          
        
        =
        
          
            
              
                
                  ⟨
                  
                    X
                    
                      2
                    
                  
                  ⟩
                
                −
                
                  
                    ⟨
                    X
                    ⟩
                  
                  
                    2
                  
                
              
            
          
        
        ,
      
    
    {\displaystyle \sigma _{X}={\textstyle {\sqrt {\left\langle X^{2}\right\rangle -\left\langle X\right\rangle ^{2}}}},}
  
and likewise for the momentum:
  
    
      
        
          σ
          
            P
          
        
        =
        
          
            
              ⟨
              
                P
                
                  2
                
              
              ⟩
            
            −
            
              
                ⟨
                P
                ⟩
              
              
                2
              
            
          
        
        .
      
    
    {\displaystyle \sigma _{P}={\sqrt {\left\langle P^{2}\right\rangle -\left\langle P\right\rangle ^{2}}}.}
  
The uncertainty principle states that
  
    
      
        
          σ
          
            X
          
        
        
          σ
          
            P
          
        
        ≥
        
          
            ℏ
            2
          
        
        .
      
    
    {\displaystyle \sigma _{X}\sigma _{P}\geq {\frac {\hbar }{2}}.}
  
Either standard deviation can in principle be made arbitrarily small, but not both simultaneously. This inequality generalizes to arbitrary pairs of self-adjoint operators 
  
    
      
        A
      
    
    {\displaystyle A}
  
 and 
  
    
      
        B
      
    
    {\displaystyle B}
  
. The commutator of these two operators is
  
    
      
        [
        A
        ,
        B
        ]
        =
        A
        B
        −
        B
        A
        ,
      
    
    {\displaystyle [A,B]=AB-BA,}
  
and this provides the lower bound on the product of standard deviations:
  
    
      
        
          σ
          
            A
          
        
        
          σ
          
            B
          
        
        ≥
        
          
            
              1
              2
            
          
        
        
          |
          
            
              
                ⟨
              
            
            [
            A
            ,
            B
            ]
            
              
                ⟩
              
            
          
          |
        
        .
      
    
    {\displaystyle \sigma _{A}\sigma _{B}\geq {\tfrac {1}{2}}\left|{\bigl \langle }[A,B]{\bigr \rangle }\right|.}
  
Another consequence of the canonical commutation relation is that the position and momentum operators are Fourier transforms of each other, so that a description of an object according to its momentum is the Fourier transform of its description according to its position. The fact that dependence in momentum is the Fourier transform of the dependence in position means that the momentum operator is equivalent (up to an 
  
    
      
        i
        
          /
        
        ℏ
      
    
    {\displaystyle i/\hbar }
  
 factor) to taking the derivative according to the position, since in Fourier analysis differentiation corresponds to multiplication in the dual space. This is why in quantum equations in position space, the momentum 
  
    
      
        
          p
          
            i
          
        
      
    
    {\displaystyle p_{i}}
  
 is replaced by 
  
    
      
        −
        i
        ℏ
        
          
            ∂
            
              ∂
              x
            
          
        
      
    
    {\displaystyle -i\hbar {\frac {\partial }{\partial x}}}
  
, and in particular in the non-relativistic Schrödinger equation in position space the momentum-squared term is replaced with a Laplacian times 
  
    
      
        −
        
          ℏ
          
            2
          
        
      
    
    {\displaystyle -\hbar ^{2}}
  
.
=
When two different quantum systems are considered together, the Hilbert space of the combined system is the tensor product of the Hilbert spaces of the two components. For example, let A and B be two quantum systems, with Hilbert spaces 
  
    
      
        
          
            
              H
            
          
          
            A
          
        
      
    
    {\displaystyle {\mathcal {H}}_{A}}
  
 and 
  
    
      
        
          
            
              H
            
          
          
            B
          
        
      
    
    {\displaystyle {\mathcal {H}}_{B}}
  
, respectively. The Hilbert space of the composite system is then
  
    
      
        
          
            
              H
            
          
          
            A
            B
          
        
        =
        
          
            
              H
            
          
          
            A
          
        
        ⊗
        
          
            
              H
            
          
          
            B
          
        
        .
      
    
    {\displaystyle {\mathcal {H}}_{AB}={\mathcal {H}}_{A}\otimes {\mathcal {H}}_{B}.}
  
If the state for the first system is the vector 
  
    
      
        
          ψ
          
            A
          
        
      
    
    {\displaystyle \psi _{A}}
  
 and the state for the second system is 
  
    
      
        
          ψ
          
            B
          
        
      
    
    {\displaystyle \psi _{B}}
  
, then the state of the composite system is
  
    
      
        
          ψ
          
            A
          
        
        ⊗
        
          ψ
          
            B
          
        
        .
      
    
    {\displaystyle \psi _{A}\otimes \psi _{B}.}
  
Not all states in the joint Hilbert space 
  
    
      
        
          
            
              H
            
          
          
            A
            B
          
        
      
    
    {\displaystyle {\mathcal {H}}_{AB}}
  
 can be written in this form, however, because the superposition principle implies that linear combinations of these "separable" or "product states" are also valid. For example, if 
  
    
      
        
          ψ
          
            A
          
        
      
    
    {\displaystyle \psi _{A}}
  
 and 
  
    
      
        
          ϕ
          
            A
          
        
      
    
    {\displaystyle \phi _{A}}
  
 are both possible states for system 
  
    
      
        A
      
    
    {\displaystyle A}
  
, and likewise 
  
    
      
        
          ψ
          
            B
          
        
      
    
    {\displaystyle \psi _{B}}
  
 and 
  
    
      
        
          ϕ
          
            B
          
        
      
    
    {\displaystyle \phi _{B}}
  
 are both possible states for system 
  
    
      
        B
      
    
    {\displaystyle B}
  
, then
  
    
      
        
          
            
              1
              
                2
              
            
          
        
        
          (
          
            
              ψ
              
                A
              
            
            ⊗
            
              ψ
              
                B
              
            
            +
            
              ϕ
              
                A
              
            
            ⊗
            
              ϕ
              
                B
              
            
          
          )
        
      
    
    {\displaystyle {\tfrac {1}{\sqrt {2}}}\left(\psi _{A}\otimes \psi _{B}+\phi _{A}\otimes \phi _{B}\right)}
  
is a valid joint state that is not separable. States that are not separable are called entangled.
If the state for a composite system is entangled, it is impossible to describe either component system A or system B by a state vector. One can instead define reduced density matrices that describe the statistics that can be obtained by making measurements on either component system alone. This necessarily causes a loss of information, though: knowing the reduced density matrices of the individual systems is not enough to reconstruct the state of the composite system. Just as density matrices specify the state of a subsystem of a larger system, analogously, positive operator-valued measures (POVMs) describe the effect on a subsystem of a measurement performed on a larger system. POVMs are extensively used in quantum information theory.
As described above, entanglement is a key feature of models of measurement processes in which an apparatus becomes entangled with the system being measured. Systems interacting with the environment in which they reside generally become entangled with that environment, a phenomenon known as quantum decoherence. This can explain why, in practice, quantum effects are difficult to observe in systems larger than microscopic.
=
There are many mathematically equivalent formulations of quantum mechanics. One of the oldest and most common is the "transformation theory" proposed by Paul Dirac, which unifies and generalizes the two earliest formulations of quantum mechanics – matrix mechanics (invented by Werner Heisenberg) and wave mechanics (invented by Erwin Schrödinger). An alternative formulation of quantum mechanics is Feynman's path integral formulation, in which a quantum-mechanical amplitude is considered as a sum over all possible classical and non-classical paths between the initial and final states. This is the quantum-mechanical counterpart of the action principle in classical mechanics.
=
The Hamiltonian 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is known as the generator of time evolution, since it defines a unitary time-evolution operator 
  
    
      
        U
        (
        t
        )
        =
        
          e
          
            −
            i
            H
            t
            
              /
            
            ℏ
          
        
      
    
    {\displaystyle U(t)=e^{-iHt/\hbar }}
  
 for each value of 
  
    
      
        t
      
    
    {\displaystyle t}
  
. From this relation between 
  
    
      
        U
        (
        t
        )
      
    
    {\displaystyle U(t)}
  
 and 
  
    
      
        H
      
    
    {\displaystyle H}
  
, it follows that any observable 
  
    
      
        A
      
    
    {\displaystyle A}
  
 that commutes with 
  
    
      
        H
      
    
    {\displaystyle H}
  
 will be conserved: its expectation value will not change over time. This statement generalizes, as mathematically, any Hermitian operator 
  
    
      
        A
      
    
    {\displaystyle A}
  
 can generate a family of unitary operators parameterized by a variable 
  
    
      
        t
      
    
    {\displaystyle t}
  
. Under the evolution generated by 
  
    
      
        A
      
    
    {\displaystyle A}
  
, any observable 
  
    
      
        B
      
    
    {\displaystyle B}
  
 that commutes with 
  
    
      
        A
      
    
    {\displaystyle A}
  
 will be conserved. Moreover, if 
  
    
      
        B
      
    
    {\displaystyle B}
  
 is conserved by evolution under 
  
    
      
        A
      
    
    {\displaystyle A}
  
, then 
  
    
      
        A
      
    
    {\displaystyle A}
  
 is conserved under the evolution generated by 
  
    
      
        B
      
    
    {\displaystyle B}
  
. This implies a quantum version of the result proven by Emmy Noether in classical (Lagrangian) mechanics: for every differentiable symmetry of a Hamiltonian, there exists a corresponding conservation law.
=
The simplest example of a quantum system with a position degree of freedom is a free particle in a single spatial dimension. A free particle is one which is not subject to external influences, so that its Hamiltonian consists only of its kinetic energy:
  
    
      
        H
        =
        
          
            1
            
              2
              m
            
          
        
        
          P
          
            2
          
        
        =
        −
        
          
            
              ℏ
              
                2
              
            
            
              2
              m
            
          
        
        
          
            
              d
              
                2
              
            
            
              d
              
                x
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle H={\frac {1}{2m}}P^{2}=-{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}}{dx^{2}}}.}
  
The general solution of the Schrödinger equation is given by
  
    
      
        ψ
        (
        x
        ,
        t
        )
        =
        
          
            1
            
              2
              π
            
          
        
        
          ∫
          
            −
            ∞
          
          
            ∞
          
        
        
          
            
              ψ
              ^
            
          
        
        (
        k
        ,
        0
        )
        
          e
          
            i
            (
            k
            x
            −
            
              
                
                  ℏ
                  
                    k
                    
                      2
                    
                  
                
                
                  2
                  m
                
              
            
            t
            )
          
        
        
          d
        
        k
        ,
      
    
    {\displaystyle \psi (x,t)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{\infty }{\hat {\psi }}(k,0)e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}\mathrm {d} k,}
  
which is a superposition of all possible plane waves 
  
    
      
        
          e
          
            i
            (
            k
            x
            −
            
              
                
                  ℏ
                  
                    k
                    
                      2
                    
                  
                
                
                  2
                  m
                
              
            
            t
            )
          
        
      
    
    {\displaystyle e^{i(kx-{\frac {\hbar k^{2}}{2m}}t)}}
  
, which are eigenstates of the momentum operator with momentum 
  
    
      
        p
        =
        ℏ
        k
      
    
    {\displaystyle p=\hbar k}
  
. The coefficients of the superposition are 
  
    
      
        
          
            
              ψ
              ^
            
          
        
        (
        k
        ,
        0
        )
      
    
    {\displaystyle {\hat {\psi }}(k,0)}
  
, which is the Fourier transform of the initial quantum state 
  
    
      
        ψ
        (
        x
        ,
        0
        )
      
    
    {\displaystyle \psi (x,0)}
  
.
It is not possible for the solution to be a single momentum eigenstate, or a single position eigenstate, as these are not normalizable quantum states. Instead, we can consider a Gaussian wave packet:
  
    
      
        ψ
        (
        x
        ,
        0
        )
        =
        
          
            1
            
              
                π
                a
              
              
                4
              
            
          
        
        
          e
          
            −
            
              
                
                  x
                  
                    2
                  
                
                
                  2
                  a
                
              
            
          
        
      
    
    {\displaystyle \psi (x,0)={\frac {1}{\sqrt[{4}]{\pi a}}}e^{-{\frac {x^{2}}{2a}}}}
  
which has Fourier transform, and therefore momentum distribution
  
    
      
        
          
            
              ψ
              ^
            
          
        
        (
        k
        ,
        0
        )
        =
        
          
            
              a
              π
            
            
              4
            
          
        
        
          e
          
            −
            
              
                
                  a
                  
                    k
                    
                      2
                    
                  
                
                2
              
            
          
        
        .
      
    
    {\displaystyle {\hat {\psi }}(k,0)={\sqrt[{4}]{\frac {a}{\pi }}}e^{-{\frac {ak^{2}}{2}}}.}
  
We see that as we make 
  
    
      
        a
      
    
    {\displaystyle a}
  
 smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making 
  
    
      
        a
      
    
    {\displaystyle a}
  
 larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle.
As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant.
=
The particle in a one-dimensional potential energy box is the most mathematically simple example where restraints lead to the quantization of energy levels. The box is defined as having zero potential energy everywhere inside a certain region, and therefore infinite potential energy everywhere outside that region. For the one-dimensional case in the 
  
    
      
        x
      
    
    {\displaystyle x}
  
 direction, the time-independent Schrödinger equation may be written
  
    
      
        −
        
          
            
              ℏ
              
                2
              
            
            
              2
              m
            
          
        
        
          
            
              
                d
                
                  2
                
              
              ψ
            
            
              d
              
                x
                
                  2
                
              
            
          
        
        =
        E
        ψ
        .
      
    
    {\displaystyle -{\frac {\hbar ^{2}}{2m}}{\frac {d^{2}\psi }{dx^{2}}}=E\psi .}
  
With the differential operator defined by
  
    
      
        
          
            
              
                p
                ^
              
            
          
          
            x
          
        
        =
        −
        i
        ℏ
        
          
            d
            
              d
              x
            
          
        
      
    
    {\displaystyle {\hat {p}}_{x}=-i\hbar {\frac {d}{dx}}}
  
the previous equation is evocative of the classic kinetic energy analogue,
  
    
      
        
          
            1
            
              2
              m
            
          
        
        
          
            
              
                p
                ^
              
            
          
          
            x
          
          
            2
          
        
        =
        E
        ,
      
    
    {\displaystyle {\frac {1}{2m}}{\hat {p}}_{x}^{2}=E,}
  
with state 
  
    
      
        ψ
      
    
    {\displaystyle \psi }
  
 in this case having energy 
  
    
      
        E
      
    
    {\displaystyle E}
  
 coincident with the kinetic energy of the particle.
The general solutions of the Schrödinger equation for the particle in a box are
  
    
      
        ψ
        (
        x
        )
        =
        A
        
          e
          
            i
            k
            x
          
        
        +
        B
        
          e
          
            −
            i
            k
            x
          
        
        
        
        E
        =
        
          
            
              
                ℏ
                
                  2
                
              
              
                k
                
                  2
                
              
            
            
              2
              m
            
          
        
      
    
    {\displaystyle \psi (x)=Ae^{ikx}+Be^{-ikx}\qquad \qquad E={\frac {\hbar ^{2}k^{2}}{2m}}}
  
or, from Euler's formula,
  
    
      
        ψ
        (
        x
        )
        =
        C
        sin
        ⁡
        (
        k
        x
        )
        +
        D
        cos
        ⁡
        (
        k
        x
        )
        .
        
      
    
    {\displaystyle \psi (x)=C\sin(kx)+D\cos(kx).\!}
  
The infinite potential walls of the box determine the values of 
  
    
      
        C
        ,
        D
        ,
      
    
    {\displaystyle C,D,}
  
 and 
  
    
      
        k
      
    
    {\displaystyle k}
  
 at 
  
    
      
        x
        =
        0
      
    
    {\displaystyle x=0}
  
 and 
  
    
      
        x
        =
        L
      
    
    {\displaystyle x=L}
  
 where 
  
    
      
        ψ
      
    
    {\displaystyle \psi }
  
 must be zero. Thus, at 
  
    
      
        x
        =
        0
      
    
    {\displaystyle x=0}
  
,
  
    
      
        ψ
        (
        0
        )
        =
        0
        =
        C
        sin
        ⁡
        (
        0
        )
        +
        D
        cos
        ⁡
        (
        0
        )
        =
        D
      
    
    {\displaystyle \psi (0)=0=C\sin(0)+D\cos(0)=D}
  
and 
  
    
      
        D
        =
        0
      
    
    {\displaystyle D=0}
  
. At 
  
    
      
        x
        =
        L
      
    
    {\displaystyle x=L}
  
,
  
    
      
        ψ
        (
        L
        )
        =
        0
        =
        C
        sin
        ⁡
        (
        k
        L
        )
        ,
      
    
    {\displaystyle \psi (L)=0=C\sin(kL),}
  
in which 
  
    
      
        C
      
    
    {\displaystyle C}
  
 cannot be zero as this would conflict with the postulate that 
  
    
      
        ψ
      
    
    {\displaystyle \psi }
  
 has norm 1. Therefore, since 
  
    
      
        sin
        ⁡
        (
        k
        L
        )
        =
        0
      
    
    {\displaystyle \sin(kL)=0}
  
, 
  
    
      
        k
        L
      
    
    {\displaystyle kL}
  
 must be an integer multiple of 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
,
  
    
      
        k
        =
        
          
            
              n
              π
            
            L
          
        
        
        
        n
        =
        1
        ,
        2
        ,
        3
        ,
        …
        .
      
    
    {\displaystyle k={\frac {n\pi }{L}}\qquad \qquad n=1,2,3,\ldots .}
  
This constraint on 
  
    
      
        k
      
    
    {\displaystyle k}
  
 implies a constraint on the energy levels, yielding
  
    
      
        
          E
          
            n
          
        
        =
        
          
            
              
                ℏ
                
                  2
                
              
              
                π
                
                  2
                
              
              
                n
                
                  2
                
              
            
            
              2
              m
              
                L
                
                  2
                
              
            
          
        
        =
        
          
            
              
                n
                
                  2
                
              
              
                h
                
                  2
                
              
            
            
              8
              m
              
                L
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle E_{n}={\frac {\hbar ^{2}\pi ^{2}n^{2}}{2mL^{2}}}={\frac {n^{2}h^{2}}{8mL^{2}}}.}
  
A finite potential well is the generalization of the infinite potential well problem to potential wells having finite depth. The finite potential well problem is mathematically more complicated than the infinite particle-in-a-box problem as the wave function is not pinned to zero at the walls of the well. Instead, the wave function must satisfy more complicated mathematical boundary conditions as it is nonzero in regions outside the well. Another related problem is that of the rectangular potential barrier, which furnishes a model for the quantum tunneling effect that plays an important role in the performance of modern technologies such as flash memory and scanning tunneling microscopy.
=
As in the classical case, the potential for the quantum harmonic oscillator is given by
  
    
      
        V
        (
        x
        )
        =
        
          
            1
            2
          
        
        m
        
          ω
          
            2
          
        
        
          x
          
            2
          
        
        .
      
    
    {\displaystyle V(x)={\frac {1}{2}}m\omega ^{2}x^{2}.}
  
This problem can either be treated by directly solving the Schrödinger equation, which is not trivial, or by using the more elegant "ladder method" first proposed by Paul Dirac. The eigenstates are given by
  
    
      
        
          ψ
          
            n
          
        
        (
        x
        )
        =
        
          
            
              1
              
                
                  2
                  
                    n
                  
                
                
                n
                !
              
            
          
        
        ⋅
        
          
            (
            
              
                
                  m
                  ω
                
                
                  π
                  ℏ
                
              
            
            )
          
          
            1
            
              /
            
            4
          
        
        ⋅
        
          e
          
            −
            
              
                
                  m
                  ω
                  
                    x
                    
                      2
                    
                  
                
                
                  2
                  ℏ
                
              
            
          
        
        ⋅
        
          H
          
            n
          
        
        
          (
          
            
              
                
                  
                    m
                    ω
                  
                  ℏ
                
              
            
            x
          
          )
        
        ,
        
      
    
    {\displaystyle \psi _{n}(x)={\sqrt {\frac {1}{2^{n}\,n!}}}\cdot \left({\frac {m\omega }{\pi \hbar }}\right)^{1/4}\cdot e^{-{\frac {m\omega x^{2}}{2\hbar }}}\cdot H_{n}\left({\sqrt {\frac {m\omega }{\hbar }}}x\right),\qquad }
  
  
    
      
        n
        =
        0
        ,
        1
        ,
        2
        ,
        …
        .
      
    
    {\displaystyle n=0,1,2,\ldots .}
  
where Hn are the Hermite polynomials
  
    
      
        
          H
          
            n
          
        
        (
        x
        )
        =
        (
        −
        1
        
          )
          
            n
          
        
        
          e
          
            
              x
              
                2
              
            
          
        
        
          
            
              d
              
                n
              
            
            
              d
              
                x
                
                  n
                
              
            
          
        
        
          (
          
            e
            
              −
              
                x
                
                  2
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle H_{n}(x)=(-1)^{n}e^{x^{2}}{\frac {d^{n}}{dx^{n}}}\left(e^{-x^{2}}\right),}
  
and the corresponding energy levels are
  
    
      
        
          E
          
            n
          
        
        =
        ℏ
        ω
        
          (
          
            n
            +
            
              
                1
                2
              
            
          
          )
        
        .
      
    
    {\displaystyle E_{n}=\hbar \omega \left(n+{1 \over 2}\right).}
  
This is another example illustrating the discretization of energy for bound states.
=
The Mach–Zehnder interferometer (MZI) illustrates the concepts of superposition and interference with linear algebra in dimension 2, rather than differential equations. It can be seen as a simplified version of the double-slit experiment, but it is of interest in its own right, for example in the delayed choice quantum eraser, the Elitzur–Vaidman bomb tester, and in studies of quantum entanglement.
We can model a photon going through the interferometer by considering that at each point it can be in a superposition of only two paths: the "lower" path which starts from the left, goes straight through both beam splitters, and ends at the top, and the "upper" path which starts from the bottom, goes straight through both beam splitters, and ends at the right. The quantum state of the photon is therefore a vector 
  
    
      
        ψ
        ∈
        
          
            C
          
          
            2
          
        
      
    
    {\displaystyle \psi \in \mathbb {C} ^{2}}
  
 that is a superposition of the "lower" path 
  
    
      
        
          ψ
          
            l
          
        
        =
        
          
            (
            
              
                
                  1
                
              
              
                
                  0
                
              
            
            )
          
        
      
    
    {\displaystyle \psi _{l}={\begin{pmatrix}1\\0\end{pmatrix}}}
  
 and the "upper" path 
  
    
      
        
          ψ
          
            u
          
        
        =
        
          
            (
            
              
                
                  0
                
              
              
                
                  1
                
              
            
            )
          
        
      
    
    {\displaystyle \psi _{u}={\begin{pmatrix}0\\1\end{pmatrix}}}
  
, that is, 
  
    
      
        ψ
        =
        α
        
          ψ
          
            l
          
        
        +
        β
        
          ψ
          
            u
          
        
      
    
    {\displaystyle \psi =\alpha \psi _{l}+\beta \psi _{u}}
  
 for complex 
  
    
      
        α
        ,
        β
      
    
    {\displaystyle \alpha ,\beta }
  
. In order to respect the postulate that 
  
    
      
        ⟨
        ψ
        ,
        ψ
        ⟩
        =
        1
      
    
    {\displaystyle \langle \psi ,\psi \rangle =1}
  
 we require that 
  
    
      
        
          |
        
        α
        
          
            |
          
          
            2
          
        
        +
        
          |
        
        β
        
          
            |
          
          
            2
          
        
        =
        1
      
    
    {\displaystyle |\alpha |^{2}+|\beta |^{2}=1}
  
.
Both beam splitters are modelled as the unitary matrix 
  
    
      
        B
        =
        
          
            1
            
              2
            
          
        
        
          
            (
            
              
                
                  1
                
                
                  i
                
              
              
                
                  i
                
                
                  1
                
              
            
            )
          
        
      
    
    {\displaystyle B={\frac {1}{\sqrt {2}}}{\begin{pmatrix}1&i\\i&1\end{pmatrix}}}
  
, which means that when a photon meets the beam splitter it will either stay on the same path with a probability amplitude of 
  
    
      
        1
        
          /
        
        
          
            2
          
        
      
    
    {\displaystyle 1/{\sqrt {2}}}
  
, or be reflected to the other path with a probability amplitude of 
  
    
      
        i
        
          /
        
        
          
            2
          
        
      
    
    {\displaystyle i/{\sqrt {2}}}
  
. The phase shifter on the upper arm is modelled as the unitary matrix 
  
    
      
        P
        =
        
          
            (
            
              
                
                  1
                
                
                  0
                
              
              
                
                  0
                
                
                  
                    e
                    
                      i
                      Δ
                      Φ
                    
                  
                
              
            
            )
          
        
      
    
    {\displaystyle P={\begin{pmatrix}1&0\\0&e^{i\Delta \Phi }\end{pmatrix}}}
  
, which means that if the photon is on the "upper" path it will gain a relative phase of 
  
    
      
        Δ
        Φ
      
    
    {\displaystyle \Delta \Phi }
  
, and it will stay unchanged if it is in the lower path.
A photon that enters the interferometer from the left will then be acted upon with a beam splitter 
  
    
      
        B
      
    
    {\displaystyle B}
  
, a phase shifter 
  
    
      
        P
      
    
    {\displaystyle P}
  
, and another beam splitter 
  
    
      
        B
      
    
    {\displaystyle B}
  
, and so end up in the state
  
    
      
        B
        P
        B
        
          ψ
          
            l
          
        
        =
        i
        
          e
          
            i
            Δ
            Φ
            
              /
            
            2
          
        
        
          
            (
            
              
                
                  −
                  sin
                  ⁡
                  (
                  Δ
                  Φ
                  
                    /
                  
                  2
                  )
                
              
              
                
                  cos
                  ⁡
                  (
                  Δ
                  Φ
                  
                    /
                  
                  2
                  )
                
              
            
            )
          
        
        ,
      
    
    {\displaystyle BPB\psi _{l}=ie^{i\Delta \Phi /2}{\begin{pmatrix}-\sin(\Delta \Phi /2)\\\cos(\Delta \Phi /2)\end{pmatrix}},}
  
and the probabilities that it will be detected at the right or at the top are given respectively by
  
    
      
        p
        (
        u
        )
        =
        
          |
        
        ⟨
        
          ψ
          
            u
          
        
        ,
        B
        P
        B
        
          ψ
          
            l
          
        
        ⟩
        
          
            |
          
          
            2
          
        
        =
        
          cos
          
            2
          
        
        ⁡
        
          
            
              Δ
              Φ
            
            2
          
        
        ,
      
    
    {\displaystyle p(u)=|\langle \psi _{u},BPB\psi _{l}\rangle |^{2}=\cos ^{2}{\frac {\Delta \Phi }{2}},}
  
  
    
      
        p
        (
        l
        )
        =
        
          |
        
        ⟨
        
          ψ
          
            l
          
        
        ,
        B
        P
        B
        
          ψ
          
            l
          
        
        ⟩
        
          
            |
          
          
            2
          
        
        =
        
          sin
          
            2
          
        
        ⁡
        
          
            
              Δ
              Φ
            
            2
          
        
        .
      
    
    {\displaystyle p(l)=|\langle \psi _{l},BPB\psi _{l}\rangle |^{2}=\sin ^{2}{\frac {\Delta \Phi }{2}}.}
  
One can therefore use the Mach–Zehnder interferometer to estimate the phase shift by estimating these probabilities.
It is interesting to consider what would happen if the photon were definitely in either the "lower" or "upper" paths between the beam splitters. This can be accomplished by blocking one of the paths, or equivalently by removing the first beam splitter (and feeding the photon from the left or the bottom, as desired). In both cases, there will be no interference between the paths anymore, and the probabilities are given by 
  
    
      
        p
        (
        u
        )
        =
        p
        (
        l
        )
        =
        1
        
          /
        
        2
      
    
    {\displaystyle p(u)=p(l)=1/2}
  
, independently of the phase 
  
    
      
        Δ
        Φ
      
    
    {\displaystyle \Delta \Phi }
  
. From this we can conclude that the photon does not take one path or another after the first beam splitter, but rather that it is in a genuine quantum superposition of the two paths.
Quantum mechanics has had enormous success in explaining many of the features of our universe, with regard to small-scale and discrete quantities and interactions which cannot be explained by classical methods. Quantum mechanics is often the only theory that can reveal the individual behaviors of the subatomic particles that make up all forms of matter (electrons, protons, neutrons, photons, and others). Solid-state physics and materials science are dependent upon quantum mechanics.
In many aspects, modern technology operates at a scale where quantum effects are significant. Important applications of quantum theory include quantum chemistry, quantum optics, quantum computing, superconducting magnets, light-emitting diodes, the optical amplifier and the laser, the transistor and semiconductors such as the microprocessor, medical and research imaging such as magnetic resonance imaging and electron microscopy. Explanations for many biological and physical phenomena are rooted in the nature of the chemical bond, most notably the macro-molecule DNA.
=
The rules of quantum mechanics assert that the state space of a system is a Hilbert space and that observables of the system are Hermitian operators acting on vectors in that space – although they do not tell us which Hilbert space or which operators. These can be chosen appropriately in order to obtain a quantitative description of a quantum system, a necessary step in making physical predictions. An important guide for making these choices is the correspondence principle, a heuristic which states that the predictions of quantum mechanics reduce to those of classical mechanics in the regime of large quantum numbers. One can also start from an established classical model of a particular system, and then try to guess the underlying quantum model that would give rise to the classical model in the correspondence limit. This approach is known as quantization.
When quantum mechanics was originally formulated, it was applied to models whose correspondence limit was non-relativistic classical mechanics. For instance, the well-known model of the quantum harmonic oscillator uses an explicitly non-relativistic expression for the kinetic energy of the oscillator, and is thus a quantum version of the classical harmonic oscillator.
Complications arise with chaotic systems, which do not have good quantum numbers, and quantum chaos studies the relationship between classical and quantum descriptions in these systems.
Quantum decoherence is a mechanism through which quantum systems lose coherence, and thus become incapable of displaying many typically quantum effects: quantum superpositions become simply probabilistic mixtures, and quantum entanglement becomes simply classical correlations. Quantum coherence is not typically evident at macroscopic scales, though at temperatures approaching absolute zero quantum behavior may manifest macroscopically.
Many macroscopic properties of a classical system are a direct consequence of the quantum behavior of its parts. For example, the stability of bulk matter (consisting of atoms and molecules which would quickly collapse under electric forces alone), the rigidity of solids, and the mechanical, thermal, chemical, optical and magnetic properties of matter are all results of the interaction of electric charges under the rules of quantum mechanics.
=
Early attempts to merge quantum mechanics with special relativity involved the replacement of the Schrödinger equation with a covariant equation such as the Klein–Gordon equation or the Dirac equation. While these theories were successful in explaining many experimental results, they had certain unsatisfactory qualities stemming from their neglect of the relativistic creation and annihilation of particles. A fully relativistic quantum theory required the development of quantum field theory, which applies quantization to a field (rather than a fixed set of particles). The first complete quantum field theory, quantum electrodynamics, provides a fully quantum description of the electromagnetic interaction. Quantum electrodynamics is, along with general relativity, one of the most accurate physical theories ever devised.
The full apparatus of quantum field theory is often unnecessary for describing electrodynamic systems. A simpler approach, one that has been used since the inception of quantum mechanics, is to treat charged particles as quantum mechanical objects being acted on by a classical electromagnetic field. For example, the elementary quantum model of the hydrogen atom describes the electric field of the hydrogen atom using a classical 
  
    
      
        
          −
          
            e
            
              2
            
          
          
            /
          
          (
          4
          π
          
            ϵ
            
              
                
                
                  0
                
              
            
          
          r
          )
        
      
    
    {\displaystyle \textstyle -e^{2}/(4\pi \epsilon _{_{0}}r)}
  
 Coulomb potential. Likewise, in a Stern–Gerlach experiment, a charged particle is modeled as a quantum system, while the background magnetic field is described classically. This "semi-classical" approach fails if quantum fluctuations in the electromagnetic field play an important role, such as in the emission of photons by charged particles.
Quantum field theories for the strong nuclear force and the weak nuclear force have also been developed. The quantum field theory of the strong nuclear force is called quantum chromodynamics, and describes the interactions of subnuclear particles such as quarks and gluons. The weak nuclear force and the electromagnetic force were unified, in their quantized forms, into a single quantum field theory (known as electroweak theory), by the physicists Abdus Salam, Sheldon Glashow and Steven Weinberg.
=
Even though the predictions of both quantum theory and general relativity have been supported by rigorous and repeated empirical evidence, their abstract formalisms contradict each other and they have proven extremely difficult to incorporate into one consistent, cohesive model. Gravity is negligible in many areas of particle physics, so that unification between general relativity and quantum mechanics is not an urgent issue in those particular applications. However, the lack of a correct theory of quantum gravity is an important issue in physical cosmology and the search by physicists for an elegant "Theory of Everything" (TOE). Consequently, resolving the inconsistencies between both theories has been a major goal of 20th- and 21st-century physics. This TOE would combine not only the models of subatomic physics but also derive the four fundamental forces of nature from a single force or phenomenon.
One proposal for doing so is string theory, which posits that the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force.
Another popular theory is loop quantum gravity (LQG), which describes quantum properties of gravity and is thus a theory of quantum spacetime. LQG is an attempt to merge and adapt standard quantum mechanics and standard general relativity. This theory describes space as an extremely fine fabric "woven" of finite loops called spin networks. The evolution of a spin network over time is called a spin foam. The characteristic length scale of a spin foam is the Planck length, approximately 1.616×10−35 m, and so lengths shorter than the Planck length are not physically meaningful in LQG.
Since its inception, the many counter-intuitive aspects and results of quantum mechanics have provoked strong philosophical debates and many interpretations. The arguments centre on the probabilistic nature of quantum mechanics, the difficulties with wavefunction collapse and the related measurement problem, and quantum nonlocality. Perhaps the only consensus that exists about these issues is that there is no consensus. Richard Feynman once said, "I think I can safely say that nobody understands quantum mechanics." According to Steven Weinberg, "There is now in my opinion no entirely satisfactory interpretation of quantum mechanics."
The views of Niels Bohr, Werner Heisenberg and other physicists are often grouped together as the "Copenhagen interpretation". According to these views, the probabilistic nature of quantum mechanics is not a temporary feature which will eventually be replaced by a deterministic theory, but is instead a final renunciation of the classical idea of "causality". Bohr in particular emphasized that any well-defined application of the quantum mechanical formalism must always make reference to the experimental arrangement, due to the complementary nature of evidence obtained under different experimental situations. Copenhagen-type interpretations were adopted by Nobel laureates in quantum physics, including Bohr, Heisenberg, Schrödinger, Feynman, and Zeilinger as well as 21st-century researchers in quantum foundations.
Albert Einstein, himself one of the founders of quantum theory, was troubled by its apparent failure to respect some cherished metaphysical principles, such as determinism and locality. Einstein's long-running exchanges with Bohr about the meaning and status of quantum mechanics are now known as the Bohr–Einstein debates. Einstein believed that underlying quantum mechanics must be a theory that explicitly forbids action at a distance. He argued that quantum mechanics was incomplete, a theory that was valid but not fundamental, analogous to how thermodynamics is valid, but the fundamental theory behind it is statistical mechanics. In 1935, Einstein and his collaborators Boris Podolsky and Nathan Rosen published an argument that the principle of locality implies the incompleteness of quantum mechanics, a thought experiment later termed the Einstein–Podolsky–Rosen paradox. In 1964, John Bell showed that EPR's principle of locality, together with determinism, was actually incompatible with quantum mechanics: they implied constraints on the correlations produced by distance systems, now known as Bell inequalities, that can be violated by entangled particles. Since then several experiments have been performed to obtain these correlations, with the result that they do in fact violate Bell inequalities, and thus falsify the conjunction of locality with determinism.
Bohmian mechanics shows that it is possible to reformulate quantum mechanics to make it deterministic, at the price of making it explicitly nonlocal. It attributes not only a wave function to a physical system, but in addition a real position, that evolves deterministically under a nonlocal guiding equation. The evolution of a physical system is given at all times by the Schrödinger equation together with the guiding equation; there is never a collapse of the wave function. This solves the measurement problem.
Everett's many-worlds interpretation, formulated in 1956, holds that all the possibilities described by quantum theory simultaneously occur in a multiverse composed of mostly independent parallel universes. This is a consequence of removing the axiom of the collapse of the wave packet. All possible states of the measured system and the measuring apparatus, together with the observer, are present in a real physical quantum superposition. While the multiverse is deterministic, we perceive non-deterministic behavior governed by probabilities, because we do not observe the multiverse as a whole, but only one parallel universe at a time. Exactly how this is supposed to work has been the subject of much debate. Several attempts have been made to make sense of this and derive the Born rule, with no consensus on whether they have been successful.
Relational quantum mechanics appeared in the late 1990s as a modern derivative of Copenhagen-type ideas, and QBism was developed some years later.
Quantum mechanics was developed in the early decades of the 20th century, driven by the need to explain phenomena that, in some cases, had been observed in earlier times. Scientific inquiry into the wave nature of light began in the 17th and 18th centuries, when scientists such as Robert Hooke, Christiaan Huygens and Leonhard Euler proposed a wave theory of light based on experimental observations. In 1803 English polymath Thomas Young described the famous double-slit experiment. This experiment played a major role in the general acceptance of the wave theory of light.
During the early 19th century, chemical research by John Dalton and Amedeo Avogadro lent weight to the atomic theory of matter, an idea that James Clerk Maxwell, Ludwig Boltzmann and others built upon to establish the kinetic theory of gases. The successes of kinetic theory gave further credence to the idea that matter is composed of atoms, yet the theory also had shortcomings that would only be resolved by the development of quantum mechanics. While the early conception of atoms from Greek philosophy had been that they were indivisible units – the word "atom" deriving from the Greek for 'uncuttable' – the 19th century saw the formulation of hypotheses about subatomic structure. One important discovery in that regard was Michael Faraday's 1838 observation of a glow caused by an electrical discharge inside a glass tube containing gas at low pressure. Julius Plücker, Johann Wilhelm Hittorf and Eugen Goldstein carried on and improved upon Faraday's work, leading to the identification of cathode rays, which J. J. Thomson found to consist of subatomic particles that would be called electrons.
The black-body radiation problem was discovered by Gustav Kirchhoff in 1859. In 1900, Max Planck proposed the hypothesis that energy is radiated and absorbed in discrete "quanta" (or energy packets), yielding a calculation that precisely matched the observed patterns of black-body radiation. The word quantum derives from the Latin, meaning "how great" or "how much". According to Planck, quantities of energy could be thought of as divided into "elements" whose size (E) would be proportional to their frequency (ν):
  
    
      
        E
        =
        h
        ν
         
      
    
    {\displaystyle E=h\nu \ }
  
,
where h is the Planck constant. Planck cautiously insisted that this was only an aspect of the processes of absorption and emission of radiation and was not the physical reality of the radiation. In fact, he considered his quantum hypothesis a mathematical trick to get the right answer rather than a sizable discovery. However, in 1905 Albert Einstein interpreted Planck's quantum hypothesis realistically and used it to explain the photoelectric effect, in which shining light on certain materials can eject electrons from the material. Niels Bohr then developed Planck's ideas about radiation into a model of the hydrogen atom that successfully predicted the spectral lines of hydrogen. Einstein further developed this idea to show that an electromagnetic wave such as light could also be described as a particle (later called the photon), with a discrete amount of energy that depends on its frequency. In his paper "On the Quantum Theory of Radiation", Einstein expanded on the interaction between energy and matter to explain the absorption and emission of energy by atoms. Although overshadowed at the time by his general theory of relativity, this paper articulated the mechanism underlying the stimulated emission of radiation, which became the basis of the laser.
This phase is known as the old quantum theory. Never complete or self-consistent, the old quantum theory was rather a set of heuristic corrections to classical mechanics. The theory is now understood as a semi-classical approximation to modern quantum mechanics. Notable results from this period include, in addition to the work of Planck, Einstein and Bohr mentioned above, Einstein and Peter Debye's work on the specific heat of solids, Bohr and Hendrika Johanna van Leeuwen's proof that classical physics cannot account for diamagnetism, and Arnold Sommerfeld's extension of the Bohr model to include special-relativistic effects.
In the mid-1920s quantum mechanics was developed to become the standard formulation for atomic physics. In 1923, the French physicist Louis de Broglie put forward his theory of matter waves by stating that particles can exhibit wave characteristics and vice versa. Building on de Broglie's approach, modern quantum mechanics was born in 1925, when the German physicists Werner Heisenberg, Max Born, and Pascual Jordan developed matrix mechanics and the Austrian physicist Erwin Schrödinger invented wave mechanics. Born introduced the probabilistic interpretation of Schrödinger's wave function in July 1926. Thus, the entire field of quantum physics emerged, leading to its wider acceptance at the Fifth Solvay Conference in 1927.
By 1930, quantum mechanics had been further unified and formalized by David Hilbert, Paul Dirac and John von Neumann with greater emphasis on measurement, the statistical nature of our knowledge of reality, and philosophical speculation about the 'observer'. It has since permeated many disciplines, including quantum chemistry, quantum electronics, quantum optics, and quantum information science. It also provides a useful framework for many features of the modern periodic table of elements, and describes the behaviors of atoms during chemical bonding and the flow of electrons in computer semiconductors, and therefore plays a crucial role in many modern technologies. While quantum mechanics was constructed to describe the world of the very small, it is also needed to explain some macroscopic phenomena such as superconductors and superfluids.
Introduction to quantum mechanics by Timon Idema
Quantum Physics Made Relatively Simple: three video lectures by Hans Bethe.
 Course material 
Quantum Cook Book and PHYS 201: Fundamentals of Physics II by Ramamurti Shankar, Yale OpenCourseware.
Modern Physics: With waves, thermodynamics, and optics – an online textbook.
MIT OpenCourseWare: Chemistry and Physics. See 8.04, 8.05 and 8.06.
⁠5+1/2⁠ Examples in Quantum Mechanics.
 Philosophy 
Ismael, Jenann. "Quantum Mechanics". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. ISSN 1095-5054. OCLC 429049174.
Zalta, Edward N. (ed.). "Philosophical Issues in Quantum Theory". Stanford Encyclopedia of Philosophy. ISSN 1095-5054. OCLC 429049174.

--- TOPIC: General_relativity ---
General relativity, also known as the general theory of relativity, and as Einstein's theory of gravity, is the geometric theory of gravitation published by Albert Einstein in May 1916 and is the accepted description of gravitation in modern physics. General relativity generalizes special relativity and refines Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or four-dimensional spacetime. In particular, the curvature of spacetime is directly related to the energy, momentum and stress of whatever is present, including matter and radiation. The relation is specified by the Einstein field equations, a system of second-order partial differential equations.
Newton's law of universal gravitation, which describes gravity in classical mechanics, can be seen as a prediction of general relativity for the almost flat spacetime geometry around stationary mass distributions. Some predictions of general relativity, however, are beyond Newton's law of universal gravitation in classical physics. These predictions concern the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light, and include gravitational time dilation, gravitational lensing, the gravitational redshift of light, the Shapiro time delay and singularities/black holes. So far, all tests of general relativity have been in agreement with the theory. The time-dependent solutions of general relativity enable us to extrapolate the history of the universe into the past and future, and have provided the modern framework for cosmology, thus leading to the discovery of the Big Bang and cosmic microwave background radiation. Despite the introduction of a number of alternative theories, general relativity continues to be the simplest theory consistent with experimental data.
Reconciliation of general relativity with the laws of quantum physics remains a problem, however, as no self-consistent theory of quantum gravity has been found. It is not yet known how gravity can be unified with the three non-gravitational interactions: strong, weak and electromagnetic.
Einstein's theory has astrophysical implications, including the prediction of black holes—regions of space in which space and time are distorted in such a way that nothing, not even light, can escape from them. Black holes are the end-state for massive stars. Microquasars and active galactic nuclei are believed to be stellar black holes and supermassive black holes. It also predicts gravitational lensing, where the bending of light results in distorted and multiple images of the same distant astronomical phenomenon. Other predictions include the existence of gravitational waves, which have been observed directly by the physics collaboration LIGO and other observatories. In addition, general relativity has provided the basis for cosmological models of an expanding universe.
Widely acknowledged as a theory of extraordinary beauty, general relativity has often been described as the most beautiful of all existing physical theories.
Henri Poincaré's 1905 theory of the dynamics of the electron was a relativistic theory which he applied to all forces, including gravity.  While others thought that gravity was instantaneous or of electromagnetic origin, he suggested that relativity was "something due to our methods of measurement". In his theory, he showed that gravitational waves propagate at the speed of light. Soon afterwards, Einstein started thinking about how to incorporate gravity into his relativistic framework. In 1907, beginning with a simple thought experiment involving an observer in free fall (FFO), he embarked on what would be an eight-year search for a relativistic theory of gravity. After numerous detours and false starts, his work culminated in the presentation to the Prussian Academy of Science in November 1915 of what are known as the Einstein field equations, which form the core of Einstein's general theory of relativity. These equations specify how the geometry of space and time is influenced by whatever matter and radiation are present. A version of non-Euclidean geometry, called Riemannian geometry, enabled Einstein to develop general relativity by providing the key mathematical framework on which he fit his physical ideas of gravity. This idea was pointed out by mathematician Marcel Grossmann and published by Grossmann and Einstein in 1913.
The Einstein field equations are nonlinear and are considered difficult to solve. Einstein used approximation methods in working out initial predictions of the theory. But in 1916, the astrophysicist Karl Schwarzschild found the first non-trivial exact solution to the Einstein field equations, the Schwarzschild metric. This solution laid the groundwork for the description of the final stages of gravitational collapse, and the objects known today as black holes. In the same year, the first steps towards generalizing Schwarzschild's solution to electrically charged objects were taken, eventually resulting in the Reissner–Nordström solution, which is associated with electrically charged black holes. In 1917, Einstein applied his theory to the universe as a whole, initiating the field of relativistic cosmology. In line with contemporary thinking, he assumed a static universe, adding a new parameter to his original field equations—the cosmological constant—to match that observational presumption. By 1929, however, the work of Hubble and others had shown that the universe is expanding. This is readily described by the expanding cosmological solutions found by Friedmann in 1922, which do not require a cosmological constant. Lemaître used these solutions to formulate the earliest version of the Big Bang models, in which the universe has evolved from an extremely hot and dense earlier state. Einstein later declared the cosmological constant the biggest blunder of his life.
During that period, general relativity remained something of a curiosity among physical theories. It was clearly superior to Newtonian gravity, being consistent with special relativity and accounting for several effects unexplained by the Newtonian theory. Einstein showed in 1915 how his theory explained the anomalous perihelion advance of the planet Mercury without any arbitrary parameters ("fudge factors"), and in 1919 an expedition led by Eddington confirmed general relativity's prediction for the deflection of starlight by the Sun during the total solar eclipse of 29 May 1919, instantly making Einstein famous. Yet the theory remained outside the mainstream of theoretical physics and astrophysics until developments between approximately 1960 and 1975 known as the golden age of general relativity. Physicists began to understand the concept of a black hole, and to identify quasars as one of these objects' astrophysical manifestations. Ever more precise solar system tests confirmed the theory's predictive power, and relativistic cosmology also became amenable to direct observational tests.
General relativity has acquired a reputation as a theory of extraordinary beauty. Subrahmanyan Chandrasekhar has noted that at multiple levels, general relativity exhibits what Francis Bacon has termed a "strangeness in the proportion" (i.e. elements that excite wonderment and surprise). It juxtaposes fundamental concepts (space and time versus matter and motion) which had previously been considered as entirely independent. Chandrasekhar also noted that Einstein's only guides in his search for an exact theory were the principle of equivalence and his sense that a proper description of gravity should be geometrical at its basis, so that there was an "element of revelation" in the manner in which Einstein arrived at his theory. Other elements of beauty associated with the general theory of relativity are its simplicity and symmetry, the manner in which it incorporates invariance and unification, and its perfect logical consistency.
In the preface to Relativity: The Special and the General Theory, Einstein said "The present book is intended, as far as possible, to give an exact insight into the theory of Relativity to those readers who, from a general scientific and philosophical point of view, are interested in the theory, but who are not conversant with the mathematical apparatus of theoretical physics. The work presumes a standard of education corresponding to that of a university matriculation examination, and, despite the shortness of the book, a fair amount of patience and force of will on the part of the reader. The author has spared himself no pains in his endeavour to present the main ideas in the simplest and most intelligible form, and on the whole, in the sequence and connection in which they actually originated."
General relativity can be understood by examining its similarities with and departures from classical physics. The first step is the realization that classical mechanics and Newton's law of gravity admit a geometric description. The combination of this description with the laws of special relativity results in a heuristic derivation of general relativity.
=
At the base of classical mechanics is the notion that a body's motion can be described as a combination of free (or inertial) motion, and deviations from this free motion. Such deviations are caused by external forces acting on a body in accordance with Newton's second law of motion, which states that the net force acting on a body is equal to that body's (inertial) mass multiplied by its acceleration. The preferred inertial motions are related to the geometry of space and time: in the standard reference frames of classical mechanics, objects in free motion move along straight lines at constant speed. In modern parlance, their paths are geodesics, straight world lines in curved spacetime.
Conversely, one might expect that inertial motions, once identified by observing the actual motions of bodies and making allowances for the external forces (such as electromagnetism or friction), can be used to define the geometry of space, as well as a time coordinate. However, there is an ambiguity once gravity comes into play. According to Newton's law of gravity, and independently verified by experiments such as that of Eötvös and its successors (see Eötvös experiment), there is a universality of free fall (also known as the weak equivalence principle, or the universal equality of inertial and passive-gravitational mass): the trajectory of a test body in free fall depends only on its position and initial speed, but not on any of its material properties. A simplified version of this is embodied in Einstein's elevator experiment, illustrated in the figure on the right: for an observer in an enclosed room, it is impossible to decide, by mapping the trajectory of bodies such as a dropped ball, whether the room is stationary in a gravitational field and the ball accelerating, or in free space aboard a rocket that is accelerating at a rate equal to that of the gravitational field versus the ball which upon release has nil acceleration.
Given the universality of free fall, there is no observable distinction between inertial motion and motion under the influence of the gravitational force. This suggests the definition of a new class of inertial motion, namely that of objects in free fall under the influence of gravity. This new class of preferred motions, too, defines a geometry of space and time—in mathematical terms, it is the geodesic motion associated with a specific connection which depends on the gradient of the gravitational potential. Space, in this construction, still has the ordinary Euclidean geometry. However, spacetime as a whole is more complicated. As can be shown using simple thought experiments following the free-fall trajectories of different test particles, the result of transporting spacetime vectors that can denote a particle's velocity (time-like vectors) will vary with the particle's trajectory; mathematically speaking, the Newtonian connection is not integrable. From this, one can deduce that spacetime is curved. The resulting Newton–Cartan theory is a geometric formulation of Newtonian gravity using only covariant concepts, i.e. a description which is valid in any desired coordinate system. In this geometric description, tidal effects—the relative acceleration of bodies in free fall—are related to the derivative of the connection, showing how the modified geometry is caused by the presence of mass.
=
As intriguing as geometric Newtonian gravity may be, its basis, classical mechanics, is merely a limiting case of (special) relativistic mechanics. In the language of symmetry: where gravity can be neglected, physics is Lorentz invariant as in special relativity rather than Galilei invariant as in classical mechanics. (The defining symmetry of special relativity is the Poincaré group, which includes translations, rotations, boosts and reflections.) The differences between the two become significant when dealing with speeds approaching the speed of light, and with high-energy phenomena.
With Lorentz symmetry, additional structures come into play. They are defined by the set of light cones (see image). The light-cones define a causal structure: for each event A, there is a set of events that can, in principle, either influence or be influenced by A via signals or interactions that do not need to travel faster than light (such as event B in the image), and a set of events for which such an influence is impossible (such as event C in the image). These sets are observer-independent. In conjunction with the world-lines of freely falling particles, the light-cones can be used to reconstruct the spacetime's semi-Riemannian metric, at least up to a positive scalar factor. In mathematical terms, this defines a conformal structure or conformal geometry.
Special relativity is defined in the absence of gravity. For practical applications, it is a suitable model whenever gravity can be neglected. Bringing gravity into play, and assuming the universality of free fall motion, an analogous reasoning as in the previous section applies: there are no global inertial frames. Instead there are approximate inertial frames moving alongside freely falling particles. Translated into the language of spacetime: the straight time-like lines that define a gravity-free inertial frame are deformed to lines that are curved relative to each other, suggesting that the inclusion of gravity necessitates a change in spacetime geometry.
A priori, it is not clear whether the new local frames in free fall coincide with the reference frames in which the laws of special relativity hold—that theory is based on the propagation of light, and thus on electromagnetism, which could have a different set of preferred frames. But using different assumptions about the special-relativistic frames (such as their being earth-fixed, or in free fall), one can derive different predictions for the gravitational redshift, that is, the way in which the frequency of light shifts as the light propagates through a gravitational field (cf. below). The actual measurements show that free-falling frames are the ones in which light propagates as it does in special relativity. The generalization of this statement, namely that the laws of special relativity hold to good approximation in freely falling (and non-rotating) reference frames, is known as the Einstein equivalence principle, a crucial guiding principle for generalizing special-relativistic physics to include gravity.
The same experimental data shows that time as measured by clocks in a gravitational field—proper time, to give the technical term—does not follow the rules of special relativity. In the language of spacetime geometry, it is not measured by the Minkowski metric. As in the Newtonian case, this is suggestive of a more general geometry. At small scales, all reference frames that are in free fall are equivalent, and approximately Minkowskian. Consequently, we are dealing with a curved generalization of Minkowski space. The metric tensor that defines the geometry—in particular, how lengths and angles are measured—is not the Minkowski metric of special relativity, it is a generalization known as a semi- or pseudo-Riemannian metric. Furthermore, each Riemannian metric is naturally associated with one particular kind of connection, the Levi-Civita connection, and this is, in fact, the connection that satisfies the equivalence principle and makes space locally Minkowskian (that is, in suitable locally inertial coordinates, the metric is Minkowskian, and its first partial derivatives and the connection coefficients vanish).
=
Having formulated the relativistic, geometric version of the effects of gravity, the question of gravity's source remains. In Newtonian gravity, the source is mass. In special relativity, mass turns out to be part of a more general quantity called the stress–energy tensor, which includes both energy and momentum densities as well as stress: pressure and shear. Using the equivalence principle, this tensor is readily generalized to curved spacetime. Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume for a small cloud of test particles that are initially at rest, and then fall freely. In special relativity, conservation of energy–momentum corresponds to the statement that the stress–energy tensor is divergence-free. This formula, too, is readily generalized to curved spacetime by replacing partial derivatives with their curved-manifold counterparts, covariant derivatives studied in differential geometry. With this additional condition—the covariant divergence of the stress–energy tensor, and hence of whatever is on the other side of the equation, is zero—the simplest nontrivial set of equations are what are called Einstein's (field) equations:
On the left-hand side is the Einstein tensor, 
  
    
      
        
          G
          
            μ
            ν
          
        
      
    
    {\displaystyle G_{\mu \nu }}
  
, which is symmetric and a specific divergence-free combination of the Ricci tensor 
  
    
      
        
          R
          
            μ
            ν
          
        
      
    
    {\displaystyle R_{\mu \nu }}
  
 and the metric. In particular,
  
    
      
        R
        =
        
          g
          
            μ
            ν
          
        
        
          R
          
            μ
            ν
          
        
      
    
    {\displaystyle R=g^{\mu \nu }R_{\mu \nu }}
  
is the curvature scalar. The Ricci tensor itself is related to the more general Riemann curvature tensor as
  
    
      
        
          R
          
            μ
            ν
          
        
        =
        
          
            
              R
              
                α
              
            
          
          
            μ
            α
            ν
          
        
        .
      
    
    {\displaystyle R_{\mu \nu }={R^{\alpha }}_{\mu \alpha \nu }.}
  
On the right-hand side, 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
  
 is a constant and 
  
    
      
        
          T
          
            μ
            ν
          
        
      
    
    {\displaystyle T_{\mu \nu }}
  
 is the stress–energy tensor. All tensors are written in abstract index notation. Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
  
 is found to be 
  
    
      
        κ
        =
        
          8
          π
          G
        
        
          /
        
        
          
            c
            
              4
            
          
        
      
    
    {\textstyle \kappa ={8\pi G}/{c^{4}}}
  
, where 
  
    
      
        G
      
    
    {\displaystyle G}
  
 is the Newtonian constant of gravitation and 
  
    
      
        c
      
    
    {\displaystyle c}
  
 the speed of light in vacuum. When there is no matter present, so that the stress–energy tensor vanishes, the results are the vacuum Einstein equations,
  
    
      
        
          R
          
            μ
            ν
          
        
        =
        0.
      
    
    {\displaystyle R_{\mu \nu }=0.}
  
In general relativity, the world line of a particle free from all external, non-gravitational force is a particular type of geodesic in curved spacetime. In other words, a freely moving or falling particle always moves along a geodesic.
The geodesic equation is:
  
    
      
        
          
            
              
                d
                
                  2
                
              
              
                x
                
                  μ
                
              
            
            
              d
              
                s
                
                  2
                
              
            
          
        
        +
        
          Γ
          
            μ
          
        
        
          
          
          
            α
            β
          
        
        
          
            
              d
              
                x
                
                  α
                
              
            
            
              d
              s
            
          
        
        
          
            
              d
              
                x
                
                  β
                
              
            
            
              d
              s
            
          
        
        =
        0
        ,
      
    
    {\displaystyle {d^{2}x^{\mu } \over ds^{2}}+\Gamma ^{\mu }{}_{\alpha \beta }{dx^{\alpha } \over ds}{dx^{\beta } \over ds}=0,}
  
where 
  
    
      
        s
      
    
    {\displaystyle s}
  
 is a scalar parameter of motion (e.g. the proper time), and 
  
    
      
        
          Γ
          
            μ
          
        
        
          
          
          
            α
            β
          
        
      
    
    {\displaystyle \Gamma ^{\mu }{}_{\alpha \beta }}
  
 are Christoffel symbols (sometimes called the affine connection coefficients or Levi-Civita connection coefficients) which is symmetric in the two lower indices. Greek indices may take the values: 0, 1, 2, 3 and the summation convention is used for repeated indices 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  
 and 
  
    
      
        β
      
    
    {\displaystyle \beta }
  
. The quantity on the left-hand-side of this equation is the acceleration of a particle, and so this equation is analogous to Newton's laws of motion which likewise provide formulae for the acceleration of a particle. This equation of motion employs the Einstein notation, meaning that repeated indices are summed (i.e. from zero to three). The Christoffel symbols are functions of the four spacetime coordinates, and so are independent of the velocity or acceleration or other characteristics of a test particle whose motion is described by the geodesic equation.
=
In general relativity, the effective gravitational potential energy of an object of mass m revolving around a massive central body M is given by
  
    
      
        
          U
          
            f
          
        
        (
        r
        )
        =
        −
        
          
            
              G
              M
              m
            
            r
          
        
        +
        
          
            
              L
              
                2
              
            
            
              2
              m
              
                r
                
                  2
                
              
            
          
        
        −
        
          
            
              G
              M
              
                L
                
                  2
                
              
            
            
              m
              
                c
                
                  2
                
              
              
                r
                
                  3
                
              
            
          
        
      
    
    {\displaystyle U_{f}(r)=-{\frac {GMm}{r}}+{\frac {L^{2}}{2mr^{2}}}-{\frac {GML^{2}}{mc^{2}r^{3}}}}
  
A conservative total force can then be obtained as its negative gradient
  
    
      
        
          F
          
            f
          
        
        (
        r
        )
        =
        −
        
          
            
              G
              M
              m
            
            
              r
              
                2
              
            
          
        
        +
        
          
            
              L
              
                2
              
            
            
              m
              
                r
                
                  3
                
              
            
          
        
        −
        
          
            
              3
              G
              M
              
                L
                
                  2
                
              
            
            
              m
              
                c
                
                  2
                
              
              
                r
                
                  4
                
              
            
          
        
      
    
    {\displaystyle F_{f}(r)=-{\frac {GMm}{r^{2}}}+{\frac {L^{2}}{mr^{3}}}-{\frac {3GML^{2}}{mc^{2}r^{4}}}}
  
where L is the angular momentum. The first term represents the force of Newtonian gravity, which is described by the inverse-square law. The second term represents the centrifugal force in the circular motion. The third term represents the relativistic effect.
=
There are alternatives to general relativity built upon the same premises, which include additional rules and/or constraints, leading to different field equations. Examples are Whitehead's theory, Brans–Dicke theory, teleparallelism, f(R) gravity and Einstein–Cartan theory.
The derivation outlined in the previous section contains all the information needed to define general relativity, describe its key properties, and address a question of crucial importance in physics, namely how the theory can be used for model-building.
=
General relativity is a metric theory of gravitation. At its core are Einstein's equations, which describe the relation between the geometry of a four-dimensional pseudo-Riemannian manifold representing spacetime, and the distribution of energy, momentum and stress contained in that spacetime. Phenomena that in classical mechanics are ascribed to the action of the force of gravity (such as free-fall, orbital motion, and spacecraft trajectories), correspond to inertial motion within a curved geometry of spacetime in general relativity; there is no gravitational force deflecting objects from their natural, straight paths. Instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow. The curvature is, in turn, caused by the stress–energy of matter. Paraphrasing the relativist John Archibald Wheeler, spacetime tells matter how to move; matter tells spacetime how to curve.
While general relativity replaces the scalar gravitational potential of classical physics by a symmetric rank-two tensor, the latter reduces to the former in certain limiting cases. For weak gravitational fields and low speed relative to the speed of light, the theory's predictions converge on those of Newton's law of universal gravitation.
As it is constructed using tensors, general relativity exhibits general covariance: its laws—and further laws formulated within the general relativistic framework—take on the same form in all coordinate systems. Furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background-independent. It thus satisfies a more stringent general principle of relativity, namely that the laws of physics are the same for all observers. Locally, as expressed in the equivalence principle, spacetime is Minkowskian, and the laws of physics exhibit local Lorentz invariance.
=
The core concept of general-relativistic model-building is that of a solution of Einstein's equations. Given both Einstein's equations and suitable equations for the properties of matter, such a solution consists of a specific semi-Riemannian manifold (usually defined by giving the metric in specific coordinates), and specific matter fields defined on that manifold. Matter and geometry must satisfy Einstein's equations, so in particular, the matter's stress–energy tensor must be divergence-free. The matter must, of course, also satisfy whatever additional equations were imposed on its properties. In short, such a solution is a model universe that satisfies the laws of general relativity, and possibly additional laws governing whatever matter might be present.
Einstein's equations are nonlinear partial differential equations and, as such, difficult to solve exactly. Nevertheless, a number of exact solutions are known, although only a few have direct physical applications. The best-known exact solutions, and also those most interesting from a physics point of view, are the Schwarzschild solution, the Reissner–Nordström solution and the Kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe, and the Friedmann–Lemaître–Robertson–Walker and de Sitter universes, each describing an expanding cosmos. Exact solutions of great theoretical interest include the Gödel universe (which opens up the intriguing possibility of time travel in curved spacetimes), the Taub–NUT solution (a model universe that is homogeneous, but anisotropic), and anti-de Sitter space (which has recently come to prominence in the context of what is called the Maldacena conjecture).
Given the difficulty of finding exact solutions, Einstein's field equations are also solved frequently by numerical integration on a computer, or by considering small perturbations of exact solutions. In the field of numerical relativity, powerful computers are employed to simulate the geometry of spacetime and to solve Einstein's equations for interesting situations such as two colliding black holes. In principle, such methods may be applied to any system, given sufficient computer resources, and may address fundamental questions such as naked singularities. Approximate solutions may also be found by perturbation theories such as linearized gravity and its generalization, the post-Newtonian expansion, both of which were developed by Einstein. The latter provides a systematic approach to solving for the geometry of a spacetime that contains a distribution of matter that moves slowly compared with the speed of light. The expansion involves a series of terms; the first terms represent Newtonian gravity, whereas the later terms represent ever smaller corrections to Newton's theory due to general relativity. An extension of this expansion is the parametrized post-Newtonian (PPN) formalism, which allows quantitative comparisons between the predictions of general relativity and alternative theories.
General relativity has a number of physical consequences. Some follow directly from the theory's axioms, whereas others have become clear only in the course of many years of research that followed Einstein's initial publication.
=
Assuming that the equivalence principle holds, gravity influences the passage of time. Light sent down into a gravity well is blueshifted, whereas light sent in the opposite direction (i.e., climbing out of the gravity well) is redshifted; collectively, these two effects are known as the gravitational frequency shift. More generally, processes close to a massive body run more slowly when compared with processes taking place farther away; this effect is known as gravitational time dilation.
Gravitational redshift has been measured in the laboratory and using astronomical observations. Gravitational time dilation in the Earth's gravitational field has been measured numerous times using atomic clocks, while ongoing validation is provided as a side effect of the operation of the Global Positioning System (GPS). Tests in stronger gravitational fields are provided by the observation of binary pulsars. All results are in agreement with general relativity. However, at the existing level of accuracy, these observations cannot distinguish between general relativity and other theories in which the equivalence principle is valid.
In the vicinity of a non-rotating sphere, the time dilation due to gravity, derived from the Schwarzschild metric, is 
  
    
      
        
          t
          
            0
          
        
        =
        
          t
          
            f
          
        
        
          
            1
            −
            
              
                
                  2
                  G
                  M
                
                
                  r
                  
                    c
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle t_{0}=t_{f}{\sqrt {1-{\frac {2GM}{rc^{2}}}}}}
  
where
  
    
      
        
          t
          
            0
          
        
      
    
    {\displaystyle t_{0}}
  
 is the proper time between two events for an observer close to the massive sphere, i.e. deep within the gravitational field
  
    
      
        
          t
          
            f
          
        
      
    
    {\displaystyle t_{f}}
  
 is the coordinate time between the events for an observer at an arbitrarily large distance from the massive object (this assumes the far-away observer is using Schwarzschild coordinates, a coordinate system where a clock at infinite distance from the massive sphere would tick at one second per second of coordinate time, while closer clocks would tick at less than that rate),
  
    
      
        G
      
    
    {\displaystyle G}
  
 is the gravitational constant,
  
    
      
        M
      
    
    {\displaystyle M}
  
 is the mass of the object creating the gravitational field,
  
    
      
        r
      
    
    {\displaystyle r}
  
 is the radial coordinate of the observer within the gravitational field (this coordinate is analogous to the classical distance from the center of the object, but is actually a Schwarzschild coordinate; the equation in this form has real solutions for 
  
    
      
        r
        >
        
          r
          
            
              s
            
          
        
      
    
    {\displaystyle r>r_{\rm {s}}}
  
),
  
    
      
        c
      
    
    {\displaystyle c}
  
 is the speed of light.
=
General relativity predicts that the path of light will follow the curvature of spacetime as it passes near a massive object. This effect was initially confirmed by observing the light of stars or distant quasars being deflected as it passes the Sun.
This and related predictions follow from the fact that light follows what is called a light-like or null geodesic—a generalization of the straight lines along which light travels in classical physics. Such geodesics are the generalization of the invariance of lightspeed in special relativity. As one examines suitable model spacetimes (either the exterior Schwarzschild solution or, for more than a single mass, the post-Newtonian expansion), several effects of gravity on light propagation emerge. Although the bending of light can also be derived by extending the universality of free fall to light, the angle of deflection resulting from such calculations is only half the value given by general relativity.
Closely related to light deflection is the Shapiro time delay, the phenomenon that light signals take longer to move through a gravitational field than they would in the absence of that field. There have been numerous successful tests of this prediction. In the parameterized post-Newtonian formalism (PPN), measurements of both the deflection of light and the gravitational time delay determine a parameter called γ, which encodes the influence of gravity on the geometry of space.
=
Predicted in 1916 by Albert Einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. These are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. On 11 February 2016, the Advanced LIGO team announced that they had directly detected gravitational waves from a pair of black holes merging.
The simplest type of such a wave can be visualized by its action on a ring of freely floating particles. A sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right). Since Einstein's equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. However, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on Earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by 10−21 or less. Data analysis methods routinely make use of the fact that these linearized waves can be Fourier decomposed.
Some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space or Gowdy universes, varieties of an expanding cosmos filled with gravitational waves. But for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are the only way to construct appropriate models.
=
General relativity differs from classical mechanics in a number of predictions concerning orbiting bodies. It predicts an overall rotation (precession) of planetary orbits, as well as orbital decay caused by the emission of gravitational waves and effects related to the relativity of direction.
 Precession of apsides 
In general relativity, the apsides of any orbit (the point of the orbiting body's closest approach to the system's center of mass) will precess; the orbit is not an ellipse, but akin to an ellipse that rotates on its focus, resulting in a rose curve-like shape (see image). Einstein first derived this result by using an approximate metric representing the Newtonian limit and treating the orbiting body as a test particle. For him, the fact that his theory gave a straightforward explanation of Mercury's anomalous perihelion shift, discovered earlier by Urbain Le Verrier in 1859, was important evidence that he had at last identified the correct form of the gravitational field equations.
The effect can also be derived by using either the exact Schwarzschild metric (describing spacetime around a spherical mass) or the much more general post-Newtonian formalism. It is due to the influence of gravity on the geometry of space and to the contribution of self-energy to a body's gravity (encoded in the nonlinearity of Einstein's equations). Relativistic precession has been observed for all planets that allow for accurate precession measurements (Mercury, Venus, and Earth), as well as in binary pulsar systems, where it is larger by five orders of magnitude.
In general relativity the perihelion shift 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
, expressed in radians per revolution, is approximately given by:
  
    
      
        σ
        =
        
          
            
              24
              
                π
                
                  3
                
              
              
                L
                
                  2
                
              
            
            
              
                T
                
                  2
                
              
              
                c
                
                  2
                
              
              (
              1
              −
              
                e
                
                  2
                
              
              )
            
          
        
         
        ,
      
    
    {\displaystyle \sigma ={\frac {24\pi ^{3}L^{2}}{T^{2}c^{2}(1-e^{2})}}\ ,}
  
where:
  
    
      
        L
      
    
    {\displaystyle L}
  
 is the semi-major axis
  
    
      
        T
      
    
    {\displaystyle T}
  
 is the orbital period
  
    
      
        c
      
    
    {\displaystyle c}
  
 is the speed of light in a vacuum
  
    
      
        e
      
    
    {\displaystyle e}
  
 is the orbital eccentricity
 Orbital decay 
According to general relativity, a binary system will emit gravitational waves, thereby losing energy. Due to this loss, the distance between the two orbiting bodies decreases, and so does their orbital period. Within the Solar System or for ordinary double stars, the effect is too small to be observable. This is not the case for a close binary pulsar, a system of two orbiting neutron stars, one of which is a pulsar: from the pulsar, observers on Earth receive a regular series of radio pulses that can serve as a highly accurate clock, which allows precise measurements of the orbital period. Because neutron stars are immensely compact, significant amounts of energy are emitted in the form of gravitational radiation.
The first observation of a decrease in orbital period due to the emission of gravitational waves was made by Hulse and Taylor, using the binary pulsar PSR1913+16 they had discovered in 1974. This was the first detection of gravitational waves, albeit indirect, for which they were awarded the 1993 Nobel Prize in physics. Since then, several other binary pulsars have been found, in particular the double pulsar PSR J0737−3039, where both stars are pulsars and which was last reported to also be in agreement with general relativity in 2021 after 16 years of observations.
 Geodetic precession and frame-dragging 
Several relativistic effects are directly related to the relativity of direction. One is geodetic precession: the axis direction of a gyroscope in free fall in curved spacetime will change when compared, for instance, with the direction of light received from distant stars—even though such a gyroscope represents the way of keeping a direction as stable as possible ("parallel transport"). For the Moon–Earth system, this effect has been measured with the help of lunar laser ranging. More recently, it has been measured for test masses aboard the satellite Gravity Probe B to a precision of better than 0.3%.
Near a rotating mass, there are gravitomagnetic or frame-dragging effects. A distant observer will determine that objects close to the mass get "dragged around". This is most extreme for rotating black holes where, for any object entering a zone known as the ergosphere, rotation is inevitable. Such effects can again be tested through their influence on the orientation of gyroscopes in free fall. Somewhat controversial tests have been performed using the LAGEOS satellites, confirming the relativistic prediction. Also the Mars Global Surveyor probe around Mars has been used.
=
The deflection of light by gravity is responsible for a new class of astronomical phenomena. If a massive object is situated between the astronomer and a distant target object with appropriate mass and relative distances, the astronomer will see multiple distorted images of the target. Such effects are known as gravitational lensing. Depending on the configuration, scale, and mass distribution, there can be two or more images, a bright ring known as an Einstein ring, or partial rings called arcs.
The earliest example was discovered in 1979; since then, more than a hundred gravitational lenses have been observed. Even if the multiple images are too close to each other to be resolved, the effect can still be measured, e.g., as an overall brightening of the target object; a number of such "microlensing events" have been observed.
Gravitational lensing has developed into a tool of observational astronomy. It is used to detect the presence and distribution of dark matter, provide a "natural telescope" for observing distant galaxies, and to obtain an independent estimate of the Hubble constant. Statistical evaluations of lensing data provide valuable insight into the structural evolution of galaxies.
=
Observations of binary pulsars provide strong indirect evidence for the existence of gravitational waves (see Orbital decay, above). Detection of these waves is a major goal of contemporary relativity-related research. Several land-based gravitational wave detectors are in operation, for example the interferometric detectors GEO 600, LIGO (two detectors), TAMA 300 and VIRGO. Various pulsar timing arrays are using millisecond pulsars to detect gravitational waves in the 10−9 to 10−6 hertz frequency range, which originate from binary supermassive blackholes. A European space-based detector, eLISA / NGO, is under development, with a precursor mission (LISA Pathfinder) having launched in December 2015.
Observations of gravitational waves promise to complement observations in the electromagnetic spectrum. They are expected to yield information about black holes and other dense objects such as neutron stars and white dwarfs, about certain kinds of supernova implosions, and about processes in the very early universe, including the signature of certain types of hypothetical cosmic string. In February 2016, the Advanced LIGO team announced that they had detected gravitational waves from a black hole merger.
=
Whenever the ratio of an object's mass to its radius becomes sufficiently large, general relativity predicts the formation of a black hole, a region of space from which nothing, not even light, can escape. In the accepted models of stellar evolution, neutron stars of around 1.4 solar masses, and stellar black holes with a few to a few dozen solar masses, are thought to be the final state for the evolution of massive stars. Usually a galaxy has one supermassive black hole with a few million to a few billion solar masses in its center, and its presence is thought to have played an important role in the formation of the galaxy and larger cosmic structures.
Astronomically, the most important property of compact objects is that they provide a supremely efficient mechanism for converting gravitational energy into electromagnetic radiation. Accretion, the falling of dust or gaseous matter onto stellar or supermassive black holes, is thought to be responsible for some spectacularly luminous astronomical objects, especially diverse kinds of active galactic nuclei on galactic scales and stellar-size objects such as microquasars. In particular, accretion can lead to relativistic jets, focused beams of highly energetic particles that are being flung into space at almost light speed.
General relativity plays a central role in modelling all these phenomena, and observations provide strong evidence for the existence of black holes with the properties predicted by the theory.
Black holes are also sought-after targets in the search for gravitational waves (cf. section § Gravitational waves, above). Merging black hole binaries should lead to some of the strongest gravitational wave signals reaching detectors on Earth, and the phase directly before the merger ("chirp") could be used as a "standard candle" to deduce the distance to the merger events–and hence serve as a probe of cosmic expansion at large distances. The gravitational waves produced as a stellar black hole plunges into a supermassive one should provide direct information about the supermassive black hole's geometry.
=
The existing models of cosmology are based on Einstein's field equations, which include the cosmological constant 
  
    
      
        Λ
      
    
    {\displaystyle \Lambda }
  
 since it has important influence on the large-scale dynamics of the cosmos,
  
    
      
        
          R
          
            μ
            ν
          
        
        −
        
          
            
              1
            
            2
          
        
        R
        
        
          g
          
            μ
            ν
          
        
        +
        Λ
         
        
          g
          
            μ
            ν
          
        
        =
        
          
            
              8
              π
              G
            
            
              c
              
                4
              
            
          
        
        
        
          T
          
            μ
            ν
          
        
      
    
    {\displaystyle R_{\mu \nu }-{\textstyle 1 \over 2}R\,g_{\mu \nu }+\Lambda \ g_{\mu \nu }={\frac {8\pi G}{c^{4}}}\,T_{\mu \nu }}
  
where 
  
    
      
        
          g
          
            μ
            ν
          
        
      
    
    {\displaystyle g_{\mu \nu }}
  
 is the spacetime metric. Isotropic and homogeneous solutions of these enhanced equations, the Friedmann–Lemaître–Robertson–Walker solutions, allow physicists to model a universe that has evolved over the past 14 billion years from a hot, early Big Bang phase. Once a small number of parameters (for example the universe's mean matter density) have been fixed by astronomical observation, further observational data can be used to put the models to the test. Predictions, all successful, include the initial abundance of chemical elements formed in a period of primordial nucleosynthesis, the large-scale structure of the universe, and the existence and properties of a "thermal echo" from the early cosmos, the cosmic background radiation.
Astronomical observations of the cosmological expansion rate allow the total amount of matter in the universe to be estimated, although the nature of that matter remains mysterious in part. About 90% of all matter appears to be dark matter, which has mass (or, equivalently, gravitational influence), but does not interact electromagnetically and, hence, cannot be observed directly. There is no generally accepted description of this new kind of matter, within the framework of known particle physics or otherwise. Observational evidence from redshift surveys of distant supernovae and measurements of the cosmic background radiation also show that the evolution of the universe is significantly influenced by a cosmological constant resulting in an acceleration of cosmic expansion or, equivalently, by a form of energy with an unusual equation of state, known as dark energy, the nature of which remains unclear.
An inflationary phase, an additional phase of strongly accelerated expansion at cosmic times of around 10−33 seconds, was hypothesized in 1980 to account for several puzzling observations that were unexplained by classical cosmological models, such as the nearly perfect homogeneity of the cosmic background radiation. Recent measurements of the cosmic background radiation have resulted in the first evidence for this scenario. However, there are a bewildering variety of possible inflationary scenarios, which cannot be restricted by existing observations. An even larger question is the physics of the earliest universe, prior to the inflationary phase and close to where the classical models predict the big bang singularity. An authoritative answer would require a complete theory of quantum gravity, which has not yet been developed (cf. the section on quantum gravity, below).
=
Kurt Gödel showed that solutions to Einstein's equations exist that contain closed timelike curves (CTCs), which allow for loops in time. The solutions require extreme physical conditions unlikely ever to occur in practice, and it remains an open question whether further laws of physics will eliminate them completely. Since then, other—similarly impractical—GR solutions containing CTCs have been found, such as the Tipler cylinder and traversable wormholes. Stephen Hawking introduced chronology protection conjecture, which is an assumption beyond those of standard general relativity to prevent time travel.
Some exact solutions in general relativity such as Alcubierre drive offer examples of warp drive but these solutions require exotic matter distribution, and generally suffer from semiclassical instability.
=
The spacetime symmetry group for special relativity is the Poincaré group, which is a ten-dimensional group of three Lorentz boosts, three rotations, and four spacetime translations. It is logical to ask what symmetries, if any, might apply in General Relativity. A tractable case might be to consider the symmetries of spacetime as seen by observers located far away from all sources of the gravitational field. The naive expectation for asymptotically flat spacetime symmetries might be simply to extend and reproduce the symmetries of flat spacetime of special relativity, viz., the Poincaré group.
In 1962 Hermann Bondi, M. G. van der Burg, A. W. Metzner and Rainer K. Sachs addressed this asymptotic symmetry problem in order to investigate the flow of energy at infinity due to propagating gravitational waves. Their first step was to decide on some physically sensible boundary conditions to place on the gravitational field at light-like infinity to characterize what it means to say a metric is asymptotically flat, making no a priori assumptions about the nature of the asymptotic symmetry group—not even the assumption that such a group exists. Then after designing what they considered to be the most sensible boundary conditions, they investigated the nature of the resulting asymptotic symmetry transformations that leave invariant the form of the boundary conditions appropriate for asymptotically flat gravitational fields. What they found was that the asymptotic symmetry transformations actually do form a group and the structure of this group does not depend on the particular gravitational field that happens to be present. This means that, as expected, one can separate the kinematics of spacetime from the dynamics of the gravitational field at least at spatial infinity. The puzzling surprise in 1962 was their discovery of a rich infinite-dimensional group (the so-called BMS group) as the asymptotic symmetry group, instead of the finite-dimensional Poincaré group, which is a subgroup of the BMS group. Not only are the Lorentz transformations asymptotic symmetry transformations, there are also additional transformations that are not Lorentz transformations but are asymptotic symmetry transformations. In fact, they found an additional infinity of transformation generators known as supertranslations. This implies the conclusion that General Relativity (GR) does not reduce to special relativity in the case of weak fields at long distances. It turns out that the BMS symmetry, suitably modified, could be seen as a restatement of the universal soft graviton theorem in quantum field theory (QFT), which relates universal infrared (soft) QFT with GR asymptotic spacetime symmetries.
=
In general relativity, no material body can catch up with or overtake a light pulse. No influence from an event A can reach any other location X before light sent out at A to X. In consequence, an exploration of all light worldlines (null geodesics) yields key information about the spacetime's causal structure. This structure can be displayed using Penrose–Carter diagrams in which infinitely large regions of space and infinite time intervals are shrunk ("compactified") so as to fit onto a finite map, while light still travels along diagonals as in standard spacetime diagrams.
Aware of the importance of causal structure, Roger Penrose and others developed what is known as global geometry. In global geometry, the object of study is not one particular solution (or family of solutions) to Einstein's equations. Rather, relations that hold true for all geodesics, such as the Raychaudhuri equation, and additional non-specific assumptions about the nature of matter (usually in the form of energy conditions) are used to derive general results.
=
Using global geometry, some spacetimes can be shown to contain boundaries called horizons, which demarcate one region from the rest of spacetime. The best-known examples are black holes: if mass is compressed into a sufficiently compact region of space (as specified in the hoop conjecture, the relevant length scale is the Schwarzschild radius, given by the equation 
  
    
      
        
          r
          
            s
          
        
        =
        
          
            
              2
              G
              M
            
            
              c
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle r_{\text{s}}={\frac {2GM}{c^{2}}},}
  
), no light from inside can escape to the outside. Since no object can overtake a light pulse, all interior matter is imprisoned as well. Passage from the exterior to the interior is still possible, showing that the boundary, the black hole's horizon, is not a physical barrier.
Early studies of black holes relied on explicit solutions of Einstein's equations, notably the spherically symmetric Schwarzschild solution (used to describe a static black hole) and the axisymmetric Kerr solution (used to describe a rotating, stationary black hole, and introducing interesting features such as the ergosphere). Using global geometry, later studies have revealed more general properties of black holes. With time they become rather simple objects characterized by eleven parameters specifying: electric charge, mass–energy, linear momentum, angular momentum, and location at a specified time. This is stated by the black hole uniqueness theorem: "black holes have no hair", that is, no distinguishing marks like the hairstyles of humans. Irrespective of the complexity of a gravitating object collapsing to form a black hole, the object that results (having emitted gravitational waves) is very simple.
Even more remarkably, there is a general set of laws known as black hole mechanics, which is analogous to the laws of thermodynamics. For instance, by the second law of black hole mechanics, the area of the event horizon of a general black hole will never decrease with time, analogous to the entropy of a thermodynamic system. This limits the energy that can be extracted by classical means from a rotating black hole (e.g. by the Penrose process). There is strong evidence that the laws of black hole mechanics are, in fact, a subset of the laws of thermodynamics, and that the black hole area is proportional to its entropy. This leads to a modification of the original laws of black hole mechanics: for instance, as the second law of black hole mechanics becomes part of the second law of thermodynamics, it is possible for the black hole area to decrease as long as other processes ensure that entropy increases overall. As thermodynamical objects with nonzero temperature, black holes should emit thermal radiation. Semiclassical calculations indicate that indeed they do, with the surface gravity playing the role of temperature in Planck's law. This radiation is known as Hawking radiation (cf. the quantum theory section, below).
There are many other types of horizons. In an expanding universe, an observer may find that some regions of the past cannot be observed ("particle horizon"), and some regions of the future cannot be influenced (event horizon). Even in flat Minkowski space, when described by an accelerated observer (Rindler space), there will be horizons associated with a semiclassical radiation known as Unruh radiation.
=
Another general feature of general relativity is the appearance of spacetime boundaries known as singularities. Spacetime can be explored by following up on timelike and lightlike geodesics—all possible ways that light and particles in free fall can travel. But some solutions of Einstein's equations have "ragged edges"—regions known as spacetime singularities, where the paths of light and falling particles come to an abrupt end, and geometry becomes ill-defined. In the more interesting cases, these are "curvature singularities", where geometrical quantities characterizing spacetime curvature, such as the Ricci scalar, take on infinite values. Well-known examples of spacetimes with future singularities—where worldlines end—are the Schwarzschild solution, which describes a singularity inside an eternal static black hole, or the Kerr solution with its ring-shaped singularity inside an eternal rotating black hole. The Friedmann–Lemaître–Robertson–Walker solutions and other spacetimes describing universes have past singularities on which worldlines begin, namely Big Bang singularities, and some have future singularities (Big Crunch) as well.
Given that these examples are all highly symmetric—and thus simplified—it is tempting to conclude that the occurrence of singularities is an artifact of idealization. The famous singularity theorems, proved using the methods of global geometry, say otherwise: singularities are a generic feature of general relativity, and unavoidable once the collapse of an object with realistic matter properties has proceeded beyond a certain stage and also at the beginning of a wide class of expanding universes. However, the theorems say little about the properties of singularities, and much of current research is devoted to characterizing these entities' generic structure (hypothesized e.g. by the BKL conjecture). The cosmic censorship hypothesis states that all realistic future singularities (no perfect symmetries, matter with realistic properties) are safely hidden away behind a horizon, and thus invisible to all distant observers. While no formal proof yet exists, numerical simulations offer supporting evidence of its validity.
=
Each solution of Einstein's equation encompasses the whole history of a universe—it is not just some snapshot of how things are, but a whole, possibly matter-filled, spacetime. It describes the state of matter and geometry everywhere and at every moment in that particular universe. Due to its general covariance, Einstein's theory is not sufficient by itself to determine the time evolution of the metric tensor. It must be combined with a coordinate condition, which is analogous to gauge fixing in other field theories.
To understand Einstein's equations as partial differential equations, it is helpful to formulate them in a way that describes the evolution of the universe over time. This is done in "3+1" formulations, where spacetime is split into three space dimensions and one time dimension. The best-known example is the ADM formalism. These decompositions show that the spacetime evolution equations of general relativity are well-behaved: solutions always exist, and are uniquely defined, once suitable initial conditions have been specified. Such formulations of Einstein's field equations are the basis of numerical relativity.
=
The notion of evolution equations is intimately tied in with another aspect of general relativistic physics. In Einstein's theory, it turns out to be impossible to find a general definition for a seemingly simple property such as a system's total mass (or energy). The main reason is that the gravitational field—like any physical field—must be ascribed a certain energy, but that it proves to be fundamentally impossible to localize that energy.
Nevertheless, there are possibilities to define a system's total mass, either using a hypothetical "infinitely distant observer" (ADM mass) or suitable symmetries (Komar mass). If one excludes from the system's total mass the energy being carried away to infinity by gravitational waves, the result is the Bondi mass at null infinity. Just as in classical physics, it can be shown that these masses are positive. Corresponding global definitions exist for momentum and angular momentum. There have also been a number of attempts to define quasi-local quantities, such as the mass of an isolated system formulated using only quantities defined within a finite region of space containing that system. The hope is to obtain a quantity useful for general statements about isolated systems, such as a more precise formulation of the hoop conjecture.
If general relativity were considered to be one of the two pillars of modern physics, then quantum theory, the basis of understanding matter from elementary particles to solid-state physics, would be the other. However, how to reconcile quantum theory with general relativity is still an open question.
=
Ordinary quantum field theories, which form the basis of modern elementary particle physics, are defined in flat Minkowski space, which is an excellent approximation when it comes to describing the behavior of microscopic particles in weak gravitational fields like those found on Earth. In order to describe situations in which gravity is strong enough to influence (quantum) matter, yet not strong enough to require quantization itself, physicists have formulated quantum field theories in curved spacetime. These theories rely on general relativity to describe a curved background spacetime, and define a generalized quantum field theory to describe the behavior of quantum matter within that spacetime. Using this formalism, it can be shown that black holes emit a blackbody spectrum of particles known as Hawking radiation leading to the possibility that they evaporate over time. As briefly mentioned above, this radiation plays an important role for the thermodynamics of black holes.
=
The demand for consistency between a quantum description of matter and a geometric description of spacetime, as well as the appearance of singularities (where curvature length scales become microscopic), indicate the need for a full theory of quantum gravity: for an adequate description of the interior of black holes, and of the very early universe, a theory is required in which gravity and the associated geometry of spacetime are described in the language of quantum physics. Despite major efforts, no complete and consistent theory of quantum gravity is currently known, even though a number of candidates exist.
Attempts to generalize ordinary quantum field theories, used in elementary particle physics to describe fundamental interactions, so as to include gravity have led to serious problems. Some have argued that at low energies, this approach proves successful, in that it results in an acceptable effective (quantum) field theory of gravity. At very high energies, however, the perturbative results are badly divergent and lead to models devoid of predictive power ("perturbative non-renormalizability").
One attempt to overcome these limitations is string theory, a quantum theory not of point particles, but of minute one-dimensional extended objects. The theory promises to be a unified description of all particles and interactions, including gravity; the price to pay is unusual features such as six extra dimensions of space in addition to the usual three. In what is called the second superstring revolution, it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity.
Another approach starts with the canonical quantization procedures of quantum theory. Using the initial-value-formulation of general relativity (cf. evolution equations above), the result is the Wheeler–deWitt equation (an analogue of the Schrödinger equation) which turns out to be ill-defined without a proper ultraviolet (lattice) cutoff. However, with the introduction of what are now known as Ashtekar variables, this leads to a model known as loop quantum gravity. Space is represented by a web-like structure called a spin network, evolving over time in discrete steps.
Depending on which features of general relativity and quantum theory are accepted unchanged, and on what level changes are introduced, there are numerous other attempts to arrive at a viable theory of quantum gravity, some examples being the lattice theory of gravity based on the Feynman Path Integral approach and Regge calculus, dynamical triangulations, causal sets, twistor models or the path integral based models of quantum cosmology.
All candidate theories still have major formal and conceptual problems to overcome. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests (and thus to decide between the candidates where their predictions vary), although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.
General relativity has emerged as a highly successful model of gravitation and cosmology, which has so far unambiguously fitted observational and experimental data. However, there are strong theoretical reasons to consider the theory to be incomplete. The problem of quantum gravity and the question of the reality of spacetime singularities remain open. Observational data that is taken as evidence for dark energy and dark matter could also indicate the need to consider alternatives or modifications of general relativity.
Even taken as is, general relativity provides many possibilities for further exploration. Mathematical relativists seek to understand the nature of singularities and the fundamental properties of Einstein's equations, while numerical relativists run increasingly powerful computer simulations, such as those describing merging black holes. In February 2016, it was announced that gravitational waves were directly detected by the Advanced LIGO team on 14 September 2015. A century after its introduction, general relativity remains a highly active area of research.
Alcubierre drive – Hypothetical FTL transportation by warping space (warp drive)
Alternatives to general relativity – Proposed theories of gravity
Contributors to general relativity
Derivations of the Lorentz transformations
Ehrenfest paradox – Paradox in special relativity
Einstein–Hilbert action – Concept in general relativity
Einstein's thought experiments – Albert Einstein's hypothetical situations to argue scientific points
General relativity priority dispute – Debate about credit for general relativity
Introduction to the mathematics of general relativity
Nordström's theory of gravitation – Predecessor to the theory of relativity
Ricci calculus – Tensor index notation for tensor-based calculations
Timeline of gravitational physics and relativity
=
Einstein, Albert (2015). Relativity: The Special and the General Theory. Princeton University Press. ISBN 978-0-691-16633-9. Reprint of the original 1916 title, with commentary by Hanoch Gutfreund and Jürgen Renn.
Geroch, Robert (1981), General Relativity from A to B, University of Chicago Press, ISBN 978-0-226-28864-2.
Lieber, Lillian (2008), The Einstein Theory of Relativity: A Trip to the Fourth Dimension, Philadelphia: Paul Dry Books, Inc., ISBN 978-1-58988-044-3
Schutz, Bernard F. (2001), "Gravitational radiation", in Murdin, Paul (ed.), Encyclopedia of Astronomy and Astrophysics, Institute of Physics Pub., ISBN 978-1-56159-268-5
Thorne, Kip (1994). Black Holes and Time Warps: Einstein's Outrageous Legacy. New York: W. W. Norton & Company. ISBN 978-0-393-31276-8. Foreword by Stephen Hawking.
Wald, Robert M. (1992), Space, Time, and Gravity: the Theory of the Big Bang and Black Holes, Chicago: University of Chicago Press, ISBN 978-0-226-87029-8.
Wheeler, John; Ford, Kenneth (1998), Geons, Black Holes, & Quantum Foam: a life in physics, New York: W. W. Norton, ISBN 978-0-393-31991-0
=
Yvonne Choquet-Bruhat (2014). Introduction to General Relativity, Black Holes, and Cosmology. Oxford University Press. ISBN 978-0-19-193650-0.
Taylor, Edwin F.; Wheeler, John A. (2000). Exploring Black Holes: Introduction to General Relativity. Addison Wesley-Longman. ISBN 978-0-201-38423-9.
=
Cheng, Ta-Pei (2004). Relativity, Gravitation, and Cosmology: A Basic Introduction. Oxford University Press. ISBN 978-0-198-52957-6.
Crowell, Ben (2020). General Relativity.
Dirac, Paul (1975). General Theory of Relativity. Princeton University Press. ISBN 978-0-691-01146-2.
Gron, O.; Hervik, S. (2007), Einstein's General theory of Relativity, Springer, ISBN 978-0-387-69199-2.
Hartle, James B. (2003), Gravity: an Introduction to Einstein's General Relativity, San Francisco: Addison-Wesley, ISBN 978-0-8053-8662-2.
Hughston, L. P.; Tod, K. P. (1991), Introduction to General Relativity, Cambridge: Cambridge University Press, ISBN 978-0-521-33943-8
d'Inverno, Ray (1992), Introducing Einstein's Relativity, Oxford: Oxford University Press, ISBN 978-0-19-859686-8.
Ludyk, Günter (2013). Einstein in Matrix Form (1st ed.). Berlin: Springer. ISBN 978-3-642-35797-8.
Møller, Christian (1955) [1952], The Theory of Relativity, Oxford University Press, OCLC 7644624.
Moore, Thomas A (2012), A General Relativity Workbook, University Science Books, ISBN 978-1-891389-82-5.
Schutz, Bernard F. (2009). A First Course in General Relativity (2nd ed.). Cambridge University Press. ISBN 978-0-521-88705-2..
Zee, Anthony (2013). Einstein Gravity in a Nutshell. Princeton University Press. ISBN 978-0-691-14558-7.
=
Carroll, Sean M. (2003). Spacetime and Geometry: An Introduction to General Relativity. Addison-Wesley. ISBN 978-0-8053-8732-2.. Reprinted 2019 by Cambridge University Press, ISBN 978-1-108-48839-6.
Grøn, Øyvind; Hervik, Sigbjørn (2007), Einstein's General Theory of Relativity, New York: Springer, ISBN 978-0-387-69199-2
Landau, Lev D.; Lifshitz, Evgeny F. (1980), Course of Theoretical Physics Volume 2: The Classical Theory of Fields (4th ed.), London: Butterworth-Heinemann, ISBN 978-0-7506-2768-9.
Landsman, Klaas (2021). Foundations of General Relativity: From Einstein to Black Holes. Radboud University Press. ISBN 978-90-831789-2-9.
Stephani, Hans (1990), General Relativity: An Introduction to the Theory of the Gravitational Field, Cambridge: Cambridge University Press, Bibcode:1990grit.book.....S, ISBN 978-0-521-37941-0.
Misner, Charles; Thorne, Kip; Wheeler, John (1973). Gravitation. W. H. Freeman. ISBN 978-0-716-70344-0. Reprinted 2017 by Princeton University Press, ISBN 978-0-691-17779-3.
Sachs, Rainer K.; Hu, Hung-hsi (1983). General Relativity for Mathematicians. Springer-Verlag. ISBN 978-0387902180.
Wald, Robert M. (1984). General Relativity. Chicago: University of Chicago Press. ISBN 0-226-87032-4. OCLC 10018614.
Weinberg, Steven (1972). Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity. Wiley. ISBN 978-0-471-92567-5.
=
Baumann, Daniel (2022). Cosmology. Cambridge University Press. ISBN 978-1-108-83807-8.
Chandrasekhar, Subrahmanyan (1983). The Mathematical Theory of Black Holes. Oxford University Press. ISBN 978-0-198-51291-2.
Hawking, Stephen; Ellis, George (1975). The Large Scale Structure of Space-time. Cambridge University Press. ISBN 978-0-521-09906-6.
Poisson, Eric (2007). A Relativist's Toolkit: The Mathematics of Black-Hole Mechanics. Cambridge University Press. ISBN 978-0-521-53780-3.
Stephani, Hans; Kramer, Dietrich; MacCallum, Malcolm; Hoenselaers, Cornelius; Herlt, Eduard (2003). Exact Solutions of Einstein's Field Equations (2nd ed.). Cambridge University Press. ISBN 978-0-521-46702-5.
=
Einstein, Albert (1916), "Die Grundlage der allgemeinen Relativitätstheorie", Annalen der Physik, 49 (7): 769–822, Bibcode:1916AnP...354..769E, doi:10.1002/andp.19163540702 See also English translation at Einstein Papers Project
Flanagan, Éanna É.; Hughes, Scott A. (2005), "The basics of gravitational wave theory", New J. Phys., 7 (1): 204, arXiv:gr-qc/0501041, Bibcode:2005NJPh....7..204F, doi:10.1088/1367-2630/7/1/204
Landgraf, M.; Hechler, M.; Kemble, S. (2005), "Mission design for LISA Pathfinder", Class. Quantum Grav., 22 (10): S487–S492, arXiv:gr-qc/0411071, Bibcode:2005CQGra..22S.487L, doi:10.1088/0264-9381/22/10/048, S2CID 119476595
Nieto, Michael Martin (2006), "The quest to understand the Pioneer anomaly" (PDF), Europhysics News, 37 (6): 30–34, arXiv:gr-qc/0702017, Bibcode:2006ENews..37f..30N, doi:10.1051/epn:2006604, archived (PDF) from the original on 24 September 2015
Shapiro, I. I.; Pettengill, Gordon; Ash, Michael; Stone, Melvin; Smith, William; Ingalls, Richard; Brockelman, Richard (1968), "Fourth test of general relativity: preliminary results", Phys. Rev. Lett., 20 (22): 1265–1269, Bibcode:1968PhRvL..20.1265S, doi:10.1103/PhysRevLett.20.1265
Valtonen, M. J.; Lehto, H. J.; Nilsson, K.; Heidt, J.; Takalo, L. O.; Sillanpää, A.; Villforth, C.; Kidger, M.; et al. (2008), "A massive binary black-hole system in OJ 287 and a test of general relativity", Nature, 452 (7189): 851–853, arXiv:0809.1280, Bibcode:2008Natur.452..851V, doi:10.1038/nature06896, PMID 18421348, S2CID 4412396
Einstein Online Archived 1 June 2014 at the Wayback Machine – Articles on a variety of aspects of relativistic physics for a general audience; hosted by the Max Planck Institute for Gravitational Physics
GEO600 home page, the official website of the GEO600 project.
LIGO Laboratory
NCSA Spacetime Wrinkles – produced by the numerical relativity group at the NCSA, with an elementary introduction to general relativity
Einstein's General Theory of Relativity on YouTube (lecture by Leonard Susskind recorded 22 September 2008 at Stanford University).
Series of lectures on General Relativity given in 2006 at the Institut Henri Poincaré (introductory/advanced).
General Relativity Tutorials by John Baez.
Brown, Kevin. "Reflections on relativity". Mathpages.com. Archived from the original on 18 December 2015. Retrieved 29 May 2005.
Carroll, Sean M. (1997). "Lecture Notes on General Relativity". arXiv:gr-qc/9712019.
Moor, Rafi. "Understanding General Relativity". Retrieved 11 July 2006.
Waner, Stefan. "Introduction to Differential Geometry and General Relativity". Retrieved 5 April 2015.
The Feynman Lectures on Physics Vol. II Ch. 42: Curved Space

--- TOPIC: Thermodynamics ---
Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, entropy, and the physical properties of matter and radiation. The behavior of these quantities is governed by the four laws of thermodynamics, which convey a quantitative description using measurable macroscopic physical quantities but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to various topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering, and mechanical engineering, as well as other complex fields such as meteorology.
Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars. Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, "Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency."  German physicist and mathematician Rudolf Clausius restated Carnot's principle known as the Carnot cycle and gave the theory of heat a truer and sounder basis. His most important paper, "On the Moving Force of Heat", published in 1850, first stated the second law of thermodynamics. In 1865 he introduced the concept of entropy. In 1870 he introduced the virial theorem, which applied to heat.
The initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.
A description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. The first law specifies that energy can be transferred between physical systems as heat, as work, and with the transfer of matter. The second law defines the existence of a quantity called entropy, which describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and which can be used to quantify the useful work that can be extracted from the system.
In thermodynamics, interactions between large ensembles of objects are studied and categorized. Central to this are the concepts of the thermodynamic system and its surroundings. A system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. Properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes.
With these tools, thermodynamics can be used to describe how systems respond to changes in their environment. This can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. The results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.
This article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. Non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.
The history of thermodynamics as a scientific discipline generally begins with Otto von Guericke who, in 1650, built and designed the world's first vacuum pump and demonstrated a vacuum using his Magdeburg hemispheres. Guericke was driven to make a vacuum in order to disprove Aristotle's long-held supposition that 'nature abhors a vacuum'. Shortly after Guericke, the Anglo-Irish physicist and chemist Robert Boyle had learned of Guericke's designs and, in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed a correlation between pressure, temperature, and volume. In time, Boyle's Law was formulated, which states that pressure and volume are inversely proportional. Then, in 1679, based on these concepts, an associate of Boyle's named Denis Papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated.
Later designs implemented a steam release valve that kept the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and a cylinder engine. He did not, however, follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine, followed by Thomas Newcomen in 1712. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time.
The fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by Professor Joseph Black at the University of Glasgow, where James Watt was employed as an instrument maker. Black and Watt performed experiments together, but it was Watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. Drawing on all the previous work led Sadi Carnot, the "father of thermodynamics", to publish Reflections on the Motive Power of Fire (1824), a discourse on heat, power, energy and engine efficiency. The book outlined the basic energetic relations between the Carnot engine, the Carnot cycle, and motive power. It marked the start of thermodynamics as a modern science.
The first thermodynamic textbook was written in 1859 by William Rankine, originally trained as a physicist and a civil and mechanical engineering professor at the University of Glasgow. The first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of William Rankine, Rudolf Clausius, and William Thomson (Lord Kelvin).
The foundations of statistical thermodynamics were set out by physicists such as James Clerk Maxwell, Ludwig Boltzmann, Max Planck, Rudolf Clausius and J. Willard Gibbs.
Clausius, who first stated the basic ideas of the second law in his paper "On the Moving Force of Heat", published in 1850, and is called "one of the founding fathers of thermodynamics", introduced the concept of entropy in 1865.
During the years 1873–76 the American mathematical physicist Josiah Willard Gibbs published a series of three papers, the most famous being On the Equilibrium of Heterogeneous Substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. Also Pierre Duhem in the 19th century wrote about chemical thermodynamics. During the early 20th century, chemists such as Gilbert N. Lewis, Merle Randall, and E. A. Guggenheim applied the mathematical methods of Gibbs to the analysis of chemical processes.
Thermodynamics has an intricate etymology.
By a surface-level analysis, the word consists of two parts that can be traced back to Ancient Greek. Firstly, thermo- ("of heat"; used in words such as thermometer) can be traced back to the root θέρμη therme, meaning "heat". Secondly, the word dynamics ("science of force [or power]") can be traced back to the root δύναμις dynamis, meaning "power".
In 1849, the adjective thermo-dynamic is used by William Thomson.
In 1854, the noun thermo-dynamics is used by Thomson and William Rankine to represent the science of generalized heat engines.
Pierre Perrot claims that the term thermodynamics was coined by James Joule in 1858 to designate the science of relations between heat and power, however, Joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to Thomson's 1849 phraseology.
The study of thermodynamic systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems.
=
Classical thermodynamics is the description of the states of thermodynamic systems at near-equilibrium, that uses macroscopic, measurable properties. It is used to model exchanges of energy, work and heat based on the laws of thermodynamics. The qualifier classical reflects the fact that it represents the first level of understanding of the subject as it developed in the 19th century and describes the changes of a system in terms of macroscopic empirical (large scale, and measurable) parameters. A microscopic interpretation of these concepts was later provided by the development of statistical mechanics.
=
Statistical mechanics, also known as statistical thermodynamics, emerged with the development of atomic and molecular theories in the late 19th century and early 20th century, and supplemented classical thermodynamics with an interpretation of the microscopic interactions between individual particles or quantum-mechanical states. This field relates the microscopic properties of individual atoms and molecules to the macroscopic, bulk properties of materials that can be observed on the human scale, thereby explaining classical thermodynamics as a natural result of statistics, classical mechanics, and quantum theory at the microscopic level.
=
Chemical thermodynamics is the study of the interrelation of energy with chemical reactions or with a physical change of state within the confines of the laws of thermodynamics. The primary objective of chemical thermodynamics is to determine the spontaneity of a given transformation.
=
Equilibrium thermodynamics is the study of transfers of matter and energy in systems or bodies that, by agencies in their surroundings, can be driven from one state of thermodynamic equilibrium to another. The term 'thermodynamic equilibrium' indicates a state of balance, in which all macroscopic flows are zero; in the case of the simplest systems or bodies, their intensive properties are homogeneous, and their pressures are perpendicular to their boundaries. In an equilibrium state there are no unbalanced potentials, or driving forces, between macroscopically distinct parts of the system. A central aim in equilibrium thermodynamics is: given a system in a well-defined initial equilibrium state, and given its surroundings, and given its constitutive walls, to calculate what will be the final equilibrium state of the system after a specified thermodynamic operation has changed its walls or surroundings.
=
Non-equilibrium thermodynamics is a branch of thermodynamics that deals with systems that are not in thermodynamic equilibrium. Most systems found in nature are not in thermodynamic equilibrium because they are not in stationary states, and are continuously and discontinuously subject to flux of matter and energy to and from other systems. The thermodynamic study of non-equilibrium systems requires more general concepts than are dealt with by equilibrium thermodynamics. Many natural systems still today remain beyond the scope of currently known macroscopic thermodynamic methods.
Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following.
=
The zeroth law of thermodynamics states: If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.
This statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration. Systems are said to be in equilibrium if the small, random exchanges between them (e.g. Brownian motion) do not lead to a net change in energy. This law is tacitly assumed in every measurement of temperature. Thus, if one seeks to decide whether two bodies are at the same temperature, it is not necessary to bring them into contact and measure any changes of their observable properties in time. The law provides an empirical definition of temperature, and justification for the construction of practical thermometers.
The zeroth law was not initially recognized as a separate law of thermodynamics, as its basis in thermodynamical equilibrium was implied in the other laws. The first, second, and third laws had been explicitly stated already, and found common acceptance in the physics community before the importance of the zeroth law for the definition of temperature was realized. As it was impractical to renumber the other laws, it was named the zeroth law.
=
The first law of thermodynamics states: In a process without transfer of matter, the change in internal energy, 
  
    
      
        Δ
        U
      
    
    {\displaystyle \Delta U}
  
, of a thermodynamic system is equal to the energy gained as heat, 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
, less the thermodynamic work, 
  
    
      
        W
      
    
    {\displaystyle W}
  
, done by the system on its surroundings.
  
    
      
        Δ
        U
        =
        Q
        −
        W
      
    
    {\displaystyle \Delta U=Q-W}
  
.
where 
  
    
      
        Δ
        U
      
    
    {\displaystyle \Delta U}
  
 denotes the change in the internal energy of a closed system (for which heat or work through the system boundary are possible, but matter transfer is not possible), 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 denotes the quantity of energy supplied to the system as heat, and 
  
    
      
        W
      
    
    {\displaystyle W}
  
 denotes the amount of thermodynamic work done by the system on its surroundings. An equivalent statement is that perpetual motion machines of the first kind are impossible; work 
  
    
      
        W
      
    
    {\displaystyle W}
  
 done by a system on its surrounding requires that the system's internal energy 
  
    
      
        U
      
    
    {\displaystyle U}
  
 decrease or be consumed, so that the amount of internal energy lost by that work must be resupplied as heat 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 by an external energy source or as work by an external machine acting on the system (so that 
  
    
      
        U
      
    
    {\displaystyle U}
  
 is recovered) to make the system work continuously.
For processes that include transfer of matter, a further statement is needed: With due account of the respective fiducial reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then
  
    
      
        
          U
          
            0
          
        
        =
        
          U
          
            1
          
        
        +
        
          U
          
            2
          
        
      
    
    {\displaystyle U_{0}=U_{1}+U_{2}}
  
,
where U0 denotes the internal energy of the combined system, and U1 and U2 denote the internal energies of the respective separated systems.
Adapted for thermodynamics, this law is an expression of the principle of conservation of energy, which states that energy can be transformed (changed from one form to another), but cannot be created or destroyed.
Internal energy is a principal property of the thermodynamic state, while heat and work are modes of energy transfer by which a process may change this state. A change of internal energy of a system may be achieved by any combination of heat added or removed and work performed on or by the system. As a function of state, the internal energy does not depend on the manner, or on the path through intermediate steps, by which the system arrived at its state.
=
A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter body.
The second law refers to a system of matter and radiation, initially with inhomogeneities in temperature, pressure, chemical potential, and other intensive properties, that are due to internal 'constraints', or impermeable rigid walls, within it, or to externally imposed forces. The law observes that, when the system is isolated from the outside world and from those forces, there is a definite thermodynamic quantity, its entropy, that increases as the constraints are removed, eventually reaching a maximum value at thermodynamic equilibrium, when the inhomogeneities practically vanish. For systems that are initially far from thermodynamic equilibrium, though several have been proposed, there is known no general physical principle that determines the rates of approach to thermodynamic equilibrium, and thermodynamics does not deal with such rates. The many versions of the second law all express the general irreversibility of the transitions involved in systems approaching thermodynamic equilibrium.
In macroscopic thermodynamics, the second law is a basic observation applicable to any actual thermodynamic process; in statistical thermodynamics, the second law is postulated to be a consequence of molecular chaos.
=
The third law of thermodynamics states: As the temperature of a system approaches absolute zero, all processes cease and the entropy of the system approaches a minimum value.
This law of thermodynamics is a statistical law of nature regarding entropy and the impossibility of reaching absolute zero of temperature. This law provides an absolute reference point for the determination of entropy. The entropy determined relative to this point is the absolute entropy. Alternative definitions include "the entropy of all systems and of all states of a system is smallest at absolute zero," or equivalently "it is impossible to reach the absolute zero of temperature by any finite number of processes".
Absolute zero, at which all activity would stop if it were possible to achieve, is −273.15 °C (degrees Celsius), or −459.67 °F (degrees Fahrenheit), or 0 K (kelvin), or 0° R (degrees Rankine).
An important concept in thermodynamics is the thermodynamic system, which is a precisely defined region of the universe under study. Everything in the universe except the system is called the surroundings. A system is separated from the remainder of the universe by a boundary which may be a physical or notional, but serve to confine the system to a finite volume. Segments of the boundary are often described as walls; they have respective defined 'permeabilities'. Transfers of energy as work, or as heat, or of matter, between the system and the surroundings, take place through the walls, according to their respective permeabilities.
Matter or energy that pass across the boundary so as to effect a change in the internal energy of the system need to be accounted for in the energy balance equation. The volume contained by the walls can be the region surrounding a single atom resonating energy, such as Max Planck defined in 1900; it can be a body of steam or air in a steam engine, such as Sadi Carnot defined in 1824. The system could also be just one nuclide (i.e. a system of quarks) as hypothesized in quantum thermodynamics. When a looser viewpoint is adopted, and the requirement of thermodynamic equilibrium is dropped, the system can be the body of a tropical cyclone, such as Kerry Emanuel theorized in 1986 in the field of atmospheric thermodynamics, or the event horizon of a black hole.
Boundaries are of four types: fixed, movable, real, and imaginary. For example, in an engine, a fixed boundary means the piston is locked at its position, within which a constant volume process might occur. If the piston is allowed to move that boundary is movable while the cylinder and cylinder head boundaries are fixed. For closed systems, boundaries are real while for open systems boundaries are often imaginary. In the case of a jet engine, a fixed imaginary boundary might be assumed at the intake of the engine, fixed boundaries along the surface of the case and a second fixed imaginary boundary across the exhaust nozzle.
Generally, thermodynamics distinguishes three classes of systems, defined in terms of what is allowed to cross their boundaries:
As time passes in an isolated system, internal differences of pressures, densities, and temperatures tend to even out. A system in which all equalizing processes have gone to completion is said to be in a state of thermodynamic equilibrium.
Once in thermodynamic equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to be reversible processes.
When a system is at equilibrium under a given set of conditions, it is said to be in a definite thermodynamic state. The state of the system can be described by a number of state quantities that do not depend on the process by which the system arrived at its state. They are called intensive variables or extensive variables according to how they change when the size of the system changes. The properties of the system can be described by an equation of state which specifies the relationship between these variables. State may be thought of as the instantaneous quantitative description of a system with a set number of variables held constant.
A thermodynamic process may be defined as the energetic evolution of a thermodynamic system proceeding from an initial state to a final state. It can be described by process quantities. Typically, each thermodynamic process is distinguished from other processes in energetic character according to what parameters, such as temperature, pressure, or volume, etc., are held fixed; Furthermore, it is useful to group these processes into pairs, in which each variable held constant is one member of a conjugate pair.
Several commonly studied thermodynamic processes are:
Adiabatic process: occurs without loss or gain of energy by heat
Isenthalpic process: occurs at a constant enthalpy
Isentropic process: a reversible adiabatic process, occurs at a constant entropy
Isobaric process: occurs at constant pressure
Isochoric process: occurs at constant volume (also called isometric/isovolumetric)
Isothermal process: occurs at a constant temperature
Steady-state process: occurs without a change in the internal energy
There are two types of thermodynamic instruments, the meter and the reservoir. A thermodynamic meter is any device which measures any parameter of a thermodynamic system. In some cases, the thermodynamic parameter is actually defined in terms of an idealized measuring instrument. For example, the zeroth law states that if two bodies are in thermal equilibrium with a third body, they are also in thermal equilibrium with each other. This principle, as noted by James Maxwell in 1872, asserts that it is possible to measure temperature. An idealized thermometer is a sample of an ideal gas at constant pressure. From the ideal gas law pV=nRT, the volume of such a sample can be used as an indicator of temperature; in this manner it defines temperature. Although pressure is defined mechanically, a pressure-measuring device, called a barometer may also be constructed from a sample of an ideal gas held at a constant temperature. A calorimeter is a device which is used to measure and define the internal energy of a system.
A thermodynamic reservoir is a system which is so large that its state parameters are not appreciably altered when it is brought into contact with the system of interest. When the reservoir is brought into contact with the system, the system is brought into equilibrium with the reservoir. For example, a pressure reservoir is a system at a particular pressure, which imposes that pressure upon the system to which it is mechanically connected. The Earth's atmosphere is often used as a pressure reservoir. The ocean can act as temperature reservoir when used to cool power plants.
The central concept of thermodynamics is that of energy, the ability to do work. By the First Law, the total energy of a system and its surroundings is conserved. Energy may be transferred into a system by heating, compression, or addition of matter, and extracted from a system by cooling, expansion, or extraction of matter. In mechanics, for example, energy transfer equals the product of the force applied to a body and the resulting displacement.
Conjugate variables are pairs of thermodynamic concepts, with the first being akin to a "force" applied to some thermodynamic system, the second being akin to the resulting "displacement", and the product of the two equaling the amount of energy transferred. The common conjugate variables are:
Pressure–volume (the mechanical parameters);
Temperature–entropy (thermal parameters);
Chemical potential–particle number (material parameters).
Thermodynamic potentials are different quantitative measures of the stored energy in a system. Potentials are used to measure the energy changes in systems as they evolve from an initial state to a final state. The potential used depends on the constraints of the system, such as constant temperature or pressure. For example, the Helmholtz and Gibbs energies are the energies available in a system to do useful work when the temperature and volume or the pressure and temperature are fixed, respectively. Thermodynamic potentials cannot be measured in laboratories, but can be computed using molecular thermodynamics.
The five most well known potentials are:
where 
  
    
      
        T
      
    
    {\displaystyle T}
  
 is the temperature, 
  
    
      
        S
      
    
    {\displaystyle S}
  
 the entropy, 
  
    
      
        p
      
    
    {\displaystyle p}
  
 the pressure, 
  
    
      
        V
      
    
    {\displaystyle V}
  
 the volume, 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 the chemical potential, 
  
    
      
        N
      
    
    {\displaystyle N}
  
 the number of particles in the system, and 
  
    
      
        i
      
    
    {\displaystyle i}
  
 is the count of particles types in the system.
Thermodynamic potentials can be derived from the energy balance equation applied to a thermodynamic system. Other thermodynamic potentials can also be obtained through Legendre transformation.
Axiomatic thermodynamics is a mathematical discipline that aims to describe thermodynamics in terms of rigorous axioms, for example by finding a mathematically rigorous way to express the familiar laws of thermodynamics.
The first attempt at an axiomatic theory of thermodynamics was Constantin Carathéodory's 1909 work Investigations on the Foundations of Thermodynamics, which made use of Pfaffian systems and the concept of adiabatic accessibility, a notion that was introduced by Carathéodory himself. In this formulation, thermodynamic concepts such as heat, entropy, and temperature are derived from quantities that are more directly measurable. Theories that came after, differed in the sense that they made assumptions regarding thermodynamic processes with arbitrary initial and final states, as opposed to considering only neighboring states.
Thermodynamic process path
=
List of important publications in thermodynamics
List of textbooks on thermodynamics and statistical mechanics
List of thermal conductivities
List of thermodynamic properties
Table of thermodynamic equations
Timeline of thermodynamics
Thermodynamic equations
Goldstein, Martin & Inge F. (1993). The Refrigerator and the Universe. Harvard University Press. ISBN 978-0-674-75325-9. OCLC 32826343. A nontechnical introduction, good on historical and interpretive matters.
Kazakov, Andrei; Muzny, Chris D.; Chirico, Robert D.; Diky, Vladimir V.; Frenkel, Michael (2008). "Web Thermo Tables – an On-Line Version of the TRC Thermodynamic Tables". Journal of Research of the National Institute of Standards and Technology. 113 (4): 209–220. doi:10.6028/jres.113.016. ISSN 1044-677X. PMC 4651616. PMID 27096122.
Gibbs J.W. (1928). The Collected Works of J. Willard Gibbs Thermodynamics. New York: Longmans, Green and Co. Vol. 1, pp. 55–349.
Guggenheim E.A. (1933). Modern thermodynamics by the methods of Willard Gibbs. London: Methuen & co. ltd.
Denbigh K. (1981). The Principles of Chemical Equilibrium: With Applications in Chemistry and Chemical Engineering. London: Cambridge University Press.
Stull, D.R., Westrum Jr., E.F. and Sinke, G.C. (1969). The Chemical Thermodynamics of Organic Compounds. London: John Wiley and Sons, Inc.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Bazarov I.P. (2010). Thermodynamics: Textbook. St. Petersburg: Lan publishing house. p. 384. ISBN 978-5-8114-1003-3. 5th ed. (in Russian)
Bawendi Moungi G., Alberty Robert A. and Silbey Robert J. (2004). Physical Chemistry. J. Wiley & Sons, Incorporated.
Alberty Robert A. (2003). Thermodynamics of Biochemical Reactions. Wiley-Interscience.
Alberty Robert A. (2006). Biochemical Thermodynamics: Applications of Mathematica. Vol. 48. John Wiley & Sons, Inc. pp. 1–458. ISBN 978-0-471-75798-6. PMID 16878778. {{cite book}}: |journal= ignored (help)
Dill Ken A., Bromberg Sarina (2011). Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience. Garland Science. ISBN 978-0-8153-4430-8.
M. Scott Shell (2015). Thermodynamics and Statistical Mechanics: An Integrated Approach. Cambridge University Press. ISBN 978-1107656789.
Douglas E. Barrick (2018). Biomolecular Thermodynamics: From Theory to Applications. CRC Press. ISBN 978-1-4398-0019-5.
The following titles are more technical:
Bejan, Adrian (2016). Advanced Engineering Thermodynamics (4 ed.). Wiley. ISBN 978-1-119-05209-8.
Cengel, Yunus A., & Boles, Michael A. (2002). Thermodynamics – an Engineering Approach. McGraw Hill. ISBN 978-0-07-238332-4. OCLC 45791449.{{cite book}}:  CS1 maint: multiple names: authors list (link)
Dunning-Davies, Jeremy (1997). Concise Thermodynamics: Principles and Applications. Horwood Publishing. ISBN 978-1-8985-6315-0. OCLC 36025958.
Kroemer, Herbert & Kittel, Charles (1980). Thermal Physics. W.H. Freeman Company. ISBN 978-0-7167-1088-2. OCLC 32932988.
 Media related to Thermodynamics at Wikimedia Commons
Callendar, Hugh Longbourne (1911). "Thermodynamics" . Encyclopædia Britannica. Vol. 26 (11th ed.). pp. 808–814.
Thermodynamics Data & Property Calculation Websites Archived 11 February 2014 at the Wayback Machine
Thermodynamics Educational Websites Archived 14 June 2015 at the Wayback Machine
Biochemistry Thermodynamics
Thermodynamics and Statistical Mechanics
Engineering Thermodynamics – A Graphical Approach
Thermodynamics and Statistical Mechanics by Richard Fitzpatrick

--- TOPIC: Biology ---
Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis).
Biology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science.
Life on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss.
From Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'. The compound appears in the title of Volume 3 of Michael Christoph Hanow's Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia, published in 1766. The term biology in its modern sense appears to have been introduced independently by Thomas Beddoes (in 1799), Karl Friedrich Burdach (in 1800), Gottfried Reinhold Treviranus (Biologie oder Philosophie der lebenden Natur, 1802) and Jean-Baptiste Lamarck (Hydrogéologie, 1802).
The earliest of roots of science, which included medicine, can be traced to ancient Egypt and Mesopotamia in around 3000 to 1200 BCE. Their contributions shaped ancient Greek natural philosophy. Ancient Greek philosophers such as Aristotle (384–322 BCE) contributed extensively to the development of biological knowledge. He explored biological causation and the diversity of life. His successor, Theophrastus, began the scientific study of plants. Scholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought.
Biology began to quickly develop with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop techniques of microscopic dissection and staining. Advances in microscopy had a profound impact on biological thinking. In the early 19th century, biologists pointed to the central importance of the cell. In 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells, continuing to support spontaneous generation. However, Robert Remak and Rudolf Virchow were able to reify the third tenet, and by the 1860s most biologists accepted all three tenets which consolidated into cell theory.
Meanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735, and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent.
Serious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who presented a coherent theory of evolution. The British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions.
The basis for modern genetics began with the work of Gregor Mendel in 1865. This outlined the principles of biological inheritance. However, the significance of his work was not realized until the early 20th century when evolution became a unified theory as the modern synthesis reconciled Darwinian evolution with classical genetics. In the 1940s and early 1950s, a series of experiments by Alfred Hershey and Martha Chase pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double-helical structure of DNA by James Watson and Francis Crick in 1953, marked the transition to the era of molecular genetics. From the 1950s onwards, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. The Human Genome Project was launched in 1990 to map the human genome.
=
All organisms are made up of chemical elements; oxygen, carbon, hydrogen, and nitrogen account for most (96%) of the mass of all organisms, with calcium, phosphorus, sulfur, sodium, chlorine, and magnesium constituting essentially all the remainder. Different elements can combine to form compounds such as water, which is fundamental to life. Biochemistry is the study of chemical processes within and relating to living organisms. Molecular biology is the branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including molecular synthesis, modification, mechanisms, and interactions.
=
Life arose from the Earth's first ocean, which formed some 3.8 billion years ago. Since then, water continues to be the most abundant molecule in every organism. Water is important to life because it is an effective solvent, capable of dissolving solutes such as sodium and chloride ions or other small molecules to form an aqueous solution. Once dissolved in water, these solutes are more likely to come in contact with one another and therefore take part in chemical reactions that sustain life. 
In terms of its molecular structure, water is a small polar molecule with a bent shape formed by the polar covalent bonds of two hydrogen (H) atoms to one oxygen (O) atom (H2O). Because the O–H bonds are polar, the oxygen atom has a slight negative charge and the two hydrogen atoms have a slight positive charge. This polar property of water allows it to attract other water molecules via hydrogen bonds, which makes water cohesive. Surface tension results from the cohesive force due to the attraction between molecules at the surface of the liquid. 
Water is also adhesive as it is able to adhere to the surface of any polar or charged non-water molecules. Water is denser as a liquid than it is as a solid (or ice). This unique property of water allows ice to float above liquid water such as ponds, lakes, and oceans, thereby insulating the liquid below from the cold air above. Water has the capacity to absorb energy, giving it a higher specific heat capacity than other solvents such as ethanol. Thus, a large amount of energy is needed to break the hydrogen bonds between water molecules to convert liquid water into water vapor. 
As a molecule, water is not completely stable as each water molecule continuously dissociates into hydrogen and hydroxyl ions before reforming into a water molecule again. In pure water, the number of hydrogen ions balances (or equals) the number of hydroxyl ions, resulting in a pH that is neutral.
=
Organic compounds are molecules that contain carbon bonded to another element such as hydrogen. With the exception of water, nearly all the molecules that make up each organism contain carbon. Carbon can form covalent bonds with up to four other atoms, enabling it to form diverse, large, and complex molecules. For example, a single carbon atom can form four single covalent bonds such as in methane, two double covalent bonds such as in carbon dioxide (CO2), or a triple covalent bond such as in carbon monoxide (CO). Moreover, carbon can form very long chains of interconnecting carbon–carbon bonds such as octane or ring-like structures such as glucose.
The simplest form of an organic molecule is the hydrocarbon, which is a large family of organic compounds that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other elements such as oxygen (O), hydrogen (H), phosphorus (P), and sulfur (S), which can change the chemical behavior of that compound. Groups of atoms that contain these elements (O-, H-, P-, and S-) and are bonded to a central carbon atom or skeleton are called functional groups. There are six prominent functional groups that can be found in organisms: amino group, carboxyl group, carbonyl group, hydroxyl group, phosphate group, and sulfhydryl group.
In 1953, the Miller–Urey experiment showed that organic compounds could be synthesized abiotically within a closed system mimicking the conditions of early Earth, thus suggesting that complex organic molecules could have arisen spontaneously in early Earth (see abiogenesis).
=
Macromolecules are large molecules made up of smaller subunits or monomers. Monomers include sugars, amino acids, and nucleotides. Carbohydrates include monomers and polymers of sugars.
Lipids are the only class of macromolecules that are not made up of polymers. They include steroids, phospholipids, and fats, largely nonpolar and hydrophobic (water-repelling) substances.
Proteins are the most diverse of the macromolecules. They include enzymes, transport proteins, large signaling molecules, antibodies, and structural proteins. The basic unit (or monomer) of a protein is an amino acid. Twenty amino acids are used in proteins.
Nucleic acids are polymers of nucleotides. Their function is to store, transmit, and express hereditary information.
Cell theory states that cells are the fundamental units of life, that all living things are composed of one or more cells, and that all cells arise from preexisting cells through cell division. Most cells are very small, with diameters ranging from 1 to 100 micrometers and are therefore only visible under a light or electron microscope. There are generally two types of cells: eukaryotic cells, which contain a nucleus, and prokaryotic cells, which do not. Prokaryotes are single-celled organisms such as bacteria, whereas eukaryotes can be single-celled or multicellular. In multicellular organisms, every cell in the organism's body is derived ultimately from a single cell in a fertilized egg.
=
Every cell is enclosed within a cell membrane that separates its cytoplasm from the extracellular space. A cell membrane consists of a lipid bilayer, including cholesterols that sit between phospholipids to maintain their fluidity at various temperatures. Cell membranes are semipermeable, allowing small molecules such as oxygen, carbon dioxide, and water to pass through while restricting the movement of larger molecules and charged particles such as ions. Cell membranes also contain membrane proteins, including integral membrane proteins that go across the membrane serving as membrane transporters, and peripheral proteins that loosely attach to the outer side of the cell membrane, acting as enzymes shaping the cell. Cell membranes are involved in various cellular processes such as cell adhesion, storing electrical energy, and cell signalling and serve as the attachment surface for several extracellular structures such as a cell wall, glycocalyx, and cytoskeleton.
Within the cytoplasm of a cell, there are many biomolecules such as proteins and nucleic acids. In addition to biomolecules, eukaryotic cells have specialized structures called organelles that have their own lipid bilayers or are spatially units. These organelles include the cell nucleus, which contains most of the cell's DNA, or mitochondria, which generate adenosine triphosphate (ATP) to power cellular processes. Other organelles such as endoplasmic reticulum and Golgi apparatus play a role in the synthesis and packaging of proteins, respectively. Biomolecules such as proteins can be engulfed by lysosomes, another specialized organelle. Plant cells have additional organelles that distinguish them from animal cells such as a cell wall that provides support for the plant cell, chloroplasts that harvest sunlight energy to produce sugar, and vacuoles that provide storage and structural support as well as being involved in reproduction and breakdown of plant seeds. Eukaryotic cells also have cytoskeleton that is made up of microtubules, intermediate filaments, and microfilaments, all of which provide support for the cell and are involved in the movement of the cell and its organelles. In terms of their structural composition, the microtubules are made up of tubulin (e.g., α-tubulin and β-tubulin) whereas intermediate filaments are made up of fibrous proteins. Microfilaments are made up of actin molecules that interact with other strands of proteins.
=
All cells require energy to sustain cellular processes. Metabolism is the set of chemical reactions in an organism. The three main purposes of metabolism are: the conversion of food to energy to run cellular processes; the conversion of food/fuel to monomer building blocks; and the elimination of metabolic wastes. These enzyme-catalyzed reactions allow organisms to grow and reproduce, maintain their structures, and respond to their environments. Metabolic reactions may be categorized as catabolic—the breaking down of compounds (for example, the breaking down of glucose to pyruvate by cellular respiration); or anabolic—the building up (synthesis) of compounds (such as proteins, carbohydrates, lipids, and nucleic acids). Usually, catabolism releases energy, and anabolism consumes energy. The chemical reactions of metabolism are organized into metabolic pathways, in which one chemical is transformed through a series of steps into another chemical, each step being facilitated by a specific enzyme. Enzymes are crucial to metabolism because they allow organisms to drive desirable reactions that require energy that will not occur by themselves, by coupling them to spontaneous reactions that release energy. Enzymes act as catalysts—they allow a reaction to proceed more rapidly without being consumed by it—by reducing the amount of activation energy needed to convert reactants into products. Enzymes also allow the regulation of the rate of a metabolic reaction, for example in response to changes in the cell's environment or to signals from other cells.
=
Cellular respiration is a set of metabolic reactions and processes that take place in cells to convert chemical energy from nutrients into adenosine triphosphate (ATP), and then release waste products. The reactions involved in respiration are catabolic reactions, which break large molecules into smaller ones, releasing energy. Respiration is one of the key ways a cell releases chemical energy to fuel cellular activity. The overall reaction occurs in a series of biochemical steps, some of which are redox reactions. Although cellular respiration is technically a combustion reaction, it clearly does not resemble one when it occurs in a cell because of the slow, controlled release of energy from the series of reactions.
Sugar in the form of glucose is the main nutrient used by animal and plant cells in respiration. Cellular respiration involving oxygen is called aerobic respiration, which has four stages: glycolysis, citric acid cycle (or Krebs cycle), electron transport chain, and oxidative phosphorylation. Glycolysis is a metabolic process that occurs in the cytoplasm whereby glucose is converted into two pyruvates, with two net molecules of ATP being produced at the same time. Each pyruvate is then oxidized into acetyl-CoA by the pyruvate dehydrogenase complex, which also generates NADH and carbon dioxide. Acetyl-CoA enters the citric acid cycle, which takes places inside the mitochondrial matrix. At the end of the cycle, the total yield from 1 glucose (or 2 pyruvates) is 6 NADH, 2 FADH2, and 2 ATP molecules. Finally, the next stage is oxidative phosphorylation, which in eukaryotes, occurs in the mitochondrial cristae. Oxidative phosphorylation comprises the electron transport chain, which is a series of four protein complexes that transfer electrons from one complex to another, thereby releasing energy from NADH and FADH2 that is coupled to the pumping of protons (hydrogen ions) across the inner mitochondrial membrane (chemiosmosis), which generates a proton motive force. Energy from the proton motive force drives the enzyme ATP synthase to synthesize more ATPs by phosphorylating ADPs. The transfer of electrons terminates with molecular oxygen being the final electron acceptor.
If oxygen were not present, pyruvate would not be metabolized by cellular respiration but undergoes a process of fermentation. The pyruvate is not transported into the mitochondrion but remains in the cytoplasm, where it is converted to waste products that may be removed from the cell. This serves the purpose of oxidizing the electron carriers so that they can perform glycolysis again and removing the excess pyruvate. Fermentation oxidizes NADH to NAD+ so it can be re-used in glycolysis. In the absence of oxygen, fermentation prevents the buildup of NADH in the cytoplasm and provides NAD+ for glycolysis. This waste product varies depending on the organism. In skeletal muscles, the waste product is lactic acid. This type of fermentation is called lactic acid fermentation. In strenuous exercise, when energy demands exceed energy supply, the respiratory chain cannot process all of the hydrogen atoms joined by NADH. During anaerobic glycolysis, NAD+ regenerates when pairs of hydrogen combine with pyruvate to form lactate. Lactate formation is catalyzed by lactate dehydrogenase in a reversible reaction. Lactate can also be used as an indirect precursor for liver glycogen. During recovery, when oxygen becomes available, NAD+ attaches to hydrogen from lactate to form ATP. In yeast, the waste products are ethanol and carbon dioxide. This type of fermentation is known as alcoholic or ethanol fermentation. The ATP generated in this process is made by substrate-level phosphorylation, which does not require oxygen.
=
Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's metabolic activities via cellular respiration. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water. In most cases, oxygen is released as a waste product. Most plants, algae, and cyanobacteria perform photosynthesis, which is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies most of the energy necessary for life on Earth.
Photosynthesis has four stages: Light absorption, electron transport, ATP synthesis, and carbon fixation. Light absorption is the initial step of photosynthesis whereby light energy is absorbed by chlorophyll pigments attached to proteins in the thylakoid membranes. The absorbed light energy is used to remove electrons from a donor (water) to a primary electron acceptor, a quinone designated as Q. In the second stage, electrons move from the quinone primary electron acceptor through a series of electron carriers until they reach a final electron acceptor, which is usually the oxidized form of NADP+, which is reduced to NADPH, a process that takes place in a protein complex called photosystem I (PSI). The transport of electrons is coupled to the movement of protons (or hydrogen) from the stroma to the thylakoid membrane, which forms a pH gradient across the membrane as hydrogen becomes more concentrated in the lumen than in the stroma. This is analogous to the proton-motive force generated across the inner mitochondrial membrane in aerobic respiration.
During the third stage of photosynthesis, the movement of protons down their concentration gradients from the thylakoid lumen to the stroma through the ATP synthase is coupled to the synthesis of ATP by that same ATP synthase. The NADPH and ATPs generated by the light-dependent reactions in the second and third stages, respectively, provide the energy and electrons to drive the synthesis of glucose by fixing atmospheric carbon dioxide into existing organic carbon compounds, such as ribulose bisphosphate (RuBP) in a sequence of light-independent (or dark) reactions called the Calvin cycle.
=
Cell signaling (or communication) is the ability of cells to receive, process, and transmit signals with its environment and with itself. Signals can be non-chemical such as light, electrical impulses, and heat, or chemical signals (or ligands) that interact with receptors, which can be found embedded in the cell membrane of another cell or located deep inside a cell. There are generally four types of chemical signals: autocrine, paracrine, juxtacrine, and hormones. In autocrine signaling, the ligand affects the same cell that releases it. Tumor cells, for example, can reproduce uncontrollably because they release signals that initiate their own self-division. In paracrine signaling, the ligand diffuses to nearby cells and affects them. For example, brain cells called neurons release ligands called neurotransmitters that diffuse across a synaptic cleft to bind with a receptor on an adjacent cell such as another neuron or muscle cell. In juxtacrine signaling, there is direct contact between the signaling and responding cells. Finally, hormones are ligands that travel through the circulatory systems of animals or vascular systems of plants to reach their target cells. Once a ligand binds with a receptor, it can influence the behavior of another cell, depending on the type of receptor. For instance, neurotransmitters that bind with an ionotropic receptor can alter the excitability of a target cell. Other types of receptors include protein kinase receptors (e.g., receptor for the hormone insulin) and G protein-coupled receptors. Activation of G protein-coupled receptors can initiate second messenger cascades. The process by which a chemical or physical signal is transmitted through a cell as a series of molecular events is called signal transduction.
=
The cell cycle is a series of events that take place in a cell that cause it to divide into two daughter cells. These events include the duplication of its DNA and some of its organelles, and the subsequent partitioning of its cytoplasm into two daughter cells in a process called cell division. In eukaryotes (i.e., animal, plant, fungal, and protist cells), there are two distinct types of cell division: mitosis and meiosis. Mitosis is part of the cell cycle, in which replicated chromosomes are separated into two new nuclei. Cell division gives rise to genetically identical cells in which the total number of chromosomes is maintained. In general, mitosis (division of the nucleus) is preceded by the S stage of interphase (during which the DNA is replicated) and is often followed by telophase and cytokinesis; which divides the cytoplasm, organelles and cell membrane of one cell into two new cells containing roughly equal shares of these cellular components. The different stages of mitosis all together define the mitotic phase of an animal cell cycle—the division of the mother cell into two genetically identical daughter cells. The cell cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. In contrast to mitosis, meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division (meiosis I), and sister chromatids are separated in the second division (meiosis II). Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor.
Prokaryotes (i.e., archaea and bacteria) can also undergo cell division (or binary fission). Unlike the processes of mitosis and meiosis in eukaryotes, binary fission in prokaryotes takes place without the formation of a spindle apparatus on the cell. Before binary fission, DNA in the bacterium is tightly coiled. After it has uncoiled and duplicated, it is pulled to the separate poles of the bacterium as it increases the size to prepare for splitting. Growth of a new cell wall begins to separate the bacterium (triggered by FtsZ polymerization and "Z-ring" formation). The new cell wall (septum) fully develops, resulting in the complete split of the bacterium. The new daughter cells have tightly coiled DNA rods, ribosomes, and plasmids.
=
Meiosis is a central feature of sexual reproduction in eukaryotes, and the most fundamental function of meiosis appears to be conservation of the integrity of the genome that is passed on to progeny by parents. Two aspects of sexual reproduction, meiotic recombination and outcrossing, are likely maintained respectively by the adaptive advantages of recombinational repair of genomic DNA damage and genetic complementation which masks the expression of deleterious recessive mutations.
The beneficial effect of genetic complementation, derived from outcrossing (cross-fertilization) is also referred to as hybrid vigor or heterosis. Charles Darwin in his 1878 book The Effects of Cross and Self-Fertilization in the Vegetable Kingdom at the start of chapter XII noted "The first and most important of the conclusions which may be drawn from the observations given in this volume, is that generally cross-fertilisation is beneficial and self-fertilisation often injurious, at least with the plants on which I experimented." Genetic variation, often produced as a byproduct of sexual reproduction, may provide long-term advantages to those sexual lineages that engage in outcrossing.
=
Genetics is the scientific study of inheritance. Mendelian inheritance, specifically, is the process by which genes and traits are passed on from parents to offspring. It has several principles. The first is that genetic characteristics, alleles, are discrete and have alternate forms (e.g., purple vs. white or tall vs. dwarf), each inherited from one of two parents. Based on the law of dominance and uniformity, which states that some alleles are dominant while others are recessive; an organism with at least one dominant allele will display the phenotype of that dominant allele. During gamete formation, the alleles for each gene segregate, so that each gamete carries only one allele for each gene. Heterozygotic individuals produce gametes with an equal frequency of two alleles. Finally, the law of independent assortment, states that genes of different traits can segregate independently during the formation of gametes, i.e., genes are unlinked. An exception to this rule would include traits that are sex-linked. Test crosses can be performed to experimentally determine the underlying genotype of an organism with a dominant phenotype. A Punnett square can be used to predict the results of a test cross. The chromosome theory of inheritance, which states that genes are found on chromosomes, was supported by Thomas Morgans's experiments with fruit flies, which established the sex linkage between eye color and sex in these insects.
=
A gene is a unit of heredity that corresponds to a region of deoxyribonucleic acid (DNA) that carries genetic information that controls form or function of an organism. DNA is composed of two polynucleotide chains that coil around each other to form a double helix. It is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell is collectively known as its genome. In eukaryotes, DNA is mainly in the cell nucleus. In prokaryotes, the DNA is held within the nucleoid. The genetic information is held within genes, and the complete assemblage in an organism is called its genotype.
DNA replication is a semiconservative process whereby each strand serves as a template for a new strand of DNA. Mutations are heritable changes in DNA. They can arise spontaneously as a result of replication errors that were not corrected by proofreading or can be induced by an environmental mutagen such as a chemical (e.g., nitrous acid, benzopyrene) or radiation (e.g., x-ray, gamma ray, ultraviolet radiation, particles emitted by unstable isotopes). Mutations can lead to phenotypic effects such as loss-of-function, gain-of-function, and conditional mutations.
Some mutations are beneficial, as they are a source of genetic variation for evolution. Others are harmful if they were to result in a loss of function of genes needed for survival.
=
Gene expression is the molecular process by which a genotype encoded in DNA gives rise to an observable phenotype in the proteins of an organism's body. This process is summarized by the central dogma of molecular biology, which was formulated by Francis Crick in 1958. According to the Central Dogma, genetic information flows from DNA to RNA to protein. There are two gene expression processes: transcription (DNA to RNA) and translation (RNA to protein).
=
The regulation of gene expression by environmental factors and during different stages of development can occur at each step of the process such as transcription, RNA splicing, translation, and post-translational modification of a protein. Gene expression can be influenced by positive or negative regulation, depending on which of the two types of regulatory proteins called transcription factors bind to the DNA sequence close to or at a promoter. A cluster of genes that share the same promoter is called an operon, found mainly in prokaryotes and some lower eukaryotes (e.g., Caenorhabditis elegans). In positive regulation of gene expression, the activator is the transcription factor that stimulates transcription when it binds to the sequence near or at the promoter. Negative regulation occurs when another transcription factor called a repressor binds to a DNA sequence called an operator, which is part of an operon, to prevent transcription. Repressors can be inhibited by compounds called inducers (e.g., allolactose), thereby allowing transcription to occur. Specific genes that can be activated by inducers are called inducible genes, in contrast to constitutive genes that are almost constantly active. In contrast to both, structural genes encode proteins that are not involved in gene regulation. In addition to regulatory events involving the promoter, gene expression can also be regulated by epigenetic changes to chromatin, which is a complex of DNA and protein found in eukaryotic cells.
=
Development is the process by which a multicellular organism (plant or animal) goes through a series of changes, starting from a single cell, and taking on various forms that are characteristic of its life cycle. There are four key processes that underlie development: Determination, differentiation, morphogenesis, and growth. Determination sets the developmental fate of a cell, which becomes more restrictive during development. Differentiation is the process by which specialized cells arise from less specialized cells such as stem cells. Stem cells are undifferentiated or partially differentiated cells that can differentiate into various types of cells and proliferate indefinitely to produce more of the same stem cell. Cellular differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals, which are largely due to highly controlled modifications in gene expression and epigenetics. With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself. Thus, different cells can have very different physical characteristics despite having the same genome. Morphogenesis, or the development of body form, is the result of spatial differences in gene expression. A small fraction of the genes in an organism's genome called the developmental-genetic toolkit control the development of that organism. These toolkit genes are highly conserved among phyla, meaning that they are ancient and very similar in widely separated groups of animals. Differences in deployment of toolkit genes affect the body plan and the number, identity, and pattern of body parts. Among the most important toolkit genes are the Hox genes. Hox genes determine where repeating parts, such as the many vertebrae of snakes, will grow in a developing embryo or larva.
=
Evolution is a central organizing concept in biology. It is the change in heritable characteristics of populations over successive generations. In artificial selection, animals were selectively bred for specific traits.
 Given that traits are inherited, populations contain a varied mix of traits, and reproduction is able to increase any population, Darwin argued that in the natural world, it was nature that played the role of humans in selecting for specific traits. Darwin inferred that individuals who possessed heritable traits better adapted to their environments are more likely to survive and produce more offspring than other individuals. He further inferred that this would lead to the accumulation of favorable traits over successive generations, thereby increasing the match between the organisms and their environment.
=
A species is a group of organisms that mate with one another and speciation is the process by which one lineage splits into two lineages as a result of having evolved independently from each other. For speciation to occur, there has to be reproductive isolation. Reproductive isolation can result from incompatibilities between genes as described by Bateson–Dobzhansky–Muller model. Reproductive isolation also tends to increase with genetic divergence. Speciation can occur when there are physical barriers that divide an ancestral species, a process known as allopatric speciation.
=
A phylogeny is an evolutionary history of a specific group of organisms or their genes. It can be represented using a phylogenetic tree, a diagram showing lines of descent among organisms or their genes. Each line drawn on the time axis of a tree represents a lineage of descendants of a particular species or population. When a lineage divides into two, it is represented as a fork or split on the phylogenetic tree. Phylogenetic trees are the basis for comparing and grouping different species. Different species that share a feature inherited from a common ancestor are described as having homologous features (or synapomorphy). Phylogeny provides the basis of biological classification. This classification system is rank-based, with the highest rank being the domain followed by kingdom, phylum, class, order, family, genus, and species. All organisms can be classified as belonging to one of three domains: Archaea (originally Archaebacteria), Bacteria (originally eubacteria), or Eukarya (includes the fungi, plant, and animal kingdoms).
=
The history of life on Earth traces how organisms have evolved from the earliest emergence of life to present day. Earth formed about 4.5 billion years ago and all life on Earth, both living and extinct, descended from a last universal common ancestor that lived about 3.5 billion years ago. Geologists have developed a geologic time scale that divides the history of the Earth into major divisions, starting with four eons (Hadean, Archean, Proterozoic, and Phanerozoic), the first three of which are collectively known as the Precambrian, which lasted approximately 4 billion years. Each eon can be divided into eras, with the Phanerozoic eon that began 539 million years ago being subdivided into Paleozoic, Mesozoic, and Cenozoic eras. These three eras together comprise eleven periods (Cambrian, Ordovician, Silurian, Devonian, Carboniferous, Permian, Triassic, Jurassic, Cretaceous, Tertiary, and Quaternary).
The similarities among all known present-day species indicate that they have diverged through the process of evolution from their common ancestor. Biologists regard the ubiquity of the genetic code as evidence of universal common descent for all bacteria, archaea, and eukaryotes. Microbial mats of coexisting bacteria and archaea were the dominant form of life in the early Archean eon and many of the major steps in early evolution are thought to have taken place in this environment. The earliest evidence of eukaryotes dates from 1.85 billion years ago, and while they may have been present earlier, their diversification accelerated when they started using oxygen in their metabolism. Later, around 1.7 billion years ago, multicellular organisms began to appear, with differentiated cells performing specialised functions.
Algae-like multicellular land plants are dated back to about 1 billion years ago, although evidence suggests that microorganisms formed the earliest terrestrial ecosystems, at least 2.7 billion years ago. Microorganisms are thought to have paved the way for the inception of land plants in the Ordovician period. Land plants were so successful that they are thought to have contributed to the Late Devonian extinction event.
Ediacara biota appear during the Ediacaran period, while vertebrates, along with most other modern phyla originated about 525 million years ago during the Cambrian explosion. During the Permian period, synapsids, including the ancestors of mammals, dominated the land, but most of this group became extinct in the Permian–Triassic extinction event 252 million years ago. During the recovery from this catastrophe, archosaurs became the most abundant land vertebrates; one archosaur group, the dinosaurs, dominated the Jurassic and Cretaceous periods. After the Cretaceous–Paleogene extinction event 66 million years ago killed off the non-avian dinosaurs, mammals increased rapidly in size and diversity. Such mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.
=
Bacteria are a type of cell that constitute a large domain of prokaryotic microorganisms. Typically a few micrometers in length, bacteria have a number of shapes, ranging from spheres to rods and spirals. Bacteria were among the first life forms to appear on Earth, and are present in most of its habitats. Bacteria inhabit soil, water, acidic hot springs, radioactive waste, and the deep biosphere of the Earth's crust. Bacteria also live in symbiotic and parasitic relationships with plants and animals. Most bacteria have not been characterised, and only about 27 percent of the bacterial phyla have species that can be grown in the laboratory.
Archaea constitute the other domain of prokaryotic cells and were initially classified as bacteria, receiving the name archaebacteria (in the Archaebacteria kingdom), a term that has fallen out of use. Archaeal cells have unique properties separating them from the other two domains, Bacteria and Eukaryota. Archaea are further divided into multiple recognized phyla. Archaea and bacteria are generally similar in size and shape, although a few archaea have very different shapes, such as the flat and square cells of Haloquadratum walsbyi. Despite this morphological similarity to bacteria, archaea possess genes and several metabolic pathways that are more closely related to those of eukaryotes, notably for the enzymes involved in transcription and translation. Other aspects of archaeal biochemistry are unique, such as their reliance on ether lipids in their cell membranes, including archaeols. Archaea use more energy sources than eukaryotes: these range from organic compounds, such as sugars, to ammonia, metal ions or even hydrogen gas. Salt-tolerant archaea (the Haloarchaea) use sunlight as an energy source, and other species of archaea fix carbon, but unlike plants and cyanobacteria, no known species of archaea does both. Archaea reproduce asexually by binary fission, fragmentation, or budding; unlike bacteria, no known species of Archaea form endospores.
The first observed archaea were extremophiles, living in extreme environments, such as hot springs and salt lakes with no other organisms. Improved molecular detection tools led to the discovery of archaea in almost every habitat, including soil, oceans, and marshlands. Archaea are particularly numerous in the oceans, and the archaea in plankton may be one of the most abundant groups of organisms on the planet.
Archaea are a major part of Earth's life. They are part of the microbiota of all organisms. In the human microbiome, they are important in the gut, mouth, and on the skin. Their morphological, metabolic, and geographical diversity permits them to play multiple ecological roles: carbon fixation; nitrogen cycling; organic compound turnover; and maintaining microbial symbiotic and syntrophic communities, for example.
=
Eukaryotes are hypothesized to have split from archaea, which was followed by their endosymbioses with bacteria (or symbiogenesis) that gave rise to mitochondria and chloroplasts, both of which are now part of modern-day eukaryotic cells. The major lineages of eukaryotes diversified in the Precambrian about 1.5 billion years ago and can be classified into eight major clades: alveolates, excavates, stramenopiles, plants, rhizarians, amoebozoans, fungi, and animals. Five of these clades are collectively known as protists, which are mostly microscopic eukaryotic organisms that are not plants, fungi, or animals. While it is likely that protists share a common ancestor (the last eukaryotic common ancestor), protists by themselves do not constitute a separate clade as some protists may be more closely related to plants, fungi, or animals than they are to other protists. Like groupings such as algae, invertebrates, or protozoans, the protist grouping is not a formal taxonomic group but is used for convenience. Most protists are unicellular; these are called microbial eukaryotes.
Plants are mainly multicellular organisms, predominantly photosynthetic eukaryotes of the kingdom Plantae, which would exclude fungi and some algae. Plant cells were derived by endosymbiosis of a cyanobacterium into an early eukaryote about one billion years ago, which gave rise to chloroplasts. The first several clades that emerged following primary endosymbiosis were aquatic and most of the aquatic photosynthetic eukaryotic organisms are collectively described as algae, which is a term of convenience as not all algae are closely related. Algae comprise several distinct clades such as glaucophytes, which are microscopic freshwater algae that may have resembled in form to the early unicellular ancestor of Plantae. Unlike glaucophytes, the other algal clades such as red and green algae are multicellular. Green algae comprise three major clades: chlorophytes, coleochaetophytes, and stoneworts.
Fungi are eukaryotes that digest foods outside their bodies, secreting digestive enzymes that break down large food molecules before absorbing them through their cell membranes. Many fungi are also saprobes, feeding on dead organic matter, making them important decomposers in ecological systems.
Animals are multicellular eukaryotes. With few exceptions, animals consume organic material, breathe oxygen, are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million animal species in total. They have complex interactions with each other and their environments, forming intricate food webs.
=
Viruses are submicroscopic infectious agents that replicate inside the cells of organisms. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea. More than 6,000 virus species have been described in detail. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity.
The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Because viruses possess some but not all characteristics of life, they have been described as "organisms at the edge of life", and as self-replicators.
Ecology is the study of the distribution and abundance of life, the interaction between organisms and their environment.
=
The community of living (biotic) organisms in conjunction with the nonliving (abiotic) components (e.g., water, light, radiation, temperature, humidity, atmosphere, acidity, and soil) of their environment is called an ecosystem. These biotic and abiotic components are linked together through nutrient cycles and energy flows. Energy from the sun enters the system through photosynthesis and is incorporated into plant tissue. By feeding on plants and on one another, animals move matter and energy through the system. They also influence the quantity of plant and microbial biomass present. By breaking down dead organic matter, decomposers release carbon back to the atmosphere and facilitate nutrient cycling by converting nutrients stored in dead biomass back to a form that can be readily used by plants and other microbes.
=
A population is the group of organisms of the same species that occupies an area and reproduce from generation to generation. Population size can be estimated by multiplying population density by the area or volume. The carrying capacity of an environment is the maximum population size of a species that can be sustained by that specific environment, given the food, habitat, water, and other resources that are available. The carrying capacity of a population can be affected by changing environmental conditions such as changes in the availability of resources and the cost of maintaining them. In human populations, new technologies such as the Green revolution have helped increase the Earth's carrying capacity for humans over time, which has stymied the attempted predictions of impending population decline, the most famous of which was by Thomas Malthus in the 18th century.
=
A community is a group of populations of species occupying the same geographical area at the same time. A biological interaction is the effect that a pair of organisms living together in a community have on each other. They can be either of the same species (intraspecific interactions), or of different species (interspecific interactions). These effects may be short-term, like pollination and predation, or long-term; both often strongly influence the evolution of the species involved. A long-term interaction is called a symbiosis. Symbioses range from mutualism, beneficial to both partners, to competition, harmful to both partners. Every species participates as a consumer, resource, or both in consumer–resource interactions, which form the core of food chains or food webs. There are different trophic levels within any food web, with the lowest level being the primary producers (or autotrophs) such as plants and algae that convert energy and inorganic material into organic compounds, which can then be used by the rest of the community. At the next level are the heterotrophs, which are the species that obtain energy by breaking apart organic compounds from other organisms. Heterotrophs that consume plants are primary consumers (or herbivores) whereas heterotrophs that consume herbivores are secondary consumers (or carnivores). And those that eat secondary consumers are tertiary consumers and so on. Omnivorous heterotrophs are able to consume at multiple levels. Finally, there are decomposers that feed on the waste products or dead bodies of organisms.
On average, the total amount of energy incorporated into the biomass of a trophic level per unit of time is about one-tenth of the energy of the trophic level that it consumes. Waste and dead material used by decomposers as well as heat lost from metabolism make up the other ninety percent of energy that is not consumed by the next trophic level.
=
In the global ecosystem or biosphere, matter exists as different interacting compartments, which can be biotic or abiotic as well as accessible or inaccessible, depending on their forms and locations. For example, matter from terrestrial autotrophs are both biotic and accessible to other organisms whereas the matter in rocks and minerals are abiotic and inaccessible. A biogeochemical cycle is a pathway by which specific elements of matter are turned over or moved through the biotic (biosphere) and the abiotic (lithosphere, atmosphere, and hydrosphere) compartments of Earth. There are biogeochemical cycles for nitrogen, carbon, and water.
=
Conservation biology is the study of the conservation of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is concerned with factors that influence the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity. The concern stems from estimates suggesting that up to 50% of all species on the planet will disappear within the next 50 years, which has contributed to poverty, starvation, and will reset the course of evolution on this planet. Biodiversity affects the functioning of ecosystems, which provide a variety of services upon which people depend. Conservation biologists research and educate on the trends of biodiversity loss, species extinctions, and the negative effect these are having on our capabilities to sustain the well-being of human society. Organizations and citizens are responding to the current biodiversity crisis through conservation action plans that direct research, monitoring, and education programs that engage concerns at local through global scales.
OSU's Phylocode
Biology Dictionary Online
MIT video lecture series on biology
OneZoom Tree of Life
Journal of the History of Biology at Springer Nature
Journal links
PLOS Biology A peer-reviewed, open-access journal published by the Public Library of Science
Current Biology: General journal publishing original research from all areas of biology
Biology Letters: A high-impact Royal Society journal publishing peer-reviewed biology papers of general interest
Science: Internationally renowned AAAS science journal – see sections of the life sciences
International Journal of Biological Sciences: A biological journal publishing significant peer-reviewed scientific papers
Perspectives in Biology and Medicine: An interdisciplinary scholarly journal publishing essays of broad relevance

--- TOPIC: DNA ---
Deoxyribonucleic acid (; DNA) is a polymer composed of two polynucleotide chains that coil around each other to form a double helix. The polymer carries genetic instructions for the development, functioning, growth and reproduction of all known organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids. Alongside proteins, lipids and complex carbohydrates (polysaccharides), nucleic acids are one of the four major types of macromolecules that are essential for all known forms of life.
The two DNA strands are known as polynucleotides as they are composed of simpler monomeric units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds (known as the phosphodiester linkage) between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA. The complementary nitrogenous bases are divided into two groups, the single-ringed pyrimidines and the double-ringed purines. In DNA, the pyrimidines are thymine and cytosine; the purines are adenine and guanine.
Both strands of double-stranded DNA store the same biological information. This information is replicated when the two strands separate. The two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (or bases). It is the sequence of these four nucleobases along the backbone that encodes genetic information. RNA strands are created using DNA strands as a template in a process called transcription, where DNA bases are exchanged for their corresponding bases except in the case of thymine (T), for which RNA substitutes uracil (U). Under the genetic code, these RNA strands specify the sequence of amino acids within proteins in a process called translation.
Within eukaryotic cells, DNA is organized into long structures called chromosomes. Before typical cell division, these chromosomes are duplicated in the process of DNA replication, providing a complete set of chromosomes for each daughter cell. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus as nuclear DNA, and some in the mitochondria as mitochondrial DNA or in chloroplasts as chloroplast DNA. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm, in circular chromosomes. Within eukaryotic chromosomes, chromatin proteins, such as histones, compact and organize DNA. These compacting structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.
DNA is a long polymer made from repeating units called nucleotides. The structure of DNA is dynamic along its length, being capable of coiling into tight loops and other shapes. In all species it is composed of two helical chains, bound to each other by hydrogen bonds. Both chains are coiled around the same axis, and have the same pitch of 34 ångströms (3.4 nm). The pair of chains have a radius of 10 Å (1.0 nm). According to another study, when measured in a different solution, the DNA chain measured 22–26 Å (2.2–2.6 nm) wide, and one nucleotide unit measured 3.3 Å (0.33 nm) long. The buoyant density of most DNA is 1.7g/cm3.
DNA does not usually exist as a single strand, but instead as a pair of strands that are held tightly together. These two long strands coil around each other, in the shape of a double helix. The nucleotide contains both a segment of the backbone of the molecule (which holds the chain together) and a nucleobase (which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called a nucleoside, and a base linked to a sugar and to one or more phosphate groups is called a nucleotide. A biopolymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.
The backbone of the DNA strand is made from alternating phosphate and sugar groups. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These are known as the 3′-end (three prime end), and 5′-end (five prime end) carbons, the prime symbol being used to distinguish these carbon atoms from those of the base to which the deoxyribose forms a glycosidic bond.
Therefore, any DNA strand normally has one end at which there is a phosphate group attached to the 5′ carbon of a ribose (the 5′ phosphoryl) and another end at which there is a free hydroxyl group attached to the 3′ carbon of a ribose (the 3′ hydroxyl). The orientation of the 3′ and 5′ carbons along the sugar-phosphate backbone confers directionality (sometimes called polarity) to each DNA strand. In a nucleic acid double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are antiparallel. The asymmetric ends of DNA strands are said to have a directionality of five prime end (5′ ), and three prime end (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the related pentose sugar ribose in RNA.
The DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. The four bases found in DNA are adenine (A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine, forming A-T and G-C base pairs.
=
The nucleobases are classified into two types: the purines, A and G, which are fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA, many artificial nucleic acid analogues have been created to study the properties of nucleic acids, or for use in biotechnology.
=
Modified bases occur in DNA. The first of these recognized was 5-methylcytosine, which was found in the genome of Mycobacterium tuberculosis in 1925. The reason for the presence of these noncanonical bases in bacterial viruses (bacteriophages) is to avoid the restriction enzymes present in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses. Modifications of the bases cytosine and adenine, the more common and modified DNA bases, play vital roles in the epigenetic control of gene expression in plants and animals.
A number of noncanonical bases are known to occur in DNA. Most of these are modifications of the canonical bases plus uracil.
Modified Adenine
N6-carbamoyl-methyladenine
N6-methyadenine
Modified Guanine
7-Deazaguanine
7-Methylguanine
Modified Cytosine
N4-Methylcytosine
5-Carboxylcytosine
5-Formylcytosine
5-Glycosylhydroxymethylcytosine
5-Hydroxycytosine
5-Methylcytosine
Modified Thymidine
α-Glutamythymidine
α-Putrescinylthymine
Uracil and modifications
Base J
Uracil
5-Dihydroxypentauracil
5-Hydroxymethyldeoxyuracil
Others
Deoxyarchaeosine
2,6-Diaminopurine (2-Aminoadenine)
=
Twin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. The major groove is 22 ångströms (2.2 nm) wide, while the minor groove is 12 Å (1.2 nm) in width. Due to the larger width of the major groove, the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such as transcription factors that can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell (see below), but the major and minor grooves are always named to reflect the differences in width that would be seen if the DNA was twisted back into the ordinary B form.
=
In a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix (from six-carbon ring to six-carbon ring) is called a Watson-Crick base pair. DNA with high GC-content is more stable than DNA with low GC-content. A Hoogsteen base pair (hydrogen bonding the 6-carbon ring to the 5-carbon ring) is a rare variation of base-pairing. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can thus be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this base pair complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. This reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in organisms.
 ssDNA vs. dsDNA 
Most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double-stranded (dsDNA) structure is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart—a process known as melting—to form two single-stranded DNA (ssDNA) molecules. Melting occurs at high temperatures, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).
The stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the melting temperature (also called Tm value), which is the temperature at which 50% of the double-strand molecules are converted to single-strand molecules; melting temperature is dependent on ionic strength and the concentration of DNA. As a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have more strongly interacting strands, while short helices with high AT content have more weakly interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.
In the laboratory, the strength of this interaction can be measured by finding the melting temperature Tm necessary to break half of the hydrogen bonds. When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules have no single common shape, but some conformations are more stable than others.
=
In humans, the total female diploid nuclear genome per cell extends for 6.37 Gigabase pairs (Gbp), is 208.23 cm long and weighs 6.51 picograms (pg). Male values are 6.27 Gbp, 205.00 cm, 6.41 pg. Each DNA polymer can contain hundreds of millions of nucleotides, such as in chromosome 1. Chromosome 1 is the largest human chromosome with approximately 220 million base pairs, and would be 85 mm long if straightened.
In eukaryotes, in addition to nuclear DNA, there is also mitochondrial DNA (mtDNA) which encodes certain proteins used by the mitochondria. The mtDNA is usually relatively small in comparison to the nuclear DNA. For example, the human mitochondrial DNA forms closed circular molecules, each of which contains 16,569 DNA base pairs, with each such molecule normally containing a full set of the mitochondrial genes. Each human mitochondrion contains, on average, approximately 5 such mtDNA molecules. Each human cell contains approximately 100 mitochondria, giving a total number of mtDNA molecules per human cell of approximately 500. However, the amount of mitochondria per cell also varies by cell type, and an egg cell can contain 100,000 mitochondria, corresponding to up to 1,500,000 copies of the mitochondrial genome (constituting up to 90% of the DNA of the cell).
=
A DNA sequence is called a "sense" sequence if it is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the "antisense" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.
A few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.
=
DNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its "relaxed" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.
=
DNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, and the presence of polyamines in solution.
The first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson functions that provided only a limited amount of structural information for oriented fibers of DNA. An alternative analysis was proposed by Wilkins et al. in 1953 for the in vivo B-DNA X-ray diffraction-scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double helix.
Although the B-DNA form is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.
Compared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partly dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, and in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.
=
For many years, exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1 was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.
=
At the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.
These guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases, known as a guanine tetrad, form a flat plate. These flat four-base units then stack on top of each other to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.
In addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.
=
In DNA, fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.
=
Several artificial nucleobases have been synthesized, and successfully incorporated in the eight-base DNA analogue named Hachimoji DNA. Dubbed S, B, P, and Z, these artificial bases are capable of bonding with each other in a predictable way (S–B and P–Z), maintain the double helix structure of DNA, and be transcribed to RNA. Their existence could be seen as an indication that there is nothing special about the four natural nucleobases that evolved on Earth. On the other hand, DNA is tightly related to RNA which does not only act as a transcript of DNA but also performs as molecular machines many tasks in cells. For this purpose it has to fold into a structure. It has been shown that to allow to create all possible structures at least four bases are required for the corresponding RNA, while a higher number is also possible but this would be against the natural principle of least effort.
=
The phosphate groups of DNA give it similar acidic properties to phosphoric acid and it can be considered as a strong acid. It will be fully ionized at a normal cellular pH, releasing protons which leave behind negative charges on the phosphate groups. These negative charges protect DNA from breakdown by hydrolysis by repelling nucleophiles which could hydrolyze it.
=
Pure DNA extracted from cells forms white, stringy clumps.
=
The expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.
For one example, cytosine methylation produces 5-methylcytosine, which is important for X-inactivation of chromosomes. The average level of methylation varies between organisms—the worm Caenorhabditis elegans lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the "J-base" in kinetoplastids.
=
DNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions, deletions from the DNA sequence, and chromosomal translocations. These mutations can cause cancer. Because of inherent limits in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.
Many mutagens fit into the space between two adjacent base pairs, this is called intercalation. Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo[a]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.
DNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In an alternative fashion, a cell may copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.
=
Genomic DNA is tightly and orderly packed in the process called DNA condensation, to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. 
The functional genetic information in a genome is located in genes, regulatory sequences, origins of replication, centromeres, telomeres, and segments required for the three-dimensional structure of chromatin. In many complex eukaryotes, only a small fraction of the total sequence is devoted to the various functional elements. For example, in humans less than 10% of the genome has a defined functional role and the rest (90%) is probably junk DNA. (See also [Non-coding DNA].)
=
A gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called codons formed from a sequence of three nucleotides (e.g., ACT, CAG, TTT).
In transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (43 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAG, TAA, and TGA codons, (UAG, UAA, and UGA on the mRNA).
=
Cell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand's complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.
=
Naked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L. Various possible functions have been proposed for eDNA: it may be involved in horizontal gene transfer; it may provide nutrients; and it may act as a buffer to recruit or titrate ions or antibiotics. Extracellular DNA acts as a functional extracellular matrix component in the biofilms of several bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm; it may contribute to biofilm formation; and it may contribute to the biofilm's physical strength and resistance to biological stress.
Cell-free fetal DNA is found in the blood of the mother, and can be sequenced to determine a great deal of information about the developing fetus.
Under the name of environmental DNA eDNA has seen increased use in the natural sciences as a survey tool for ecology, monitoring the movements and presence of species in water, air, or on land, and assessing an area's biodiversity.
All the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.
=
Structural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes, this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones, making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are thus largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation, and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.
A distinct group of DNA-binding proteins is the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination, and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.
In contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.
As these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to "read" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.
=
 Nucleases and ligases 
Nucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the horizontal line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.
Enzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.
 Topoisomerases and helicases 
Topoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.
Helicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly adenosine triphosphate (ATP), to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.
 Polymerases 
Polymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products is created based on existing polynucleotide chains—which are called templates. These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.
In DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. To preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.
RNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. For example, HIV reverse transcriptase is an enzyme for AIDS virus replication. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure. It synthesizes telomeres at the ends of chromosomes. Telomeres prevent fusion of the ends of neighboring chromosomes and protect chromosome ends from damage.
Transcription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.
A DNA helix usually does not interact with other segments of DNA, and in human cells, the different chromosomes even occupy separate areas in the nucleus called "chromosome territories". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is in chromosomal crossover which occurs during sexual reproduction, when genetic recombination occurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.
Recombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.
The most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA. Only strands of like polarity exchange DNA during recombination. There are two types of cleavage: east-west cleavage and north–south cleavage. The north–south cleavage nicks both strands of DNA, while the east–west cleavage has one strand of DNA intact. The formation of a Holliday junction during recombination makes it possible for genetic diversity, genes to exchange on chromosomes, and expression of wild-type viral genomes.
DNA contains the genetic information that allows all forms of life to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes. However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.
Building blocks of DNA (adenine, guanine, and related organic molecules) may have been formed extraterrestrially in outer space. Complex DNA and RNA organic compounds of life, including uracil, cytosine, and thymine, have also been formed in the laboratory under conditions mimicking those found in outer space, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar cosmic dust and gas clouds.
Ancient DNA has been recovered from ancient organisms at a timescale where genome evolution can be directly observed, including from extinct organisms up to millions of years old, such as the woolly mammoth.
=
Methods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.
=
Forensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator. This process is formally termed DNA profiling, also called DNA fingerprinting. In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.
The development of forensic science and the ability to now obtain genetic matching on minute samples of blood, skin, saliva, or hair has led to re-examining many cases. Evidence can now be uncovered that was scientifically impossible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where prior trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defense to DNA matches obtained forensically is to claim that cross-contamination of evidence has occurred. This has resulted in meticulous strict handling procedures with new cases of serious crime.
DNA profiling is also used successfully to positively identify victims of mass casualty incidents, bodies or body parts in serious accidents, and individual victims in mass war graves, via matching to family members.
DNA profiling is also used in DNA paternity testing to determine if someone is the biological parent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. Usually DNA sequencing are carried out after birth, but there are new methods to test paternity while a mother is still pregnant.
=
Deoxyribozymes, also called DNAzymes or catalytic DNA, were first discovered in 1994. They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach called in vitro selection or systematic evolution of ligands by exponential enrichment (SELEX). DNAzymes catalyze variety of chemical reactions including RNA-DNA cleavage, RNA-DNA ligation, amino acids phosphorylation-dephosphorylation, carbon-carbon bond formation, etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction. The most extensively studied class of DNAzymes is RNA-cleaving types which have been used to detect different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific). The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in cells.
=
Bioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning, and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.
=
DNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the DNA origami method) and three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins. DNA and other nucleic acids are the basis of aptamers, synthetic oligonucleotide ligands for specific target molecules used in a range of biotechnology and biomedical applications.
=
Because DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology.
=
DNA as a storage device for information has enormous potential since it has much higher storage density compared to electronic devices. However, high costs, slow read and write times (memory latency), and insufficient reliability has prevented its practical use.
DNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it "nuclein". In 1878, Albrecht Kossel isolated the non-protein component of "nuclein", nucleic acid, and later isolated its five primary nucleobases.
In 1909, Phoebus Levene identified the base, sugar, and phosphate nucleotide unit of RNA (then named "yeast nucleic acid"). In 1929, Levene identified deoxyribose sugar in "thymus nucleic acid" (DNA). Levene suggested that DNA consisted of a string of four nucleotide units linked together through the phosphate groups ("tetranucleotide hypothesis"). Levene thought the chain was short and the bases repeated in a fixed order. In 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a "giant hereditary molecule" made up of "two mirror strands that would replicate in a semi-conservative fashion using each strand as a template". In 1928, Frederick Griffith in his experiment discovered that traits of the "smooth" form of Pneumococcus could be transferred to the "rough" form of the same bacteria by mixing killed "smooth" bacteria with the live "rough" form. This system provided the first clear suggestion that DNA carries genetic information.
In 1933, while studying virgin sea urchin eggs, Jean Brachet suggested that DNA is found in the cell nucleus and that RNA is present exclusively in the cytoplasm. At the time, "yeast nucleic acid" (RNA) was thought to occur only in plants, while "thymus nucleic acid" (DNA) only in animals. The latter was thought to be a tetramer, with the function of buffering cellular pH. In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.
In 1943, Oswald Avery, along with co-workers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle, supporting Griffith's suggestion (Avery–MacLeod–McCarty experiment). Erwin Chargaff developed and published observations now known as Chargaff's rules, stating that in DNA from any species of any organism, the amount of guanine should be equal to cytosine and the amount of adenine should be equal to thymine.
By 1951, Alec Todd and collaborators at the University of Cambridge had determined by biochemical methods how the backbone of DNA is structured via the successive linking of carbon atoms 3 and 5 of the sugar to phosphates. This would help to corroborate Watson and Crick's later X-ray structural work. Todd would later be awarded the 1957 Nobel Prize in Chemistry for this and other discoveries related to DNA.
Late in 1951, Francis Crick started working with James Watson at the Cavendish Laboratory within the University of Cambridge. DNA's role in heredity was confirmed in 1952 when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the enterobacteria phage T2.
In May 1952, Raymond Gosling, a graduate student working under the supervision of Rosalind Franklin, took an X-ray diffraction image, labeled as "Photo 51", at high hydration levels of DNA. This photo was given to Watson and Crick by Maurice Wilkins and was critical to their obtaining the correct structure of DNA. Franklin told Crick and Watson that the backbones had to be on the outside. Before then, Linus Pauling, and Watson and Crick, had erroneous models with the chains inside and the bases pointing outwards. Franklin's identification of the space group for DNA crystals proved her correct. In February 1953, Linus Pauling and Robert Corey proposed a model for nucleic acids containing three intertwined chains, with the phosphates near the axis, and the bases on the outside. Watson and Crick completed their model, which is now accepted as the first correct model of the double helix of DNA. On 28 February 1953 Crick interrupted patrons' lunchtime at The Eagle pub in Cambridge, England to announce that he and Watson had "discovered the secret of life".
The 25 April 1953 issue of the journal Nature published a series of five articles giving the Watson and Crick double-helix structure DNA and evidence supporting it. The structure was reported in a letter titled "MOLECULAR STRUCTURE OF NUCLEIC ACIDS A Structure for Deoxyribose Nucleic Acid", in which they said, "It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material." This letter was followed by a letter from Franklin and Gosling, which was the first publication of their own X-ray diffraction data and of their original analysis method. Then followed a letter by Wilkins and two of his colleagues, which contained an analysis of in vivo B-DNA X-ray patterns, and which supported the presence in vivo of the Watson and Crick structure.
In April 2023, scientists, based on new evidence, concluded that Rosalind Franklin was a contributor and "equal player" in the discovery process of DNA, rather than otherwise, as may have been presented subsequently after the time of the discovery. In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.
In an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the "adaptor hypothesis". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and co-workers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley, and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.
In 1986, DNA analysis was first used in a criminal investigation when police in the UK requested Alec Jeffreys of the University of Leicester to prove or disprove the involvement in a particular case of a suspect who claimed innocence in the matter. Although the suspect had already confessed to committing a recent rape-murder, he was denying any involvement in a similar crime committed three years earlier. Yet the details of the two cases were so alike that the police concluded both crimes had been committed by the same person. However, all charges against the suspect were dropped when Jeffreys' DNA testing exonerated the suspect — from both the earlier murder and the one to which he'd confessed. Further such DNA profiling led to positive identification of another suspect (Colin Pitchfork) who, in 1988, was found guilty of both rape-murders.
DNA binding site prediction on protein
DNA the Double Helix Game From the official Nobel Prize web site
DNA under electron microscope
Dolan DNA Learning Center
Double Helix: 50 years of DNA, Nature
Proteopedia DNA
Proteopedia Forms_of_DNA
ENCODE threads explorer ENCODE home page at Nature
Double Helix 1953–2003 National Centre for Biotechnology Education
Genetic Education Modules for Teachers – DNA from the Beginning Study Guide
PDB Molecule of the Month DNA
"Clue to chemistry of heredity found". The New York Times, June 1953. First American newspaper coverage of the discovery of the DNA structure
DNA from the Beginning Another DNA Learning Center site on DNA, genes, and heredity from Mendel to the human genome project.
The Register of Francis Crick Personal Papers 1938 – 2007 at Mandeville Special Collections Library, University of California, San Diego
Seven-page, handwritten letter that Crick sent to his 12-year-old son Michael in 1953 describing the structure of DNA. See Crick's medal goes under the hammer, Nature, 5 April 2013.

--- TOPIC: Evolution ---
Evolution is the change in the heritable characteristics of biological populations over successive generations. It occurs when evolutionary processes such as genetic drift and natural selection act on genetic variation, resulting in certain characteristics becoming more or less common within a population over successive generations. The process of evolution has given rise to biodiversity at every level of biological organisation.
The scientific theory of evolution by natural selection was conceived independently by two British naturalists, Charles Darwin and Alfred Russel Wallace, in the mid-19th century as an explanation for why organisms are adapted to their physical and biological environments. The theory was first set out in detail in Darwin's book On the Origin of Species. Evolution by natural selection is established by observable facts about living organisms: (1) more offspring are often produced than can possibly survive; (2) traits vary among individuals with respect to their morphology, physiology, and behaviour; (3) different traits confer different rates of survival and reproduction (differential fitness); and (4) traits can be passed from generation to generation (heritability of fitness). In successive generations, members of a population are therefore more likely to be replaced by the offspring of parents with favourable characteristics for that environment.
In the early 20th century, competing ideas of evolution were refuted and evolution was combined with Mendelian inheritance and population genetics to give rise to modern evolutionary theory. In this synthesis the basis for heredity is in DNA molecules that pass information from generation to generation. The processes that change DNA in a population include natural selection, genetic drift, mutation, and gene flow.
All life on Earth—including humanity—shares a last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. The fossil record includes a progression from early biogenic graphite to microbial mat fossils to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped by repeated formations of new species (speciation), changes within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth. Morphological and biochemical traits tend to be more similar among species that share a more recent common ancestor, which historically was used to reconstruct phylogenetic trees, although direct comparison of genetic sequences is a more common method today.
Evolutionary biologists have continued to study various aspects of evolution by forming and testing hypotheses as well as constructing theories based on evidence from the field or laboratory and on data generated by the methods of mathematical and theoretical biology. Their discoveries have influenced not just the development of biology but also other fields including agriculture, medicine, and computer science.
Evolution in organisms occurs through changes in heritable characteristics—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the "brown-eye trait" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.
The complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. Some of these traits come from the interaction of its genotype with the environment while others are neutral. Some observable characteristics are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. The phenotype is the ability of the skin to tan when exposed to sunlight. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.
Heritable characteristics are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specifies the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, each long strand of DNA is called a chromosome. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are influenced by multiple genes in a quantitative or epistatic manner.
Evolution can occur if there is genetic variation within a population. Variation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is very similar among all individuals of that species. However, discoveries in the field of evolutionary developmental biology have demonstrated that even relatively small differences in genotype can lead to dramatic differences in phenotype both within and between species.
An individual organism's phenotype results from both its genotype and the influence of the environment it has lived in. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.
=
Mutations are changes in the DNA sequence of a cell's genome and are the ultimate source of genetic variation in all organisms. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect.
About half of the mutations in the coding regions of protein-coding genes are deleterious — the other half are neutral. A small percentage of the total mutations in this region confer a fitness benefit. Some of the mutations in other parts of the genome are deleterious but the vast majority are neutral. A few are beneficial.
Mutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.
New genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA, a phenomenon termed de novo gene birth.
The generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions (exon shuffling). When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to 100 independent domains that each catalyse one step in the overall process, like a step in an assembly line.
One example of mutation is wild boar piglets. They are camouflage coloured and show a characteristic pattern of dark and light longitudinal stripes. However, mutations in the melanocortin 1 receptor (MC1R) disrupt the pattern. The majority of pig breeds carry MC1R mutations disrupting wild-type colour and different mutations causing dominant black colouring.
=
In asexual organisms, genes are inherited together, or linked, as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.
The two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. This cost does not apply to hermaphroditic species, like most plants and many invertebrates. The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment. Another hypothesis is that sexual reproduction is primarily an adaptation for promoting accurate recombinational repair of damage in germline DNA, and that increased diversity is a byproduct of this process that may sometimes be adaptively beneficial.
=
Gene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy-metal-tolerant and heavy-metal-sensitive populations of grasses.
Gene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast Saccharomyces cerevisiae and the adzuki bean weevil Callosobruchus chinensis has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.
Large-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.
=
Some heritable changes cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.
From a neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms, for example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, and mutation bias.
=
Evolution by natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It embodies three principles:
Variation exists within populations of organisms with respect to morphology, physiology and behaviour (phenotypic variation).
Different traits confer different rates of survival and reproduction (differential fitness).
These traits can be passed from generation to generation (heritability of fitness).
More offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage. This teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. Consequences of selection include nonrandom mating and genetic hitchhiking.
The central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.
If an allele increases fitness more than the other alleles of that gene, then with each generation this allele has a higher probability of becoming common within the population. These traits are said to be selected for. Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele likely becoming rarer—they are selected against.
Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form. However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. "Throwbacks" such as these are known as atavisms.
Natural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.
Natural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. "Nature" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: "Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity, and material cycles (i.e., exchange of materials between living and nonliving parts) within the system...." Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.
Natural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation.
=
Genetic drift is the random fluctuation of allele frequencies within a population from one generation to the next. When selective forces are absent or relatively weak, allele frequencies are equally likely to drift upward or downward in each successive generation because the alleles are subject to sampling error. This drift halts when an allele eventually becomes fixed, either by disappearing from the population or by replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that begin with the same genetic structure to drift apart into two divergent populations with different sets of alleles.
According to the neutral theory of molecular evolution most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. In this model, most genetic changes in a population are thus the result of constant mutation pressure and genetic drift. This form of the neutral theory has been debated since it does not seem to fit some genetic variation seen in nature. A better-supported version of this model is the nearly neutral theory, according to which a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft. Another concept is constructive neutral evolution (CNE), which explains that complex systems can emerge and spread into a population through neutral transitions due to the principles of excess capacity, presuppression, and ratcheting, and it has been applied in areas ranging from the origins of the spliceosome to the complex interdependence of microbial communities.
The time it takes a neutral allele to become fixed by genetic drift depends on population size; fixation is more rapid in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.
It is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.
=
Mutation bias is usually conceived as a difference in expected rates for two different kinds of mutation, e.g., transition-transversion bias, GC-AT bias, deletion-insertion bias. This is related to the idea of developmental bias. J. B. S. Haldane and Ronald Fisher argued that, because mutation is a weak pressure easily overcome by selection, tendencies of mutation would be ineffectual except under conditions of neutral evolution or extraordinarily high mutation rates. This opposing-pressures argument was long used to dismiss the possibility of internal tendencies in evolution, until the molecular era prompted renewed interest in neutral evolution.
Noboru Sueoka and Ernst Freese proposed that systematic biases in mutation might be responsible for systematic differences in genomic GC composition between species. The identification of a GC-biased E. coli mutator strain in 1967, along with the proposal of the neutral theory, established the plausibility of mutational explanations for molecular patterns, which are now common in the molecular evolution literature.
For instance, mutation biases are frequently invoked in models of codon usage. Such models also include effects of selection, following the mutation-selection-drift model, which allows both for mutation biases and differential selection based on effects on translation. Hypotheses of mutation bias have played an important role in the development of thinking about the evolution of genome composition, including isochores. Different insertion vs. deletion biases in different taxa can lead to the evolution of different genome sizes. The hypothesis of Lynch regarding genome size relies on mutational biases toward increase or decrease in genome size.
However, mutational hypotheses for the evolution of composition suffered a reduction in scope when it was discovered that (1) GC-biased gene conversion makes an important contribution to composition in diploid organisms such as mammals and (2) bacterial genomes frequently have AT-biased mutation.
Contemporary thinking about the role of mutation biases reflects a different theory from that of Haldane and Fisher. More recent work showed that the original "pressures" theory assumes that evolution is based on standing variation: when evolution depends on events of mutation that introduce new alleles, mutational and developmental biases in the introduction of variation (arrival biases) can impose biases on evolution without requiring neutral evolution or high mutation rates.
Several studies report that the mutations implicated in adaptation reflect common mutation biases though others dispute this interpretation.
 Genetic hitchhiking 
Recombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.
 Sexual selection 
A special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.
Evolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed. These outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction, whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in allele frequency and adaptation. Macroevolution is the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.
A common misconception is that evolution has goals, long-term plans, or an innate tendency for "progress", as expressed in beliefs such as orthogenesis and evolutionism; realistically, however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing, and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to evolutionary research since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.
=
Adaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term adaptation for the evolutionary process and adaptive trait for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:
Adaptation is the evolutionary process whereby an organism becomes better able to live in its habitat or habitats.
Adaptedness is the state of being adapted: the degree to which an organism is able to live and reproduce in a given set of habitats.
An adaptive trait is an aspect of the developmental pattern of the organism which enables or enhances the probability of that organism surviving and reproducing.
Adaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria Escherichia coli evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, Flavobacterium evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium Sphingobium evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).
Adaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.
During evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.
However, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard Holaspis guentheri, which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.
An area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.
=
Interactions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.
=
Not all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.
Coalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.
Such cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the helping individual contains alleles which promote the helping activity, it is likely that its kin will also contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.
=
Speciation is the process where a species diverges into two or more descendant species.
There are multiple ways to define the concept of "species". The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by evolutionary biologist Ernst Mayr in 1942, the BSC states that "species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups." Despite its wide and long-term use, the BSC like other species concepts is not without controversy, for example, because genetic recombination among prokaryotes is not an intrinsic aspect of reproduction; this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.
Barriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the grey tree frog being a particularly well-studied example.
Speciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.
The second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.
The third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass Anthoxanthum odoratum, which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.
Finally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and nonrandom mating, to allow reproductive isolation to evolve.
One type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species Arabidopsis thaliana and Arabidopsis arenosa crossbred to give the new species Arabidopsis suecica. This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.
Speciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short "bursts" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.
=
Extinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future. Despite the estimated extinction of more than 99% of all species that ever lived on Earth, about 1 trillion species are estimated to be on Earth currently with only one-thousandth of 1% described.
The role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous "low-level" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.
Concepts and models used in evolutionary biology, such as natural selection, have many applications.
Artificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.
Understanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.
Evolutionary theory has many applications in medicine. Many human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as to pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.
In computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.
=
The Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as "remains of biotic life" found in 4.1 billion-year-old rocks in Western Australia. Commenting on the Australian findings, Stephen Blair Hedges wrote: "If life arose relatively quickly on Earth, then it could be common in the universe."  In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.
More than 99% of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80% not yet described.
Highly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.
=
All organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits. Fourth, organisms can be classified using these similarities into a hierarchy of nested groups, similar to a family tree.
Due to horizontal gene transfer, this "tree of life" may be more complicated than a simple branching tree, since some genes have spread independently between distantly related species. To solve this problem and others, some authors prefer to use the "Coral of life" as a metaphor or a mathematical model to illustrate the evolution of life. This view dates back to an idea briefly mentioned by Darwin but later abandoned.
Past species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, palaeontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.
More recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.
=
Prokaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6 and 2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.
The history of life was that of the unicellular eukaryotes, prokaryotes and archaea until around 1.7 billion years ago, when multicellular organisms began to appear, with differentiated cells performing specialised functions. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.
Approximately 538.8 million years ago, a remarkable amount of biological diversity appeared over a span of around 10 million years in what is called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.
About 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from "reptile"-like lineages), mammals around 129 million years ago, Homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.
=
The proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork De rerum natura (lit. 'On the Nature of Things').
=
In contrast to these materialistic views, Aristotelianism had considered all natural things as actualisations of fixed natural possibilities, known as forms. This became part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.
A number of Arab Muslim scholars wrote about evolution, most notably Ibn Khaldun, who wrote the book Muqaddimah in 1377, in which he asserted that humans developed from "the world of the monkeys", in a process by which "species become more numerous".
=
The "New Science" of the 17th century rejected the Aristotelian approach. It sought to explain natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences: the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, "species", to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.
Other naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon, suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or "filament"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's "transmutation" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level, these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the Natural Theology or Evidences of the Existence and Attributes of the Deity (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.
=
The crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin and Alfred Wallace in terms of variable populations. Darwin used the expression descent with modification rather than evolution. Partly influenced by An Essay on the Principle of Population (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a "struggle for existence" in which favourable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of "natural selection" from 1838 onwards and was writing up his "big book" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his "abstract" as On the Origin of Species explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using palaeontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.
Othniel C. Marsh, America's first palaeontologist, was the first to provide solid fossil evidence to support Darwin's theory of evolution by unearthing the ancestors of the modern horse. In 1877, Marsh delivered a very influential speech before the annual meeting of the American Association for the Advancement of Science, providing a demonstrative argument for evolution. For the first time, Marsh traced the evolution of vertebrates from fish all the way through humans. Sparing no detail, he listed a wealth of fossil examples of past life forms. The significance of this speech was immediately recognised by the scientific community, and it was printed in its entirety in several scientific journals.
In 1880, Marsh caught the attention of the scientific world with the publication of Odontornithes: a Monograph on Extinct Birds of North America, which included his discoveries of birds with teeth. These skeletons helped bridge the gap between dinosaurs and birds, and provided invaluable support for Darwin's theory of evolution. Darwin wrote to Marsh saying, "Your work on these old birds & on the many fossil animals of N. America has afforded the best support to the theory of evolution, which has appeared within the last 20 years" (since Darwin's publication of Origin of Species).
=
The mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cell's structure. De Vries was also one of the researchers who made Mendel's work well known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.
=
In the 1920s and 1930s, the modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that included random genetic drift, mutation, and gene flow. This new version of evolutionary theory focused on changes in allele frequencies in population. It explained patterns observed across species in populations, through fossil transitions in palaeontology.
=
Since then, further syntheses have extended evolution's explanatory power in the light of numerous discoveries, to cover biological phenomena across the whole of the biological hierarchy from genes to populations.
The publication of the structure of DNA by James Watson and Francis Crick with contribution of Rosalind Franklin in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved understanding of the relationship between genotype and phenotype. Advances were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that "nothing in biology makes sense except in the light of evolution", because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.
One extension, known as evolutionary developmental biology and informally called "evo-devo", emphasises how changes between generations (evolution) act on patterns of change within individual organisms (development). Since the beginning of the 21st century, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological inheritance and cultural inheritance, and evolvability.
In the 19th century, particularly after the publication of On the Origin of Species in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.
While various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of Vestiges of the Natural History of Creation in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.
The teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 Epperson v. Arkansas decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 Kitzmiller v. Dover Area School District case. The debate over Darwin's ideas did not generate significant controversy in China.
Devolution (biology) – Notion that species can revert to primitive forms
Chronospecies
Cosmic ray#Potential role in biological evolution
 
General information
"Evolution" on In Our Time at the BBC
"Evolution Resources from the National Academies". Washington, D.C.: National Academy of Sciences. Retrieved 30 May 2011.
"Understanding Evolution: your one-stop resource for information on evolution". Berkeley, California: University of California, Berkeley. Retrieved 30 May 2011.
"Evolution of Evolution – 150 Years of Darwin's 'On the Origin of Species'". Arlington County, Virginia: National Science Foundation. Archived from the original on 19 May 2011. Retrieved 30 May 2011.
"Human Evolution Timeline Interactive". Smithsonian Institution, National Museum of Natural History. 28 January 2010. Retrieved 14 July 2018. Adobe Flash required.
"History of Evolution in the United States". Salon. Retrieved 2021-08-24.
Video (1980; Cosmos animation; 8:01): "Evolution" – Carl Sagan on YouTube
Experiments
Lenski, Richard E. "Experimental Evolution". East Lansing, Michigan: Michigan State University. Retrieved 31 July 2013.
Chastain, Erick; Livnat, Adi; Papadimitriou, Christos; Vazirani, Umesh (22 July 2014). "Algorithms, games, and evolution". PNAS. 111 (29): 10620–10623. Bibcode:2014PNAS..11110620C. doi:10.1073/pnas.1406556111. ISSN 0027-8424. PMC 4115542. PMID 24979793.
Online lectures
"Evolution Matters Lecture Series". Harvard Online Learning. Cambridge, Massachusetts: Harvard University. Archived from the original on 18 December 2017. Retrieved 15 July 2018.
Stearns, Stephen C. "EEB 122: Principles of Evolution, Ecology and Behavior". Open Yale Courses. New Haven, Connecticut: Yale University. Archived from the original on 1 December 2017. Retrieved 14 July 2018.

--- TOPIC: Photosynthesis ---
Photosynthesis ( FOH-tə-SINTH-ə-sis) is a system of biological processes by which photopigment-bearing autotrophic organisms, such as most plants, algae and cyanobacteria, convert light energy — typically from sunlight — into the chemical energy necessary to fuel their metabolism. The term photosynthesis usually refers to oxygenic photosynthesis, a process that releases oxygen as a byproduct of water splitting. Photosynthetic organisms store the converted chemical energy within the bonds of intracellular organic compounds (complex compounds containing carbon), typically carbohydrates like sugars (mainly glucose, fructose and sucrose), starches, phytoglycogen and cellulose. When needing to use this stored energy, an organism's cells then metabolize the organic compounds through cellular respiration. Photosynthesis plays a critical role in producing and maintaining the oxygen content of the Earth's atmosphere, and it supplies most of the biological energy necessary for complex life on Earth.
Some organisms also perform anoxygenic photosynthesis, which does not produce oxygen. Some bacteria (e.g. purple bacteria) use bacteriochlorophyll to split hydrogen sulfide as a reductant instead of water, releasing sulfur instead of oxygen, which was a dominant form of photosynthesis in the euxinic Canfield oceans during the Boring Billion. Archaea such as Halobacterium also perform a type of non-carbon-fixing anoxygenic photosynthesis, where the simpler photopigment retinal and its microbial rhodopsin derivatives are used to absorb green light and produce a proton (hydron) gradient across the cell membrane, and the subsequent ion movement powers transmembrane proton pumps to directly synthesize adenosine triphosphate (ATP), the "energy currency" of cells. Such archaeal photosynthesis might have been the earliest form of photosynthesis that evolved on Earth, as far back as the Paleoarchean, preceding that of cyanobacteria (see Purple Earth hypothesis).
While the details may differ between species, the process always begins when light energy is absorbed by the reaction centers, proteins that contain photosynthetic pigments or chromophores. In plants, these pigments are chlorophylls (a porphyrin derivative that absorbs the red and blue spectra of light, thus reflecting green) held inside chloroplasts, abundant in leaf cells. In cyanobacteria, they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two important molecules that participate in energetic processes: reduced nicotinamide adenine dinucleotide phosphate (NADPH) and ATP.
In plants, algae, and cyanobacteria, sugars are synthesized by a subsequent sequence of light-independent reactions called the Calvin cycle. In this process, atmospheric carbon dioxide is incorporated into already existing organic compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose. In other bacteria, different mechanisms like the reverse Krebs cycle are used to achieve the same end.
The first photosynthetic organisms probably evolved early in the evolutionary history of life using reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. The average rate of energy captured by global photosynthesis is approximately 130 terawatts, which is about eight times the total power consumption of human civilization. Photosynthetic organisms also convert around 100–115 billion tons (91–104 Pg petagrams, or billions of metric tons), of carbon into biomass per year. Photosynthesis was discovered in 1779 by Jan Ingenhousz who showed that plants need light, not just soil and water.
Most photosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from carbon dioxide and water using energy from light. However, not all organisms use carbon dioxide as a source of carbon atoms to carry out photosynthesis; photoheterotrophs use organic compounds, rather than carbon dioxide, as a source of carbon.
In plants, algae, and cyanobacteria, photosynthesis releases oxygen. This oxygenic photosynthesis is by far the most common type of photosynthesis used by living organisms. Some shade-loving plants (sciophytes) produce such low levels of oxygen during photosynthesis that they use all of it themselves instead of releasing it to the atmosphere.
Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties of anoxygenic photosynthesis, used mostly by bacteria, which consume carbon dioxide but do not release oxygen or which produce elemental sulfur instead of molecular oxygen.
Carbon dioxide is converted into sugars in a process called carbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide into carbohydrates. Carbon fixation is an endothermic redox reaction. In general outline, photosynthesis is the opposite of cellular respiration: while photosynthesis is a process of reduction of carbon dioxide to carbohydrates, cellular respiration is the oxidation of carbohydrates or other nutrients to carbon dioxide. Nutrients used in cellular respiration include carbohydrates, amino acids and fatty acids. These nutrients are oxidized to produce carbon dioxide and water, and to release chemical energy to drive the organism's metabolism.
Photosynthesis and cellular respiration are distinct processes, as they take place through different sequences of chemical reactions and in different cellular compartments (cellular respiration in mitochondria).
The general equation for photosynthesis as first proposed by Cornelis van Niel is:
CO2carbondioxide + 2H2Aelectron donor + photonslight energy → [CH2O]carbohydrate + 2Aoxidizedelectrondonor + H2Owater
Since water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:
CO2carbondioxide + 2H2Owater + photonslight energy → [CH2O]carbohydrate + O2oxygen + H2Owater
This equation emphasizes that water is both a reactant in the light-dependent reaction and a product of the light-independent reaction, but canceling n water molecules from each side gives the net equation:
CO2carbondioxide + H2O water  + photonslight energy → [CH2O]carbohydrate + O2 oxygen 
Other processes substitute other compounds (such as arsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite to arsenate: The equation for this reaction is:
CO2carbondioxide + (AsO3−3)arsenite + photonslight energy → (AsO3−4)arsenate + COcarbonmonoxide(used to build other compounds in subsequent reactions)
Photosynthesis occurs in two stages. In the first stage, light-dependent reactions or light reactions capture the energy of light and use it to make the hydrogen carrier NADPH and the energy-storage molecule ATP. During the second stage, the light-independent reactions use these products to capture and reduce carbon dioxide.
Most organisms that use oxygenic photosynthesis use visible light for the light-dependent reactions, although at least three use shortwave infrared or, more specifically, far-red radiation.
Some organisms employ even more radical variants of photosynthesis. Some archaea use a simpler method that employs a pigment similar to those used for vision in animals. The bacteriorhodopsin changes its configuration in response to sunlight, acting as a proton pump. This produces a proton gradient more directly, which is then converted to chemical energy. The process does not involve carbon dioxide fixation and does not release oxygen, and seems to have evolved separately from the more common types of photosynthesis.
In photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded in cell membranes. In its simplest form, this involves the membrane surrounding the cell itself. However, the membrane may be tightly folded into cylindrical sheets called thylakoids, or bunched up into round vesicles called intracytoplasmic membranes. These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb.
In plants and algae, photosynthesis takes place in organelles called chloroplasts. A typical plant cell contains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral and peripheral membrane protein complexes of the photosynthetic system.
Plants absorb light primarily using the pigment chlorophyll. The green part of the light spectrum is not absorbed but is reflected, which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such as carotenes and xanthophylls. Algae also use chlorophyll, but various other pigments are present, such as phycocyanin, carotenes, and xanthophylls in green algae, phycoerythrin in red algae (rhodophytes) and fucoxanthin in brown algae and diatoms resulting in a wide variety of colors.
These pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.
Although all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to minimize heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.
In the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is taken up by a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases oxygen.
The overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:
Not all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above-ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.
=
In plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.
In the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram "Z-scheme"). The absorption of a photon by the antenna complex loosens an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That loosened electron is taken up by the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), a chemiosmotic potential is generated by pumping proton cations (H+) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the Z-scheme. The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the coenzyme NADP with an H+ to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.
The cyclic reaction is similar to that of the non-cyclic but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name cyclic reaction.
=
Linear electron transport through a photosystem will leave the reaction center of that photosystem oxidized. Elevating another electron will first require re-reduction of the reaction center. The excited electrons lost from the reaction center (P700) of photosystem I are replaced by transfer from plastocyanin, whose electrons come from electron transport through photosystem II. Photosystem II, as the first step of the Z-scheme, requires an external source of electrons to reduce its oxidized chlorophyll a reaction center. The source of electrons for photosynthesis in green plants and cyanobacteria is water. Two water molecules are oxidized by the energy of four successive charge-separation reactions of photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions. The electrons yielded are transferred to a redox-active tyrosine residue that is oxidized by the energy of P680+. This resets the ability of P680 to absorb another photon and release another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Kok's S-state diagrams). The hydrogen ions are released in the thylakoid lumen and therefore contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen and its energy for cellular respiration, including photosynthetic organisms.
=
In the light-independent (or "dark") reactions, the enzyme RuBisCO captures CO2 from the atmosphere and, in a process called the Calvin cycle, uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is
Carbon fixation produces the three-carbon sugar intermediate, which is then converted into the final carbohydrate products. The simple carbon sugars photosynthesis produces are then used to form other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the carbon and energy from plants is passed through a food chain.
The fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (five out of six molecules) of the glyceraldehyde 3-phosphate produced are used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus "recycled" often condense to form hexose phosphates, which ultimately yield sucrose, starch, and cellulose, as well as glucose and fructose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.
=
 On land 
In hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, CO2 will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) and decrease in carbon fixation. Some plants have evolved mechanisms to increase the CO2 concentration in the leaves under these conditions.
Plants that use the C4 carbon fixation process chemically fix carbon dioxide in the cells of the mesophyll by adding it to the three-carbon molecule phosphoenolpyruvate (PEP), a reaction catalyzed by an enzyme called PEP carboxylase, creating the four-carbon organic acid oxaloacetic acid. Oxaloacetic acid or malate synthesized by this process is then translocated to specialized bundle sheath cells where the enzyme RuBisCO and other Calvin cycle enzymes are located, and where CO2 released by decarboxylation of the four-carbon acids is then fixed by RuBisCO activity to the three-carbon 3-phosphoglyceric acids. The physical separation of RuBisCO from the oxygen-generating light reactions reduces photorespiration and increases CO2 fixation and, thus, the photosynthetic capacity of the leaf. C4 plants can produce more sugar than C3 plants in conditions of high light and temperature. Many important crop plants are C4 plants, including maize, sorghum, sugarcane, and millet. Plants that do not use PEP-carboxylase in carbon fixation are called C3 plants because the primary carboxylation reaction, catalyzed by RuBisCO, produces the three-carbon 3-phosphoglyceric acids directly in the Calvin-Benson cycle. Over 90% of plants use C3 carbon fixation, compared to 3% that use C4 carbon fixation; however, the evolution of C4 in over sixty plant lineages makes it a striking example of convergent evolution. C2 photosynthesis, which involves carbon-concentration by selective breakdown of photorespiratory glycine, is both an evolutionary precursor to C4 and a useful carbon-concentrating mechanism in its own right.
Xerophytes, such as cacti and most succulents, also use PEP carboxylase to capture carbon dioxide in a process called Crassulacean acid metabolism (CAM). In contrast to C4 metabolism, which spatially separates the CO2 fixation to PEP from the Calvin cycle, CAM temporally separates these two processes. CAM plants have a different leaf anatomy from C3 plants, and fix the CO2 at night, when their stomata are open. CAM plants store the CO2 mostly in the form of malic acid via carboxylation of phosphoenolpyruvate to oxaloacetate, which is then reduced to malate. Decarboxylation of malate during the day releases CO2 inside the leaves, thus allowing carbon fixation to 3-phosphoglycerate by RuBisCO. CAM is used by 16,000 species of plants.
Calcium-oxalate-accumulating plants, such as Amaranthus hybridus and Colobanthus quitensis, show a variation of photosynthesis where calcium oxalate crystals function as dynamic carbon pools, supplying carbon dioxide (CO2) to photosynthetic cells when stomata are partially or totally closed. This process was named alarm photosynthesis. Under stress conditions (e.g., water deficit), oxalate released from calcium oxalate crystals is converted to CO2 by an oxalate oxidase enzyme, and the produced CO2 can support the Calvin cycle reactions. Reactive hydrogen peroxide (H2O2), the byproduct of oxalate oxidase reaction, can be neutralized by catalase. Alarm photosynthesis represents a photosynthetic variant to be added to the well-known C4 and CAM pathways. However, alarm photosynthesis, in contrast to these pathways, operates as a biochemical pump that collects carbon from the organ interior (or from the soil) and not from the atmosphere.
 In water 
Cyanobacteria possess carboxysomes, which increase the concentration of CO2 around RuBisCO to increase the rate of photosynthesis. An enzyme, carbonic anhydrase, located within the carboxysome, releases CO2 from dissolved hydrocarbonate ions (HCO−3). Before the CO2 can diffuse out, RuBisCO concentrated within the carboxysome quickly sponges it up. HCO−3 ions are made from CO2 outside the cell by another carbonic anhydrase and are actively pumped into the cell by a membrane protein. They cannot cross the membrane as they are charged, and within the cytosol they turn back into CO2 very slowly without the help of carbonic anhydrase. This causes the HCO−3 ions to accumulate within the cell from where they diffuse into the carboxysomes. Pyrenoids in algae and hornworts also act to concentrate CO2 around RuBisCO.
The overall process of photosynthesis takes place in four stages:
Plants usually convert light into chemical energy with a photosynthetic efficiency of 3–6%. 
Absorbed light that is unconverted is dissipated primarily as heat, with a small fraction (1–2%) reemitted as chlorophyll fluorescence at longer (redder) wavelengths. This fact allows measurement of the light reaction of photosynthesis by using chlorophyll fluorometers.
Actual plants' photosynthetic efficiency varies with the frequency of the light being converted, light intensity, temperature, and proportion of carbon dioxide in the atmosphere, and can vary from 0.1% to 8%. By comparison, solar panels convert light into electric energy at an efficiency of approximately 6–20% for mass-produced panels, and above 40% in laboratory devices.
Scientists are studying photosynthesis in hopes of developing plants with increased yield.
The efficiency of both light and dark reactions can be measured, but the relationship between the two can be complex. For example, the light reaction creates ATP and NADPH energy molecules, which C3 plants can use for carbon fixation or photorespiration. Electrons may also flow to other electron sinks. For this reason, it is not uncommon for authors to differentiate between work done under non-photorespiratory conditions and under photorespiratory conditions.
Chlorophyll fluorescence of photosystem II can measure the light reaction, and infrared gas analyzers can measure the dark reaction. An integrated chlorophyll fluorometer and gas exchange system can investigate both light and dark reactions when researchers use the two separate systems together. Infrared gas analyzers and some moisture sensors are sensitive enough to measure the photosynthetic assimilation of CO2 and of ΔH2O using reliable methods. CO2 is commonly measured in μmols/(m2/s), parts per million, or volume per million; and H2O is commonly measured in mmols/(m2/s) or in mbars. By measuring CO2 assimilation, ΔH2O, leaf temperature, barometric pressure, leaf area, and photosynthetically active radiation (PAR), it becomes possible to estimate, "A" or carbon assimilation, "E" or transpiration, "gs" or stomatal conductance, and "Ci" or intracellular CO2. However, it is more common to use chlorophyll fluorescence for plant stress measurement, where appropriate, because the most commonly used parameters FV/FM and Y(II) or F/FM' can be measured in a few seconds, allowing the investigation of larger plant populations.
Gas exchange systems that offer control of CO2 levels, above and below ambient, allow the common practice of measurement of A/Ci curves, at different CO2 levels, to characterize a plant's photosynthetic response.
Integrated chlorophyll fluorometer – gas exchange systems allow a more precise measure of photosynthetic response and mechanisms. While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO2 levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of CC, the estimation of CO2 concentration at the site of carboxylation in the chloroplast, to replace Ci. CO2 concentration in the chloroplast becomes possible to estimate with the measurement of mesophyll conductance or gm using an integrated system.
Photosynthesis measurement systems are not designed to directly measure the amount of light the leaf absorbs, but analysis of chlorophyll fluorescence, P700- and P515-absorbance, and gas exchange measurements reveal detailed information about, e.g., the photosystems, quantum efficiency and the CO2 assimilation rates. With some instruments, even wavelength dependency of the photosynthetic efficiency can be analyzed.
A phenomenon known as quantum walk increases the efficiency of the energy transport of light significantly. In the photosynthetic cell of an alga, bacterium, or plant, there are light-sensitive molecules called chromophores arranged in an antenna-shaped structure called a photocomplex. When a photon is absorbed by a chromophore, it is converted into a quasiparticle referred to as an exciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form accessible to the cell's metabolism. The exciton's wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously "choose" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time.
Because that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances. Obstacles in the form of destructive interference cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic "hop". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks.
Fossils of what are thought to be filamentous photosynthetic organisms have been dated at 3.4 billion years old. More recent studies also suggest that photosynthesis may have begun about 3.4 billion years ago, though the first direct evidence of photosynthesis comes from thylakoid membranes preserved in 1.75-billion-year-old cherts.
Oxygenic photosynthesis is the main source of oxygen in the Earth's atmosphere, and its earliest appearance is sometimes referred to as the oxygen catastrophe. Geological evidence suggests that oxygenic photosynthesis, such as that in cyanobacteria, became important during the Paleoproterozoic era around two billion years ago. Modern photosynthesis in plants and most photosynthetic prokaryotes is oxygenic, using water as an electron donor, which is oxidized to molecular oxygen in the photosynthetic reaction center.
=
Several groups of animals have formed symbiotic relationships with photosynthetic algae. These are most common in corals, sponges, and sea anemones. Scientists presume that this is due to the particularly simple body plans and large surface areas of these animals compared to their volumes. In addition, a few marine mollusks, such as Elysia viridis and Elysia chlorotica, also maintain a symbiotic relationship with chloroplasts they capture from the algae in their diet and then store in their bodies (see Kleptoplasty). This allows the mollusks to survive solely by photosynthesis for several months at a time. Some of the genes from the plant cell nucleus have even been transferred to the slugs, so that the chloroplasts can be supplied with proteins they need to survive.
An even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosynthetic bacteria, including a circular chromosome, prokaryotic-type ribosome, and similar proteins in the photosynthetic reaction center. The endosymbiotic theory suggests that photosynthetic bacteria were acquired (by endocytosis) by early eukaryotic cells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Like mitochondria, chloroplasts possess their own DNA, separate from the nuclear DNA of their plant host cells and the genes in this chloroplast DNA resemble those found in cyanobacteria. DNA in chloroplasts codes for redox proteins such as those found in the photosynthetic reaction centers. The CoRR Hypothesis proposes that this co-location of genes with their gene products is required for redox regulation of gene expression, and accounts for the persistence of DNA in bioenergetic organelles.
=
Symbiotic and kleptoplastic organisms excluded:
The glaucophytes and the red and green algae—clade Archaeplastida (uni- and multicellular)
The cryptophytes—clade Cryptista (unicellular)
The haptophytes—clade Haptista (unicellular)
The dinoflagellates and chromerids in the superphylum Myzozoa, and Pseudoblepharisma in the phylum Ciliophora—clade Alveolata (unicellular)
The ochrophytes—clade Stramenopila (uni- and multicellular)
The chlorarachniophytes and three species of Paulinella in the phylum Cercozoa—clade Rhizaria (unicellular)
The euglenids—clade Excavata (unicellular)
Except for the euglenids, which are found within the Excavata, all of these belong to the Diaphoretickes. Archaeplastida and the photosynthetic Paulinella got their plastids, which are surrounded by two membranes, through primary endosymbiosis in two separate events, by engulfing a cyanobacterium. The plastids in all the other groups have either a red or green algal origin, and are referred to as the "red lineages" and the "green lineages". The only known exception is the ciliate Pseudoblepharisma tenue, which in addition to its plastids that originated from green algae also has a purple sulfur bacterium as symbiont. In dinoflagellates and euglenids the plastids are surrounded by three membranes, and in the remaining lines by four. A nucleomorph, remnants of the original algal nucleus located between the inner and outer membranes of the plastid, is present in the cryptophytes (from a red alga) and chlorarachniophytes (from a green alga).
Some dinoflagellates that lost their photosynthetic ability later regained it again through new endosymbiotic events with different algae.
While able to perform photosynthesis, many of these eukaryotic groups are mixotrophs and practice heterotrophy to various degrees.
=
Early photosynthetic systems, such as those in green and purple sulfur and green and purple nonsulfur bacteria, are thought to have been anoxygenic, and used various other molecules than water as electron donors. Green and purple sulfur bacteria are thought to have used hydrogen and sulfur as electron donors. Green nonsulfur bacteria used various amino and other organic acids as electron donors. Purple nonsulfur bacteria used a variety of nonspecific organic molecules. The use of these molecules is consistent with the geological evidence that Earth's early atmosphere was highly reducing at that time.
With a possible exception of Heimdallarchaeota, photosynthesis is not found in archaea. Haloarchaea are photoheterotrophic; they can absorb energy from the sun, but do not harvest carbon from the atmosphere and are therefore not photosynthetic. Instead of chlorophyll they use rhodopsins, which convert light-energy to ion gradients but cannot mediate electron transfer reactions.
In bacteria eight photosynthetic lineages are currently known:
Cyanobacteria, the only prokaryotes performing oxygenic photosynthesis and the only prokaryotes that contain two types of photosystems (type I (RCI), also known as Fe-S type, and type II (RCII), also known as quinone type). The seven remaining prokaryotes have anoxygenic photosynthesis and use versions of either type I or type II.
Chlorobi (green sulfur bacteria) Type I
Heliobacteria Type I
Chloracidobacterium Type I
Proteobacteria (purple sulfur bacteria and purple non-sulfur bacteria) Type II (see: Purple bacteria)
Chloroflexota (green non-sulfur bacteria) Type II
Gemmatimonadota Type II
Eremiobacterota Type II
=
The biochemical capacity to use water as the source for electrons in photosynthesis evolved once, in a common ancestor of extant cyanobacteria (formerly called blue-green algae). The geological record indicates that this transforming event took place early in Earth's history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier. Because the Earth's atmosphere contained almost no oxygen during the estimated development of photosynthesis, it is believed that the first photosynthetic cyanobacteria did not generate oxygen. Available evidence from geobiological studies of Archean (>2500 Ma) sedimentary rocks indicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterial evolution opened about 2000 Ma, revealing an already-diverse biota of cyanobacteria. Cyanobacteria remained the principal primary producers of oxygen throughout the Proterozoic Eon (2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable of nitrogen fixation. Green algae joined cyanobacteria as the major primary producers of oxygen on continental shelves near the end of the Proterozoic, but only with the Mesozoic (251–66 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did the primary production of oxygen in marine shelf waters take modern form. Cyanobacteria remain critical to marine ecosystems as primary producers of oxygen in oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as the plastids of marine algae.
=
Although some of the steps in photosynthesis are still not completely understood, the overall photosynthetic equation has been known since the 19th century.
Jan van Helmont began the research of the process in the mid-17th century when he carefully measured the mass of the soil a plant was using and the mass of the plant as it grew. After noticing that the soil mass changed very little, he hypothesized that the mass of the growing plant must come from the water, the only substance he added to the potted plant. His hypothesis was partially accurate – much of the gained mass comes from carbon dioxide as well as water. However, this was a signaling point to the idea that the bulk of a plant's biomass comes from the inputs of photosynthesis, not the soil itself.
Joseph Priestley, a chemist and minister, discovered that when he isolated a volume of air under an inverted jar and burned a candle in it (which gave off CO2), the candle would burn out very quickly, much before it ran out of wax. He further discovered that a mouse could similarly "injure" air. He then showed that a plant could restore the air the candle and the mouse had "injured".
In 1779, Jan Ingenhousz repeated Priestley's experiments. He discovered that it was the influence of sunlight on the plant that could cause it to revive a mouse in a matter of hours.
In 1796, Jean Senebier, a Swiss pastor, botanist, and naturalist, demonstrated that green plants consume carbon dioxide and release oxygen under the influence of light. Soon afterward, Nicolas-Théodore de Saussure showed that the increase in mass of the plant as it grows could not be due only to uptake of CO2 but also to the incorporation of water. Thus, the basic reaction by which organisms use photosynthesis to produce food (such as glucose) was outlined.
=
Cornelis Van Niel made key discoveries explaining the chemistry of photosynthesis. By studying purple sulfur bacteria and green bacteria, he was the first to demonstrate that photosynthesis is a light-dependent redox reaction in which hydrogen reduces (donates its atoms as electrons and protons to) carbon dioxide.
Robert Emerson discovered two light reactions by testing plant productivity using different wavelengths of light. With the red alone, the light reactions were suppressed. When blue and red were combined, the output was much more substantial. Thus, there were two photosystems, one absorbing up to 600 nm wavelengths, the other up to 700 nm. The former is known as PSII, the latter is PSI. PSI contains only chlorophyll "a", PSII contains primarily chlorophyll "a" with most of the available chlorophyll "b", among other pigments. These include phycobilins, which are the red and blue pigments of red and blue algae, respectively, and fucoxanthol for brown algae and diatoms. The process is most productive when the absorption of quanta is equal in both PSII and PSI, assuring that input energy from the antenna complex is divided between the PSI and PSII systems, which in turn powers the photochemistry.
Robert Hill thought that a complex of reactions consisted of an intermediate to cytochrome b6 (now a plastoquinone), and that another was from cytochrome f to a step in the carbohydrate-generating mechanisms. These are linked by plastoquinone, which does require energy to reduce cytochrome f. Further experiments to prove that the oxygen developed during the photosynthesis of green plants came from water were performed by Hill in 1937 and 1939. He showed that isolated chloroplasts give off oxygen in the presence of unnatural reducing agents like iron oxalate, ferricyanide or benzoquinone after exposure to light. In the Hill reaction:
2 H2O + 2 A + (light, chloroplasts) → 2 AH2 + O2
A is the electron acceptor. Therefore, in light, the electron acceptor is reduced and oxygen is evolved. Samuel Ruben and Martin Kamen used radioactive isotopes to determine that the oxygen liberated in photosynthesis came from the water.
Melvin Calvin and Andrew Benson, along with James Bassham, elucidated the path of carbon assimilation (the photosynthetic carbon reduction cycle) in plants. The carbon reduction cycle is known as the Calvin cycle, but many scientists refer to it as the Calvin-Benson, Benson-Calvin, or even Calvin-Benson-Bassham (or CBB) Cycle.
Nobel Prize–winning scientist Rudolph A. Marcus was later able to discover the function and significance of the electron transport chain.
Otto Heinrich Warburg and Dean Burk discovered the I-quantum photosynthesis reaction that splits CO2, activated by the respiration.
In 1950, first experimental evidence for the existence of photophosphorylation in vivo was presented by Otto Kandler using intact Chlorella cells and interpreting his findings as light-dependent ATP formation.
In 1954, Daniel I. Arnon et al. discovered photophosphorylation in vitro in isolated chloroplasts with the help of P32.
Louis N. M. Duysens and Jan Amesz discovered that chlorophyll "a" will absorb one light, oxidize cytochrome f, while chlorophyll "a" (and other pigments) will absorb another light but will reduce this same oxidized cytochrome, stating the two light reactions are in series.
=
In 1893, the American botanist Charles Reid Barnes proposed two terms, photosyntax and photosynthesis, for the biological process of synthesis of complex carbon compounds out of carbonic acid, in the presence of chlorophyll, under the influence of light. The term photosynthesis is derived from the Greek phōs (φῶς, gleam) and sýnthesis (σύνθεσις, arranging together), while another word that he designated was photosyntax, from sýntaxis (σύνταξις, configuration). Over time, the term photosynthesis came into common usage. Later discovery of anoxygenic photosynthetic bacteria and photophosphorylation necessitated redefinition of the term.
=
In the late 1940s at the University of California, Berkeley, the details of photosynthetic carbon metabolism were sorted out by the chemists Melvin Calvin, Andrew Benson, James Bassham and a score of students and researchers utilizing the carbon-14 isotope and paper chromatography techniques. The pathway of CO2 fixation by the algae Chlorella in a fraction of a second in light resulted in a three carbon molecule called phosphoglyceric acid (PGA). For that original and ground-breaking work, a Nobel Prize in Chemistry was awarded to Melvin Calvin in 1961. In parallel, plant physiologists studied leaf gas exchanges using the new method of infrared gas analysis and a leaf chamber where the net photosynthetic rates ranged from 10 to 13 μmol CO2·m−2·s−1, with the conclusion that all terrestrial plants have the same photosynthetic capacities, that are light saturated at less than 50% of sunlight.
Later in 1958–1963 at Cornell University, field grown maize was reported to have much greater leaf photosynthetic rates of 40 μmol CO2·m−2·s−1 and not be saturated at near full sunlight. This higher rate in maize was almost double of those observed in other species such as wheat and soybean, indicating that large differences in photosynthesis exist among higher plants. At the University of Arizona, detailed gas exchange research on more than 15 species of monocots and dicots uncovered for the first time that differences in leaf anatomy are crucial factors in differentiating photosynthetic capacities among species. In tropical grasses, including maize, sorghum, sugarcane, Bermuda grass and in the dicot amaranthus, leaf photosynthetic rates were around 38−40 μmol CO2·m−2·s−1, and the leaves have two types of green cells, i.e. outer layer of mesophyll cells surrounding a tightly packed cholorophyllous vascular bundle sheath cells. This type of anatomy was termed Kranz anatomy in the 19th century by the botanist Gottlieb Haberlandt while studying leaf anatomy of sugarcane. Plant species with the greatest photosynthetic rates and Kranz anatomy showed no apparent photorespiration, very low CO2 compensation point, high optimum temperature, high stomatal resistances and lower mesophyll resistances for gas diffusion and rates never saturated at full sun light. The research at Arizona was designated a Citation Classic in 1986. These species were later termed C4 plants as the first stable compound of CO2 fixation in light has four carbons as malate and aspartate. Other species that lack Kranz anatomy were termed C3 type such as cotton and sunflower, as the first stable carbon compound is the three-carbon PGA. At 1000 ppm CO2 in measuring air, both the C3 and C4 plants had similar leaf photosynthetic rates around 60 μmol CO2·m−2·s−1 indicating the suppression of photorespiration in C3 plants.
There are four main factors influencing photosynthesis and several corollary factors. The four main are:
Light irradiance and wavelength
Water absorption
Carbon dioxide concentration
Temperature.
Total photosynthesis is limited by a range of environmental factors. These include the amount of light available, the amount of leaf area a plant has to capture light (shading by other plants is a major limitation of photosynthesis), the rate at which carbon dioxide can be supplied to the chloroplasts to support photosynthesis, the availability of water, and the availability of suitable temperatures for carrying out photosynthesis.
=
The process of photosynthesis provides the main input of free energy into the biosphere, and is one of four main ways in which radiation is important for plant life.
The radiation climate within plant communities is extremely variable, in both time and space.
In the early 20th century, Frederick Blackman and Gabrielle Matthaei investigated the effects of light intensity (irradiance) and temperature on the rate of carbon assimilation.
At constant temperature, the rate of carbon assimilation varies with irradiance, increasing as the irradiance increases, but reaching a plateau at higher irradiance.
At low irradiance, increasing the temperature has little influence on the rate of carbon assimilation. At constant high irradiance, the rate of carbon assimilation increases as the temperature is increased.
These two experiments illustrate several important points: First, it is known that, in general, photochemical reactions are not affected by temperature. However, these experiments clearly show that temperature affects the rate of carbon assimilation, so there must be two sets of reactions in the full process of carbon assimilation. These are the light-dependent 'photochemical' temperature-independent stage, and the light-independent, temperature-dependent stage. Second, Blackman's experiments illustrate the concept of limiting factors. Another limiting factor is the wavelength of light. Cyanobacteria, which reside several meters underwater, cannot receive the correct wavelengths required to cause photoinduced charge separation in conventional photosynthetic pigments. To combat this problem, Cyanobacteria have a light-harvesting complex called Phycobilisome. This complex is made up of a series of proteins with different pigments which surround the reaction center.
=
As carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.
RuBisCO oxygenase activity is disadvantageous to plants for several reasons:
One product of oxygenase activity is phosphoglycolate (2 carbon) instead of 3-phosphoglycerate (3 carbon). Phosphoglycolate cannot be metabolized by the Calvin-Benson cycle and represents carbon lost from the cycle. A high oxygenase activity, therefore, drains the sugars that are required to recycle ribulose 5-bisphosphate and for the continuation of the Calvin-Benson cycle.
Phosphoglycolate is quickly metabolized to glycolate that is toxic to a plant at a high concentration; it inhibits photosynthesis.
Salvaging glycolate is an energetically expensive process that uses the glycolate pathway, and only 75% of the carbon is returned to the Calvin-Benson cycle as 3-phosphoglycerate. The reactions also produce ammonia (NH3), which is able to diffuse out of the plant, leading to a loss of nitrogen.
A highly simplified summary is:
2 glycolate + ATP → 3-phosphoglycerate + carbon dioxide + ADP + NH3
The salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.
=
=
A collection of photosynthesis pages for all levels from a renowned expert (Govindjee)
In depth, advanced treatment of photosynthesis, also from Govindjee
Science Aid: Photosynthesis Article appropriate for high school science
Metabolism, Cellular Respiration and Photosynthesis – The Virtual Library of Biochemistry and Cell Biology
Overall examination of Photosynthesis at an intermediate level
Overall Energetics of Photosynthesis
The source of oxygen produced by photosynthesis Interactive animation, a textbook tutorial
Marshall J (2011-03-29). "First practical artificial leaf makes debut". Discovery News. Archived from the original on 2012-03-22. Retrieved 2011-03-29.
Photosynthesis – Light Dependent & Light Independent Stages Archived 2011-09-10 at the Wayback Machine
Khan Academy, video introduction

--- TOPIC: Chemistry ---
Chemistry is the scientific study of the properties and behavior of matter. It is a physical science within the natural sciences that studies the chemical elements that make up matter and compounds made of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during reactions with other substances. Chemistry also addresses the nature of chemical bonds in chemical compounds.
In the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the Moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).
Chemistry has existed under various names since ancient times. It has evolved, and now chemistry encompasses various areas of specialisation, or subdisciplines, that continue to increase in number and interrelate to create further interdisciplinary fields of study. The applications of various fields of chemistry are used frequently for economic purposes in the chemical industry.
The word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.
The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'.
The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory.
The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.
A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws.
Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are:
=
In chemistry, matter is defined as anything that has rest mass and volume (it takes up space) and is made up of particles. The particles that make up matter have rest mass as well – not all particles have rest mass, such as the photon. Matter can be a pure chemical substance or a mixture of substances.
 Atom 
The atom is the basic unit of chemistry. It consists of a dense core called the atomic nucleus surrounded by a space occupied by an electron cloud. The nucleus is made up of positively charged protons and uncharged neutrons (together called nucleons), while the electron cloud consists of negatively charged electrons which orbit the nucleus. In a neutral atom, the negatively charged electrons balance out the positive charge of the protons. The nucleus is dense; the mass of a nucleon is approximately 1,836 times that of an electron, yet the radius of an atom is about 10,000 times that of its nucleus.
The atom is also the smallest entity that can be envisaged to retain the chemical properties of the element, such as electronegativity, ionization potential, preferred oxidation state(s), coordination number, and preferred types of bonds to form (e.g., metallic, ionic, covalent).
 Element 
A chemical element is a pure substance which is composed of a single type of atom, characterized by its particular number of protons in the nuclei of its atoms, known as the atomic number and represented by the symbol Z. The mass number is the sum of the number of protons and neutrons in a nucleus. Although all the nuclei of all atoms belonging to one element will have the same atomic number, they may not necessarily have the same mass number; atoms of an element which have different mass numbers are known as isotopes. For example, all atoms with 6 protons in their nuclei are atoms of the chemical element carbon, but atoms of carbon may have mass numbers of 12 or 13.
The standard presentation of the chemical elements is in the periodic table, which orders elements by atomic number. The periodic table is arranged in groups, or columns, and periods, or rows. The periodic table is useful in identifying periodic trends.
 Compound 
A compound is a pure chemical substance composed of more than one element. The properties of a compound bear little similarity to those of its elements. The standard nomenclature of compounds is set by the International Union of Pure and Applied Chemistry (IUPAC). Organic compounds are named according to the organic nomenclature system. The names for inorganic compounds are created according to the inorganic nomenclature system. When a compound has more than one component, then they are divided into two classes, the electropositive and the electronegative components. In addition the Chemical Abstracts Service (CAS) has devised a method to index chemical substances. In this scheme each chemical substance is identifiable by a number known as its CAS registry number.
 Molecule 
A molecule is the smallest indivisible portion of a pure chemical substance that has its unique set of chemical properties, that is, its potential to undergo a certain set of chemical reactions with other substances. However, this definition only works well for substances that are composed of molecules, which is not true of many substances (see below). Molecules are typically a set of atoms bound together by covalent bonds, such that the structure is electrically neutral and all valence electrons are paired with other electrons either in bonds or in lone pairs.
Thus, molecules exist as electrically neutral units, unlike ions. When this rule is broken, giving the "molecule" a charge, the result is sometimes named a molecular ion or a polyatomic ion. However, the discrete and separate nature of the molecular concept usually requires that molecular ions be present only in well-separated form, such as a directed beam in a vacuum in a mass spectrometer. Charged polyatomic collections residing in solids (for example, common sulfate or nitrate ions) are generally not considered "molecules" in chemistry. Some molecules contain one or more unpaired electrons, creating radicals. Most radicals are comparatively reactive, but some, such as nitric oxide (NO) can be stable.
The "inert" or noble gas elements (helium, neon, argon, krypton, xenon and radon) are composed of lone atoms as their smallest discrete unit, but the other isolated chemical elements consist of either molecules or networks of atoms bonded to each other in some way. Identifiable molecules compose familiar substances such as water, air, and many organic compounds like alcohol, sugar, gasoline, and the various pharmaceuticals.
However, not all substances or chemical compounds consist of discrete molecules, and indeed most of the solid substances that make up the solid crust, mantle, and core of the Earth are chemical compounds without molecules. These other types of substances, such as ionic compounds and network solids, are organized in such a way as to lack the existence of identifiable molecules per se. Instead, these substances are discussed in terms of formula units or unit cells as the smallest repeating structure within the substance. Examples of such substances are mineral salts (such as table salt), solids like carbon and diamond, metals, and familiar silica and silicate minerals such as quartz and granite.
One of the main characteristics of a molecule is its geometry often called its structure. While the structure of diatomic, triatomic or tetra-atomic molecules may be trivial, (linear, angular pyramidal etc.) the structure of polyatomic molecules, that are constituted of more than six atoms (of several elements) can be crucial for its chemical nature.
 Substance and mixture 
A chemical substance is a kind of matter with a definite composition and set of properties. A collection of substances is called a mixture. Examples of mixtures are air and alloys.
 Mole and amount of substance 
The mole is a unit of measurement that denotes an amount of substance (also called chemical amount). One mole is defined to contain exactly 6.02214076×1023 particles (atoms, molecules, ions, or electrons), where the number of particles per mole is known as the Avogadro constant. Molar concentration is the amount of a particular substance per volume of solution, and is commonly reported in mol/dm3.
=
In addition to the specific chemical properties that distinguish different chemical classifications, chemicals can exist in several phases. For the most part, the chemical classifications are independent of these bulk phase classifications; however, some more exotic phases are incompatible with certain chemical properties. A phase is a set of states of a chemical system that have similar bulk structural properties, over a range of conditions, such as pressure or temperature.
Physical properties, such as density and refractive index tend to fall within values characteristic of the phase. The phase of matter is defined by the phase transition, which is when energy put into or taken out of the system goes into rearranging the structure of the system, instead of changing the bulk conditions.
Sometimes the distinction between phases can be continuous instead of having a discrete boundary; in this case the matter is considered to be in a supercritical state. When three states meet based on the conditions, it is known as a triple point and since this is invariant, it is a convenient way to define a set of conditions.
The most familiar examples of phases are solids, liquids, and gases. Many substances exhibit multiple solid phases. For example, there are three phases of solid iron (alpha, gamma, and delta) that vary based on temperature and pressure. A principal difference between solid phases is the crystal structure, or arrangement, of the atoms. Another phase commonly encountered in the study of chemistry is the aqueous phase, which is the state of substances dissolved in aqueous solution (that is, in water).
Less familiar phases include plasmas, Bose–Einstein condensates and fermionic condensates and the paramagnetic and ferromagnetic phases of magnetic materials. While most familiar phases deal with three-dimensional systems, it is also possible to define analogs in two-dimensional systems, which has received attention for its relevance to systems in biology.
=
Atoms sticking together in molecules or crystals are said to be bonded with one another. A chemical bond may be visualized as the multipole balance between the positive charges in the nuclei and the negative charges oscillating about them. More than simple attraction and repulsion, the energies and distributions characterize the availability of an electron to bond to another atom.
The chemical bond can be a covalent bond, an ionic bond, a hydrogen bond or just because of Van der Waals force. Each of these kinds of bonds is ascribed to some potential. These potentials create the interactions which hold atoms together in molecules or crystals. In many simple compounds, valence bond theory, the Valence Shell Electron Pair Repulsion model (VSEPR), and the concept of oxidation number can be used to explain molecular structure and composition.
An ionic bond is formed when a metal loses one or more of its electrons, becoming a positively charged cation, and the electrons are then gained by the non-metal atom, becoming a negatively charged anion. The two oppositely charged ions attract one another, and the ionic bond is the electrostatic force of attraction between them. For example, sodium (Na), a metal, loses one electron to become an Na+ cation while chlorine (Cl), a non-metal, gains this electron to become Cl−. The ions are held together due to electrostatic attraction, and that compound sodium chloride (NaCl), or common table salt, is formed.
In a covalent bond, one or more pairs of valence electrons are shared by two atoms: the resulting electrically neutral group of bonded atoms is termed a molecule. Atoms will share valence electrons in such a way as to create a noble gas electron configuration (eight electrons in their outermost shell) for each atom. Atoms that tend to combine in such a way that they each have eight electrons in their valence shell are said to follow the octet rule. However, some elements like hydrogen and lithium need only two electrons in their outermost shell to attain this stable configuration; these atoms are said to follow the duet rule, and in this way they are reaching the electron configuration of the noble gas helium, which has two electrons in its outer shell.
Similarly, theories from classical physics can be used to predict many ionic structures. With more complicated compounds, such as metal complexes, valence bond theory is less applicable and alternative approaches, such as the molecular orbital theory, are generally used.
=
In the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structures, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants.
A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. A reaction is said to be exothermic if the reaction releases heat to the surroundings; in the case of endothermic reactions, the reaction absorbs heat from the surroundings.
Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The speed of a chemical reaction (at given temperature T) is related to the activation energy E, by the Boltzmann's population factor 
  
    
      
        
          e
          
            −
            E
            
              /
            
            k
            T
          
        
      
    
    {\displaystyle e^{-E/kT}}
  
 – that is the probability of a molecule to have energy greater than or equal to E at the given temperature T. This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation. The activation energy necessary for a chemical reaction to occur can be in the form of heat, light, electricity or mechanical force in the form of ultrasound.
A related concept free energy, which also incorporates entropy considerations, is a very useful means for predicting the feasibility of a reaction and determining the state of equilibrium of a chemical reaction, in chemical thermodynamics. A reaction is feasible only if the total change in the Gibbs free energy is negative, 
  
    
      
        Δ
        G
        ≤
        0
        
      
    
    {\displaystyle \Delta G\leq 0\,}
  
; if it is equal to zero the chemical reaction is said to be at equilibrium.
There exist only limited possible states of energy for electrons, atoms and molecules. These are determined by the rules of quantum mechanics, which require quantization of energy of a bound system. The atoms/molecules in a higher energy state are said to be excited. The molecules/atoms of substance in an excited energy state are often much more reactive; that is, more amenable to chemical reactions.
The phase of a substance is invariably determined by its energy and the energy of its surroundings. When the intermolecular forces of a substance are such that the energy of the surroundings is not sufficient to overcome them, it occurs in a more ordered phase like liquid or solid as is the case with water (H2O); a liquid at room temperature because its molecules are bound by hydrogen bonds. Whereas hydrogen sulfide (H2S) is a gas at room temperature and standard pressure, as its molecules are bound by weaker dipole–dipole interactions.
The transfer of energy from one chemical substance to another depends on the size of energy quanta emitted from one substance. However, heat energy is often transferred more easily from almost any substance to another because the phonons responsible for vibrational and rotational energy levels in a substance have much less energy than photons invoked for the electronic energy transfer. Thus, because vibrational and rotational energy levels are more closely spaced than electronic energy levels, heat is more easily transferred between substances relative to light or other forms of electronic energy. For example, ultraviolet electromagnetic radiation is not transferred with as much efficacy from one substance to another as thermal or electrical energy.
The existence of characteristic energy levels for different chemical substances is useful for their identification by the analysis of spectral lines. Different kinds of spectra are often used in chemical spectroscopy, e.g. IR, microwave, NMR, ESR, etc. Spectroscopy is also used to identify the composition of remote objects – like stars and distant galaxies – by analyzing their radiation spectra.
The term chemical energy is often used to indicate the potential of a chemical substance to undergo a transformation through a chemical reaction or to transform other chemical substances.
=
When a chemical substance is transformed as a result of its interaction with another substance or with energy, a chemical reaction is said to have occurred. A chemical reaction is therefore a concept related to the "reaction" of a substance when it comes in close contact with another, whether as a mixture or a solution; exposure to some form of energy, or both. It results in some energy exchange between the constituents of the reaction as well as with the system environment, which may be designed vessels—often laboratory glassware.
Chemical reactions can result in the formation or dissociation of molecules, that is, molecules breaking apart to form two or more molecules or rearrangement of atoms within or across molecules. Chemical reactions usually involve the making or breaking of chemical bonds. Oxidation, reduction, dissociation, acid–base neutralization and molecular rearrangement are some examples of common chemical reactions.
A chemical reaction can be symbolically depicted through a chemical equation. While in a non-nuclear chemical reaction the number and kind of atoms on both sides of the equation are equal, for a nuclear reaction this holds true only for the nuclear particles viz. protons and neutrons.
The sequence of steps in which the reorganization of chemical bonds may be taking place in the course of a chemical reaction is called its mechanism. A chemical reaction can be envisioned to take place in a number of steps, each of which may have a different speed. Many reaction intermediates with variable stability can thus be envisaged during the course of a reaction. Reaction mechanisms are proposed to explain the kinetics and the relative product mix of a reaction. Many physical chemists specialize in exploring and proposing the mechanisms of various chemical reactions. Several empirical rules, like the Woodward–Hoffmann rules often come in handy while proposing a mechanism for a chemical reaction.
According to the IUPAC gold book, a chemical reaction is "a process that results in the interconversion of chemical species." Accordingly, a chemical reaction may be an elementary reaction or a stepwise reaction. An additional caveat is made, in that this definition includes cases where the interconversion of conformers is experimentally observable. Such detectable chemical reactions normally involve sets of molecular entities as indicated by this definition, but it is often conceptually convenient to use the term also for changes involving single molecular entities (i.e. 'microscopic chemical events').
=
An ion is a charged species, an atom or a molecule, that has lost or gained one or more electrons. When an atom loses an electron and thus has more protons than electrons, the atom is a positively charged ion or cation. When an atom gains an electron and thus has more electrons than protons, the atom is a negatively charged ion or anion. Cations and anions can form a crystalline lattice of neutral salts, such as the Na+ and Cl− ions forming sodium chloride, or NaCl. Examples of polyatomic ions that do not split up during acid–base reactions are hydroxide (OH−) and phosphate (PO43−).
Plasma is composed of gaseous matter that has been completely ionized, usually through high temperature.
=
A substance can often be classified as an acid or a base. There are several different theories which explain acid–base behavior. The simplest is Arrhenius theory, which states that an acid is a substance that produces hydronium ions when it is dissolved in water, and a base is one that produces hydroxide ions when dissolved in water. According to Brønsted–Lowry acid–base theory, acids are substances that donate a positive hydrogen ion to another substance in a chemical reaction; by extension, a base is the substance which receives that hydrogen ion.
A third common theory is Lewis acid–base theory, which is based on the formation of new chemical bonds. Lewis theory explains that an acid is a substance which is capable of accepting a pair of electrons from another substance during the process of bond formation, while a base is a substance which can provide a pair of electrons to form a new bond. There are several other ways in which a substance may be classified as an acid or a base, as is evident in the history of this concept.
Acid strength is commonly measured by two methods. One measurement, based on the Arrhenius definition of acidity, is pH, which is a measurement of the hydronium ion concentration in a solution, as expressed on a negative logarithmic scale. Thus, solutions that have a low pH have a high hydronium ion concentration and can be said to be more acidic. The other measurement, based on the Brønsted–Lowry definition, is the acid dissociation constant (Ka), which measures the relative ability of a substance to act as an acid under the Brønsted–Lowry definition of an acid. That is, substances with a higher Ka are more likely to donate hydrogen ions in chemical reactions than those with lower Ka values.
=
Redox (reduction-oxidation) reactions include all chemical reactions in which atoms have their oxidation state changed by either gaining electrons (reduction) or losing electrons (oxidation). Substances that have the ability to oxidize other substances are said to be oxidative and are known as oxidizing agents, oxidants or oxidizers. An oxidant removes electrons from another substance. Similarly, substances that have the ability to reduce other substances are said to be reductive and are known as reducing agents, reductants, or reducers.
A reductant transfers electrons to another substance and is thus oxidized itself. And because it "donates" electrons it is also called an electron donor. Oxidation and reduction properly refer to a change in oxidation number—the actual transfer of electrons may never occur. Thus, oxidation is better defined as an increase in oxidation number, and reduction as a decrease in oxidation number.
=
Although the concept of equilibrium is widely used across sciences, in the context of chemistry, it arises whenever a number of different states of the chemical composition are possible, as for example, in a mixture of several chemical compounds that can react with one another, or when a substance can be present in more than one kind of phase.
A system of chemical substances at equilibrium, even though having an unchanging composition, is most often not static; molecules of the substances continue to react with one another thus giving rise to a dynamic equilibrium. Thus the concept describes the state in which the parameters such as chemical composition remain unchanged over time.
=
Chemical reactions are governed by certain laws, which have become fundamental concepts in chemistry. Some of them are:
The history of chemistry spans a period from the ancient past to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze.
Chemistry was preceded by its protoscience, alchemy, which operated a non-scientific approach to understanding the constituents of matter and their interactions. Despite being unsuccessful in explaining the nature of matter and its transformations, alchemists set the stage for modern chemistry by performing experiments and recording the results. Robert Boyle, although skeptical of elements and convinced of alchemy, played a key part in elevating the "sacred art" as an independent, fundamental and philosophical discipline in his work The Sceptical Chymist (1661).
While both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry, as a body of knowledge distinct from alchemy, became an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena. The history of chemistry afterwards is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.
=
The definition of chemistry has changed over time, as new discoveries and theories add to the functionality of the science. The term "chymistry", in the view of noted scientist Robert Boyle in 1661, meant the subject of the material principles of mixed bodies. In 1663, the chemist Christopher Glaser described "chymistry" as a scientific art, by which one learns to dissolve bodies, and draw from them the different substances on their composition, and how to unite them again, and exalt them to a higher perfection.
The 1730 definition of the word "chemistry", as used by Georg Ernst Stahl, meant the art of resolving mixed, compound, or aggregate bodies into their principles; and of composing such bodies from those principles. In 1837, Jean-Baptiste Dumas considered the word "chemistry" to refer to the science concerned with the laws and effects of molecular forces. This definition further evolved until, in 1947, it came to mean the science of substances: their structure, their properties, and the reactions that change them into other substances—a characterization accepted by Linus Pauling. More recently, in 1998, Professor Raymond Chang broadened the definition of "chemistry" to mean the study of matter and the changes it undergoes.
=
Early civilizations, such as the Egyptians, Babylonians, and Indians, amassed practical knowledge concerning the arts of metallurgy, pottery and dyes, but did not develop a systematic theory.
A basic chemical hypothesis first emerged in Classical Greece with the theory of four elements as propounded definitively by Aristotle stating that fire, air, earth and water were the fundamental elements from which everything is formed as a combination. Greek atomism dates back to 440 BC, arising in works by philosophers such as Democritus and Epicurus. In 50 BCE, the Roman philosopher Lucretius expanded upon the theory in his poem De rerum natura (On The Nature of Things). Unlike modern concepts of science, Greek atomism was purely philosophical in nature, with little concern for empirical observations and no concern for chemical experiments.
An early form of the idea of conservation of mass is the notion that "Nothing comes from nothing" in Ancient Greek philosophy, which can be found in Empedocles (approx. 4th century BC): "For it is impossible for anything to come to be from what is not, and it cannot be brought about or heard of that what is should be utterly destroyed." and Epicurus (3rd century BC), who, describing the nature of the Universe, wrote that "the totality of things was always such as it is now, and always will be".
In the Hellenistic world the art of alchemy first proliferated, mingling magic and occultism into the study of natural substances with the ultimate goal of transmuting elements into gold and discovering the elixir of eternal life. Work, particularly the development of distillation, continued in the early Byzantine period with the most famous practitioner being the 4th century Greek-Egyptian Zosimos of Panopolis. Alchemy continued to be developed and practised throughout the Arab world after the Muslim conquests, and from there, and from the Byzantine remnants, diffused into medieval and Renaissance Europe through Latin translations.
The Arabic works attributed to Jabir ibn Hayyan introduced a systematic classification of chemical substances, and provided instructions for deriving an inorganic compound (sal ammoniac or ammonium chloride) from organic substances (such as plants, blood, and hair) by chemical means. Some Arabic Jabirian works (e.g., the "Book of Mercy", and the "Book of Seventy") were later translated into Latin under the Latinized name "Geber", and in 13th-century Europe an anonymous writer, usually referred to as pseudo-Geber, started to produce alchemical and metallurgical writings under this name. Later influential Muslim philosophers, such as Abū al-Rayhān al-Bīrūnī and Avicenna disputed the theories of alchemy, particularly the theory of the transmutation of metals.
Improvements of the refining of ores and their extractions to smelt metals was widely used source of information for early chemists in the 16th century, among them Georg Agricola (1494–1555), who published his major work De re metallica in 1556. His work, describing highly developed and complex processes of mining metal ores and metal extraction, were the pinnacle of metallurgy during that time. His approach removed all mysticism associated with the subject, creating the practical base upon which others could and would build. The work describes the many kinds of furnaces used to smelt ore, and stimulated interest in minerals and their composition. Agricola has been described as the "father of metallurgy" and the founder of geology as a scientific discipline.
Under the influence of the Scientific Revolution and its new empirical methods propounded by Sir Francis Bacon and others, a group of chemists at Oxford, Robert Boyle, Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline. Boyle in particular questioned some commonly held chemical theories and argued for chemical practitioners to be more "philosophical" and less commercially focused in The Sceptical Chemyst. He formulated Boyle's law, rejected the classical "four elements" and proposed a mechanistic alternative of atoms and chemical reactions that could be subject to rigorous experiment.
In the following decades, many important discoveries were made, such as the nature of 'air' which was discovered to be composed of many different gases. The Scottish chemist Joseph Black and the Flemish Jan Baptist van Helmont discovered carbon dioxide, or what Black called 'fixed air' in 1754; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and, independently, Carl Wilhelm Scheele isolated pure oxygen. The theory of phlogiston (a substance at the root of all combustion) was propounded by the German Georg Ernst Stahl in the early 18th century and was only overturned by the end of the century by the French chemist Antoine Lavoisier, the chemical analogue of Newton in physics. Lavoisier did more than any other to establish the new science on proper theoretical footing, by elucidating the principle of conservation of mass and developing a new system of chemical nomenclature used to this day.
English scientist John Dalton proposed the modern theory of atoms; that all substances are composed of indivisible 'atoms' of matter and that different atoms have varying atomic weights.
The development of the electrochemical theory of chemical combinations occurred in the early 19th century as the result of the work of two scientists in particular, Jöns Jacob Berzelius and Humphry Davy, made possible by the prior invention of the voltaic pile by Alessandro Volta. Davy discovered nine new elements including the alkali metals by extracting them from their oxides with electric current.
British William Prout first proposed ordering all the elements by their atomic weight as all atoms had a weight that was an exact multiple of the atomic weight of hydrogen. J.A.R. Newlands devised an early table of elements, which was then developed into the modern periodic table of elements in the 1860s by Dmitri Mendeleev and independently by several other scientists including Julius Lothar Meyer. The inert gases, later called the noble gases were discovered by William Ramsay in collaboration with Lord Rayleigh at the end of the century, thereby filling in the basic structure of the table.
Organic chemistry was developed by Justus von Liebig and others, following Friedrich Wöhler's synthesis of urea. Other crucial 19th century advances were; an understanding of valence bonding (Edward Frankland in 1852) and the application of thermodynamics to chemistry (J. W. Gibbs and Svante Arrhenius in the 1870s).
At the turn of the twentieth century the theoretical underpinnings of chemistry were finally understood due to a series of remarkable discoveries that succeeded in probing and discovering the very nature of the internal structure of atoms. In 1897, J.J. Thomson of the University of Cambridge discovered the electron and soon after the French scientist Becquerel as well as the couple Pierre and Marie Curie investigated the phenomenon of radioactivity. In a series of pioneering scattering experiments Ernest Rutherford at the University of Manchester discovered the internal structure of the atom and the existence of the proton, classified and explained the different types of radioactivity and successfully transmuted the first element by bombarding nitrogen with alpha particles.
His work on atomic structure was improved on by his students, the Danish physicist Niels Bohr, the Englishman Henry Moseley and the German Otto Hahn, who went on to father the emerging nuclear chemistry and discovered nuclear fission. The electronic theory of chemical bonds and molecular orbitals was developed by the American scientists Linus Pauling and Gilbert N. Lewis.
The year 2011 was declared by the United Nations as the International Year of Chemistry. It was an initiative of the International Union of Pure and Applied Chemistry, and of the United Nations Educational, Scientific, and Cultural Organization and involves chemical societies, academics, and institutions worldwide and relied on individual initiatives to organize local and regional activities.
In the practice of chemistry, pure chemistry is the study of the fundamental principles of chemistry, while applied chemistry applies that knowledge to develop technology and solve real-world problems.
=
Chemistry is typically divided into several major sub-disciplines. There are also several main cross-disciplinary and more specialized fields of chemistry.
Analytical chemistry is the analysis of material samples to gain an understanding of their chemical composition and structure. Analytical chemistry incorporates standardized experimental methods in chemistry. These methods may be used in all subdisciplines of chemistry, excluding purely theoretical chemistry.
Biochemistry is the study of the chemicals, chemical reactions and interactions that take place at a molecular level in living organisms. Biochemistry is highly interdisciplinary, covering medicinal chemistry, neurochemistry, molecular biology, forensics, plant science and genetics.
Inorganic chemistry is the study of the properties and reactions of inorganic compounds, such as metals and minerals. The distinction between organic and inorganic disciplines is not absolute and there is much overlap, most importantly in the sub-discipline of organometallic chemistry.
Materials chemistry is the preparation, characterization, and understanding of solid state components or devices with a useful current or future function. The field is a new breadth of study in graduate programs, and it integrates elements from all classical areas of chemistry like organic chemistry, inorganic chemistry, and crystallography with a focus on fundamental issues that are unique to materials. Primary systems of study include the chemistry of condensed phases (solids, liquids, polymers) and interfaces between different phases.
Neurochemistry is the study of neurochemicals; including transmitters, peptides, proteins, lipids, sugars, and nucleic acids; their interactions, and the roles they play in forming, maintaining, and modifying the nervous system.
Nuclear chemistry is the study of how subatomic particles come together and make nuclei. Modern transmutation is a large component of nuclear chemistry, and the table of nuclides is an important result and tool for this field. In addition to medical applications, nuclear chemistry encompasses nuclear engineering which explores the topic of using nuclear power sources for generating energy.
Organic chemistry is the study of the structure, properties, composition, mechanisms, and reactions of organic compounds. An organic compound is defined as any compound based on a carbon skeleton. Organic compounds can be classified, organized and understood in reactions by their functional groups, unit atoms or molecules that show characteristic chemical properties in a compound.
Physical chemistry is the study of the physical and fundamental basis of chemical systems and processes. In particular, the energetics and dynamics of such systems and processes are of interest to physical chemists. Important areas of study include chemical thermodynamics, chemical kinetics, electrochemistry, statistical mechanics, spectroscopy, and more recently, astrochemistry. Physical chemistry has large overlap with molecular physics. Physical chemistry involves the use of infinitesimal calculus in deriving equations. It is usually associated with quantum chemistry and theoretical chemistry. Physical chemistry is a distinct discipline from chemical physics, but again, there is very strong overlap.
Theoretical chemistry is the study of chemistry via fundamental theoretical reasoning (usually within mathematics or physics). In particular the application of quantum mechanics to chemistry is called quantum chemistry. Since the end of the Second World War, the development of computers has allowed a systematic development of computational chemistry, which is the art of developing and applying computer programs for solving chemical problems. Theoretical chemistry has large overlap with (theoretical and experimental) condensed matter physics and molecular physics.
Other subdivisions include electrochemistry, femtochemistry, flavor chemistry, flow chemistry, immunohistochemistry, hydrogenation chemistry, mathematical chemistry, molecular mechanics, natural product chemistry, organometallic chemistry, petrochemistry, photochemistry, physical organic chemistry, polymer chemistry, radiochemistry, sonochemistry, supramolecular chemistry, synthetic chemistry, and many others.
=
Interdisciplinary fields include agrochemistry, astrochemistry (and cosmochemistry), atmospheric chemistry, chemical engineering, chemical biology, chemo-informatics, environmental chemistry, geochemistry, green chemistry, immunochemistry, marine chemistry, materials science, mechanochemistry, medicinal chemistry, molecular biology, nanotechnology, oenology, pharmacology, phytochemistry, solid-state chemistry, surface science, thermochemistry, and many others.
=
The chemical industry represents an important economic activity worldwide. The global top 50 chemical producers in 2013 had sales of US$980.5 billion with a profit margin of 10.3%.
=
Popular reading
Atkins, P. W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8
Atkins, P. W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8
Kean, Sam. The Disappearing Spoon – and Other True Tales from the Periodic Table (Black Swan) London, England, 2010 ISBN 978-0-552-77750-6
Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7
Stwertka, A. A Guide to the Elements (Oxford University Press) ISBN 0-19-515027-9
"Dictionary of the History of Ideas". Archived from the original on 10 March 2008.
"Chemistry" . Encyclopædia Britannica. Vol. 6 (11th ed.). 1911. pp. 33–76.
Introductory undergraduate textbooks
Atkins, P.W., Overton, T., Rourke, J., Weller, M. and Armstrong, F. Shriver and Atkins Inorganic Chemistry (4th ed.) 2006 (Oxford University Press) ISBN 0-19-926463-5
Chang, Raymond. Chemistry 6th ed. Boston, Massachusetts: James M. Smith, 1998. ISBN 0-07-115221-0
Clayden, Jonathan; Greeves, Nick; Warren, Stuart; Wothers, Peter (2001). Organic Chemistry (1st ed.). Oxford University Press. ISBN 978-0-19-850346-0.
Voet and Voet. Biochemistry (Wiley) ISBN 0-471-58651-X
Advanced undergraduate-level or graduate textbooks
Atkins, P. W. Physical Chemistry (Oxford University Press) ISBN 0-19-879285-9
Atkins, P. W. et al. Molecular Quantum Mechanics (Oxford University Press)
McWeeny, R. Coulson's Valence (Oxford Science Publications) ISBN 0-19-855144-4
Pauling, L. The Nature of the chemical bond (Cornell University Press) ISBN 0-8014-0333-2
Pauling, L., and Wilson, E. B. Introduction to Quantum Mechanics with Applications to Chemistry (Dover Publications) ISBN 0-486-64871-0
Smart and Moore. Solid State Chemistry: An Introduction (Chapman and Hall) ISBN 0-412-40040-5
Stephenson, G. Mathematical Methods for Science Students (Longman) ISBN 0-582-44416-0
General Chemistry principles, patterns and applications.

--- TOPIC: Atom ---
Atoms are the basic particles of the chemical elements and the fundamental building blocks of matter. An atom consists of a nucleus of protons and generally neutrons, surrounded by an electromagnetically bound swarm of electrons.  The chemical elements are distinguished from each other by the number of protons that are in their atoms. For example, any atom that contains 11 protons is sodium, and any atom that contains 29 protons is copper. Atoms with the same number of protons but a different number of neutrons are called isotopes of the same element.
Atoms are extremely small, typically around 100 picometers across. A human hair is about a million carbon atoms wide. Atoms are smaller than the shortest wavelength of visible light, which means humans cannot see atoms with conventional microscopes. They are so small that accurately predicting their behavior using classical physics is not possible due to quantum effects.
More than 99.94% of an atom's mass is in the nucleus. Protons have a positive electric charge and neutrons have no charge, so the nucleus is positively charged. The electrons are negatively charged, and this opposing charge is what binds them to the nucleus. If the numbers of protons and electrons are equal, as they normally are, then the atom is electrically neutral as a whole. A charged atom is called an ion. If an atom has more electrons than protons, then it has an overall negative charge and is called a negative ion (or anion). Conversely, if it has more protons than electrons, it has a positive charge and is called a positive ion (or cation).
The electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.
Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules or crystals. The ability of atoms to attach and detach from each other is responsible for most of the physical changes observed in nature. Chemistry is the science that studies these changes.
The word atom is derived from the ancient Greek word atomos, which means "uncuttable". However, this ancient idea was based in philosophical reasoning rather than scientific reasoning. Modern atomic theory is not based on these old concepts. In the early 19th century, the scientist John Dalton found evidence that matter really is composed of discrete units, and so applied the word atom to those units.
=
The basic idea that matter is made up of tiny indivisible particles is an old idea that appeared in many ancient cultures. In ancient Greece, the philosophers Leucippus and his student Democritus proposed that all matter consists of indivisible units called atomos..A similar idea developed in ancient India with Kanāda of the Vaiśeṣika school, who proposed indivisible particles (paramāṇu), while later Buddhist Abhidharma traditions also articulated atomistic views emphasizing momentary and causally dependent material phenomena. Although these early concepts differ fundamentally from the modern scientific understanding of atoms, both Greek and Indian philosophical traditions included speculative ideas that matter is composed of extremely small, indivisible units, developed independently of the experimental and quantitative framework later used in modern atomic theory.
=
In the early 1800s, John Dalton compiled experimental data gathered by him and other scientists and discovered a pattern now known as the "law of multiple proportions". He noticed that in any group of chemical compounds which all contain two particular chemical elements, the amount of Element A per measure of Element B will differ across these compounds by ratios of small whole numbers. This pattern suggested that each element combines with other elements in multiples of a basic unit of weight, with each element having a unit of unique weight. Dalton decided to call these units "atoms".
For example, there are two types of tin oxide: one is a grey powder that is 88.1% tin and 11.9% oxygen, and the other is a white powder that is 78.7% tin and 21.3% oxygen. Adjusting these figures, in the grey powder there is about 13.5 g of oxygen for every 100 g of tin, and in the white powder there is about 27 g of oxygen for every 100 g of tin. 13.5 and 27 form a ratio of 1:2. Dalton concluded that in the grey oxide there is one atom of oxygen for every atom of tin, and in the white oxide there are two atoms of oxygen for every atom of tin (SnO and SnO2).
Dalton also analyzed iron oxides. There is one type of iron oxide that is a black powder which is 78.1% iron and 21.9% oxygen; and there is another iron oxide that is a red powder which is 70.4% iron and 29.6% oxygen. Adjusting these figures, in the black powder there is about 28 g of oxygen for every 100 g of iron, and in the red powder there is about 42 g of oxygen for every 100 g of iron. 28 and 42 form a ratio of 2:3. Dalton concluded that in these oxides, for every two atoms of iron, there are two or three atoms of oxygen respectively. These substances are known today as iron(II) oxide and iron(III) oxide, and their formulas are FeO and Fe2O3 respectively. Iron(II) oxide's formula is normally written as FeO, but since it is a crystalline substance we could alternately write it as Fe2O2, and when we contrast that with Fe2O3, the 2:3 ratio for the oxygen is plain to see.
As a final example: nitrous oxide is 63.3% nitrogen and 36.7% oxygen, nitric oxide is 44.05% nitrogen and 55.95% oxygen, and nitrogen dioxide is 29.5% nitrogen and 70.5% oxygen. Adjusting these figures, in nitrous oxide there is 80 g of oxygen for every 140 g of nitrogen, in nitric oxide there is about 160 g of oxygen for every 140 g of nitrogen, and in nitrogen dioxide there is 320 g of oxygen for every 140 g of nitrogen. 80, 160, and 320 form a ratio of 1:2:4. The respective formulas for these oxides are N2O, NO, and NO2.
=
In 1897, J. J. Thomson discovered that cathode rays can be deflected by electric and magnetic fields, which meant that cathode rays are not a form of light but made of electrically charged particles, and their charge was negative given the direction the particles were deflected in. He measured these particles to be 1,700 times lighter than hydrogen (the lightest atom). He called these new particles corpuscles but they were later renamed electrons since these are the particles that carry electricity. Thomson also showed that electrons were identical to particles given off by photoelectric and radioactive materials. Thomson explained that an electric current is the passing of electrons from one atom to the next, and when there was no current the electrons embedded themselves in the atoms. This in turn meant that atoms were not indivisible as scientists thought. The atom was composed of electrons whose negative charge was balanced out by some source of positive charge to create an electrically neutral atom. Ions, Thomson explained, must be atoms which have an excess or shortage of electrons.
=
The electrons in the atom logically had to be balanced out by a commensurate amount of positive charge, but Thomson had no idea where this positive charge came from, so he tentatively proposed that it was everywhere in the atom, the atom being in the shape of a sphere. This was the mathematically simplest hypothesis to fit the available evidence, or lack thereof. Following from this, Thomson imagined that the balance of electrostatic forces would distribute the electrons throughout the sphere in a more or less even manner. Thomson's model is popularly known as the plum pudding model, though neither Thomson nor his colleagues used this analogy. Thomson's model was incomplete, it was unable to predict any other properties of the elements such as emission spectra and valencies. It was soon rendered obsolete by the discovery of the atomic nucleus.
Between 1908 and 1913, Ernest Rutherford and his colleagues Hans Geiger and Ernest Marsden performed a series of experiments in which they bombarded thin foils of metal with a beam of alpha particles. They did this to measure the scattering patterns of the alpha particles. They spotted a small number of alpha particles being deflected by angles greater than 90°. This shouldn't have been possible according to the Thomson model of the atom, whose charges were too diffuse to produce a sufficiently strong electric field. The deflections should have all been negligible. Rutherford proposed that the positive charge of the atom is concentrated in a tiny volume at the center of the atom and that the electrons surround this nucleus in a diffuse cloud. This nucleus carried almost all of the atom's mass. Only such an intense concentration of charge, anchored by its high mass, could produce an electric field that could deflect the alpha particles so strongly.
=
A problem in classical mechanics is that an accelerating charged particle radiates electromagnetic radiation, causing the particle to lose kinetic energy. Circular motion counts as acceleration, which means that an electron orbiting a central charge should spiral down into that nucleus as it loses speed. In 1913, the physicist Niels Bohr proposed a new model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits, and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of a photon. This quantization was used to explain why the electrons' orbits are stable and why elements absorb and emit electromagnetic radiation in discrete spectra. Bohr's model could only predict the emission spectra of hydrogen, not atoms with more than one electron.
=
Back in 1815, William Prout observed that the atomic weights of many elements were multiples of hydrogen's atomic weight, which is in fact true for all of them if one takes isotopes into account. In 1898, J. J. Thomson found that the positive charge of a hydrogen ion is equal to the negative charge of an electron, and these were then the smallest known charged particles. Thomson later found that the positive charge in an atom is a positive multiple of an electron's negative charge. In 1913, Henry Moseley discovered that the frequencies of X-ray emissions from an excited atom were a mathematical function of its atomic number and hydrogen's nuclear charge. In 1919, Rutherford bombarded nitrogen gas with alpha particles and detected hydrogen ions being emitted from the gas, and concluded that they were produced by alpha particles hitting and splitting the nuclei of the nitrogen atoms.
These observations led Rutherford to conclude that the hydrogen nucleus is a singular particle with a positive charge equal to the electron's negative charge. He named this particle "proton" in 1920. The number of protons in an atom (which Rutherford called the "atomic number") was found to be equal to the element's ordinal number on the periodic table and therefore provided a simple and clear-cut way of distinguishing the elements from each other. The atomic weight of each element is higher than its proton number, so Rutherford hypothesized that the surplus weight was carried by unknown particles with no electric charge and a mass equal to that of the proton.
In 1928, Walter Bothe observed that beryllium emitted a highly penetrating, electrically neutral radiation when bombarded with alpha particles. It was later discovered that this radiation could knock hydrogen atoms out of paraffin wax. Initially it was thought to be high-energy gamma radiation, since gamma radiation had a similar effect on electrons in metals, but James Chadwick found that the ionization effect was too strong for it to be due to electromagnetic radiation, so long as energy and momentum were conserved in the interaction. In 1932, Chadwick exposed various elements, such as hydrogen and nitrogen, to the mysterious "beryllium radiation", and by measuring the energies of the recoiling charged particles, he deduced that the radiation was actually composed of electrically neutral particles which could not be massless like the gamma ray, but instead were required to have a mass similar to that of a proton. Chadwick now claimed these particles as Rutherford's neutrons.
=
In 1925, Werner Heisenberg published the first consistent mathematical formulation of quantum mechanics (matrix mechanics). One year earlier, Louis de Broglie had proposed that all particles behave like waves to some extent, and in 1926 Erwin Schrödinger used this idea to develop the Schrödinger equation, which describes electrons as three-dimensional waveforms rather than points in space. A consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at a given point in time. This became known as the uncertainty principle, formulated by Werner Heisenberg in 1927. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be found. This model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen.
=
Though the word atom originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton, and the neutron.
The electron is the least massive of these particles by four orders of magnitude at 9.11×10−31 kg, with a negative electrical charge and a size that is too small to be measured using available techniques. It was the lightest particle with a positive rest mass measured, until the discovery of neutrino mass. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. Electrons have been known since the late 19th century, mostly thanks to J.J. Thomson; see history of subatomic physics for details.
Protons have a positive charge and a mass of 1.6726×10−27 kg. The number of protons in an atom is called its atomic number. Ernest Rutherford (1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920, he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton.
Neutrons have no electrical charge and have a mass of 1.6749×10−27 kg. Neutrons are the heaviest of the three constituent particles, but their mass can be reduced by the nuclear binding energy. Neutrons and protons (collectively known as nucleons) have comparable dimensions—on the order of 2.5×10−15 m—although the 'surface' of these particles is not sharply defined. The neutron was discovered in 1932 by the English physicist James Chadwick.
In the Standard Model of physics, electrons are truly elementary particles with no internal structure, whereas protons and neutrons are composite particles composed of elementary particles called quarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of two up quarks (each with charge +⁠2/3⁠) and one down quark (with a charge of −⁠1/3⁠). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.
The quarks are held together by the strong interaction (or strong force), which is mediated by gluons. The protons and neutrons, in turn, are held to each other in the nucleus by the nuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.
=
All the bound protons and neutrons in an atom make up a tiny atomic nucleus, and are collectively called nucleons. The radius of a nucleus is approximately equal to 
  
    
      
        1.07
        
          
            A
            
              3
            
          
        
      
    
    {\displaystyle 1.07{\sqrt[{3}]{A}}}
  
 femtometres, where 
  
    
      
        A
      
    
    {\displaystyle A}
  
 is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 105 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other.
Atoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay.
The proton, the electron, and the neutron are classified as fermions. Fermions obey the Pauli exclusion principle which prohibits identical fermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud.
A nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with matching numbers of protons and neutrons are more stable against decay, but with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus.
The number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3 to 10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.
If the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass–energy equivalence formula, E = mc2, where m is the mass loss and c is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.
The fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction.  For heavier nuclei, the binding energy per nucleon begins to decrease. That means that a fusion process producing a nucleus that has an atomic number higher than about 26, and a mass number higher than about 60, is an endothermic process. Thus, more massive nuclei cannot undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.
=
The electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.
Electrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and differ from each other in size, shape and orientation.
Each atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines.
The amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 million eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals.
=
By definition, any two atoms with an identical number of protons in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of neutrons are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form, also called protium), one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The known elements form a set of atomic numbers, from the single-proton element hydrogen up to the 118-proton element oganesson. All known isotopes of elements with atomic numbers greater than 82 are radioactive, although the radioactivity of element 83 (bismuth) is so slight as to be practically negligible.
About 339 nuclides occur naturally on Earth, of which 251 (about 74%) have not been observed to decay, and are referred to as "stable isotopes". Only 90 nuclides are stable theoretically, while another 161 (bringing the total to 251) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as "stable". An additional 35 radioactive nuclides have half-lives longer than 100 million years, and are long-lived enough to have been present since the birth of the Solar System. This collection of 286 nuclides are known as primordial nuclides. Finally, an additional 53 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such as radium from uranium), or as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).
For 80 of the chemical elements, at least one stable isotope exists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.1 stable isotopes per element. Twenty-six "monoisotopic elements" have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes.
Stability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain "magic numbers" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 251 known stable nuclides, only four have both an odd number of protons and odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10, and nitrogen-14. (Tantalum-180m is odd-odd and observationally stable, but is predicted to decay with a very long half-life.) Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138, and lutetium-176. Most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
=
The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called "nucleons") in a given atom is called the mass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is "carbon-12," which has 12 nucleons (six protons and six neutrons).
The actual mass of an atom at rest is often expressed in daltons (Da), also called the unified atomic mass unit (u). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately 1.66×10−27 kg. Hydrogen-1 (the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 Da. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the dalton (for example the mass of a nitrogen-14 is roughly 14 Da), but this number will not be exactly an integer except (by definition) in the case of carbon-12. The heaviest stable atom is lead-208, with a mass of 207.9766521 Da.
As even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about 6.022×1023). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the dalton, each carbon-12 atom has an atomic mass of exactly 12 Da, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.
=
Atoms lack a well-defined outer boundary, so their dimensions are usually described in terms of an atomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus. This assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm.
When subjected to external forces, like electrical fields, the shape of an atom may deviate from spherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown by group-theoretical considerations. Aspherical deviations might be elicited for instance in crystals, where large crystal-electrical fields may occur at low-symmetry lattice sites. Significant ellipsoidal deformations have been shown to occur for sulfur ions and chalcogen ions in pyrite-type compounds.
Atomic dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they cannot be viewed using an optical microscope, although individual atoms can be observed using a scanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion (2×1021) atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of 2×10−4 kg contains about 10 sextillion (1022) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.
=
Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.
The most common forms of radioactive decay are:
Alpha decay: this process is caused when the nucleus emits an alpha particle, which is a helium nucleus consisting of two protons and two neutrons. The result of the emission is a new element with a lower atomic number.
Beta decay (and electron capture): these processes are regulated by the weak force, and result from a transformation of a neutron into a proton, or a proton into a neutron. The neutron to proton transition is accompanied by the emission of an electron and an antineutrino, while proton to neutron transition (except in electron capture) causes the emission of a positron and a neutrino. The electron or positron emissions are called beta particles. Beta decay either increases or decreases the atomic number of the nucleus by one. Electron capture is more common than positron emission, because it requires less energy. In this type of decay, an electron is absorbed by the nucleus, rather than a positron emitted from the nucleus. A neutrino is still emitted in this process, and a proton changes to a neutron.
Gamma decay: this process results from a change in the energy level of the nucleus to a lower state, resulting in the emission of electromagnetic radiation. The excited state of a nucleus which results in gamma emission usually occurs following the emission of an alpha or a beta particle. Thus, gamma decay usually follows alpha or beta decay.
Other more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission.
Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.
=
Elementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin 1⁄2 ħ, or "spin-1⁄2". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.
The magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field, but the most dominant contribution comes from electron spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.
In ferromagnetic elements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a spontaneous process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.
The nucleus of an atom will have no spin when it has even numbers of both neutrons and protons, but for other cases of odd numbers, the nucleus may have a spin. Normally nuclei with spin are aligned in random directions because of thermal equilibrium, but for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.
=
The potential energy of an electron in an atom is negative relative to when the distance from the nucleus goes to infinity; its dependence on the electron's position reaches the minimum inside the nucleus, roughly in inverse proportion to the distance. In the quantum-mechanical model, a bound electron can occupy only a set of states centered on the nucleus, and each state corresponds to a specific energy level; see time-independent Schrödinger equation for a theoretical explanation. An energy level can be measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). The lowest energy state of a bound electron is called the ground state, i.e., stationary state, while an electron transition to a higher level results in an excited state. The electron's energy increases along with n because the (average) distance to the nucleus increases. Dependence of the energy on ℓ is caused not by the electrostatic potential of the nucleus, but by interaction between electrons.
For an electron to transition between two different states, e.g. ground state to first excited state, it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels, according to the Niels Bohr model, what can be precisely calculated by the Schrödinger equation. Electrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; see Electron properties.
The energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.
When a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series of emission lines from the photons emitted by the atoms. Spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined.
Close examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin–orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect.
If a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.
=
Valency is the combining power of an element. It is determined by the number of bonds it can form to other atoms or groups. The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in
that shell are called valence electrons. The number of valence electrons determines the bonding
behavior with other atoms. Atoms tend to chemically react with each other in a manner that fills (or empties) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. Many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds.
The chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.
=
Quantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases, and plasmas. Within a state, a material can also exist in different allotropes. An example of this is solid carbon, which can exist as graphite or diamond. Gaseous allotropes exist as well, such as dioxygen and ozone.
At temperatures close to absolute zero, atoms can form a Bose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms then behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.
While atoms are too small to be seen, devices such as the scanning tunneling microscope (STM) enable their visualization at the surfaces of solids. The microscope uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would be insurmountable in the classical perspective. Electrons tunnel through the vacuum between two biased electrodes, providing a tunneling current that is exponentially dependent on their separation. One electrode is a sharp tip ideally ending with a single atom. At each point of the scan of the surface the tip's height is adjusted so as to keep the tunneling current at a set value. How much the tip moves to and away from the surface is interpreted as the height profile. For low bias, the microscope images the averaged electron orbitals across closely packed energy levels—the local density of the electronic states near the Fermi level. Because of the distances involved, both electrodes need to be extremely stable; only then periodicities can be observed that correspond to individual atoms. The method alone is not chemically specific, and cannot identify the atomic species present at the surface.
Atoms can be easily identified by their mass. If an atom is ionized by removing one of its electrons, its trajectory when it passes through a magnetic field will bend. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.
The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry.
Electron emission techniques such as X-ray photoelectron spectroscopy (XPS) and Auger electron spectroscopy (AES), which measure the binding energies of the core electrons, are used to identify the atomic species present in a sample in a non-destructive way. With proper focusing both can be made area-specific. Another such method is electron energy loss spectroscopy (EELS), which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample.
Spectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.
Baryonic matter forms about 4% of the total energy density of the observable universe, with an average density of about 0.25 particles/m3 (mostly protons and electrons). Within a galaxy such as the Milky Way, particles have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 105 to 109 atoms/m3. The Sun is believed to be inside the Local Bubble, so the density in the solar neighborhood is only about 103 atoms/m3. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium.
Up to 95% of the Milky Way's baryonic matter are concentrated inside stars, where conditions are unfavorable for atomic matter. The total baryonic mass is about 10% of the mass of the galaxy; the remainder of the mass is an unknown dark matter. High temperature inside stars makes most "atoms" fully ionized, that is, separates all electrons from the nuclei. In stellar remnants—with exception of their surface layers—an immense pressure make electron shells impossible.
=
Electrons are thought to exist in the Universe since early stages of the Big Bang. Atomic nuclei forms in nucleosynthesis reactions. In about three minutes Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the Universe, and perhaps some of the beryllium and boron.
Ubiquitousness and stability of atoms relies on their binding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where the temperature is much higher than ionization potential, the matter exists in the form of plasma—a gas of positively charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms become statistically favorable. Atoms (complete with bound electrons) became to dominate over charged particles 380,000 years after the Big Bang—an epoch called recombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.
Since the Big Bang, which produced no carbon or heavier elements, atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium, and (via the triple-alpha process) the sequence of elements from carbon up to iron; see stellar nucleosynthesis for details.
Isotopes such as lithium-6, as well as some beryllium and boron are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.
Elements heavier than iron were produced in supernovae and colliding neutron stars through the r-process, and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.
=
Most of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay.
There are a few trace atoms on Earth that were not present at the beginning (i.e., not "primordial"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore.
The Earth contains approximately 1.33×1050 atoms. Although small numbers of independent atoms of noble gases exist, such as argon, neon, and helium, 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, including water, salt, silicates, and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.
=
 Superheavy elements 
All nuclides with atomic numbers higher than 82 (lead) are known to be radioactive. No nuclide with an atomic number exceeding 92 (uranium) exists on Earth as a primordial nuclide, and heavier elements generally have shorter half-lives. Nevertheless, an "island of stability" encompassing relatively long-lived isotopes of superheavy elements with atomic numbers 110 to 114 might exist. Predictions for the half-life of the most stable nuclide on the island range from a few minutes to millions of years. In any case, superheavy elements (with Z > 104) would not exist due to increasing Coulomb repulsion (which results in spontaneous fission with increasingly short half-lives) in the absence of any stabilizing effects.
 Exotic matter 
Each particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories of baryogenesis may offer an explanation. As a result, no antimatter atoms have been discovered in nature. In 1996, the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at the CERN laboratory in Geneva.
Other exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test fundamental predictions of physics.
Oliver Manuel (2001). Origin of Elements in the Solar System: Implications of Post-1957 Observations. Springer. ISBN 978-0-306-46562-8. OCLC 228374906.
Andrew G. van Melsen (2004) [1952]. From Atomos to Atom: The History of the Concept Atom. Translated by Henry J. Koren. Dover Publications. ISBN 0-486-49584-1.
J.P. Millington (1906). John Dalton. J. M. Dent & Co. (London); E. P. Dutton & Co. (New York).
Charles H. Holbrow; James N. Lloyd; Joseph C. Amato; Enrique Galvez; M. Elizabeth Parks (2010). Modern Introductory Physics. Springer Science & Business Media. ISBN 978-0-387-79079-4.
John Dalton (1808). A New System of Chemical Philosophy vol. 1.
John Dalton (1817). A New System of Chemical Philosophy vol. 2.
John L. Heilbron (2003). Ernest Rutherford and the Explosion of Atoms. Oxford University Press. ISBN 0-19-512378-6.
Jaume Navarro (2012). A History of the Electron: J. J. and G. P. Thomson. Cambridge University Press. ISBN 978-1-107-00522-8.
Bernard Pullman (1998). The Atom in the History of Human Thought. Translated by Axel Reisinger. Oxford University Press. ISBN 0-19-511447-7.
Jean Perrin (1910) [1909]. Brownian Movement and Molecular Reality. Translated by F. Soddy. Taylor and Francis.
Eric R. Scerri (2020). The Periodic Table, Its Story and Its Significance (2nd ed.). New York: Oxford University Press. ISBN 978-0-190-91436-3.
Atoms in Motion – The Feynman Lectures on Physics
Sharp, Tim (8 August 2017). "What is an Atom?". Live Science.

--- TOPIC: Molecule ---
A molecule is a group of two or more atoms that are held together by attractive forces known as chemical bonds; depending on context, the term may or may not include ions that satisfy this criterion. In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions.
A molecule may be homonuclear, that is, it consists of atoms of one chemical element, e.g. two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, e.g. water (two hydrogen atoms and one oxygen atom; H2O). In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This relaxes the requirement that a molecule contains two or more atoms, since the noble gases are individual atoms. Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.
Concepts similar to molecules have been discussed since ancient times, but modern investigation into the nature of molecules and their bonds began in the 17th century. Refined over time by scientists such as Robert Boyle, Amedeo Avogadro, Jean Perrin, and Linus Pauling, the study of molecules is today known as molecular physics or molecular chemistry.
According to Merriam-Webster and the Online Etymology Dictionary, the word "molecule" derives from the Latin "moles" or small unit of mass. The word is derived from French molécule (1678), from Neo-Latin molecula, diminutive of Latin moles "mass, barrier". The word, which until the late 18th century was used only in Latin form, became popular after being used in works of philosophy by Descartes.
The definition of the molecule has evolved as knowledge of the structure of molecules has increased. Earlier definitions were less precise, defining molecules as the smallest particles of pure chemical substances that still retain their composition and chemical properties. This definition often breaks down since many substances in ordinary experience, such as rocks, salts, and metals, are composed of large crystalline networks of chemically bonded atoms or ions, but are not made of discrete molecules.
The modern concept of molecules can be traced back towards pre-scientific and Greek philosophers such as Leucippus and Democritus who argued that all the universe is composed of atoms and voids. Circa 450 BC Empedocles imagined fundamental elements (fire (), earth (), air (), and water ()) and "forces" of attraction and repulsion allowing the elements to interact.
A fifth element, the incorruptible quintessence aether, was considered to be the fundamental building block of the heavenly bodies. The viewpoint of Leucippus and Empedocles, along with the aether, was accepted by Aristotle and passed to medieval and renaissance Europe.
In a more concrete manner, however, the concept of aggregates or units of bonded atoms, i.e. "molecules", traces its origins to Robert Boyle's 1661 hypothesis, in his famous treatise The Sceptical Chymist, that matter is composed of clusters of particles and that chemical change results from the rearrangement of the clusters. Boyle argued that matter's basic elements consisted of various sorts and sizes of particles, called "corpuscles", which were capable of arranging themselves into groups. In 1789, William Higgins published views on what he called combinations of "ultimate" particles, which foreshadowed the concept of valency bonds. If, for example, according to Higgins, the force between the ultimate particle of oxygen and the ultimate particle of nitrogen were 6, then the strength of the force would be divided accordingly, and similarly for the other combinations of ultimate particles.
Amedeo Avogadro created the word "molecule". His 1811 paper "Essay on Determining the Relative Masses of the Elementary Molecules of Bodies", he essentially states, i.e. according to Partington's A Short History of Chemistry, that:The smallest particles of gases are not necessarily simple atoms, but are made up of a certain number of these atoms united by attraction to form a single molecule.In coordination with these concepts, in 1833 the French chemist Marc Antoine Auguste Gaudin presented a clear account of Avogadro's hypothesis, regarding atomic weights, by making use of "volume diagrams", which clearly show both semi-correct molecular geometries, such as a linear water molecule, and correct molecular formulas, such as H2O:
In 1917, an unknown American undergraduate chemical engineer named Linus Pauling was learning the Dalton hook-and-eye bonding method, which was the mainstream description of bonds between atoms at the time. Pauling, however, was not satisfied with this method and looked to the newly emerging field of quantum physics for a new method. In 1926, French physicist Jean Perrin received the Nobel Prize in physics for proving, conclusively, the existence of molecules. He did this by calculating the Avogadro constant using three different methods, all involving liquid phase systems. First, he used a gamboge soap-like emulsion, second by doing experimental work on Brownian motion, and third by confirming Einstein's theory of particle rotation in the liquid phase.
In 1927, the physicists Fritz London and Walter Heitler applied the new quantum mechanics to the deal with the saturable, nondynamic forces of attraction and repulsion, i.e., exchange forces, of the hydrogen molecule. Their valence bond treatment of this problem, in their joint paper, was a landmark in that it brought chemistry under quantum mechanics. Their work was an influence on Pauling, who had just received his doctorate and visited Heitler and London in Zürich on a Guggenheim Fellowship.
Subsequently, in 1931, building on the work of Heitler and London and on theories found in Lewis' famous article, Pauling published his ground-breaking article "The Nature of the Chemical Bond" in which he used quantum mechanics to calculate properties and structures of molecules, such as angles between bonds and rotation about bonds. On these concepts, Pauling developed hybridization theory to account for bonds in molecules such as CH4, in which four sp³ hybridised orbitals are overlapped by hydrogen's 1s orbital, yielding four sigma (σ) bonds. The four bonds are of the same length and strength, which yields a molecular structure as shown below:
The science of molecules is called molecular chemistry or molecular physics, depending on whether the focus is on chemistry or physics. Molecular chemistry deals with the laws governing the interaction between molecules that results in the formation and breakage of chemical bonds, while molecular physics deals with the laws governing their structure and properties. In practice, however, this distinction is vague. In molecular sciences, a molecule consists of a stable system (bound state) composed of two or more atoms. Polyatomic ions may sometimes be usefully thought of as electrically charged molecules. The term unstable molecule is used for very reactive species, i.e., short-lived assemblies (resonances) of electrons and nuclei, such as radicals, molecular ions, Rydberg molecules, transition states, van der Waals complexes, or systems of colliding atoms as in Bose–Einstein condensate.
Molecules as components of matter are common. They also make up most of the oceans and atmosphere. Most organic substances are molecules. The substances of life are molecules, e.g. proteins, the amino acids of which they are composed, the nucleic acids (DNA and RNA), sugars, carbohydrates, fats, and vitamins. The nutrient minerals are generally ionic compounds, thus they are not molecules, e.g. iron sulfate.
However, the majority of familiar solid substances on Earth are made partly or completely of crystals or ionic compounds, which are not made of molecules. These include all of the minerals that make up the substance of the Earth, sand, clay, pebbles, rocks, boulders, bedrock, the molten interior, and the core of the Earth. All of these contain many chemical bonds, but are not made of identifiable molecules.
No typical molecule can be defined for salts nor for covalent crystals, although these are often composed of repeating unit cells that extend either in a plane, e.g. graphene; or three-dimensionally e.g. diamond, quartz, sodium chloride. The theme of repeated unit-cellular-structure also holds for most metals which are condensed phases with metallic bonding. Thus solid metals are not made of molecules. In glasses, which are solids that exist in a vitreous disordered state, the atoms are held together by chemical bonds with no presence of any definable molecule, nor any of the regularity of repeating unit-cellular-structure that characterizes salts, covalent crystals, and metals.
Molecules are generally held together by covalent bonding. Several non-metallic elements exist only as molecules in the environment either in compounds or as homonuclear molecules, not as free atoms: for example, hydrogen.
While some people say a metallic crystal can be considered a single giant molecule held together by metallic bonding, others point out that metals behave very differently than molecules.
=
A covalent bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are termed shared pairs or bonding pairs, and the stable balance of attractive and repulsive forces between atoms, when they share electrons, is termed covalent bonding.
=
 
Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. The ions are atoms that have lost one or more electrons (termed cations) and atoms that have gained one or more electrons (termed anions). This transfer of electrons is termed electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complicated nature, e.g. molecular ions like NH4+ or SO42−. At normal temperatures and pressures, ionic bonding mostly creates solids (or occasionally liquids) without separate identifiable molecules, but the vaporization/sublimation of such materials does produce separate molecules where electrons are still transferred fully enough for the bonds to be considered ionic rather than covalent.
Most molecules are far too small to be seen with the naked eye, although molecules of many polymers can reach macroscopic sizes, including biopolymers such as DNA. Molecules commonly used as building blocks for organic synthesis have a dimension of a few angstroms (Å) to several dozen Å, or around one billionth of a meter. Single molecules cannot usually be observed by light (as noted above), but small molecules and even the outlines of individual atoms may be traced in some circumstances by use of an atomic force microscope. Some of the largest molecules are macromolecules or supermolecules.
The smallest molecule is the diatomic hydrogen (H2), with a bond length of 0.74 Å.
Effective molecular radius is the size a molecule displays in solution.
The table of permselectivity for different substances contains examples.
=
The chemical formula for a molecule uses one line of chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, and plus (+) and minus (−) signs. These are limited to one typographic line of symbols, which may include subscripts and superscripts.
A compound's empirical formula is a very simple type of chemical formula. It is the simplest integer ratio of the chemical elements that constitute it. For example, water is always composed of a 2:1 ratio of hydrogen to oxygen atoms, and ethanol (ethyl alcohol) is always composed of carbon, hydrogen, and oxygen in a 2:6:1 ratio. However, this does not determine the kind of molecule uniquely – dimethyl ether has the same ratios as ethanol, for instance. Molecules with the same atoms in different arrangements are called isomers. Also carbohydrates, for example, have the same ratio (carbon:hydrogen:oxygen= 1:2:1) (and thus the same empirical formula) but different total numbers of atoms in the molecule.
The molecular formula reflects the exact number of atoms that compose the molecule and so characterizes different molecules. However different isomers can have the same atomic composition while being different molecules.
The empirical formula is often the same as the molecular formula but not always. For example, the molecule acetylene has molecular formula C2H2, but the simplest integer ratio of elements is CH.
The molecular mass can be calculated from the chemical formula and is typically expressed in daltons, which are equal to 1/12 of the mass of a neutral carbon-12 (12C isotope) atom. For network solids, the term formula unit is used in stoichiometric calculations.
=
For molecules with a complicated 3-dimensional structure, especially involving atoms bonded to four different substituents, a simple molecular formula or even semi-structural chemical formula may not be enough to completely specify the molecule. In this case, a graphical type of formula called a structural formula may be needed. Structural formulas may in turn be represented with a one-dimensional chemical name, but such chemical nomenclature requires many words and terms which are not part of chemical formulas.
Molecules have fixed equilibrium geometries—bond lengths and angles— about which they continuously oscillate through vibrational and rotational motions. A pure substance is composed of molecules with the same average geometrical structure. The chemical formula and the structure of a molecule are the two important factors that determine its properties, particularly its reactivity. Isomers share a chemical formula but normally have very different properties because of their different structures. Stereoisomers, a particular type of isomer, may have very similar physico-chemical properties and at the same time different biochemical activities.
Molecular spectroscopy deals with the response (spectrum) of molecules interacting with probing signals of known energy (or frequency, according to the Planck relation). Molecules have quantized energy levels that can be analyzed by detecting the molecule's energy exchange through absorbance or emission.
Spectroscopy does not generally refer to diffraction studies where particles such as neutrons, electrons, or high energy X-rays interact with a regular arrangement of molecules (as in a crystal).
Microwave spectroscopy commonly measures changes in the rotation of molecules, and can be used to identify molecules in outer space. Infrared spectroscopy measures the vibration of molecules, including stretching, bending or twisting motions. It is commonly used to identify the kinds of bonds or functional groups in molecules. Changes in the arrangements of electrons yield absorption or emission lines in ultraviolet, visible or near infrared light, and result in colour. Nuclear resonance spectroscopy measures the environment of particular nuclei in the molecule, and can be used to characterise the numbers of atoms in different positions in a molecule.
The study of molecules by molecular physics and theoretical chemistry is largely based on quantum mechanics and is essential for the understanding of the chemical bond. The simplest of molecules is the hydrogen molecule-ion, H2+, and the simplest of all the chemical bonds is the one-electron bond. H2+ is composed of two positively charged protons and one negatively charged electron, which means that the Schrödinger equation for the system can be solved more easily due to the lack of electron–electron repulsion. With the development of fast digital computers, approximate solutions for more complicated molecules became possible and are one of the main aspects of computational chemistry.
When trying to define rigorously whether an arrangement of atoms is sufficiently stable to be considered a molecule, IUPAC suggests that it "must correspond to a depression on the potential energy surface that is deep enough to confine at least one vibrational state". This definition does not depend on the nature of the interaction between the atoms, but only on the strength of the interaction. In fact, it includes weakly bound species that would not traditionally be considered molecules, such as the helium dimer, He2, which has one vibrational bound state and is so loosely bound that it is only likely to be observed at very low temperatures.
Whether or not an arrangement of atoms is sufficiently stable to be considered a molecule is inherently an operational definition. Philosophically, therefore, a molecule is not a fundamental entity (in contrast, for instance, to an elementary particle); rather, the concept of a molecule is the chemist's way of making a useful statement about the strengths of atomic-scale interactions in the world that we observe.
Molecule of the Month – School of Chemistry, University of Bristol

--- TOPIC: Chemical_bond ---
A chemical bond is the association of atoms or ions to form molecules, crystals, and other structures. The bond may result from the electrostatic force between oppositely charged ions as in ionic bonds or through the sharing of electrons as in covalent bonds, or some combination of these effects. Chemical bonds are described as having different strengths: there are "strong bonds" or "primary bonds" such as covalent, ionic and metallic bonds, and "weak bonds" or "secondary bonds" such as dipole–dipole interactions, the London dispersion force, and hydrogen bonding.  
Since opposite electric charges attract, the negatively charged electrons surrounding the nucleus and the positively charged protons within a nucleus attract each other. Electrons shared between two nuclei will be attracted to both of them. "Constructive quantum mechanical wavefunction interference" stabilizes the paired nuclei (see  Theories of chemical bonding). Bonded nuclei maintain an optimal distance (the bond distance) balancing attractive and repulsive effects explained quantitatively by quantum theory.
The atoms in molecules, crystals, metals and other forms of matter are held together by chemical bonds, which determine the structure and properties of matter.
All bonds can be described by quantum theory, but, in practice, simplified rules and other theories allow chemists to predict the strength, directionality, and polarity of bonds. The octet rule and VSEPR theory are examples. More sophisticated theories are valence bond theory, which includes orbital hybridization and resonance, and molecular orbital theory which includes the linear combination of atomic orbitals and ligand field theory. Electrostatics are used to describe bond polarities and the effects they have on chemical substances.
A chemical bond is an attraction between atoms. This attraction may be seen as the result of different behaviors of the outermost or valence electrons of atoms. These behaviors merge into each other seamlessly in various circumstances, so that there is no clear line to be drawn between them. However it remains useful and customary to differentiate between different types of bond, which result in different properties of condensed matter.
In the simplest view of a covalent bond, one or more electrons (often a pair of electrons) are drawn into the space between the two atomic nuclei. Energy is released by bond formation. This is not as a result of reduction in potential energy, because the attraction of the two electrons to the two protons is offset by the electron-electron and proton-proton repulsions. Instead, the release of energy (and hence stability of the bond) arises from the reduction in kinetic energy due to the electrons being in a more spatially distributed (i.e. longer de Broglie wavelength) orbital compared with each electron being confined closer to its respective nucleus. These bonds exist between two particular identifiable atoms and have a direction in space, allowing them to be shown as single connecting lines between atoms in drawings, or modeled as sticks between spheres in models.
In a polar covalent bond, one or more electrons are unequally shared between two nuclei. Covalent bonds often result in the formation of small collections of better-connected atoms called molecules, which in solids and liquids are bound to other molecules by forces that are often much weaker than the covalent bonds that hold the molecules internally together. Such weak intermolecular bonds give organic molecular substances, such as waxes and oils, their soft bulk character, and their low melting points (in liquids, molecules must cease most structured or oriented contact with each other). When covalent bonds link long chains of atoms in large molecules, however (as in polymers such as nylon), or when covalent bonds extend in networks through solids that are not composed of discrete molecules (such as diamond or quartz or the silicate minerals in many types of rock) then the structures that result may be both strong and tough, at least in the direction oriented correctly with networks of covalent bonds.  Also, the melting points of such covalent polymers and networks increase greatly.
In a simplified view of an ionic bond, the bonding electron is not shared at all, but transferred. In this type of bond, the outer atomic orbital of one atom has a vacancy which allows the addition of one or more electrons. These newly added electrons potentially occupy a lower energy-state (effectively closer to more nuclear charge) than they experience in a different atom. Thus, one nucleus offers a more tightly bound position to an electron than does another nucleus, with the result that one atom may transfer an electron to the other. This transfer causes one atom to assume a net positive charge, and the other to assume a net negative charge. The bond then results from electrostatic attraction between the positive and negatively charged ions. Ionic bonds may be seen as extreme examples of polarization in covalent bonds. Often, such bonds have no particular orientation in space, since they result from equal electrostatic attraction of each ion to all ions around them. Ionic bonds are strong (and thus ionic substances require high temperatures to melt) but also brittle, since the forces between ions are short-range and do not easily bridge cracks and fractures. This type of bond gives rise to the physical characteristics of crystals of classic mineral salts, such as table salt.
A less often mentioned type of bonding is metallic bonding. In this type of bonding, each atom in a metal donates one or more electrons to a "sea" of electrons that reside between many metal atoms. In this sea, each electron is free (by virtue of its wave nature) to be associated with a great many atoms at once. The bond results because the metal atoms become somewhat positively charged due to loss of their electrons while the electrons remain attracted to many atoms, without being part of any given atom. Metallic bonding may be seen as an extreme example of delocalization of electrons over a large system of covalent bonds, in which every atom participates. This type of bonding is often very strong (resulting in the tensile strength of metals). However, metallic bonding is more collective in nature than other types, and so they allow metal crystals to more easily deform, because they are composed of atoms attracted to each other, but not in any particularly-oriented ways. This results in the malleability of metals. The cloud of electrons in metallic bonding causes the characteristically good electrical and thermal conductivity of metals, and also their shiny lustre that reflects most frequencies of white light.
=
Working in the late 17th century, Robert Boyle developed the concept of a chemical element as substance different from a compound.
Near the end of the 18th century, a number of important developments in chemistry emerged without referring to the notion of an atomic theory. The first was Antoine Lavoisier who showed that compounds consist of elements in constant proportion, redefining an element as a substance which scientists could not decompose into simpler substances by experimentation. This brought an end to the ancient idea of the elements of matter being fire, earth, air, and water, which had no experimental support. Lavoisier showed that water can be decomposed into hydrogen and oxygen, which in turn he could not decompose into anything simpler, thereby proving these are elements. Lavoisier also defined the law of conservation of mass, which states that in a chemical reaction, matter does not appear nor disappear into thin air; the total mass remains the same even if the substances involved were transformed. In 1797 the French chemist Joseph Proust established the law of definite proportions, which states that if a compound is broken down into its constituent chemical elements, then the masses of those constituents will always have the same proportions by weight, regardless of the quantity or source of the original compound. This definition distinguished compounds from mixtures.
In the early years of the 17th century, Humphry Davy experimented on decomposing compounds into elements using the newly invention of the voltaic pile. This led to speculation that chemical bonding was related to electricity and in 1812 Jöns Jakob Berzelius published a theory of chemical combination stressing the electronegative and electropositive characters of the combining atoms.
By the mid 19th century, Edward Frankland, F.A. Kekulé, A.S. Couper, Alexander Butlerov, and Hermann Kolbe, building on the theory of radicals, developed the theory of valency, originally called "combining power", in which compounds were joined owing to an attraction of positive and negative poles. In 1904, Richard Abegg proposed his rule that the difference between the maximum and minimum valencies of an element is often eight. At this point, valency was still an empirical number based only on chemical properties.
=
The nature of the atom became clearer with Ernest Rutherford's 1911 discovery of an atomic nucleus surrounded by electrons. In his paper, Rutherford mentioned the model of Japanese physicist Hantaro Nagaoka, who had rejected Thomson's plum pudding model on the grounds that opposite charges are impenetrable. In 1904, Nagaoka had proposed an alternative planetary model of the atom in which a positively charged center is surrounded by a number of revolving electrons, in the manner of Saturn and its rings.
Nagaoka's model hypothesized:
a very massive atomic center (in analogy to a very massive planet)
electrons revolving around the nucleus, bound by electrostatic forces (in analogy to the rings revolving around Saturn, bound by gravitational forces.)
At the 1911 Solvay Conference, in the discussion of what could regulate energy differences between atoms, Max Planck stated: "The intermediaries could be the electrons." These nuclear models suggested that electrons determine chemical behavior.
Next came Niels Bohr's 1913 model of a nuclear atom with electron orbits.
=
In 1916, chemist Gilbert N. Lewis developed the concept of electron-pair bonds, in which two atoms may share one to six electrons, thus forming the single electron bond, a single bond, a double bond, or a triple bond; in Lewis's own words, "An electron may form a part of the shell of two different atoms and cannot be said to belong to either one exclusively."
Also in 1916, Walther Kossel put forward a theory similar to Lewis' only his model assumed complete transfers of electrons between atoms, and was thus a model of ionic bonding. Both Lewis and Kossel structured their bonding models on that of Abegg's rule (1904).
Niels Bohr also proposed a model of the chemical bond in 1913. According to his model for a diatomic molecule, the electrons of the atoms of the molecule form a rotating ring whose plane is perpendicular to the axis of the molecule and equidistant from the atomic nuclei. The dynamic equilibrium of the molecular system is achieved through the balance of forces between the forces of attraction of nuclei to the plane of the ring of electrons and the forces of mutual repulsion of the nuclei. The Bohr model of the chemical bond took into account the Coulomb repulsion – the electrons in the ring are at the maximum distance from each other.
In 1927, the first mathematically complete quantum description of a simple chemical bond, i.e. that produced by one electron in the hydrogen molecular ion, H2+, was derived by the Danish physicist Øyvind Burrau. This work showed that the quantum approach to chemical bonds could be fundamentally and quantitatively correct, but the mathematical methods used could not be extended to molecules containing more than one electron. A more practical, albeit less quantitative, approach was put forward in the same year by Walter Heitler and Fritz London. The Heitler–London method forms the basis of what is now called valence bond theory. In 1929, the linear combination of atomic orbitals molecular orbital method (LCAO) approximation was introduced by Sir John Lennard-Jones, who also suggested methods to derive electronic structures of molecules of F2 (fluorine) and O2 (oxygen) molecules, from basic quantum principles. This molecular orbital theory represented a covalent bond as an orbital formed by combining the quantum mechanical Schrödinger atomic orbitals which had been hypothesized for electrons in single atoms. The equations for bonding electrons in multi-electron atoms could not be solved to mathematical perfection (i.e., analytically), but approximations for them still gave many good qualitative predictions and results. Most quantitative calculations in modern quantum chemistry use either valence bond or molecular orbital theory as a starting point, although a third approach, density functional theory, has become increasingly popular in recent years.
In 1933, H. H. James and A. S. Coolidge carried out a calculation on the dihydrogen molecule that, unlike all previous calculation which used functions only of the distance of the electron from the atomic nucleus, used functions which also explicitly added the distance between the two electrons. With up to 13 adjustable parameters they obtained a result very close to the experimental result for the dissociation energy. Later extensions have used up to 54 parameters and gave excellent agreement with experiments. This calculation convinced the scientific community that quantum theory could give agreement with experiment. However this approach has none of the physical pictures of the valence bond and molecular orbital theories and is difficult to extend to larger molecules.
Because atoms and molecules are three-dimensional, it is difficult to use a single method to indicate orbitals and bonds. In molecular formulas the chemical bonds (binding orbitals) between atoms are indicated in different ways depending on the type of discussion. Sometimes, some details are neglected. For example, in organic chemistry one is sometimes concerned only with the functional group of the molecule. Thus, the molecular formula of ethanol may be written in conformational form, three-dimensional form, full two-dimensional form (indicating every bond with no three-dimensional directions), compressed two-dimensional form (CH3–CH2–OH), by separating the functional group from another part of the molecule (C2H5OH), or by its atomic constituents (C2H6O), according to what is discussed. Sometimes, even the non-bonding valence shell electrons (with the two-dimensional approximate directions) are marked, e.g. for elemental carbon .'C'. Some chemists may also mark the respective orbitals, e.g. the hypothetical ethene−4 anion (\/C=C/\ −4) indicating the possibility of bond formation.
Strong chemical bonds are the intramolecular forces that hold atoms together in molecules. A strong chemical bond is formed from the transfer or sharing of electrons between atomic centers and relies on the electrostatic attraction between the protons in nuclei and the electrons in the orbitals.
The types of strong bond differ due to the difference in electronegativity of the constituent elements. Electronegativity is the tendency for an atom of a given chemical element to attract shared electrons when forming a chemical bond, where the higher the associated electronegativity then the more it attracts electrons. Electronegativity serves as a simple way to quantitatively estimate the bond energy, which characterizes a bond along the continuous scale from covalent to ionic bonding. A large difference in electronegativity leads to more polar (ionic) character in the bond.
=
Ionic bonding is a type of electrostatic interaction between atoms that have a large electronegativity difference. There is no precise value that distinguishes ionic from covalent bonding, but an electronegativity difference of over 1.7 is likely to be ionic while a difference of less than 1.7 is likely to be covalent. Ionic bonding leads to separate positive and negative ions. Ionic charges are commonly between −3e to +3e. Ionic bonding commonly occurs in metal salts such as sodium chloride (table salt). A typical feature of ionic bonds is that the species form into ionic crystals, in which no ion is specifically paired with any single other ion in a specific directional bond. Rather, each species of ion is surrounded by ions of the opposite charge, and the spacing between it and each of the oppositely charged ions near it is the same for all surrounding atoms of the same type. It is thus no longer possible to associate an ion with any specific other single ionized atom near it. This is a situation unlike that in covalent crystals, where covalent bonds between specific atoms are still discernible from the shorter distances between them, as measured via such techniques as X-ray diffraction.
Ionic crystals may contain a mixture of covalent and ionic species, as for example salts of complex acids such as sodium cyanide, NaCN. X-ray diffraction shows that in NaCN, for example, the bonds between sodium cations (Na+) and the cyanide anions (CN−) are ionic, with no sodium ion associated with any particular cyanide. However, the bonds between the carbon (C) and nitrogen (N) atoms in cyanide are of the covalent type, so that each carbon is strongly bound to just one nitrogen, to which it is physically much closer than it is to other carbons or nitrogens in a sodium cyanide crystal.
When such crystals are melted into liquids, the ionic bonds are broken first because they are non-directional and allow the charged species to move freely. Similarly, when such salts dissolve into water, the ionic bonds are typically broken by the interaction with water but the covalent bonds continue to hold. For example, in solution, the cyanide ions, still bound together as single CN− ions, move independently through the solution, as do sodium ions, as Na+. In water, charged ions move apart because each of them are more strongly attracted to a number of water molecules than to each other. The attraction between ions and water molecules in such solutions is due to a type of weak dipole-dipole type chemical bond. In melted ionic compounds, the ions continue to be attracted to each other, but not in any ordered or crystalline way.
=
Covalent bonding is a common type of bonding in which two or more atoms share valence electrons more or less equally. The simplest and most common type is a single bond in which two atoms share two electrons. Other types include the double bond, the triple bond, one- and three-electron bonds, the three-center two-electron bond and three-center four-electron bond.
In non-polar covalent bonds, the electronegativity difference between the bonded atoms is small, typically 0 to 0.3. Bonds within most organic compounds are described as covalent. The figure shows methane (CH4), in which each hydrogen forms a covalent bond with the carbon. See sigma bonds and pi bonds for LCAO descriptions of such bonding.
Molecules that are formed primarily from non-polar covalent bonds are often immiscible in water or other polar solvents, but much more soluble in non-polar solvents such as hexane.
A polar covalent bond is a covalent bond with a significant ionic character. This means that the two shared electrons are closer to one of the atoms than the other, creating an imbalance of charge. Such bonds occur between two atoms with moderately different electronegativities and give rise to dipole–dipole interactions. The electronegativity difference between the two atoms in these bonds is 0.3 to 1.7.
 Single and multiple bonds 
A single bond between two atoms corresponds to the sharing of one pair of electrons. The Hydrogen (H) atom has one valence electron. Two Hydrogen atoms can then form a molecule, held together by the shared pair of electrons. Each H atom now has the noble gas electron configuration of helium (He). The pair of shared electrons forms a single covalent bond. The electron density of these two bonding electrons in the region between the two atoms increases from the density of two non-interacting H atoms.
A double bond has two shared pairs of electrons, one in a sigma bond and one in a pi bond with electron density concentrated on two opposite sides of the internuclear axis. A triple bond consists of three shared electron pairs, forming one sigma and two pi bonds. An example is nitrogen. Quadruple and higher bonds are very rare and occur only between certain transition metal atoms.
 Coordinate covalent bond (dipolar bond) 
A coordinate covalent bond is a covalent bond in which the two shared bonding electrons are from the same one of the atoms involved in the bond. For example, boron trifluoride (BF3) and ammonia (NH3) form an adduct or coordination complex F3B←NH3 with a B–N bond in which a lone pair of electrons on N is shared with an empty atomic orbital on B. BF3 with an empty orbital is described as an electron pair acceptor or Lewis acid, while NH3 with a lone pair that can be shared is described as an electron-pair donor or Lewis base. The electrons are shared roughly equally between the atoms in contrast to ionic bonding. Such bonding is shown by an arrow pointing to the Lewis acid. (In the Figure, solid lines are bonds in the plane of the diagram, wedged bonds point towards the observer, and dashed bonds point away from the observer.)
Transition metal complexes are generally bound by coordinate covalent bonds. For example, the ion Ag+ reacts as a Lewis acid with two molecules of the Lewis base NH3 to form the complex ion Ag(NH3)2+, which has two Ag←N coordinate covalent bonds.
=
In metallic bonding, bonding electrons are delocalized over a lattice of atoms. By contrast, in ionic compounds, the locations of the binding electrons and their charges are static. The free movement or delocalization of bonding electrons leads to classical metallic properties such as luster (surface light reflectivity), electrical and thermal conductivity, ductility, and high tensile strength.
There are several types of weak bonds that can be formed between two or more molecules which are not covalently bound. Intermolecular forces cause molecules to attract or repel each other. Often, these forces influence physical characteristics (such as the melting point) of a substance.
Van der Waals forces are interactions between closed-shell molecules. They include both Coulombic interactions between partial charges in polar molecules, and Pauli repulsions between closed electrons shells.
Keesom forces are the forces between the permanent dipoles of two polar molecules. London dispersion forces are the forces between induced dipoles of different molecules. There can also be an interaction between a permanent dipole in one molecule and an induced dipole in another molecule.
Hydrogen bonds of the form A--H•••B occur when A and B are two highly electronegative atoms (usually N, O or F) such that A forms a highly polar covalent bond with H so that H has a partial positive charge, and B has a lone pair of electrons which is attracted to this partial positive charge and forms a hydrogen bond. Hydrogen bonds are responsible for the high boiling points of water and ammonia with respect to their heavier analogues. In some cases a similar halogen bond can be formed by a halogen atom located between two electronegative atoms on different molecules.
At short distances, repulsive forces between atoms also become important.
In the (unrealistic) limit of "pure" ionic bonding, electrons are perfectly localized on one of the two atoms in the bond. Such bonds can be understood by classical physics. The force between the atoms depends on isotropic continuum electrostatic potentials. The magnitude of the force is in simple proportion to the product of the two ionic charges according to Coulomb's law.
Covalent bonds are better understood by valence bond (VB) theory or molecular orbital (MO) theory. The properties of the atoms involved can be understood using concepts such as oxidation number, formal charge, and electronegativity. The electron density within a bond is not assigned to individual atoms, but is instead delocalized between atoms. In valence bond theory, bonding is conceptualized as being built up from electron pairs that are localized and shared by two atoms via the overlap of atomic orbitals. The concepts of orbital hybridization and resonance augment this basic notion of the electron pair bond. In molecular orbital theory, bonding is viewed as being delocalized and apportioned in orbitals that extend throughout the molecule and are adapted to its symmetry properties, typically by considering linear combinations of atomic orbitals (LCAO). Valence bond theory is more chemically intuitive by being spatially localized, allowing attention to be focused on the parts of the molecule undergoing chemical change. In contrast, molecular orbitals are more "natural" from a quantum mechanical point of view, with orbital energies being physically significant and directly linked to experimental ionization energies from photoelectron spectroscopy. Consequently, valence bond theory and molecular orbital theory are often viewed as competing but complementary frameworks that offer different insights into chemical systems. As approaches for electronic structure theory, both MO and VB methods can give approximations to any desired level of accuracy, at least in principle. However, at lower levels, the approximations differ, and one approach may be better suited for computations involving a particular system or property than the other.
Unlike the spherically symmetrical Coulombic forces in pure ionic bonds, covalent bonds are generally directed and anisotropic. These are often classified based on their symmetry with respect to a molecular plane as sigma bonds and pi bonds. In the general case, atoms form bonds that are intermediate between ionic and covalent, depending on the relative electronegativity of the atoms involved. Bonds of this type are known as polar covalent bonds.
W. Locke (1997). Introduction to Molecular Orbital Theory. Retrieved May 18, 2005.
Carl R. Nave (2005). HyperPhysics. Retrieved May 18, 2005.
Linus Pauling and the Nature of the Chemical Bond: A Documentary History. Retrieved February 29, 2008.

--- TOPIC: Astronomy ---
Astronomy is a natural science that studies celestial objects and the phenomena that occur in the cosmos. It uses mathematics, physics, and chemistry to explain their origin and their overall evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, meteoroids, asteroids, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates beyond Earth's atmosphere. Cosmology is the branch of astronomy that studies the universe as a whole.
Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Egyptians, Babylonians, Greeks, Indians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars.
Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results.
Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets.
Astronomy (from the Greek ἀστρονομία from ἄστρον astron, "star" and -νομία -nomia from νόμος nomos, "law" or "rule") means study of celestial objects. Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. The two fields share a common origin but became distinct, astronomy being supported by physics while astrology is not.
=
"Astronomy" and "astrophysics" are broadly synonymous in modern usage. In dictionary definitions, "astronomy" is "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties", while "astrophysics" is the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". Sometimes, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" means the qualitative study of the subject, whereas "astrophysics" is the physics-oriented version of the subject. Some fields, such as astrometry, are in this sense purely astronomy rather than also astrophysics. Research departments may use "astronomy" and "astrophysics" according to whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Thus, in modern use, the two terms are often used interchangeably.
=
The initial development of astronomy was driven by practical needs like agricultural calendars. Before recorded history archeological sites such as Stonehenge provide evidence of ancient interest in astronomical observations. 
Evidence also comes from artefacts such as the Nebra sky disc which serves as an astronomical calendar, defining a year as twelve lunar months, 354 days, with intercalary months to make up the solar year. The disc is inlaid with symbols interpreted as a sun, moon, and stars including a cluster of seven stars. Megalithic structures located in Nabta Playa, Upper Egypt featured astronomy, calendar arrangements in alignment with the heliacal rising of Sirius and supported calibration the yearly calendar for the annual Nile flood.These practices have been linked with the emergence of cosmology in Old Kingdom Egypt.
=
Civilizations such as Egypt, Mesopotamia, Greece, India, China independently but with cross-cultural influences created astronomical observatories and developed ideas on the nature of the Universe, along with calendars and astronomical instruments. A key early development was the beginning of mathematical and scientific astronomy among the Babylonians, laying the foundations for astronomical traditions in other civilizations. The Babylonians discovered that lunar eclipses recurred in the saros cycle of 223 synodic months.
Following the Babylonians, significant advances were made in ancient Greece and the Hellenistic world. Greek astronomy sought a rational, physical explanation for celestial phenomena.  In the 4th century BC, Heracleides Ponticus was the first to proposed that the Earth rotates on its own axis.In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. He also observed the small drift in the positions of the equinoxes and solstices with respect to the fixed stars that we now know is caused by precession. Hipparchus also created a catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
=
After the classical Greek era, astronomy was dominated by the geocentric model of the Universe, or the Ptolemaic system, named after Claudius Ptolemy. His 13-volume astronomy work, named the Almagest in its Arabic translation, became the primary reference for over a thousand years.  In this system, the Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. While the system would eventually be discredited, it gave the most accurate predictions for the positions of astronomical bodies available at that time.
With the arrival of Hellenistic astronomy in India through trade and cultural contacts, Indian astronomy entered a new phase during the early centuries CE.  Earlier indigenous traditions, such as those recorded in the Vedāṅga Jyotiṣa, provided calendrical foundations, while Greek astronomical models were later integrated by scholars including Āryabhaṭa, Varāhamihira, and Brahmagupta. Āryabhaṭa notably improved methods for calculating planetary motions and eclipses. In the later medieval period, the Kerala school contributed to astronomy through refined observational practices and more accurate planetary and eclipse calculations.
Astronomy flourished in the medieval Islamic world. Astronomical observatories were established there by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in the last 1000 years, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Iranian scholar Al-Biruni observed that, contrary to Ptolemy, the Sun's apogee (highest point in the heavens) was mobile, not fixed. Arabic astronomers introduced many Arabic names now used for individual stars.
The ruins at Great Zimbabwe and Timbuktu may have housed astronomical observatories. In Post-classical West Africa, astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens and diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in 1583.
In medieval Europe, Richard of Wallingford (1292–1336) invented the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes. Nicole Oresme (1320–1382) discussed evidence for the rotation of the Earth. Jean Buridan (1300–1361) developed the theory of impetus, describing motions including of the celestial bodies.
For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter.
=
During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. While his model maintained circular orbits, it was sufficient to calculate the size of planetary orbits and their period. The appealing simplicity of Copernican astronomy led to its adoption among astronomers even before it was confirmed by Galileo's telescopic observations in the 1600s.
=
Sometime around 1608 the telescope was invented and by 1610, Galileo Galilei observed phases on the planet Venus similar to those of the Moon, supporting the heliocentric model. Around the same time the heliocentric model was organized quantitatively by Johannes Kepler. Analyzing two decades of careful observations by Tycho Brahe, Kepler devised a system that described the details of the motion of the planets around the Sun. While Kepler discarded the uniform circular motion of Copernicus in favor of elliptical motion, he did not succeed in formulating a theory behind the laws he wrote down. It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope.
Newton, in collaboration with Richard Bentley proposed that stars are like the Sun only much further away. 
The new telescopes also altered ideas about stars. By 1610 Galileo discovered that the band of light crossing the sky at night that we call the Milky Way was composed of numerous stars. In 1668 James Gregory compared the luminosity of Jupiter to Sirius to estimate its distance at over 83,000 AU. The English astronomer John Flamsteed, Britain's first Astronomer Royal, catalogued over 3000 stars but the data were published against his wishes in 1712. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found. Friedrich Bessel developed the technique of stellar parallax in 1838 but it was so difficult to apply that only about 100 stars were measured by 1900.
During the 18–19th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.
Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and astrophotography. In 1814–15, Joseph von Fraunhofer discovered some 574 dark lines in the spectrum of the sun and of other stars. In 1859, Gustav Kirchhoff ascribed these lines to the presence of different elements.
=
In the late 1700s William Herschel mapped the distribution of stars in different directions from Earth, concluding that the universe consisted of the Sun near the center of disk of stars, the Milky Way. After John Michell demonstrated that stars differ in intrinsic luminosity and after Herschel's own observations with more powerful telescopes that additional stars appeared in all directions, astronomers began to consider that some of the fuzzy spiral nebulae were distant island Universes. 
The existence of galaxies, including the Earth's galaxy, the Milky Way, as a group of stars was only demonstrated in the 20th century. In 1912, Henrietta Leavitt discovered Cepheid variable stars with well-defined, periodic luminosity changes which can be used to fix the star's true luminosity which then becomes an accurate tool for distance estimates. Using Cepheid variable stars, Harlow Shapley constructed the first accurate map of the Milky Way. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that the universe consists of a multitude of galaxies.
=
Albert Einstein's 1917 publication of general relativity began the modern era of theoretical models of the universe as a whole. In  1922, Alexander Friedman published simplified models for the universe showing static, expanding and contracting solutions. 
In 1929 Hubble published observations that the galaxies are all moving away from Earth with a velocity proportional to distance, a relation now known as Hubble's law. This relation is expected if the universe is expanding. The consequence that the universe was once very dense and hot, a Big Bang concept expounded by Georges Lemaître in 1927, was discussed but no experimental evidence was available to support it. From the 1940s on, nuclear reaction rates under high density conditions were studied leading to the development of a successful model of big bang nucleosynthesis in the late 1940s and early 1950s. Then in 1965 cosmic microwave background radiation was discovered, cementing the evidence for the Big Bang.
Theoretical astronomy predicted the existence of objects such as black holes and neutron stars. These have been used to explain phenomena such as quasars and pulsars. 
Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. The LIGO project detected evidence of gravitational waves in 2015.
Observational astronomy relies on many different wavelengths of electromagnetic radiation and the forms of astronomy are  categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Specific information on these subfields is given below.
=
Radio astronomy uses radiation with long wavelengths, mainly between 1 millimeter and 15 meters (frequencies from 20 MHz to 300 GHz), far outside the visible range. Hydrogen, otherwise an invisible gas, produces a spectral line at 21 cm (1420 MHz) which is observable at radio wavelengths. Objects observable at radio wavelengths include interstellar gas, pulsars, fast radio bursts, supernovae, and active galactic nuclei.
=
Infrared astronomy detects infrared radiation with wavelengths longer than red visible light, outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous galactic protostars and their host star clusters.
With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space.
The James Webb Space Telescope senses infrared radiation to detect very distant galaxies. Visible light from these galaxies was emitted billions of years ago and the expansion of the universe shifted the light in to the infrared range. By studying these distant galaxies astronomers hope to learn about the formation of the first galaxies.
=
Historically, optical astronomy, which has been also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 380 to 700 nm that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.
=
Ultraviolet astronomy employs ultraviolet wavelengths which are absorbed by the Earth's atmosphere, requiring observations from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue OB stars that are very bright at these wavelengths.
=
X-ray astronomy uses X-radiation, produced by extremely hot and high-energy processes. Since X-rays are absorbed by the Earth's atmosphere, observations must be performed at high altitude, such as from balloons, rockets, or specialized satellites. X-ray sources include X-ray binaries, supernova remnants, clusters of galaxies, and active galactic nuclei. Since the Sun's surface is relatively cool, X-ray images of the Sun and other stars give valuable information on the hot solar corona.
=
Gamma ray astronomy observes astronomical objects at the shortest wavelengths (highest energy) of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory, or by specialized telescopes called atmospheric Cherenkov telescopes. Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.
Gamma-ray astronomy provides information on the origin of cosmic rays, possible annihilation events for dark matter, relativistic particles outflows from active galactic nuclei (AGN), and, using AGN as distant sources, properties of intergalactic space.
Gamma-ray bursts, which radiate transiently, are extremely energetic events, and are the brightest (most luminous) phenomena in the universe.
=
Some events originating from great distances may be observed from the Earth using systems that do not rely on electromagnetic radiation.
In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. 
Gravitational-wave astronomy employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.
The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.
=
One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects known as astrometry. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allow astronomers to plot the movement of these systems through the Milky Way galaxy.
=
Physical cosmology, the study of large-scale structure of the Universe, seeks to understand the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, the concept that the universe begin extremely dense and hot, then expanded over the course of 13.8 billion years to its present condition. The concept of the Big Bang became widely accepted after the discovery of the microwave background radiation in 1965. Dark matter and dark energy are now thought form 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.
=
The study of objects outside our galaxy is concerned with the formation and evolution of galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. These assist the understanding of the large-scale structure of the cosmos.
=
Galactic astronomy studies galaxies including the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies and contains the Solar System. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is within the dusty outer arms, large portions of the Milky Way are obscured from view.
Kinematic studies of matter in the Milky Way and other galaxies show there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.
=
The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Aspects studied include star formation in giant molecular clouds; the formation of protostars; and the transition to nuclear fusion and main-sequence stars, carrying out nucleosynthesis. Further processes studied include stellar evolution, ending either with supernovae or white dwarfs. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole.
=
Solar astronomy is the study of the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. Processes studied by the science include the sunspot cycle, the sun's changes in luminosity, both steady and periodic, and the behavior of the sun's various layers, namely its core with its nuclear fusion, the radiation zone, the convection zone, the photosphere, the chromosphere, and the corona.
=
Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as exoplanets orbiting distant stars. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft.
Processes studied include planetary differentiation; the generation of, and effects created by, a planetary magnetic field; and the creation of heat within a planet, such as by collisions, radioactive decay, and tidal heating. In turn, that heat can drive geologic processes such as volcanism, tectonics, and surface erosion, studied by branches of geology.
=
Astrochemistry is an overlap of astronomy and chemistry. It studies the abundance and reactions of molecules in the Universe, and their interaction with radiation. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. Studies in this field contribute for example to the understanding of the formation of the Solar System.
=
Astrobiology (or exobiology) studies the origin of life and its development other than on earth. It considers whether extraterrestrial life exists, and how humans can detect it if it does. It makes use of astronomy, biochemistry, geology, microbiology, physics, and planetary science to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. That encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.
=
Astronomy and astrophysics have developed interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, using archaeological and anthropological evidence. 
Astrostatistics is the application of statistics to the analysis of large quantities of observational astrophysical data. 
As "forensic astronomy", finally, methods from astronomy have been used to solve problems of art history and occasionally of law.
Astronomy is one of the sciences to which amateurs can contribute the most. Collectively, amateur astronomers observe celestial objects and phenomena, sometimes with consumer-level equipment or equipment that they build themselves. Common targets include the Sun, the Moon, planets, stars, comets, meteor showers, and deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs throughout the world have programs to help their members set up and run observational programs such as to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues.
Most amateurs work at visible wavelengths, but some have experimented with wavelengths outside the visible spectrum. The pioneer of amateur radio astronomy, Karl Jansky, discovered a radio source at the centre of the Milky Way.
Some amateur astronomers use homemade telescopes or radio telescopes originally built for astronomy research (e.g. the One-Mile Telescope).
Amateurs can make occultation measurements to refine the orbits of minor planets. They can discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make advances in astrophotography.
In the 21st century, there remain important unanswered questions in astronomy. Some are cosmic in scope: for example, what are the dark matter and dark energy that dominate the evolution and fate of the cosmos? What will be the ultimate fate of the universe? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? Others pertain to more specific classes of phenomena. For example, is the Solar System normal or atypical? What is the origin of the stellar mass spectrum, i.e. why do astronomers observe the same distribution of stellar masses—the initial mass function—regardless of initial conditions? Likewise, questions remain about the formation of the first galaxies, the origin of supermassive black holes, the source of ultra-high-energy cosmic rays, and whether there is other life in the Universe, especially other intelligent life.
Cosmogony – Theory or model concerning the origin of the universe
Outline of astronomy – Overview of the scientific field of astronomy
Outline of space science – Overview of and topical guide to space science
Space exploration – Investigation of space, planets, and moons
Local (astronomy)
=
Glossary of astronomy
List of astronomers
List of astronomical instruments
List of astronomical observatories
List of astronomy acronyms
List of astronomical societies
List of software for astronomy research and education
Forbes, George (1909). History of Astronomy. London: Plain Label Books. ISBN 978-1-60303-159-2. Archived from the original on 28 August 2018. Retrieved 7 April 2019. {{cite book}}: ISBN / Date incompatibility (help)
Harpaz, Amos (1994). Stellar Evolution. A K Peters. ISBN 978-1-56881-012-6.
Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 978-3-540-67877-9.
NASA/IPAC Extragalactic Database (NED) (NED-Distances)
Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System
https://viewspace.org/ veiwspace.org - interactives and videos about astronomy

--- TOPIC: Black_hole ---
A black hole is an astronomical body so compact that its gravity prevents anything, including light, from escaping. Albert Einstein's theory of general relativity predicts that a sufficiently compact mass will form a black hole. The boundary of no escape is called the event horizon. In general relativity, a black hole's event horizon seals an object's fate but produces no locally detectable change when crossed. General relativity also predicts that every black hole should have a central singularity, where the curvature of spacetime is infinite.
In many ways, a black hole acts like an ideal black body, as it reflects no light. Quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly.
Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. In 1916, Karl Schwarzschild found the first modern solution of general relativity that would characterise a black hole. Due to his influential research, the Schwarzschild metric is named after him. David Finkelstein, in 1958, first published the interpretation of "black hole" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The first black hole known was Cygnus X-1, identified by several researchers independently in 1971.
Black holes typically form when massive stars collapse at the end of their life cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses may form by absorbing other stars and merging with other black holes, or via direct collapse of gas clouds. There is consensus that supermassive black holes exist in the centres of most galaxies.
The presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. In extreme cases, this creates a quasar, some of the brightest objects in the universe. Merging black holes can also be detected by observation of the gravitational waves they emit. If other stars are orbiting a black hole, their orbits can be used to determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses.
The idea of a body so massive that even light could not escape was briefly proposed by English astronomical pioneer and clergyman John Michell and independently by French scientist Pierre-Simon Laplace. Both scholars proposed very large stars in contrast to the modern concept of an extremely dense object.
Michell's idea, in a short part of a letter published in 1784, calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed the speed of light. Michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies.
In 1796, Laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the Solar System in his book Exposition du Système du Monde. Franz Xaver von Zach asked Laplace for a mathematical analysis, which Laplace provided and published in a journal edited by von Zach. Laplace omitted his comment about invisible stars in later editions of his book, perhaps because Thomas Young's wave theory of light had cast doubt on the validity of the corpuscles of light used in Laplace's mathematical analysis.
=
In 1905 Albert Einstein showed that the laws of electromagnetism would be invariant under a Lorentz transformation: they would be identical for observers travelling at different velocities relative to each other. This discovery became known as the principle of special relativity. Although the laws of mechanics had already been shown to be invariant, gravity remained yet to be included. 
To add gravity to the his theory of relativity, Einstein was guided by observations by Galileo Galilei, Isaac Newton and others which showed  inertial mass equalled gravitational mass. In 1907, Einstein published a paper proposing his equivalence principle, the hypothesis that this equality means the two forms of mass have a common cause. Using the principle, Einstein predicted the redshift effect of gravity on light.
In 1911, Einstein predicted the deflection of light by massive bodies, but his analysis was premature and off by a factor of two.
By 1917, Einstein refined these ideas into his general theory of relativity, which explained how matter affects spacetime, which in turn affects the motion of other matter. This theory formed the basis for black hole physics.
=
Only a few months after Einstein published the field equations describing general relativity, astrophysicist Karl Schwarzschild set out to apply the idea to stars. He assumed spherical symmetry with no spin and found a solution to Einstein's equations. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently gave the same solution for the point mass using a different set of coordinates. At a certain radius from the center of the mass, the Schwarzschild solution became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this radius, which later became known as the Schwarzschild radius, was not understood at the time.
Many physicists of the early 20th century were skeptical of the existence of black holes. In a 1926 popular science book, Arthur Eddington discussed the idea of a star with mass compressed to its Schwarzschild radius, but his analysis was meant to illustrate issues in the then-poorly-understood theory of general relativity rather than to seriously analyze the problem: Eddington did not believe black holes existed. 
In 1939, Einstein himself used his theory of general relativity in an attempt to prove that black holes were impossible. His work relied on increasing pressure or increasing centrifugal force balancing the force of gravity so that the object would not collapse beyond its Schwarzschild radius. He missed the possibility that implosion would drive the system below this critical value.
=
By the 1920s, astronomers had classified a number of white dwarf stars as too cool and dense to be explained by the gradual cooling of ordinary stars. In 1926, Ralph Fowler showed that quantum-mechanical degeneracy pressure was larger than thermal pressure at these densities.
In 1931, using a combination of special relativity and quantum mechanics, Subrahmanyan Chandrasekhar calculated that a non-rotating body of electron-degenerate matter below a certain limiting mass (now called the Chandrasekhar limit at 1.4 M☉) is stable, and by 1934 he showed that this explained the catalog of white dwarf stars. At the same meeting where Chandrasekhar announced his results, Eddington pointed out that stars above this limit would radiate until they were sufficiently dense to prevent light from exiting, a conclusion he considered absurd. Eddington and, later, Lev Landau argued that some yet unknown mechanism would stop the collapse. They were partially correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable. These arguments from senior scientists delayed acceptance of Chandrasekhar's model.  
In the 1930s, Fritz Zwicky and Walter Baade studied stellar novae, focusing on exceptionally bright ones they called supernovae. Zwicky promoted the idea that supernovae produced stars with the density of atomic nuclei—neutron stars—but this idea was largely ignored. In 1937, Lev Landau published a detailed model of a nuclear core model for stellar cores, which caught the attention of Robert Oppenheimer. In 1939, based on Chandrasekhar's reasoning, Oppenheimer and George Volkoff predicted that neutron stars below a certain mass limit—now known as the Tolman–Oppenheimer–Volkoff limit—would be stable due to neutron degeneracy pressure. Above that limit, they reasoned that either their model would not apply or that gravitational contraction would not stop.
John Archibald Wheeler and two of his students resolved questions about the model behind the Tolman–Oppenheimer–Volkoff (TOV) limit. Harrison and Wheeler developed the equations of state relating density to pressure for cold matter all the way from atoms through electron degeneracy to neutron degeneracy. Masami Wakano and Wheeler then used the equations to compute the equilibrium curve for stars, relating mass to circumference. They found no additional features that would invalidate the TOV limit. This meant that the only thing that could prevent black holes from forming was a dynamic process ejecting sufficient mass from a star as it cooled. Wheeler held the view that the neutrons in an imploding star would convert to electromagnetic radiation fast enough that the resulting light would not be trapped in a black hole.
=
The modern concept of black holes was formulated by Robert Oppenheimer and his student Hartland Snyder in 1939. In the paper, Oppenheimer and Snyder solved Einstein's equations of general relativity for an idealized imploding star, in a model later called the Oppenheimer–Snyder model, then described the results from far outside the star. The implosion starts as one might expect: the star material rapidly collapses inward. But as density of the star increases, gravitational time dilation increases and the collapse, viewed from afar, seems to slow down. Once the star reached a critical radius—its Schwarzschild radius—faraway viewers would no longer see the implosion. The light from the implosion would be infinitely redshifted and time dilation would be so extreme that it would appear frozen in time.
In 1958, David Finkelstein identified the Schwarzschild surface as an event horizon, calling it "a perfect unidirectional membrane: causal influences can cross it in only one direction". In this sense, events that occur inside of the black hole cannot affect events that occur outside of the black hole. Finkelstein created a new reference frame to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. A similar concept had already been found by Martin Kruskal, but its significance had not been fully understood at the time. Finkelstein's new frame of reference allowed events at the event horizon of an imploding star to be related to events far away. By 1962 the two points of view were reconciled, convincing many skeptics that implosion into a black hole made physical sense.
=
The era from the mid-1960s to the mid-1970s was the "golden age of black hole research", when general relativity and black holes became mainstream subjects of research.
In this period, more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the cylindrically symmetric solution for a black hole that is both rotating and electrically charged.
In 1967, Werner Israel found that the Schwarzschild solution was the only possible solution for a nonspinning, uncharged black hole, and couldn't have any additional parameters. In that sense, a Schwarzschild black hole would be defined by its mass alone, and any two Schwarzschild black holes with the same mass would be identical. Israel later found that Reissner-Nordstrom black holes were only defined by their mass and electric charge, while Brandon Carter discovered that Kerr black holes only had two degrees of freedom, mass and spin. Together, these findings became known as the no-hair theorem, which states that a stationary black hole is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge.
At first, it was suspected that the strange mathematical singularities found in each of the black hole solutions only appeared due to the assumption that a black hole would be perfectly spherically symmetric, and therefore the singularities would not appear in generic situations where black holes would not necessarily be symmetric. This view was held in particular by Vladimir Belinski, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions, although they would later reverse their positions. However, in 1965, Roger Penrose proved that general relativity without quantum mechanics requires that singularities appear in all black holes. Shortly afterwards, Hawking generalized Penrose's solution to find that in all but a few physically infeasible scenarios, a cosmological Big Bang singularity is inevitable unless quantum gravity intervenes.
Astronomical observations also made great strides during this era. In 1967, Antony Hewish and Jocelyn Bell Burnell discovered pulsars and by 1969, these were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities, but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse. Based on observations in Greenwich and Toronto in the early 1970s, Cygnus X-1, a galactic X-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole.
Work by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as Hawking radiation.
=
The first strong evidence for black holes came from combined X-ray and optical observations of Cygnus X-1 in 1972. The x-ray source, located in the Cygnus constellation, was discovered through a survey by two suborbital rockets,  as the blocking of x-rays by Earth's atmosphere makes it difficult to detect them from the ground. Unlike stars or pulsars, Cygnus X-1 was not associated with any prominent radio or optical source. In 1972, Louise Webster, Paul Murdin, and, independently, Charles Thomas Bolton, found that Cygnus X-1 was actually in a binary system with the supergiant star HDE 226868. Using the emission patterns of the visible star, both research teams found that the mass of Cygnus X-1 was likely too large to be a white dwarf or neutron star, indicating that it was probably a black hole. Further research strengthened their hypothesis.
While Cygnus X-1, a stellar-mass black hole, was generally accepted by the scientific community as a black hole by the end of 1973, it would be decades before a supermassive black hole would gain the same broad recognition. Although, as early as the 1960s, physicists such as Donald Lynden-Bell and Martin Rees had suggested that powerful quasars in the center of galaxies were powered by accreting supermassive black holes, little observational proof existed at the time. However, the Hubble Space Telescope, launched decades later, found that supermassive black holes were not only present in these active galactic nuclei, but that supermassive black holes in the center of galaxies were ubiquitous: Almost every galaxy had a supermassive black hole at its center, many of which were quiescent. 
In 1999, David Merritt proposed the M–sigma relation, which related the dispersion of the velocity of matter in the center bulge of a galaxy to the mass of the supermassive black hole at its core. Subsequent studies confirmed this correlation. Around the same time, based on telescope observations of the velocities of stars at the center of the Milky Way galaxy, independent work groups led by Andrea Ghez and Reinhard Genzel concluded that the compact radio source in the center of the galaxy, Sagittarius A*, was likely a supermassive black hole.
On 11 February 2016, the LIGO Scientific Collaboration and Virgo Collaboration announced the first direct detection of gravitational waves, named GW150914, representing the first observation of a black hole merger. At the time of the merger, the black holes were approximately 1.4 billion light-years away from Earth and had masses of 30 and 35 solar masses. The mass of the resulting black hole was approximately 62 solar masses, with an additional three solar masses radiated away as gravitational waves. The Laser Interferometer Gravitational-Wave Observatory (LIGO) detected the gravitational waves by using two mirrors spaced four kilometers apart to measure microscopic changes in length. In 2017, Rainer Weiss, Kip Thorne, and Barry Barish, who had spearheaded the project, were awarded the Nobel Prize in Physics for their work. Since the initial discovery in 2015, hundreds more gravitational waves have been observed by LIGO and another interferometer, Virgo.
On 10 April 2019, the first direct image of a black hole and its vicinity was published, following observations made by the Event Horizon Telescope (EHT) in 2017 of the supermassive black hole in Messier 87's galactic centre. The observations were carried out by eight observatories in six geographical locations across four days and totaled five petabytes of data. In 2022, the Event Horizon Telescope collaboration released an image of the black hole in the center of the Milky Way galaxy, Sagittarius A*; The data had been collected in 2017. Detailed analysis of the motion of stars recorded by the Gaia mission produced  evidence in 2022 and 2023 of a black hole named Gaia BH1 in a binary with a Sun-like star about 1,560 light-years (480 parsecs) away. Gaia BH1 is currently the closest known black hole to Earth. Two more black holes have since been found from Gaia data, one in a binary with a red giant and the other in a binary with a G-type star.
In 2020, the  Nobel Prize in Physics was awarded for work on black holes. Andrea Ghez and Reinhard Genzel shared one-half for their discovery that Sagittarius A* is a supermassive black hole. Penrose received the other half for his work showing that the mathematics of general relativity requires the formation of black holes. Cosmologists lamented that Hawking's extensive theoretical work on black holes would not be honored since he died in 2018.
=
In December 1967, a student reportedly suggested the phrase "black hole" at a lecture by John Wheeler; Wheeler adopted the term for its brevity and "advertising value", and Wheeler's stature in the field ensured it quickly caught on, leading some to credit Wheeler with coining the phrase.
However, the term was used by others around that time. Science writer Marcia Bartusiak traces the term "black hole" to physicist Robert H. Dicke, who in the early 1960s reportedly compared the phenomenon to the Black Hole of Calcutta, notorious as a prison where people entered but never left alive.
The term black hole was used in print by Life and Science News magazines in 1963, and by science journalist Ann Ewing in her article "'Black Holes' in Space", dated 18 January 1964, which was a report on a meeting of the American Association for the Advancement of Science held in Cleveland, Ohio.
A black hole is generally defined as a region of spacetime from which no information-carrying signals or objects can escape. However, verifying an object as a black hole by this definition would require waiting for an infinite time and at an infinite distance from the black hole to verify that indeed, nothing has escaped, and thus cannot be used to identify a physical black hole. Broadly, physicists do not have a precisely-agreed-upon definition of a black hole. Among astrophysicists, a black hole is a compact object with a mass larger than four solar masses. A black hole may also be defined as a reservoir of information or a region where space is falling inwards faster than the speed of light.
The no-hair theorem postulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. If the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. The degree to which the conjecture is true for real black holes is currently an unsolved problem.
The simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild, who discovered the solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric.
Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.
=
The simplest static black holes have mass but neither electric charge nor angular momentum.  Contrary to the popular notion of a black hole "sucking in everything" in its surroundings, from far away, the external gravitational field of a black hole is identical to that of any other body of the same mass.
While a black hole can theoretically have any positive mass, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality
  
    
      
        
          
            
              Q
              
                2
              
            
            
              4
              π
              
                ϵ
                
                  0
                
              
            
          
        
        +
        
          
            
              
                c
                
                  2
                
              
              
                J
                
                  2
                
              
            
            
              G
              
                M
                
                  2
                
              
            
          
        
        ≤
        G
        
          M
          
            2
          
        
      
    
    {\displaystyle {\frac {Q^{2}}{4\pi \epsilon _{0}}}+{\frac {c^{2}J^{2}}{GM^{2}}}\leq GM^{2}}
  
for a black hole of mass M. Black holes with the maximum possible charge or spin satisfying this inequality are called extremal black holes. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These are so-called naked singularities that can be observed from the outside. Because these singularities make the universe inherently unpredictable, many physicists believe they could not exist. The weak cosmic censorship hypothesis, proposed by Sir Roger Penrose, rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. However, this theory has not yet been proven, and some physicists believe that naked singularities could exist. It is also unknown whether black holes could even become extremal, forming naked singularities, since natural processes counteract increasing spin and charge when a black hole becomes near-extremal.
The total mass of a black hole can be estimated by analyzing the motion of objects near the black hole, such as stars or gas. 
=
All black holes spin, often fast—One supermassive black hole, GRS 1915+105 has been estimated to spin at over 1,000 revolutions per second. The Milky Way's central black hole Sagittarius A* rotates at about 90% of the maximum rate. 
The spin rate can be inferred from measurements of atomic spectral lines in the X-ray range. As gas near the black hole plunges inward, high energy X-ray emission from electron-positron pairs illuminates the gas further out, appearing red-shifted due to relativistic effects. Depending on the spin of the black hole, this plunge happens at different radii from the hole, with different degrees of redshift. Astronomers can use the gap between the x-ray emission of the outer disk and the redshifted emission from plunging material to determine the spin of the black hole.
A newer way to estimate spin is based on the temperature of gasses accreting onto the black hole. The method requires an independent measurement of the black hole mass and inclination angle of the accretion disk followed by computer modeling. Gravitational waves from coalescing binary black holes can also provide the spin of both progenitor black holes and the merged hole, but such events are rare.
A spinning black hole has angular momentum. The supermassive black hole in the center of the Messier 87 (M87) galaxy appears to have an angular momentum very close to the maximum theoretical value. That uncharged limit is 
  
    
      
        J
        ≤
        
          
            
              G
              
                M
                
                  2
                
              
            
            c
          
        
        ,
      
    
    {\displaystyle J\leq {\frac {GM^{2}}{c}},}
  
allowing definition of a dimensionless spin magnitude such that
  
    
      
        0
        ≤
        
          
            
              c
              J
            
            
              G
              
                M
                
                  2
                
              
            
          
        
        ≤
        1.
      
    
    {\displaystyle 0\leq {\frac {cJ}{GM^{2}}}\leq 1.}
  
=
Most black holes are believed to have an approximately neutral charge. For example, Michal Zajaček, Arman Tursunov, Andreas Eckart, and Silke Britzen found the electric charge of Sagittarius A* to be at least ten orders of magnitude below the theoretical maximum. A charged black hole repels other like charges just like any other charged object. If a black hole were to become charged, particles with an opposite sign of charge would be pulled in by the extra electromagnetic force, while particles with the same sign of charge would be repelled, neutralizing the black hole. This effect may not be as strong if the black hole is also spinning. The presence of charge can reduce the diameter of the black hole by up to 38%.
The charge Q for a nonspinning black hole is bounded by
  
    
      
        Q
        ≤
        
          
            G
          
        
        M
        ,
      
    
    {\displaystyle Q\leq {\sqrt {G}}M,}
  
where G is the gravitational constant and M is the black hole's mass.
Black holes can have a wide range of masses. The minimum mass of a black hole formed by stellar gravitational collapse is governed by the maximum mass of a neutron star and is believed to be approximately two-to-four solar masses. However, theoretical primordial black holes, believed to have formed soon after the Big Bang, could be far smaller, with masses as little as 10−5 grams at formation. These very small black holes are sometimes called micro black holes.
Black holes formed by stellar collapse are called stellar black holes. Estimates of their maximum mass at formation vary, but generally range from 10 to 100 solar masses, with higher estimates for black holes progenated by low-metallicity stars. The mass of a black hole formed via a supernova has a lower bound: If the progenitor star is too small, the collapse may be stopped by the degeneracy pressure of the star's constituents, allowing the condensation of matter into an exotic denser state. Degeneracy pressure occurs from the Pauli exclusion principle—Particles will resist being in the same place as each other. Smaller progenitor stars, with masses less than about 8 M☉, will be held together by the degeneracy pressure of electrons and will become a white dwarf. For more massive progenitor stars, electron degeneracy pressure is no longer strong enough to resist the force of gravity and the star will be held together by neutron degeneracy pressure, which can occur at much higher densities, forming a neutron star. If the star is still too massive, even neutron degeneracy pressure will not be able to resist the force of gravity and the star will collapse into a black hole. Stellar black holes can also gain mass via accretion of nearby matter, often from a companion object such as a star. 
Black holes that are larger than stellar black holes but smaller than supermassive black holes are called intermediate-mass black holes, with masses of approximately 102 to 105 solar masses. These black holes seem to be rarer than their stellar and supermassive counterparts, with relatively few candidates having been observed. Physicists have speculated that such black holes may form from collisions in globular and star clusters or at the center of low-mass galaxies. They may also form as the result of mergers of smaller black holes, with several LIGO observations finding merged black holes within the 110-350 solar mass range.
The black holes with the largest masses are called supermassive black holes, with masses more than 106 times that of the Sun. These black holes are believed to exist at the centers of almost every large galaxy, including the Milky Way. Some scholars have theorized that the collapse of very massive population III stars in the early universe could have produced black holes of up to 103 M☉. These black holes could be the seeds of the supermassive black holes found in the centres of most galaxies.  Some scientists have proposed a subcategory of even larger black holes, called ultramassive black holes, with masses greater than 109-1010 solar masses. Theoretical models predict that the accretion disc that feeds black holes will be unstable once a black hole reaches 50-100 billion times the mass of the Sun, setting a rough upper limit to black hole mass.
=
For a nonspinning, uncharged black hole, the radius of the event horizon, or Schwarzschild radius, is proportional to the mass, M, through
  
    
      
        
          r
          
            
              s
            
          
        
        =
        
          
            
              2
              G
              M
            
            
              c
              
                2
              
            
          
        
        ≈
        2.95
        
        
          
            M
            
              M
              
                ⊙
              
            
          
        
         
        
          k
          m
          ,
        
      
    
    {\displaystyle r_{\mathrm {s} }={\frac {2GM}{c^{2}}}\approx 2.95\,{\frac {M}{M_{\odot }}}~\mathrm {km,} }
  
where rs is the Schwarzschild radius and M☉ is the mass of the Sun. For a black hole with nonzero spin or electric charge, the radius is smaller, until an extremal black hole could have an event horizon close to
  
    
      
        
          r
          
            
              +
            
          
        
        =
        
          
            
              G
              M
            
            
              c
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle r_{\mathrm {+} }={\frac {GM}{c^{2}}},}
  
half the radius of a nonspinning, uncharged black hole of the same mass.
While black holes are conceptually invisible sinks of all matter and light, in astronomical settings, their enormous gravity alters the motion of surrounding objects and pulls nearby gas inwards at near-light speed, making the area around black holes the brightest objects in the universe. 
=
 Relativistic jets 
Some black holes have relativistic jets—thin streams of plasma travelling away from the black hole at more than one-tenth of the speed of light. A small faction of the matter falling towards the black hole gets accelerated away along the hole rotation axis. These jets can extend as far as millions of parsecs from the black hole itself. 
Black holes of any mass can have jets. However, they are typically observed around spinning black holes with strongly-magnetized accretion disks. Relativistic jets were more common in the early universe, when galaxies and their corresponding supermassive black holes were rapidly gaining mass. All black holes with jets also have an accretion disk, but the jets are usually brighter than the disk. Supermassive black holes with jets are often called quasars or even "macroquasars" and are observed in other galaxies. Stellar-mass objects with jets, typically observed in the Milky Way, are referred to as microquasars.
The mechanism of formation of jets is not yet known, but several options have been proposed. One method proposed to fuel these jets is the Blandford-Znajek process, which suggests that the dragging of magnetic field lines by a black hole's rotation could launch jets of matter into space. The Penrose process, which involves extraction of a black hole's rotational energy, has also been proposed as a potential mechanism of jet propulsion.
 Accretion disk 
Due to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disk-like structure around the object. As the disk's angular momentum is transferred outward due to internal processes, its matter falls farther inward, converting its gravitational energy into heat and releasing a large flux of x-rays. The temperature of these disks can range from thousands to millions of Kelvin, and temperatures can differ throughout a single accretion disk. Accretion disks can also emit in other parts of the electromagnetic spectrum, depending on the disk's turbulence and magnetization and the black hole's mass and angular momentum. 
Accretion disks can be defined as geometrically thin or geometrically thick. Geometrically thin disks are mostly confined to the black hole's equatorial plane and have a well-defined edge at the innermost stable circular orbit (ISCO), while geometrically thick disks are supported by internal pressure and temperature and can extend inside the ISCO. Disks with high rates of electron scattering and absorption, appearing bright and opaque, are called 'optically thick'; 'optically thin' disks are more translucent and produce fainter images when viewed from afar. Accretion disks of black holes accreting beyond the Eddington limit are often referred to as "polish donuts" due to their thick, toroidal shape that resembles that of a donut.
Quasar accretion disks are expected to usually appear blue in color. The disk for a stellar black hole, on the other hand, would likely look orange, yellow, or red, with its inner regions being the brightest. Theoretical research suggests that the hotter a disk is, the bluer it should be, although this is not always supported by observations of real astronomical objects. Accretion disk colors may also be altered by the Doppler effect, with the part of the disk travelling towards an observer appearing bluer and brighter and the part of the disk travelling away from the observer appearing redder and dimmer.
 Innermost stable circular orbit (ISCO) 
In Newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. In general relativity, however, there exists a smallest possible radius for which a massive particle can orbit stably. Any infinitesimal inward perturbations to this orbit will lead to the particle spiraling into the black hole, and any outward perturbations will, depending on the energy, cause the particle to spiral in, move to a stable orbit further from the black hole, or escape to infinity. This orbit is called the innermost stable circular orbit, or ISCO. The location of the ISCO depends on the spin of the black hole and the spin of the particle itself. In the case of a Schwarzschild black hole (spin zero) and a particle without spin, the location of the ISCO is:
  
    
      
        
          r
          
            
              I
              S
              C
              O
            
          
        
        =
        3
        
        
          r
          
            s
          
        
        =
        
          
            
              6
              
              G
              M
            
            
              c
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle r_{\rm {ISCO}}=3\,r_{\text{s}}={\frac {6\,GM}{c^{2}}},}
  
 where 
  
    
      
        
          r
          
            
              
                
                
                  I
                  S
                  C
                  O
                
              
            
          
        
      
    
    {\displaystyle r_{\rm {_{ISCO}}}}
  
 is the radius of the ISCO, 
  
    
      
        
          r
          
            s
          
        
      
    
    {\displaystyle r_{\text{s}}}
  
 is the Schwarzschild radius of the black hole, 
  
    
      
        G
      
    
    {\displaystyle G}
  
 is the gravitational constant, and 
  
    
      
        c
      
    
    {\displaystyle c}
  
 is the speed of light.
The radius of this orbit changes slightly based on particle spin. For charged black holes, the ISCO moves inwards. For spinning black holes, the ISCO is moved inwards for particles orbiting in the same direction that the black hole is spinning (prograde) and outwards for particles orbiting in the opposite direction (retrograde). For example, the ISCO for a particle orbiting retrograde can be as far out as about 
  
    
      
        9
        
          r
          
            s
          
        
      
    
    {\displaystyle 9r_{\text{s}}}
  
, while the ISCO for a particle orbiting prograde can be as close as at the event horizon itself.
 Photon sphere and shadow 
The photon sphere is a spherical boundary for which photons moving on tangents to that sphere are bent completely around the black hole, possibly orbiting multiple times. Light rays with impact parameters less than the radius of the photon sphere enter the black hole.
For Schwarzschild black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius; the radius for non-Schwarzschild black holes is at least 1.5 times the radius of the event horizon. When viewed from a great distance, the photon sphere creates an observable black hole shadow. Since no light emerges from within the black hole, this shadow is the limit for possible observations. The shadow of colliding black holes should have characteristic warped shapes, allowing scientists to detect black holes that are about to merge.
While light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Therefore, any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon. Light emitted towards the photon sphere may also curve around the black hole and return to the emitter.
For a rotating, uncharged black hole, the radius of the photon sphere depends on the spin parameter and whether the photon is orbiting prograde or retrograde. For a photon orbiting prograde, the photon sphere will be 1-3 Schwarzschild radii from the center of the black hole, while for a photon orbiting retrograde, the photon sphere will be between 3-5 Schwarzschild radii from the center of the black hole. The exact location of the photon sphere depends on the magnitude of the black hole's rotation. For a charged, nonrotating black hole, there will only be one photon sphere, and the radius of the photon sphere will decrease for increasing black hole charge. For non-extremal, charged, rotating black holes, there will always be two photon spheres, with the exact radii depending on the parameters of the black hole.
 Ergosphere 
Near a rotating black hole, spacetime rotates similar to a vortex. The rotating spacetime will drag any matter and light into rotation around the spinning black hole. This effect of general relativity, called frame dragging, gets stronger closer to the spinning mass.  The region of spacetime in which it is impossible to stay still is called the ergosphere. This is because frame dragging is so strong near the event horizon that the space around the black hole is moving faster than the speed of light.
The ergosphere of a black hole is a volume bounded by the black hole's event horizon and the ergosurface, which coincides with the event horizon at the poles but bulges out from it around the equator.
Matter and radiation can escape from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole, slowing down the rotation of the black hole. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process, is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.
 Plunging region 
The observable region of spacetime around a black hole closest to its event horizon is called the plunging region. In this area it is no longer possible for free falling matter to follow circular orbits or stop a final descent into the black hole. Instead, it will rapidly plunge toward the black hole at close to the speed of light, growing increasingly hot and producing a characteristic, detectable thermal emission. However, light and radiation emitted from this region can still escape from the black hole's gravitational pull.
=
The defining feature of a black hole is the existence of an event horizon, a boundary in spacetime through which matter and light can pass only inward towards the center of the black hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach or affect an outside observer, making it impossible to determine whether such an event occurred.
As predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass. At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.
To a distant observer, a clock near a black hole would appear to tick more slowly than one further from the black hole. This effect, known as gravitational time dilation, would also cause an object falling into a black hole to appear to slow as it approached the event horizon, never quite reaching the horizon from the perspective of an outside observer. All processes on this object would appear to slow down, and any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift. An object falling from 1/2 of a Schwarzschild radius above the event horizon would fade away until it could no longer be seen, disappearing from view within one hundredth of a second. It would also appear to flatten onto the black hole, joining all other material that had ever fallen into the hole.
On the other hand, an observer falling into a black hole would not notice any of these effects as they cross the event horizon. Their own clocks appear to them to tick normally, and they cross the event horizon after a finite time without noting any singular behaviour. In general relativity, it is impossible to determine the location of the event horizon from local observations, due to Einstein's equivalence principle.
For non-rotating black holes, the geometry of the event horizon is precisely spherical, while for rotating black holes, the event horizon is oblate.
=
 Cauchy horizon 
Black holes that are rotating and/or charged have an inner horizon, often called the Cauchy horizon, inside of the black hole. The inner horizon is divided up into two segments: an ingoing section and an outgoing section.
At the ingoing section of the Cauchy horizon, radiation and matter that fall into the black hole would build up at the horizon, causing the curvature of spacetime to go to infinity. This would cause an observer falling in to experience tidal forces. This phenomenon is often called mass inflation, since it is associated with a parameter dictating the black hole's internal mass growing exponentially, and the buildup of tidal forces is called the mass-inflation singularity or Cauchy horizon singularity. Some physicists have argued that in realistic black holes, accretion and Hawking radiation would stop mass inflation from occurring.
At the outgoing section of the inner horizon, infalling radiation would backscatter off of the black hole's spacetime curvature and travel outward, building up at the outgoing Cauchy horizon. This would cause an infalling observer to experience a gravitational shock wave and tidal forces as the spacetime curvature at the horizon grew to infinity. This buildup of tidal forces is called the shock singularity.
Both of these singularities are weak, meaning that an object crossing them would only be deformed a finite amount by tidal forces, even though the spacetime curvature would still be infinite at the singularity. This is as opposed to a strong singularity, where an object hitting the singularity would be stretched and squeezed by an infinite amount. They are also null singularities, meaning that a photon could travel parallel to the them without ever being intercepted.
 Singularity 
Mathematical models of black holes based on general relativities have singularities at their centers—points where the curvature of spacetime becomes infinite, and geodesics terminate within a finite proper time. However, it is unknown whether these singularities truly exist in real black holes. Some physicists believe that singularities do not exist, and that their existence, which would make spacetime unpredictable, signals a breakdown of general relativity and a need for a more complete understanding of quantum gravity. Others believe that such singularities could be resolved within the current framework of physics, without having to introduce quantum gravity. There are also physicists, including Kip Thorne and Charles Misner, who believe that not all singularities can be resolved, and that some likely still exist in the real universe despite the effects of quantum gravity. Finally, still others believe that singularities do not exist, and that their existence in general relativity does not matter, since general relativity is already believed to be an incomplete theory.
According to general relativity, every black hole has a singularity inside. For a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form a ring singularity that lies in the plane of rotation. In both cases, the singular region has zero volume. All of the mass of the black hole ends up in the singularity. Since the singularity has nonzero mass in an infinitely small space, it can be thought of as having infinite density.
Observers falling into a Schwarzschild black hole (i.e., non-rotating and not charged) cannot avoid being carried into the singularity once they cross the event horizon. As they fall further into the black hole, they will be torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the "noodle effect". Eventually, they will reach the singularity and be crushed into an infinitely small point.
Before the 1970s, most physicists believed that the interior of a Schwarzschild black hole curved inwards towards a sharp point at the singularity. However, in the late 1960s, Soviet physicists Vladimir Belinskii, Isaak Khalatnikov, and Evgeny Lifshitz discovered that this model was only true when the spacetime inside the black hole had not been perturbed. Any perturbations, such as those caused by matter or radiation falling in, would cause space to oscillate chaotically near the singularity. Any matter falling in would experience intense tidal forces rapidly changing in direction, all while being compressed into an increasingly small volume. Physicists termed these oscillations "Mixmaster dynamics", after a brand of mixer that was popular at the time that Belinskii, Khalatnikov, and Lifshitz made their discovery, because they have a similar effect on matter near a singularity as an electric mixer would have on dough.
In the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole. The possibility of travelling to another universe is, however, only theoretical, since any perturbation would destroy this possibility. It also appears to be possible to follow closed timelike curves (returning to one's own past) around the Kerr singularity, which leads to problems with causality like the grandfather paradox. However, processes inside the black hole, such as quantum gravity effects or mass inflation, might prevent closed timelike curves from arising.
To solve technical issues with general relativity, some models of gravity do not include black hole singularities. These theoretical black holes without singularities are called regular, or nonsingular, black holes. For example, the fuzzball model, based on string theory, states that black holes are actually made up of quantum microstates and need not have a singularity or an event horizon. The theory of loop quantum gravity proposes that the curvature and density at the center of a black hole is large, but not infinite.
Black holes are formed by gravitational collapse of massive stars, either by direct collapse or during a supernova explosion in a process called "fallback". Black holes can result from the merger of two neutron stars or a neutron star and a black hole. Other more speculative mechanisms include primordial black holes created from density fluctuations in the early universe, the collapse of dark stars, a hypothetical object powered by annihilation of dark matter, or from hypothetical self-interacting dark matter.
=
Gravitational collapse occurs when an object's internal pressure is insufficient to resist the object's own gravity. At the end of a star's life, it will run out of hydrogen to fuse, and will start fusing more and more massive elements, until it gets to iron. Since the fusion of elements heavier than iron would require more energy than it would release, nuclear fusion ceases. If the iron core of the star is too massive, the star will no longer be able to support itself and will undergo gravitational collapse.
While most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the delay growing to infinity as the emitting material reaches the event horizon. Thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.
=
Observations of quasars at redshift 
  
    
      
        z
        ∼
        7
      
    
    {\displaystyle z\sim 7}
  
, less than a billion years after the Big Bang, has led to investigations of other ways to form black holes. The accretion process to build supermassive black holes has a limiting rate of mass accumulation and 
a billion years is not enough time to reach quasar status.
One suggestion is direct collapse of nearly pure hydrogen gas (low metalicity) clouds characteristic of the young universe, forming a supermassive star which collapses into a black hole.
It has been suggested that seed black holes with typical masses of ~105 M☉ could have formed in this way which then could grow to ~109 M☉. However, the very large amount of gas required for direct collapse is not typically stable to fragmentation to form multiple stars. Thus another approach suggests massive star formation followed by collisions that seed massive black holes which ultimately merge to create a quasar.
A neutron star in a common envelope with a regular star can accrete sufficient material to collapse to a black hole or two neutron stars can merge. These avenues for the formation ob black holes are considered relatively rare.
=
In the current epoch of the universe, conditions needed to form black holes are rare and are mostly only found in stars. However, in the early universe, conditions may have allowed for black hole formations via other means. Fluctuations of spacetime soon after the Big Bang may have formed areas that were denser then their surroundings. Initially, these regions would not have been compact enough to form a black hole, but eventually, the curvature of spacetime in the regions become large enough to cause them to collapse into a black hole. Different models for the early universe vary widely in their predictions of the scale of these fluctuations. Various models predict the creation of primordial black holes ranging from a Planck mass (~2.2×10−8 kg) to hundreds of thousands of solar masses. Primordial black holes with masses less than 1015 g would have evaporated by now due to Hawking radiation.
Despite the early universe being extremely dense, it did not re-collapse into a black hole during the Big Bang, since the universe was expanding rapidly and did not have the gravitational differential necessary for black hole formation. Models for the gravitational collapse of objects of relatively constant size, such as stars, do not necessarily apply in the same way to rapidly expanding space such as the Big Bang.
=
In principle, black holes could be formed in high-energy particle collisions that achieve sufficient density, although no such events have been detected.
These hypothetical micro black holes, which could form from the collision of cosmic rays and  Earth's atmosphere or in particle accelerators like the Large Hadron Collider, would not be able to aggregate additional mass. Instead, they would evaporate in about 10−25 seconds, posing no threat to the Earth.
=
Once a black hole has formed, it can continue to grow by absorbing additional matter. Any black hole will continually absorb gas and interstellar dust from its surroundings. This growth process is one possible way through which some supermassive black holes may have been formed, although the formation of supermassive black holes is still an open field of research. A similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters. Black holes can also merge with other objects such as stars or even other black holes. This is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many smaller objects. The process has also been proposed as the origin of some intermediate-mass black holes.
Restrictions have been proposed to limit the growth rate of black holes. In theory, at a certain rate of accretion, the outward radiation pressure will become as strong as the inward gravitational force, and the black hole will be unable to accrete any faster. This limit is called the Eddington limit. However, in practicality, many black holes accrete beyond this rate due to their non-spherical geometry or instabilities in the accretion disk. Accretion beyond the limit is called Super-Eddington accretion and may have been commonplace in the early universe. Additionally, mergers of supermassive black holes may take a long time: As a binary of supermassive black holes approach each other, most nearby stars are ejected, leaving little for the remaining black holes to gravitationally interact with that would allow them to get closer to each other. This phenomenon has been called the final parsec problem, as the distance at which this happens is usually around one parsec.
=
When a black hole accretes matter, the gas in the inner accretion disk orbits at very high speeds because of its proximity to the black hole. The resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainly X-rays). These bright X-ray sources may be detected by telescopes. By the time the matter of the disk reaches the ISCO, it will have given off a significant amount of energy: Between 5.7% and 42% of its mass will have been converted to energy, depending on the black hole's spin. Most of this energy (about 90%) is released in a relatively small area, within about 20 black hole radii. In many cases, accretion disks are accompanied by relativistic jets that are emitted along the black hole's poles, which carry away much of the energy. The mechanism for the creation of these jets is currently not well understood, in part due to insufficient data.
As such, many of the universe's most energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei and quasars are believed to be the accretion disks of supermassive black holes. Similarly, X-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion. It has also been suggested that some ultraluminous X-ray sources may be the accretion disks of intermediate-mass black holes.
Stars have been observed to get torn apart by tidal forces in the immediate vicinity of supermassive black holes in galaxy nuclei, in what is known as a  tidal disruption event (TDE). Some of the material from the disrupted star forms an accretion disk around the black hole, which emits observable electromagnetic radiation.
=
The correlation between the masses of supermassive black holes in the centres of galaxies with the velocity dispersion and mass of stars in their host bulges suggests that the formation of galaxies and the formation of their central black holes are related. Black hole winds from rapid accretion, particularly when the galaxy itself is still accreting matter, can compress gas nearby, accelerating star formation. However, if the winds become too strong, the black hole may blow nearly all of the gas out of the galaxy, quenching star formation. Black hole jets may also energize nearby cavities of plasma and eject low-entropy gas from out of the galactic core, causing gas in galactic centers to be hotter than expected.
=
In 1974, Stephen Hawking predicted that black holes emit small amounts of thermal radiation at a temperature of 
  
    
      
        
          
            
              ℏ
              
                c
                
                  3
                
              
            
            
              8
              π
              G
              M
              
                k
                
                  B
                
              
            
          
        
      
    
    {\displaystyle {\frac {\hbar c^{3}}{8\pi GMk_{B}}}}
  
, where 
  
    
      
        
          ℏ
        
      
    
    {\displaystyle {\hbar }}
  
 is the reduced Planck constant, 
  
    
      
        c
      
    
    {\displaystyle c}
  
 is the speed of light, 
  
    
      
        G
      
    
    {\displaystyle G}
  
 is the gravitational constant, 
  
    
      
        M
      
    
    {\displaystyle M}
  
 is the mass of the black hole and 
  
    
      
        
          k
          
            B
          
        
      
    
    {\displaystyle k_{B}}
  
 is the Boltzmann constant. This effect has become known as Hawking radiation. By applying quantum field theory to black holes, Hawking determined that a black hole should continuously emit thermal blackbody radiation. This theory was supported by previous work by Jacob Bekenstein, who theorized that black holes should have a finite entropy proportional to their surface area, and therefore should also have a temperature. It is also analogous to a special relativistic effect called the Unruh effect, which predicts that an accelerating observer should detect a higher temperature of their surroundings than a nonaccelerating observer. The temperature experienced by an accelerating observer is the same as an observer near the horizon of a black hole with an equivalent surface gravity. This result is in accordance with the equivalence principle, which states that the effects of acceleration in flat spacetime should be the same as the effects of the equivalent acceleration due to gravity in curved spacetime.
Since Hawking's publication, many others have mathematically verified the result through different approaches. If Hawking's theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles. The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes.
A stellar black hole of 1 M☉ has a Hawking temperature of 62 nanokelvins. This is far less than the 2.7 K temperature of the cosmic microwave background radiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through Hawking radiation and thus will grow instead of shrinking. To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than the Moon. Such a black hole would have a diameter of less than a tenth of a millimetre.
If a black hole is very small, the radiation effects are expected to become very strong. A black hole with the mass of a car would have a diameter of about 10−24 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster. For a very small black hole, quantum gravity effects are expected to play an important role and could hypothetically make the black hole stable, although current developments in quantum gravity do not indicate this is the case.
The Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from Earth. A possible exception is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes, with modern research predicting that primordial black holes must make up less than a fraction of 10−7 of the universe's total mass. NASA's Fermi Gamma-ray Space Telescope, launched in 2008, has searched for these flashes, but has not yet found any.
If black holes evaporate via Hawking radiation, a non-accreting solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064 years. A supermassive black hole with a mass of 1011 M☉ will evaporate in around 2×10100 years. During the collapse of a supercluster of galaxies, supermassive black holes are predicted to grow to up to 1014 M☉. Even these would evaporate over a timescale of up to 10106 years. It is unknown exactly what would happen at the end of a black hole's evaporation. Some physicists theorize that it would leave behind a remnant, such as a naked singularity.
=
The properties of a black hole are constrained and interrelated by the theories that predict these properties. When based on general relativity, these relationships are called the laws of black hole mechanics. For a  black hole that is not still forming or accreting matter, the zeroth law of black hole mechanics states the black hole's surface gravity is constant across the event horizon. The first law relates changes in the black hole's surface area, angular momentum, and charge to changes in its energy. The second law says the surface area of a black hole never decreases on its own. Finally, the third law says that the surface gravity of a black hole is never zero. These laws are mathematical analogs of the laws of thermodynamics. They are not equivalent, however, because, according to general relativity without quantum mechanics, a black hole can never emit radiation, and thus its temperature must always be zero.
Quantum mechanics predicts that a black hole will continuously emit thermal Hawking radiation, and therefore must always have a nonzero temperature. It also predicts that all black holes have entropy which scales with their surface area. When quantum mechanics is accounted for, the laws of black hole mechanics become equivalent to the classical laws of thermodynamics. However, these conclusions are derived without a complete theory of quantum gravity, although many potential theories do predict black holes having entropy and temperature. Thus, the true quantum nature of black hole thermodynamics continues to be debated.
Millions of black holes with around 30 solar masses derived from stellar collapse are expected to exist in the Milky Way. Even a dwarf galaxy like Draco should have hundreds. Only a few of these have been detected.
By nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. The defining characteristic of a black hole is its event horizon. The horizon itself cannot be imaged so all other possible explanations for these indirect observations must be considered and eliminated before concluding that a black hole has been observed.
=
The Event Horizon Telescope (EHT) is a global system of radio telescopes capable of directly observing a black hole shadow. In April 2017, EHT began observing the black hole at the centre of Messier 87. Using petabytes of data from eight different radio observatories over a ten-day observation period, the EHT team created a composite image of the black hole, which they debuted in April 2019. The black hole's shadow appears as a dark circle in the centre of the image, bordered by the orange-red ring of its accretion disk. The bottom half of the disk is brighter than the top due to Doppler beaming: Material at the bottom of the disk, which is travelling towards the viewer at relativistic speeds, appears brighter than the material at the top of the disk, which is travelling away from the viewer. In April 2023, the EHT team presented an image of the shadow of the Messier 87 black hole and its high-energy jet, viewed together for the first time.
On 12 May 2022, the EHT released the first image of Sagittarius A*, the supermassive black hole at the centre of the Milky Way galaxy. The EHT team had previously detected magnetic field lines around the black hole, confirming theoretical predictions of magnetic fields around black holes. The imaging of Sagittarius A* was done concurrently with the imaging of the Messier 87 (M87*) black hole. Like M87*, Sagittarius A*'s shadow and accretion disk can be seen in the EHT image, with the size of the shadow matching theoretical projections. Although the image of Sagittarius A* was created through the same process as for M87*, it was significantly more complex to image Sagittarius A* because of the instability of its surroundings. Because Sagittarius A* is one thousand times less massive as M87*, its accretion disk has a much shorter orbital period, so the environment around Sagittarius A* was rapidly changing as the EHT team was trying to image it. Additionally, turbulent plasma lies between Sagittarius A* and Earth, preventing resolution of the image at longer wavelengths.
=
On 14 September 2015, the LIGO gravitational wave observatory made the first-ever successful direct observation of gravitational waves. The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses. Because the two objects were only 350 km apart just before the merger, yet were more massive than possible for a neutron star, the LIGO team concluded that the gravitational waves must have come from a merger of two black holes. The signal observed by LIGO also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state. From the ringdown, the LIGO team was able to determine that the resulting merged black hole was spinning at 67% of the maximum rate and had a mass of 62 solar masses, having lost three solar masses as gravitational waves during the merger.
The observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.
Since then, many more gravitational wave events have been observed.
=
The proper motions of stars near the centre of the Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. In 1998, by fitting the motions of the stars to Keplerian orbits, the astronomers were able to infer that Sagittarius A* must be a 2.6×106 M☉ object must be contained within a radius of 0.02 light-years.
Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass of Sagittarius A* to 4.3×106 M☉, with a radius of less than 0.002 light-years. This upper limit radius is larger than the Schwarzschild radius for the estimated mass, so the combination does not prove Sagittarius A* is a black hole. Nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes. The Event Horizon Telescope image of Sagittarius A*, released in 2022, provided further confirmation that it is indeed a black hole.
=
X-ray binaries are binary systems that emit a majority of their radiation in the X-ray part of the electromagnetic spectrum. These X-ray emissions result when a compact object accretes matter from an ordinary star. The presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole. By measuring the orbital period of the binary, the distance to the binary from Earth, and the mass of the companion star, scientists can estimate the mass of the compact object. The Tolman-Oppenheimer-Volkoff limit (TOV limit) dictates the largest mass a nonrotating neutron star can be, and is estimated to be about two solar masses. While a rotating neutron star can be slightly more massive, if the compact object is much more massive than the TOV limit, it cannot be a neutron star and is generally expected to be a black hole.
The first strong candidate for a black hole, Cygnus X-1, was discovered in this way by Charles Thomas Bolton, Louise Webster, and Paul Murdin in 1972.  Observations of rotation broadening of the optical star reported in 1986 lead to a compact object mass estimate of 16 solar masses, with 7 solar masses as the lower bound. In 2011, this estimate was updated to 14.1±1.0 M☉ for the black hole and 19.2±1.9 M☉ for the optical stellar companion.
X-ray binaries can be categorized as either low-mass or high-mass; This classification is based on the mass of the companion star, not the compact object itself. In a class of X-ray binaries called soft X-ray transients, the companion star is of relatively low mass, allowing for more accurate estimates of the black hole mass. These systems actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission, called quiescence, the accretion disk is extremely faint, allowing detailed observation of the companion star. Numerous black hole candidates have been measured by this method. Black holes are also sometimes found in binaries with other compact objects, such as white dwarfs, neutron stars, and other black holes.
=
The centre of nearly every galaxy contains a supermassive black hole. The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy's bulge, known as the M–sigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself.
 Active galactic nucleus 
Astronomers use the term "active galaxy" to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. Theoretical and observational studies have shown that the high levels of activity in the centers of these galaxies, regions called active galactic nuclei (AGN), may be explained by accretion onto supermassive black holes. These AGN consist of a central black hole that may be millions or billions of times more massive than the Sun, a disk of interstellar gas and dust called an accretion disk, and two jets perpendicular to the accretion disk.
Although supermassive black holes are expected to be found in most AGN, only some galaxies' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include the Andromeda Galaxy, Messier 32, Messier 87, the Sombrero Galaxy, and the Milky Way itself.
 Quasi-periodic oscillations 
The X-ray emissions from the disks of accreting black holes sometimes flicker at certain frequencies. These signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). Some scientists also suggest that these oscillations may be caused by the black hole's axis of rotation being out of alignment with the binary system's axis of rotation. Since the frequency of quasi-periodic oscillations is correlated with the mass and rotation rate of the compact object, it can be used as an alternative way to determine the properties of candidate black holes.
=
Another way black holes can be detected is through observation of effects caused by their strong gravitational field. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected, making objects behind them appear distorted. When the lensing object is a black hole, this effect can be strong enough to create multiple images of a star or other luminous source. However, the distance between the lensed images may be too small for contemporary telescopes to resolve—This phenomenon is called microlensing. Instead of seeing two images of a lensed star, astronomers see the star brighten slightly as the black hole moves towards the line of sight between the star and Earth and then return to its normal luminosity as the black hole moves away. The turn of the millennium saw the first 3 candidate detections of black holes in this way, and in January 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole. This was also the first determination of an isolated black hole mass, 7.1±1.3 M☉.
While there is a strong case for supermassive black holes, the model for stellar-mass black holes assumes of an upper limit for the mass of a neutron star: objects observed to have more mass are assumed to be black holes.  However, the properties of extremely dense matter are poorly understood. New exotic phases of matter could allow other kinds of massive objects. Quark stars would be made up of quark matter and supported by quark degeneracy pressure, a form of degeneracy pressure even stronger than neutron degeneracy pressure. This would halt gravitational collapse at a higher mass than for a neutron star. Even stronger stars called electroweak stars would convert quarks in their cores into leptons, providing additional pressure to stop the star from collapsing. If, as some extensions of the Standard Model posit, quarks and leptons are made up of the even-smaller fundamental particles called preons, a very compact star could be supported by preon degeneracy pressure. While none of these hypothetical models can explain all of the observations of stellar black hole candidates, a Q star is the only alternative which could significantly exceed the mass limit for neutron stars and thus provide an alternative for supermassive black holes.
A few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically, but which function via a different mechanism. A dark energy star would convert infalling matter into vacuum energy; This vacuum energy would be much larger than the vacuum energy of outside space, exerting outwards pressure and preventing a singularity from forming. A black star would be gravitationally collapsing slowly enough that quantum effects would keep it just on the cusp of fully collapsing into a black hole. A gravastar would consist of a very thin shell and a dark-energy interior providing outward pressure to stop the collapse into a black hole or formation of a singularity; It could even have another gravastar inside, called a 'nestar'.
Since the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes. The average density of a 108 M☉ black hole is comparable to that of water. Consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates.
=
According to the no-hair theorem, a black hole is defined by only three parameters: its mass, charge, and angular momentum. This seems to mean that all other information about the matter that went into forming the black hole is lost, as there is no way to determine anything about the black hole from outside other than those three parameters. When black holes were thought to persist forever, this information loss was not problematic, as the information can be thought of as existing inside the black hole. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information is seemingly gone forever. This is called the black hole information paradox.
This paradox pits a foundational principle of quantum mechanics against one of general relativity. According to quantum mechanics, a pure state with no entropy can never evolve into a mixed state with entropy, a consequence of unitarity. All Hawking radiation, as thermal radiation, is in a mixed quantum state. Over vast amounts of time, the entire black hole is converted to this mixed-state radiation. Therefore, any quantum system in a pure state that had fallen into the black hole before it radiated away would seemingly have been converted into mixed-state radiation, violating unitarity. Since quantum mechanics would not allow this conversion, some scientists have suggested that the pure state of the quantum system is destroyed at the black hole event horizon. However, this conflicts with the equivalence principle of general relativity, which states that there should be no physical difference between free-falling into a black hole and floating in empty space. According to the principle, since a pure state cannot be destroyed in empty space, it should not be destroyed at the event horizon of a black hole either. Theoretical studies analyzing the paradox have led to both further paradoxes and new ideas about the intersection of quantum mechanics and general relativity. While there is no consensus on the resolution of the paradox, work on the problem is expected to be important for a theory of quantum gravity.
=
Observations of faraway galaxies have found that ultraluminous quasars, powered by supermassive black holes, existed in the early universe as far as redshift 
  
    
      
        z
        ≥
        7
      
    
    {\displaystyle z\geq 7}
  
. These black holes have been assumed to be the products of the gravitational collapse of large population III stars. However, these stellar remnants were not massive enough to produce the quasars observed at early times without accreting beyond the Eddington limit, the theoretical maximum rate of black hole accretion.
Physicists have suggested a variety of different mechanisms by which these supermassive black holes may have formed. It has been proposed that smaller black holes may have also undergone mergers to produce the observed supermassive black holes. It is also possible that they were seeded by direct-collapse black holes, in which a large cloud of hot gas avoids fragmentation that would lead to multiple stars, due to low angular momentum or heating from a nearby galaxy. Given the right circumstances, a single supermassive star forms and collapses directly into a black hole without undergoing typical stellar evolution. Additionally, these supermassive black holes in the early universe may be high-mass primordial black holes, which could have accreted further matter in the centers of galaxies. Finally, certain mechanisms allow black holes to grow faster than the theoretical Eddington limit, such as dense gas in the accretion disk limiting outward radiation pressure that prevents the black hole from accreting. However, the formation of bipolar jets prevent super-Eddington rates.
Black holes have been portrayed in science fiction in a variety of ways. Even before the advent of the term itself, objects with characteristics of black holes appeared in stories such as the 1928 novel The Skylark of Space with its "black Sun" and the "hole in space" in the 1935 short story Starship Invincible. As black holes grew to public recognition in the 1960s and 1970s, they began to be featured in films as well as novels, such as Disney's The Black Hole (1979). Black holes have also been used in works of the 21st century, such as Christopher Nolan's science fiction epic Interstellar (2014).
Authors and screenwriters have exploited the relativistic effects of black holes, particularly gravitational time dilation. For example, Interstellar features a black hole planet with a time dilation factor of over 60,000:1, while the 1977 novel Gateway depicts a spaceship approaching but never crossing the event horizon of a black hole from the perspective of an outside observer due to time dilation effects. Black holes have also been appropriated as wormholes or other methods of faster-than-light travel, such as in the 1974 novel The Forever War, where a network of black holes is used for interstellar travel. Additionally, black holes can feature as hazards to spacefarers and planets: A black hole threatens a deep-space outpost in 1978 short story The Black Hole Passes, and a binary black hole dangerously alters the orbit of a planet in the 2018 Netflix reboot of Lost in Space.
=
=
=
Black Holes on In Our Time at the BBC
Stanford Encyclopedia of Philosophy: "Singularities and Black Holes" by Erik Curiel and Peter Bokulich.
Black Holes: Gravity's Relentless Pull – Interactive multimedia Web site about the physics and astronomy of black holes from the Space Telescope Science Institute (HubbleSite)
ESA's Black Hole Visualization Archived 3 May 2019 at the Wayback Machine
Frequently Asked Questions (FAQs) on Black Holes
Fall into a black hole
Black holes - basic (NYT; April 2021)
=
16-year-long study tracks stars orbiting Sagittarius A*
Movie of Black Hole Candidate from Max Planck Institute
Cowen, Ron (20 April 2015). "3D simulations of colliding black holes hailed as most realistic yet". Nature. doi:10.1038/nature.2015.17360.
Computer visualisation of the signal detected by LIGO
Two Black Holes Merge into One (based upon the signal GW150914)

--- TOPIC: Big_Bang ---
The Big Bang is a physical theory that describes how the universe expanded from an initial state of high density and temperature. Various cosmological models based on the Big Bang concept explain a broad range of phenomena,  including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure. The uniformity of the universe, known as the horizon and flatness problems, is explained through cosmic inflation: a phase of accelerated expansion during the earliest stages. Detailed measurements of the expansion rate of the universe place the initial singularity at an estimated 13.787±0.02 billion years ago, which is considered the age of the universe. A wide range of empirical evidence strongly favors the Big Bang event, which is now widely accepted.
Extrapolating this cosmic expansion backward in time using the known laws of physics, the models describe an extraordinarily hot and dense primordial universe. Physics lacks a widely accepted theory that can model the earliest conditions of the Big Bang. As the universe expanded, it cooled sufficiently to allow the formation of subatomic particles, and later atoms. These primordial elements—mostly hydrogen, with some helium and lithium—then coalesced under the force of gravity aided by dark matter, forming early stars and galaxies.  Measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to a concept called dark energy.
The concept of an expanding universe was introduced by the physicist Alexander Friedmann in 1922 with the mathematical derivation of the Friedmann equations. The earliest empirical observation of an expanding universe is known as Hubble's law, published in work by physicist Edwin Hubble in 1929, which discerned that galaxies are moving away from Earth at a rate that accelerates proportionally with distance. Independent of Friedmann's work, and independent of Hubble's observations, in 1931 physicist Georges Lemaître proposed that the universe emerged from a "primeval atom", introducing the modern notion of the Big Bang. In 1964, the CMB was discovered. Over the next few years measurements showed this radiation to be uniform over directions in the sky and the shape of the energy versus intensity curve, both consistent with the Big Bang models of high temperatures and densities in the distant past.  By the late 1960s most cosmologists were convinced that competing steady-state model of cosmic evolution was incorrect.
There remain aspects of the observed universe that are not yet adequately explained by the Big Bang models. These include the unequal abundances of matter and antimatter known as baryon asymmetry, the detailed nature of dark matter surrounding galaxies, and the origin of dark energy.
=
Big Bang cosmology models depend on three major assumptions: the universality of physical laws, the cosmological principle, and that the matter content can be modeled as a perfect fluid.  The universality of physical laws is one of the underlying principles of the theory of relativity. The cosmological principle states that on large scales the universe is homogeneous and isotropic—appearing the same in all directions regardless of location. A perfect fluid has no viscosity; the pressure of a perfect fluid is proportional to its density.
These ideas were initially taken as postulates, but later efforts were made to test each of them. For example, the first assumption has been tested by observations showing that the largest possible deviation of the fine-structure constant over much of the age of the universe is of order 10−5. The key physical law behind these models, general relativity has passed stringent tests on the scale of the Solar System and binary stars.
The cosmological principle has been confirmed to a level of 10−5 via observations of the temperature of the CMB. At the scale of the CMB horizon, the universe has been measured to be homogeneous with an upper bound on the order of 10% inhomogeneity, as of 1995.
=
The cosmological principle dramatically simplifies the equations of general relativity, giving the Friedmann–Lemaître–Robertson–Walker metric to describe the geometry of the universe and, with the assumption of a perfect fluid, the Friedmann equations giving the time dependence of that geometry. The only parameter at this level of description is the mass-energy density: the geometry of the universe and its expansion is a direct consequence of its density. All of the major features of Big Bang cosmology are related to these results.
=
In Big Bang cosmology, the mass–energy density controls the shape and evolution of the universe. By combining astronomical observations with known laws of thermodynamics and particle physics, cosmologists have worked out the components of the density over the lifespan of the universe. In the current universe, luminous matter, the stars, planets, and so on makes up less than 5% of the density. Dark matter accounts for 27%  and dark energy the remaining 68%.
=
An important feature of the Big Bang spacetime is the presence of particle horizons. Since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not yet had time to reach earth. This places a limit or a past horizon on the most distant objects that can be observed. Conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never "catch up" to very distant objects. This defines a future horizon, which limits the events in the future that we will be able to influence. The presence of either type of horizon depends on the details of the Friedmann–Lemaître–Robertson–Walker (FLRW) metric that describes the expansion of the universe.
Our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. So our view cannot extend further backward in time, though the horizon recedes in space. If the expansion of the universe continues to accelerate, there is a future horizon as well.
=
Some processes in the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximate thermodynamic equilibrium. Others were fast enough to reach thermalization. The parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the rate of the process (usually rate of collisions between particles) and the Hubble parameter. The larger the ratio, the more time particles had to thermalize before they were too far away from each other.
According to the Big Bang models, the universe at the beginning was very hot and very compact, and since then it has been expanding and cooling.
=
Existing theories of physics cannot tell us about the moment of the Big Bang.
Extrapolation of the expansion of the universe backwards in time using only classical general relativity yields a gravitational singularity with infinite density and temperature at a finite time in the past. However this classical gravitational theory is expected to be inadequate to describe physics under these conditions. 
Thus the meaning of this singularity in the context of the Big Bang is unclear. 
The earliest time that general relativity can be applied is called the Planck time. Earlier, during the Planck epoch, when the temperature of the universe was close to the Planck scale (around 1032 K or 1028 eV) quantum gravity effects are expected to be dominant. To date there is no accepted theory of quantum gravity; above the Planck energy scale, undiscovered physics could influence the expansion history of the universe.
=
The earliest phases of the Big Bang are subject to much speculation, given the lack of available data. In the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures, and was very rapidly expanding and cooling. The period up to 10−43 seconds into the expansion, the Planck epoch, was a phase in which the four fundamental forces—the electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational force—were unified as one. In this stage, the characteristic scale length of the universe was the Planck length, 1.6×10−35 m, and consequently had a temperature of approximately 1032 degrees Celsius. Even the very concept of a particle breaks down in these conditions. A proper understanding of this period awaits the development of a theory of quantum gravity. The Planck epoch was succeeded by the grand unification epoch beginning at 10−43 seconds, where gravitation separated from the other forces as the universe's temperature fell.
At approximately 10−37 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially, unconstrained by the light speed invariance, and temperatures dropped by a factor of 100,000. This concept is motivated by the flatness problem, where the density of matter and energy is very close to the critical density needed to produce a flat universe. That is, the shape of the universe has no overall geometric curvature due to gravitational influence. Microscopic quantum fluctuations that occurred because of Heisenberg's uncertainty principle were "frozen in" by inflation, becoming amplified into the seeds that would later form the large-scale structure of the universe. At a time around 10−36 seconds, the electroweak epoch begins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.
All of the mass-energy in all of the galaxies currently visible started in a sphere with a radius around 4 x 10-29 m then grew to a sphere with a radius around 0.9 m by the end of inflation. Reheating followed as the inflaton field decayed, until the universe obtained the temperatures required for the production of a quark–gluon plasma as well as all other elementary particles. Temperatures were so high that the random motions of particles were at relativistic speeds, and particle–antiparticle pairs of all kinds were being continuously created and destroyed in collisions. At some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptons—of the order of one part in 30 million. This resulted in the predominance of matter over antimatter in the present universe.
=
The universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. Symmetry-breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form, with the electromagnetic force and weak nuclear force separating at about 10−12 seconds.
After about 10−11 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. At about 10−6 seconds, quarks and gluons combined to form baryons such as protons and neutrons. The small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. The temperature was no longer high enough to create either new proton–antiproton or neutron–antineutron pairs. A mass annihilation immediately followed, leaving just one in 108 of the original matter particles and none of their antiparticles. A similar process happened at about 1 second for electrons and positrons. After these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos).
A few minutes into the expansion, when the temperature was about a billion kelvin and the density of matter in the universe was comparable to the current density of Earth's atmosphere, neutrons combined with protons to form the universe's deuterium and helium nuclei in a process called Big Bang nucleosynthesis (BBN). Most protons remained uncombined as hydrogen nuclei.
As the universe cooled, the rest energy density of matter came to gravitationally dominate over that of the photon and neutrino radiation at a time of about 50,000 years. At a time of about 380,000 years, the universe cooled enough that electrons and nuclei combined into neutral atoms (mostly hydrogen) in an event called recombination. This process made the previously opaque universe transparent, and the photons that last scattered during this epoch comprise the cosmic microwave background.
=
After the recombination epoch, the slightly denser regions of the uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today. The details of this process depend on the amount and type of matter in the universe. The four possible types of matter are known as cold dark matter (CDM), warm dark matter, hot dark matter, and baryonic matter. The best measurements available, from the Wilkinson Microwave Anisotropy Probe (WMAP), show that the data is well-fit by a Lambda-CDM model in which dark matter is assumed to be cold. This CDM is estimated to make up about 23% of the matter/energy of the universe, while baryonic matter makes up about 4.6%.
=
Independent lines of evidence from Type Ia supernovae and the CMB imply that the universe today is dominated by a mysterious form of energy known as dark energy, which appears to homogeneously permeate all of space. Observations suggest that 73% of the total energy density of the present day universe is in this form. When the universe was very young it was likely infused with dark energy, but with everything closer together, gravity predominated, braking the expansion. Eventually, after billions of years of expansion, the declining density of matter relative to the density of dark energy allowed the expansion of the universe to begin to accelerate.
Dark energy in its simplest formulation is modeled by a cosmological constant term in Einstein field equations of general relativity, but its composition and mechanism are unknown. More generally, the details of its equation of state and relationship with the Standard Model of particle physics continue to be investigated both through observation and theory.
All of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the lambda-CDM model of cosmology, which uses the independent frameworks of quantum mechanics and general relativity. There are no easily testable models that would describe the situation prior to approximately 10−15 seconds. Understanding this earliest of eras in the history of the universe is one of the greatest unsolved problems in physics.
=
English astronomer Fred Hoyle is credited with coining the term "Big Bang" during a talk for a March 1949 BBC Radio broadcast, saying: "These theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past." However, it did not catch on until the 1970s.
It is popularly reported that Hoyle, who favored an alternative "steady-state" cosmological model, intended this to be pejorative, but Hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models. Helge Kragh writes that the evidence for the claim that it was meant as a pejorative is "unconvincing", and mentions a number of indications that it was not a pejorative.
A primordial singularity is sometimes called "the Big Bang", but the term can also refer to a more generic early hot, dense phase. The term itself has been argued to be a misnomer because it evokes an explosion. The argument is that whereas an explosion suggests expansion into a surrounding space, the Big Bang only describes the intrinsic expansion of the contents of the universe. Another issue pointed out by Santhosh Mathew is that bang implies sound, which is not an important feature of the model. However, an attempt to find a more suitable alternative was not successful. According to Timothy Ferris:
The term 'big bang' was coined with derisive intent by Fred Hoyle, and its endurance testifies to Sir Fred's creativity and wit. Indeed, the term survived an international competition in which three judges — the television science reporter Hugh Downs, the astronomer Carl Sagan, and myself — sifted through 13,099 entries from 41 countries and concluded that none was apt enough to replace it. No winner was declared, and like it or not, we are stuck with 'big bang'.
=
Early cosmological models developed from observations of the structure of the universe and from theoretical considerations. In 1912, Vesto Slipher measured the first Doppler shift of a "spiral nebula" (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from Earth. He did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were "island universes" outside our Milky Way. Ten years later, Alexander Friedmann, a Russian cosmologist and mathematician, derived the Friedmann equations from the Einstein field equations, showing that the universe might be expanding in contrast to the static universe model advocated by Albert Einstein at that time.
In 1924, American astronomer Edwin Hubble's measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. Starting that same year, Hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the 100-inch (2.5 m) Hooker telescope at Mount Wilson Observatory. This allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by Slipher. In 1929, Hubble discovered a correlation between distance and recessional velocity—now known as Hubble's law.
Independently deriving Friedmann's equations in 1927, Georges Lemaître, a Belgian physicist and Roman Catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe. He inferred the relation that Hubble would later observe, given the cosmological principle. In 1931, Lemaître went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a "primeval atom" where and when the fabric of time and space came into existence.
In the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by an expanding universe imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory. This perception was enhanced by the fact that the originator of the expanding universe concept, Lemaître, was a Roman Catholic priest. Arthur Eddington agreed with Aristotle that the universe did not have a beginning in time, viz., that matter is eternal. A beginning in time was "repugnant" to him. Lemaître, however, disagreed:
If the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. If this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.
During the 1930s, other ideas were proposed as non-standard cosmologies to explain Hubble's observations, including the Milne model, the oscillatory universe (originally suggested by Friedmann, but advocated by Albert Einstein and Richard C. Tolman) and Fritz Zwicky's tired light hypothesis.
After World War II, two distinct possibilities emerged. One was Fred Hoyle's steady-state model, whereby new matter would be created as the universe seemed to expand. In this model the universe is roughly the same at any point in time. The other was Lemaître's expanding universe theory, advocated and developed by George Gamow, who used it to develop a theory for the abundance of chemical elements in the universe. and whose associates, Ralph Alpher and Robert Herman, predicted the cosmic background radiation.
=
Ironically, it was Hoyle who coined the phrase that came to be applied to Lemaître's theory, referring to it as "this big bang idea" during a BBC Radio broadcast in March 1949. For a while, support was split between these two theories. Eventually, the observational evidence, most notably from radio source counts, began to favor Big Bang over steady state. The discovery and confirmation of the CMB in 1964 secured the Big Bang as the best theory of the origin and evolution of the universe.
In 1968 and 1970, Roger Penrose, Stephen Hawking, and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the Big Bang. Then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the Big Bang universe and resolving outstanding problems. In 1981, Alan Guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the Big Bang models with the introduction of an epoch of rapid expansion in the early universe he called "inflation". Meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the Hubble Constant and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).
Significant progress in Big Bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as the Cosmic Background Explorer (COBE), the Hubble Space Telescope and WMAP. Cosmologists now have fairly precise and accurate measurements of many of the parameters of the Big Bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.
The Big Bang models offer a comprehensive explanation for a broad range of observed phenomena, including the abundances of the light elements, the cosmic microwave background, large-scale structure, and Hubble's law.
The earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to Hubble's law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by Big Bang nucleosynthesis (BBN). More recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures. These are sometimes called the "four pillars" of the Big Bang models.
Precise modern models of the Big Bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the Standard Model of particle physics. Of these features, dark matter is currently the subject of most active laboratory investigations. Remaining issues include the cuspy halo problem and the dwarf galaxy problem of cold dark matter. Dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible. Inflation and baryogenesis remain more speculative features of current Big Bang models. Viable, quantitative explanations for such phenomena are still being sought. These are unsolved problems in physics.
=
Observations of distant galaxies and quasars show that these objects are redshifted: the light emitted from them has been shifted to longer wavelengths. This can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission or absorption lines corresponding to atoms of the chemical elements interacting with the light. These redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. If the redshift is interpreted as a Doppler shift, the recessional velocity of the object can be calculated. For some galaxies, it is possible to estimate distances via the cosmic distance ladder. When the recessional velocities are plotted against these distances, a linear relationship known as Hubble's law is observed:
  
    
      
        v
        =
        
          H
          
            0
          
        
        D
      
    
    {\displaystyle v=H_{0}D}
  
where
  
    
      
        v
      
    
    {\displaystyle v}
  
 is the recessional velocity of the galaxy or other distant object,
  
    
      
        D
      
    
    {\displaystyle D}
  
 is the proper distance to the object, and
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  
 is Hubble's constant, measured to be 70.4+1.3−1.4 km/s/Mpc by the WMAP.
Hubble's law implies that the universe is uniformly expanding everywhere. This cosmic expansion was predicted from general relativity by Friedmann in 1922 and Lemaître in 1927, well before Hubble made his 1929 analysis and observations, and it remains the cornerstone of the Big Bang model as developed by Friedmann, Lemaître, Robertson, and Walker.
The theory requires the relation 
  
    
      
        v
        =
        H
        D
      
    
    {\displaystyle v=HD}
  
 to hold at all times, where 
  
    
      
        D
      
    
    {\displaystyle D}
  
 is the proper distance, 
  
    
      
        v
      
    
    {\displaystyle v}
  
 is the recessional velocity, and 
  
    
      
        v
      
    
    {\displaystyle v}
  
, 
  
    
      
        H
      
    
    {\displaystyle H}
  
, and 
  
    
      
        D
      
    
    {\displaystyle D}
  
 vary as the universe expands (hence we write 
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  
 to denote the present-day Hubble "constant"). For distances much smaller than the size of the observable universe, the Hubble redshift can be thought of as the Doppler shift corresponding to the recession velocity 
  
    
      
        v
      
    
    {\displaystyle v}
  
. For distances comparable to the size of the observable universe, the attribution of the cosmological redshift becomes more ambiguous, although its interpretation as a kinematic Doppler shift remains the most natural one.
An unexplained discrepancy with the determination of the Hubble constant is known as Hubble tension. Techniques based on observation of the CMB suggest a lower value of this constant compared to the quantity derived from measurements based on the cosmic distance ladder.
=
In 1964, Arno Penzias and Robert Wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band. Their discovery provided substantial confirmation of the big-bang predictions by Alpher, Herman and Gamow around 1950. Through the 1970s, the radiation was found to be approximately consistent with a blackbody spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 K. This tipped the balance of evidence in favor of the Big Bang model, and Penzias and Wilson were awarded the 1978 Nobel Prize in Physics.
The surface of last scattering corresponding to emission of the CMB occurs shortly after recombination, the epoch when neutral hydrogen becomes stable. Prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. Peaking at around 372±14 kyr, the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent.
In 1989, NASA launched COBE, which made two major advances: in 1990, high-precision spectrum measurements showed that the CMB frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726 K (more recent measurements have revised this figure down slightly to 2.7255 K); then in 1992, further COBE measurements discovered tiny fluctuations (anisotropies) in the CMB temperature across the sky, at a level of about one part in 105. John C. Mather and George Smoot were awarded the 2006 Nobel Prize in Physics for their leadership in these results.
During the following decade, CMB anisotropies were further investigated by a large number of ground-based and balloon experiments. In 2000–2001, several experiments, most notably BOOMERanG, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.
In early 2003, the first results of the Wilkinson Microwave Anisotropy Probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. The results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general. The Planck space probe was launched in May 2009. Other ground and balloon-based cosmic microwave background experiments are ongoing.
=
Using Big Bang models, it is possible to calculate the expected concentration of the isotopes helium-4 (4He), helium-3 (3He), deuterium (2H), and lithium-7 (7Li) in the universe as ratios to the amount of ordinary hydrogen. The relative abundances depend on a single parameter, the ratio of photons to baryons. This value can be calculated independently from the detailed structure of CMB fluctuations. The ratios predicted (by mass, not by abundance) are about 0.25 for 4He:H, about 10−3 for 2H:H, about 10−4 for 3He:H, and about 10−9 for 7Li:H.
The measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. The agreement is excellent for deuterium, close but formally discrepant for 4He, and off by a factor of two for 7Li (this anomaly is known as the cosmological lithium problem); in the latter two cases, there are substantial systematic uncertainties. Nonetheless, the general consistency with abundances predicted by BBN is strong evidence for the Big Bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to "tune" the Big Bang to produce much more or less than 20–30% helium. Indeed, there is no obvious reason outside of the Big Bang that, for example, the young universe before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products, should have more helium than deuterium or more deuterium than 3He, and in constant ratios, too.
=
Detailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current Big Bang models. A combination of observations and theory suggest that the first quasars and galaxies formed within a billion years after the Big Bang, and since then, larger structures have been forming, such as galaxy clusters and superclusters.
Populations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). Moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the Big Bang. These observations are strong arguments against the steady-state model. Observations of star formation, galaxy and quasar distributions and larger structures, agree well with Big Bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.
=
In 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. Before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. Despite being sensitive to carbon, oxygen, and silicon, these three elements were not detected in these two clouds. Since the clouds of gas have no detectable levels of heavy elements, they likely formed in the first few minutes after the Big Bang, during BBN.
=
The age of the universe as estimated from the Hubble expansion and the CMB is now in agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual Population II stars. It is also in agreement with age estimates based on measurements of the expansion using Type Ia supernovae and measurements of temperature fluctuations in the cosmic microwave background. The agreement of independent measurements of this age supports the Lambda-CDM (ΛCDM) model, since the model is used to relate some of the measurements to an age estimate, and all estimates turn agree. Still, some observations of objects from the relatively early universe (in particular quasar APM 08279+5255) raise concern as to whether these objects had enough time to form so early in the ΛCDM model.
The prediction that the CMB temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift. This prediction also implies that the amplitude of the Sunyaev–Zel'dovich effect in clusters of galaxies does not depend directly on redshift. Observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.
=
Future gravitational-wave observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the Big Bang.
As with any theory, a number of mysteries and problems have arisen as a result of the development of the Big Bang models. Some of these mysteries and problems have been resolved while others are still outstanding. Proposed solutions to some of the problems in the Big Bang model have revealed new mysteries of their own. For example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflation theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven. What follows are a list of the mysterious aspects of the Big Bang concept still under intense investigation by cosmologists and astrophysicists.
=
It is not yet understood why the universe has more matter than antimatter. It is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons.  Both matter and antimatter were much more abundant than today, with a tiny asymmetry of only one part in 10 billion. The matter and antimatter collided and annihilated, leaving only the residual amount of matter. Today observations suggest that the universe, including its most distant parts, is made almost entirely of normal matter with very little antimatter. 
If matter and antimatter were in complete symmetry, then annihilation would result in only photons and virtually no matter at all, which is obviously not what is observed. A process called baryogenesis was hypothesized to account for the asymmetry. For baryogenesis to occur, the Sakharov conditions must be satisfied. These require that baryon number is not conserved, that C-symmetry and CP-symmetry are violated and that the universe depart from thermodynamic equilibrium. All these conditions occur in the Standard Model, but the effects are not strong enough to explain the present baryon asymmetry.
=
Measurements of the redshift–magnitude relation for type Ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. To explain this acceleration, cosmological models require that much of the energy in the universe consists of a component with large negative pressure, dubbed "dark energy".
Dark energy, though speculative, solves numerous problems. Measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of mass/energy. But the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30% of the critical density. Since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the "missing" energy density. Dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses, and the other using the characteristic pattern of the large-scale structure--baryon acoustic oscillations--as a cosmic ruler.
Negative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the Big Bang. Results from the WMAP team in 2008 are in accordance with a universe that consists of 73% dark energy, 23% dark matter, 4.6% regular matter and less than 1% neutrinos. According to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. Therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.
The dark energy component of the universe has been explained by theorists using a variety of competing theories including Einstein's cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes. A cosmological constant problem, sometimes called the "most embarrassing problem in physics", results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from Planck units.
=
During the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. This led to the idea that up to 90% of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. In addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. In particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. While dark matter has always been controversial, it is inferred by various observations: the anisotropies in the CMB, the galaxy rotation problem, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and X-ray measurements of galaxy clusters.
Indirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. Many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.
Additionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem and the cuspy halo problem. Alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by Newton and Einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.
=
The horizon problem results from the premise that information cannot travel faster than light. In a universe of finite age this sets a limit—the particle horizon—on the separation of any two regions of space that are in causal contact. The observed isotropy of the CMB is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. There would then be no mechanism to cause wider regions to have the same temperature.
A resolution to this apparent inconsistency is offered by inflation theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). During inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each other's particle horizon. The observed isotropy of the CMB then follows from the fact that this larger region was in causal contact before the beginning of inflation.
Heisenberg's uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to a cosmic scale. These fluctuations served as the seeds for all the current structures in the universe. Inflation predicts that the primordial fluctuations are nearly scale invariant and Gaussian, which has been confirmed by measurements of the CMB.
A related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.
=
The magnetic monopole objection was raised in the late 1970s. Grand unified theories (GUTs) predicted topological defects in space that would manifest as magnetic monopoles. These objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. This problem is resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.
=
The flatness problem (also known as the oldness problem) is an observational problem associated with a FLRW. The universe may have positive, negative, or zero spatial curvature depending on its total energy density. Curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be flat. Observations indicate the universe is consistent with being flat.
The problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat. Given that a natural timescale for departure from flatness might be the Planck time, 10−43 seconds, the fact that the universe has reached neither a heat death nor a Big Crunch after billions of years requires an explanation. For instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 1014 of its critical value, or it would not exist as it does today.
In addition to  confusion about the nature of cosmic expansion, the Big Bang model itself is sometimes misunderstood. 
One of the common misconceptions about the Big Bang model is that it fully explains the origin of the universe. However, the Big Bang model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.
Another common misconception relates to the recession speeds associated with Hubble's law. These are not velocities in a relativistic sense (for example, they are not related to the spatial components of 4-velocities). Therefore, it is not remarkable that according to Hubble's law, galaxies farther than the Hubble distance recede faster than the speed of light. Such recession speeds do not correspond to faster-than-light travel.
Given current understanding, scientific extrapolations about the future of the universe are only possible for finite durations, albeit for much longer periods than the current age of the universe. Anything beyond that becomes increasingly speculative. Likewise, at present, a proper understanding of the origin of the universe can only be subject to conjecture.
=
The Big Bang explains the evolution of the universe from a starting density and temperature that is well beyond humanity's capability to replicate, so extrapolations to the most extreme conditions and earliest times are necessarily more speculative. Lemaître called this initial state the "primeval atom" while Gamow called the material "ylem". How the initial state of the universe originated is still an open question, but the Big Bang model does constrain some of its characteristics. For example, if specific laws of nature were to come to existence in a random way, inflation models show, some combinations of these are far more probable, partly explaining why our Universe is rather stable. Another possible explanation for the stability of the Universe could be a hypothetical multiverse, which assumes every possible universe to exist, and thinking species could only emerge in those stable enough. A flat universe implies a balance between gravitational potential energy and other energy forms, requiring no additional energy to be created.
The Big Bang theory is built upon the equations of classical general relativity, which are not expected to be valid at the origin of cosmic time, as the temperature of the universe approaches the Planck scale. Correcting this will require the development of a correct treatment of quantum gravity. Certain quantum gravity treatments, such as the Wheeler–DeWitt equation, imply that time itself could be an emergent property. As such, physics may conclude that time did not exist before the Big Bang.
While it is not known what could have preceded the hot dense state of the early universe or how and why it originated, or even whether such questions are sensible, speculation abounds on the subject of "cosmogony".
Some speculative proposals in this regard, each of which entails untested hypotheses, are:
The simplest models, in which the Big Bang was caused by quantum fluctuations. That scenario had very little chance of happening, but, according to the totalitarian principle, even the most improbable event will eventually happen. It took place instantly, in our perspective, due to the absence of perceived time before the Big Bang.
Emergent Universe models, which feature a low-activity past-eternal era before the Big Bang, resembling ancient ideas of a cosmic egg and birth of the world out of primordial chaos.
Models in which the whole of spacetime is finite, including the Hartle–Hawking no-boundary condition. For these cases, the Big Bang does represent the limit of time but without a singularity. In such a case, the universe is self-sufficient.
Brane cosmology models, in which inflation is due to the movement of branes in string theory; the pre-Big Bang model; the ekpyrotic model, in which the Big Bang is the result of a collision between branes; and the cyclic model, a variant of the ekpyrotic model in which collisions occur periodically. In the latter model the Big Bang was preceded by a Big Crunch and the universe cycles from one process to the other.
Eternal inflation, in which universal inflation ends locally here and there in a random fashion, each end-point leading to a bubble universe, expanding from its own big bang. This is sometimes referred to as pre-big bang inflation.
Proposals in the last two categories see the Big Bang as an event in either a much larger and older universe or in a multiverse.
=
Before observations of dark energy, cosmologists considered two scenarios for the future of the universe. If the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. It would become denser and hotter again, ending with a state similar to that in which it started—a Big Crunch.
Alternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. Star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. Collisions between these would result in mass accumulating into larger and larger black holes. The average temperature of the universe would very gradually asymptotically approach absolute zero—a Big Freeze. Moreover, if protons are unstable, then baryonic matter would disappear, leaving only radiation and black holes. Eventually, black holes would evaporate by emitting Hawking radiation. The entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.
Modern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. The eventual result is not known. The Lambda-CDM model of the universe contains dark energy in the form of a cosmological constant. This theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. Other explanations of dark energy, called phantom dark energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called Big Rip.
=
As a description of the origin of the universe, the Big Bang has significant bearing on religion and philosophy. As a result, it has become one of the liveliest areas in the discourse between science and religion. Some believe the Big Bang implies a creator, while others argue that Big Bang cosmology makes the notion of a creator superfluous.
=
=
=
Alpher, Ralph A.; Herman, Robert (August 1988). "Reflections on Early Work on 'Big Bang' Cosmology". Physics Today. 41 (8): 24–34. Bibcode:1988PhT....41h..24A. doi:10.1063/1.881126.
Barrow, John D. (1994). The Origin of the Universe. Science Masters. London: Weidenfeld & Nicolson. ISBN 978-0-297-81497-9. LCCN 94006343. OCLC 490957073.
Block, David L. (2012). "Georges Lemaître and Stigler's Law of Eponymy". In Holder, Rodney D.; Mitton, Simon (eds.). A Hubble Eclipse: Lemaitre and Censorship. Astrophysics and Space Science Library. Vol. 395. Heidelberg; New York: Springer. pp. 89–96. arXiv:1106.3928v2. Bibcode:2012ASSL..395...89B. doi:10.1007/978-3-642-32254-9_8. ISBN 978-3-642-32253-2. LCCN 2012956159. OCLC 839779611. S2CID 119205665.
Davies, Paul (1992). The Mind of God: The Scientific Basis for a Rational World. New York: Simon & Schuster. ISBN 978-0-671-71069-9. LCCN 91028606. OCLC 59940452.
Farrell, John (2005). The Day Without Yesterday: Lemaître, Einstein, and the Birth of Modern Cosmology. New York: Thunder's Mouth Press. ISBN 978-1-56025-660-1. LCCN 2006272995. OCLC 61672162.
d'Inverno, Ray (1992). Introducing Einstein's Relativity. Oxford, UK; New York: Clarendon Press; Oxford University Press. ISBN 978-0-19-859686-8. LCCN 91024894. OCLC 554124256.
Lineweaver, Charles H.; Davis, Tamara M. (March 2005). "Misconceptions about the Big Bang" (PDF). Scientific American. Vol. 292, no. 3. pp. 36–45. Archived (PDF) from the original on 9 October 2019. Retrieved 23 December 2019.
Martínez-Delgado, David, ed. (2013). Local Group Cosmology. Cambridge, UK: Cambridge University Press. ISBN 978-1-107-02380-2. LCCN 2013012345. OCLC 875920635. "Lectures presented at the XX Canary Islands Winter School of Astrophysics, held in Tenerife, Spain, November 17–18, 2008."
Mather, John C.; Boslough, John (1996). The Very First Light: The True Inside Story of the Scientific Journey Back to the Dawn of the Universe (1st ed.). New York: Basic Books. ISBN 978-0-465-01575-7. LCCN 96010781. OCLC 34357391.
Riordan, Michael; Zajc, William A. (May 2006). "The First Few Microseconds" (PDF). Scientific American. Vol. 294, no. 5. pp. 34–41. Bibcode:2006SciAm.294e..34R. doi:10.1038/scientificamerican0506-34a. Archived (PDF) from the original on 30 November 2014.
Singh, Simon (2005) [First U.S. edition published 2004]. Big Bang: The Origin of the Universe (Harper Perennial; illustrated ed.). New York, New York: Harper Perennial. ISBN 978-0-00-716221-5.
Weinberg, Steven (1993) [Originally published 1977]. The First Three Minutes: A Modern View of the Origin of the Universe (Updated ed.). New York: Basic Books. ISBN 978-0-465-02437-7. LCCN 93232406. OCLC 488469247. 1st edition is available from the Internet Archive. Retrieved 23 December 2019.
Woolfson, Michael (2013). Time, Space, Stars & Man: The Story of Big Bang (2nd ed.). London: Imperial College Press. ISBN 978-1-84816-933-3. LCCN 2013371163. OCLC 835115510.
Once Upon a Universe Archived 22 June 2020 at the Wayback Machine – STFC funded project explaining the history of the universe in easy-to-understand language
"Big Bang Cosmology" – NASA/WMAP Science Team
"The Big Bang" – NASA Science
"Big Bang, Big Bewilderment" – Big bang model with animated graphics by Johannes Koelman
"The Trouble With "The Big Bang"" – A rash of recent articles illustrates a longstanding confusion over the famous term. by Sabine Hossenfelde

--- TOPIC: Universe ---
The universe is all of space and time and their contents. It comprises all of existence, any fundamental interaction, physical process and physical constant, and therefore all forms of matter and energy, and the structures they form, from sub-atomic particles to entire galactic filaments. Since the early 20th century, the field of cosmology establishes that space and time emerged together at the Big Bang 13.787±0.020 billion years ago and that the universe has been expanding since then. The portion of the universe that can be seen by humans is approximately 93 billion light-years in diameter at present, but the total size of the universe is not known.
Some of the earliest cosmological models of the universe were geocentric, placing Earth at the center. During the European Scientific Revolution, astronomical observations led to a heliocentric model. Further observational improvements led to the realization that the Sun is one of a few hundred billion stars in the Milky Way, which is one of a few hundred billion galaxies in the observable universe. Many of the stars in a galaxy have planets. At the largest scale, galaxies are distributed uniformly and the same in all directions, meaning that the universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters, which form immense filaments and voids in space, creating a vast foam-like structure. Discoveries in the early 20th century lead to the Big Bang theory with a hot fireball, cooling and becoming less dense as the universe expanded, allowing the first subatomic particles and simple atoms to form. Giant clouds of hydrogen and helium were gradually drawn to the places where matter was most dense, forming the first galaxies, stars, and everything else seen today.
From studying the effects of gravity on both matter and light, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulae and interstellar gas. This unseen matter is known as dark matter. In the widely accepted ΛCDM cosmological model, dark matter accounts for about 25.8%±1.1% of the mass and energy in the universe while about 69.2%±1.2% is dark energy, a mysterious form of energy responsible for the acceleration of the expansion of the universe. Ordinary ('baryonic') matter therefore composes only 4.84%±0.1% of the universe. Stars, planets, and visible gas clouds only form about 6% of this ordinary matter.
There are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which the universe might be one among many.
The physical universe is defined as all of space and time (collectively referred to as spacetime) and their contents. Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space. The universe also includes the physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity.
The universe is often defined as "the totality of existence", or everything that exists, everything that has existed, and everything that will exist. In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts—such as mathematics and logic—in the definition of the universe. The word universe may also refer to concepts such as the cosmos, the world, and nature.
The word universe derives from the Old French word univers, which in turn derives from the Latin word universus, meaning 'combined into one'. The Latin word 'universum' was used by Cicero and later Latin authors in many of the same senses as the modern English word is used.
=
A term for universe among the ancient Greek philosophers from Pythagoras onwards was τὸ πᾶν (tò pân) 'the all', defined as all matter and all space, and τὸ ὅλον (tò hólon) 'all things', which did not necessarily include the void. Another synonym was ὁ κόσμος (ho kósmos) meaning 'the world, the cosmos'. Synonyms are also found in Latin authors (totum, mundus, natura) and survive in modern languages, e.g., the German words Das All, Weltall, and Natur for universe. The same synonyms are found in English, such as everything (as in the theory of everything), the cosmos (as in cosmology), the world (as in the many-worlds interpretation), and nature (as in natural laws or natural philosophy).
The prevailing model for the evolution of the universe is the Big Bang theory. The Big Bang model states that the earliest state of the universe was an extremely hot and dense one, and that the universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as the homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the universe. 
The initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10−43 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity—currently the weakest by far of the four known forces—is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. The physics controlling this very early period (including quantum gravity in the Planck epoch) is not understood, so we cannot say what, if anything, happened before time zero. Since the Planck epoch, the universe has been expanding to its present scale, with a very short but intense period of cosmic inflation speculated to have occurred within the first 10−32 seconds. This initial period of inflation would explain why space appears to be very flat.
Within the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool from its inconceivably hot state, various types of elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion.
This process, known as Big Bang nucleosynthesis, lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei.
After nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time, the universe also became transparent. The photons released ("decoupled") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB).
As the universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of each photon decreases as it is cosmologically redshifted. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era.
In the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100–300 million years, the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the universe between about 200–500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis.
The universe also contains a mysterious energy—possibly a scalar field—called dark energy, the density of which does not change over time. After about 9.8 billion years, the universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era. In this era, the expansion of the universe is accelerating due to dark energy.
Of the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.
The universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation. This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction.
=
Due to the finite speed of light, there is a limit (known as the particle horizon) to how far light can travel over the age of the universe.
The spatial region from which we can receive light is called the observable universe. The proper distance (measured at a fixed time) between Earth and the edge of the observable universe is 46 billion light-years (14 billion parsecs), making the diameter of the observable universe about 93 billion light-years (28 billion parsecs). Although the distance traveled by light from the edge of the observable universe is close to the age of the universe times the speed of light, 13.8 billion light-years (4.2×10^9 pc), the proper distance is larger because the edge of the observable universe and the Earth have since moved further apart.
For comparison, the Milky Way is roughly 87,400 light-years in diameter, and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away.
Because humans cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the universe in its totality is finite or infinite. An estimate from 2011 suggests that if the cosmological principle holds, the whole universe must be more than 250 times larger than a Hubble sphere. Some disputed estimates for the total size of the universe, if finite, reach as high as 
  
    
      
        
          10
          
            
              10
              
                
                  10
                  
                    122
                  
                
              
            
          
        
      
    
    {\displaystyle 10^{10^{10^{122}}}}
  
 megaparsecs, as implied by a suggested resolution of the No-Boundary Proposal.
=
Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the universe at 13.799 ± 0.021 billion years, as of 2015.
Over time, the universe and its contents have evolved. For example, the relative population of quasars and galaxies has changed and the universe has expanded. This expansion is inferred from the observation that the light from distant galaxies has been redshifted, which implies that the galaxies are receding from us. Analyses of Type Ia supernovae indicate that the expansion is accelerating.
The more matter there is in the universe, the stronger the mutual gravitational pull of the matter. If the universe were too dense then it would re-collapse into a gravitational singularity. However, if the universe contained too little matter then the self-gravity would be too weak for astronomical structures, like galaxies or planets, to form. Since the Big Bang, the universe has expanded monotonically. Perhaps unsurprisingly, our universe has just the right mass–energy density, equivalent to about 5 protons per cubic meter, which has allowed it to expand for the last 13.8 billion years, giving time to form the universe as observed today.
There are dynamical forces acting on the particles in the universe which affect the expansion rate. Before 1998, it was expected that the expansion rate would be decreasing as time went on due to the influence of gravitational interactions in the universe; and thus there is an additional observable quantity in the universe called the deceleration parameter, which most cosmologists expected to be positive and related to the matter density of the universe. In 1998, the deceleration parameter was measured by two different groups to be negative, approximately −0.55, which technically implies that the second derivative of the cosmic scale factor 
  
    
      
        
          
            
              a
              ¨
            
          
        
      
    
    {\displaystyle {\ddot {a}}}
  
 has been positive in the last 5–6 billion years.
=
Modern physics regards events as being organized into spacetime. This idea originated with the special theory of relativity, which predicts that if one observer sees two events happening in different places at the same time, a second observer who is moving relative to the first will see those events happening at different times. The two observers will disagree on the time 
  
    
      
        T
      
    
    {\displaystyle T}
  
 between the events, and they will disagree about the distance 
  
    
      
        D
      
    
    {\displaystyle D}
  
 separating the events, but they will agree on the speed of light 
  
    
      
        c
      
    
    {\displaystyle c}
  
, and they will measure the same value for the combination 
  
    
      
        
          c
          
            2
          
        
        
          T
          
            2
          
        
        −
        
          D
          
            2
          
        
      
    
    {\displaystyle c^{2}T^{2}-D^{2}}
  
. The square root of the absolute value of this quantity is called the interval between the two events. The interval expresses how widely separated events are, not just in space or in time, but in the combined setting of spacetime.
The special theory of relativity describes a flat spacetime. Its successor, the general theory of relativity, explains gravity as curvature of spacetime arising due to its energy content. A curved path like an orbit is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: "Spacetime tells matter how to move; matter tells spacetime how to curve", and therefore there is no point in considering one without the other. The Newtonian theory of gravity is a good approximation to the predictions of general relativity when gravitational effects are weak and objects are moving slowly compared to the speed of light.
The relation between matter distribution and spacetime curvature is given by the Einstein field equations, which require tensor calculus to express. The universe appears to be a smooth spacetime continuum consisting of three spatial dimensions and one temporal (time) dimension. Therefore, an event in the spacetime of the physical universe can be identified by a set of four coordinates: (x, y, z, t).
=
Cosmologists often work with space-like slices of spacetime that are surfaces of constant time in comoving coordinates. The geometry of these spatial slices is set by the density parameter, Omega (Ω), defined as the average matter density of the universe divided by a critical value. This selects one of three possible geometries depending on whether Ω is equal to, less than, or greater than 1. These are called, respectively, the flat, open and closed universes.
Observations, including the Cosmic Background Explorer (COBE), Wilkinson Microwave Anisotropy Probe (WMAP), and Planck maps of the CMB, suggest that the universe is infinite in extent with a finite age, as described by the Friedmann–Lemaître–Robertson–Walker (FLRW) models. These FLRW models thus support inflationary models and the standard model of cosmology, describing a flat, homogeneous universe presently dominated by dark matter and dark energy.
=
The fine-tuned universe hypothesis is the proposition that the conditions that allow the existence of observable life in the universe can only occur when certain universal fundamental physical constants lie within a very narrow range of values. According to this hypothesis, if any of several fundamental constants were only slightly different, the universe would have been unlikely to be conducive to the establishment and development of matter, astronomical structures, elemental diversity, or life as it is understood. Whether this is true, and whether that question is even logically meaningful to ask, are subjects of much debate. The proposition is discussed among philosophers, scientists, theologians, and proponents of creationism.
The universe is composed almost completely of dark energy, dark matter, and ordinary matter. Other contents are electromagnetic radiation (estimated to constitute from 0.005% to close to 0.01% of the total mass–energy of the universe) and antimatter.
The proportions of all types of matter and energy have changed over the history of the universe. The total amount of electromagnetic radiation generated within the universe has decreased by 1/2 in the past 2 billion years. Today, ordinary matter, which includes atoms, stars, galaxies, and life, accounts for only 4.9% of the contents of the universe. The present overall density of this type of matter is very low, roughly 4.5 × 10−31 grams per cubic centimeter, corresponding to a density of the order of only one proton for every four cubic meters of volume. The nature of both dark energy and dark matter is unknown. Dark matter, a mysterious form of matter that has not yet been identified, accounts for 26.8% of the cosmic contents. Dark energy, which is the energy of empty space and is causing the expansion of the universe to accelerate, accounts for the remaining 68.3% of the contents.
Matter, dark matter, and dark energy are distributed homogeneously throughout the universe over length scales longer than 300 million light-years (ly) or so. However, over shorter length-scales, matter tends to clump hierarchically; many atoms are condensed into stars, most stars into galaxies, most galaxies into clusters, superclusters and, finally, large-scale galactic filaments. The observable universe contains as many as an estimated 2 trillion galaxies and, overall, as many as an estimated 1024 stars – more stars (and earth-like planets) than all the grains of beach sand on planet Earth; but less than the total number of atoms estimated in the universe as 1082; and the estimated total number of stars in an inflationary universe (observed and unobserved), as 10100. Typical galaxies range from dwarfs with as few as ten million (107) stars up to giants with one trillion (1012) stars. Between the larger structures are voids, which are typically 10–150 Mpc (33 million–490 million ly) in diameter. The Milky Way is in the Local Group of galaxies, which in turn is in the Laniakea Supercluster. This supercluster spans over 500 million light-years, while the Local Group spans over 10 million light-years. The universe also has vast regions of relative emptiness; the largest known void measures 1.8 billion ly (550 Mpc) across.
The observable universe is isotropic on scales significantly larger than superclusters, meaning that the statistical properties of the universe are the same in all directions as observed from Earth. The universe is bathed in highly isotropic microwave radiation that corresponds to a thermal equilibrium blackbody spectrum of roughly 2.72548 kelvins. The hypothesis that the large-scale universe is homogeneous and isotropic is known as the cosmological principle. A universe that is both homogeneous and isotropic looks the same from all vantage points and has no center.
=
An explanation for why the expansion of the universe is accelerating remains elusive. It is often attributed to the gravitational influence of "dark energy", an unknown form of energy that is hypothesized to permeate space. On a mass–energy equivalence basis, the density of dark energy (~ 7 × 10−30 g/cm3) is much less than the density of ordinary matter or dark matter within galaxies. However, in the present dark-energy era, it dominates the mass–energy of the universe because it is uniform across space.
Two proposed forms for dark energy are the cosmological constant, a constant energy density filling space homogeneously, and scalar fields such as quintessence or moduli, dynamic quantities whose energy density can vary in time and space while still permeating them enough to cause the observed rate of expansion. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy.
=
Dark matter is a hypothetical kind of matter that is invisible to the entire electromagnetic spectrum, but which accounts for most of the matter in the universe. The existence and properties of dark matter are inferred from its gravitational effects on visible matter, radiation, and the large-scale structure of the universe. Other than neutrinos, a form of hot dark matter, dark matter has not been detected directly, making it one of the greatest mysteries in modern astrophysics. Dark matter neither emits nor absorbs light or any other electromagnetic radiation at any significant level. Dark matter is estimated to constitute 26.8% of the total mass–energy and 84.5%  of the total matter in the universe.
=
The remaining 4.9% of the mass–energy of the universe is ordinary matter, that is, atoms, ions, electrons and the objects they form. This matter includes stars, which produce nearly all of the light we see from galaxies, as well as interstellar gas in the interstellar and intergalactic media, planets, and all the objects from everyday life that we can bump into, touch or squeeze. The great majority of ordinary matter in the universe is unseen, since visible stars and gas inside galaxies and clusters account for less than 10 percent of the ordinary matter contribution to the mass–energy density of the universe.
Ordinary matter commonly exists in four states (or phases): solid, liquid, gas, and plasma. However, advances in experimental techniques have revealed other previously theoretical phases, such as Bose–Einstein condensates and fermionic condensates. Ordinary matter is composed of two types of elementary particles: quarks and leptons. For example, the proton is formed of two up quarks and one down quark; the neutron is formed of two down quarks and one up quark; and the electron is a kind of lepton. An atom consists of an atomic nucleus, made up of protons and neutrons (both of which are baryons), and electrons that orbit the nucleus.
Soon after the Big Bang, primordial protons and neutrons formed from the quark–gluon plasma of the early universe as it cooled below two trillion degrees. A few minutes later, in a process known as Big Bang nucleosynthesis, nuclei formed from the primordial protons and neutrons. This nucleosynthesis formed lighter elements, those with small atomic numbers up to lithium and beryllium, but the abundance of heavier elements dropped off sharply with increasing atomic number. Some boron may have been formed at this time, but the next heavier element, carbon, was not formed in significant amounts. Big Bang nucleosynthesis shut down after about 20 minutes due to the rapid drop in temperature and density of the expanding universe. Subsequent formation of heavier elements resulted from stellar nucleosynthesis and supernova nucleosynthesis.
=
Ordinary matter and the forces that act on matter can be described in terms of elementary particles. These particles are sometimes described as being fundamental, since they have an unknown substructure, and it is unknown whether or not they are composed of smaller and even more fundamental particles. In most contemporary models they are thought of as points in space. All elementary particles are currently best explained by quantum mechanics and exhibit wave–particle duality: their behavior has both particle-like and wave-like aspects, with different features dominating under different circumstances.
Of central importance is the Standard Model, a theory that is concerned with electromagnetic interactions and the weak and strong nuclear interactions. The Standard Model is supported by the experimental confirmation of the existence of particles that compose matter: quarks and leptons, and their corresponding "antimatter" duals, as well as the force particles that mediate interactions: the photon, the W and Z bosons, and the gluon. The Standard Model predicted the existence of the recently discovered Higgs boson, a particle that is a manifestation of a field within the universe that can endow particles with mass. Because of its success in explaining a wide variety of experimental results, the Standard Model is sometimes regarded as a "theory of almost everything". The Standard Model does not, however, accommodate gravity. A true force–particle "theory of everything" has not been attained.
 Hadrons 
A hadron is a composite particle made of quarks held together by the strong force. Hadrons are categorized into two families: baryons (such as protons and neutrons) made of three quarks, and mesons (such as pions) made of one quark and one antiquark. Of the hadrons, protons are stable, and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions and are thus insignificant constituents of the modern universe.
From approximately 10−6 seconds after the Big Bang, during a period known as the hadron epoch, the temperature of the universe had fallen sufficiently to allow quarks to bind together into hadrons, and the mass of the universe was dominated by hadrons. Initially, the temperature was high enough to allow the formation of hadron–anti-hadron pairs, which kept matter and antimatter in thermal equilibrium. However, as the temperature of the universe continued to fall, hadron–anti-hadron pairs were no longer produced. Most of the hadrons and anti-hadrons were then eliminated in particle–antiparticle annihilation reactions, leaving a small residual of hadrons by the time the universe was about one second old.
 Leptons 
A lepton is an elementary, half-integer spin particle that does not undergo strong interactions but is subject to the Pauli exclusion principle; no two leptons of the same species can be in exactly the same state at the same time. Two main classes of leptons exist: charged leptons (also known as the electron-like leptons), and neutral leptons (better known as neutrinos). Electrons are stable and the most common charged lepton in the universe, whereas muons and taus are unstable particles that quickly decay after being produced in high energy collisions, such as those involving cosmic rays or carried out in particle accelerators. Charged leptons can combine with other particles to form various composite particles such as atoms and positronium. The electron governs nearly all of chemistry, as it is found in atoms and is directly tied to all chemical properties. Neutrinos rarely interact with anything, and are consequently rarely observed. Neutrinos stream throughout the universe but rarely interact with normal matter.
The lepton epoch was the period in the evolution of the early universe in which the leptons dominated the mass of the universe. It started roughly 1 second after the Big Bang, after the majority of hadrons and anti-hadrons annihilated each other at the end of the hadron epoch. During the lepton epoch, the temperature of the universe was still high enough to create lepton–anti-lepton pairs, so leptons and anti-leptons were in thermal equilibrium. Approximately 10 seconds after the Big Bang, the temperature of the universe had fallen to the point where lepton–anti-lepton pairs were no longer created. Most leptons and anti-leptons were then eliminated in annihilation reactions, leaving a small residue of leptons. The mass of the universe was then dominated by photons as it entered the following photon epoch.
 Photons 
A photon is the quantum of light and all other forms of electromagnetic radiation. It is the carrier for the electromagnetic force. The effects of this force are easily observable at the microscopic and at the macroscopic level because the photon has zero rest mass; this allows long distance interactions.
The photon epoch started after most leptons and anti-leptons were annihilated at the end of the lepton epoch, about 10 seconds after the Big Bang. Atomic nuclei were created in the process of nucleosynthesis which occurred during the first few minutes of the photon epoch. For the remainder of the photon epoch the universe contained a hot dense plasma of nuclei, electrons and photons. About 380,000 years after the Big Bang, the temperature of the universe fell to the point where nuclei could combine with electrons to create neutral atoms. As a result, photons no longer interacted frequently with matter and the universe became transparent. The highly redshifted photons from this period form the cosmic microwave background. Tiny variations in the temperature of the CMB correspond to variations in the density of the universe that were the early "seeds" from which all subsequent structure formation took place.
The frequency of life in the universe has been a frequent point of investigation in astronomy and astrobiology, being the issue of the Drake equation and the different views on it, from identifying the Fermi paradox, the situation of not having found any signs of extraterrestrial life, to arguments for a biophysical cosmology, a view of life being inherent to the physical cosmology of the universe.
=
General relativity is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. It is the basis of current cosmological models of the universe. General relativity generalizes special relativity and Newton's law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or spacetime. In particular, the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present.
The relation is specified by the Einstein field equations, a system of partial differential equations. In general relativity, the distribution of matter and energy determines the geometry of spacetime, which in turn describes the acceleration of matter. Therefore, solutions of the Einstein field equations describe the evolution of the universe. Combined with measurements of the amount, type, and distribution of matter in the universe, the equations of general relativity describe the evolution of the universe over time.
With the assumption of the cosmological principle that the universe is homogeneous and isotropic everywhere, a specific solution of the field equations that describes the universe is the metric tensor called the Friedmann–Lemaître–Robertson–Walker metric,
  
    
      
        d
        
          s
          
            2
          
        
        =
        −
        
          c
          
            2
          
        
        d
        
          t
          
            2
          
        
        +
        R
        (
        t
        
          )
          
            2
          
        
        
          (
          
            
              
                
                  d
                  
                    r
                    
                      2
                    
                  
                
                
                  1
                  −
                  k
                  
                    r
                    
                      2
                    
                  
                
              
            
            +
            
              r
              
                2
              
            
            d
            
              θ
              
                2
              
            
            +
            
              r
              
                2
              
            
            
              sin
              
                2
              
            
            ⁡
            θ
            
            d
            
              ϕ
              
                2
              
            
          
          )
        
      
    
    {\displaystyle ds^{2}=-c^{2}dt^{2}+R(t)^{2}\left({\frac {dr^{2}}{1-kr^{2}}}+r^{2}d\theta ^{2}+r^{2}\sin ^{2}\theta \,d\phi ^{2}\right)}
  
where (r, θ, φ) correspond to a spherical coordinate system. This metric has only two undetermined parameters. An overall dimensionless length scale factor R describes the size scale of the universe as a function of time (an increase in R is the expansion of the universe), and a curvature index k describes the geometry. The index k is defined so that it can take only one of three values: 0, corresponding to flat Euclidean geometry; 1, corresponding to a space of positive curvature; or −1, corresponding to a space of positive or negative curvature. The value of R as a function of time t depends upon k and the cosmological constant Λ. The cosmological constant represents the energy density of the vacuum of space and could be related to dark energy. The equation describing how R varies with time is known as the Friedmann equation after its inventor, Alexander Friedmann.
The solutions for R(t) depend on k and Λ, but some qualitative features of such solutions are general. First and most importantly, the length scale R of the universe can remain constant only if the universe is perfectly isotropic with positive curvature (k = 1) and has one precise value of density everywhere, as first noted by Albert Einstein.
Second, all solutions suggest that there was a gravitational singularity in the past, when R went to zero and matter and energy were infinitely dense. It may seem that this conclusion is uncertain because it is based on the questionable assumptions of perfect homogeneity and isotropy (the cosmological principle) and that only the gravitational interaction is significant. However, the Penrose–Hawking singularity theorems show that a singularity should exist for very general conditions. Hence, according to Einstein's field equations, R grew rapidly from an unimaginably hot, dense state that existed immediately following this singularity (when R had a small, finite value); this is the essence of the Big Bang model of the universe. Understanding the singularity of the Big Bang likely requires a quantum theory of gravity, which has not yet been formulated.
Third, the curvature index k determines the sign of the curvature of constant-time spatial surfaces averaged over sufficiently large length scales (greater than about a billion light-years). If k = 1, the curvature is positive and the universe has a finite volume. A universe with positive curvature is often visualized as a three-dimensional sphere embedded in a four-dimensional space. Conversely, if k is zero or negative, the universe has an infinite volume. It may seem counter-intuitive that an infinite and yet infinitely dense universe could be created in a single instant when R = 0, but exactly that is predicted mathematically when k is nonpositive and the cosmological principle is satisfied. By analogy, an infinite plane has zero curvature but infinite area, whereas an infinite cylinder is finite in one direction and a torus is finite in both.
The ultimate fate of the universe is still unknown because it depends critically on the curvature index k and the cosmological constant Λ. If the universe were sufficiently dense, k would equal +1, meaning that its average curvature throughout is positive and the universe will eventually recollapse in a Big Crunch, possibly starting a new universe in a Big Bounce. Conversely, if the universe were insufficiently dense, k would equal 0 or −1 and the universe would expand forever, cooling off and eventually reaching the Big Freeze and the heat death of the universe. Modern data suggests that the expansion of the universe is accelerating; if this acceleration is sufficiently rapid, the universe may eventually reach a Big Rip. Observationally, the universe appears to be flat (k = 0), with an overall density that is very close to the critical value between recollapse and eternal expansion.
=
Some speculative theories have proposed that our universe is but one of a set of disconnected universes, collectively denoted as the multiverse, challenging or enhancing more limited definitions of the universe. Max Tegmark developed a four-part classification scheme for the different types of multiverses that scientists have suggested in response to various problems in physics. An example of such multiverses is the one resulting from the chaotic inflation model of the early universe.
Another is the multiverse resulting from the many-worlds interpretation of quantum mechanics. In this interpretation, parallel worlds are generated in a manner similar to quantum superposition and decoherence, with all states of the wave functions being realized in separate worlds. Effectively, in the many-worlds interpretation the multiverse evolves as a universal wavefunction. If the Big Bang that created our multiverse created an ensemble of multiverses, the wave function of the ensemble would be entangled in this sense. Whether scientifically meaningful probabilities can be extracted from this picture has been and continues to be a topic of much debate, and multiple versions of the many-worlds interpretation exist. The subject of the interpretation of quantum mechanics is in general marked by disagreement.
The least controversial, but still highly disputed, category of multiverse in Tegmark's scheme is Level I. The multiverses of this level are composed by distant spacetime events "in our own universe". Tegmark and others have argued that, if space is infinite, or sufficiently large and uniform, identical instances of the history of Earth's entire Hubble volume occur every so often, simply by chance. Tegmark calculated that our nearest so-called doppelgänger is 1010115 metres away from us (a double exponential function larger than a googolplex). However, the arguments used are of speculative nature.
It is possible to conceive of disconnected spacetimes, each existing but unable to interact with one another. An easily visualized metaphor of this concept is a group of separate soap bubbles, in which observers living on one soap bubble cannot interact with those on other soap bubbles, even in principle. According to one common terminology, each "soap bubble" of spacetime is denoted as a universe, whereas humans' particular spacetime is denoted as the universe, just as humans call Earth's moon the Moon. The entire collection of these separate spacetimes is denoted as the multiverse.
With this terminology, different universes are not causally connected to each other. In principle, the other unconnected universes may have different dimensionalities and topologies of spacetime, different forms of matter and energy, and different physical laws and physical constants, although such possibilities are purely speculative. Others consider each of several bubbles created as part of chaotic inflation to be separate universes, though in this model these universes all share a causal origin.
Historically, there have been many ideas of the cosmos (cosmologies) and its origin (cosmogonies). Theories of an impersonal universe governed by physical laws were first proposed by the Greeks and Indians. Ancient Chinese philosophy encompassed the notion of the universe including both all of space and all of time. Over the centuries, improvements in astronomical observations and theories of motion and gravitation led to ever more accurate descriptions of the universe. The modern era of cosmology began with Albert Einstein's 1915 general theory of relativity, which made it possible to quantitatively predict the origin, evolution, and conclusion of the universe as a whole. Most modern, accepted theories of cosmology are based on general relativity and, more specifically, the predicted Big Bang.
=
Many cultures have stories describing the origin of the world and universe. Cultures generally regard these stories as having some truth. There are however many differing beliefs in how these stories apply amongst those believing in a supernatural origin, ranging from a god directly creating the universe as it is now to a god just setting the "wheels in motion" (for example via mechanisms such as the big bang and evolution).
Ethnologists and anthropologists who study myths have developed various classification schemes for the various themes that appear in creation stories. For example, in one type of story, the world is born from a world egg; such stories include the Finnish epic poem Kalevala, the Chinese story of Pangu or the Indian Brahmanda Purana. In related stories, the universe is created by a single entity emanating or producing something by him- or herself, as in the Tibetan Buddhism concept of Adi-Buddha, the ancient Greek story of Gaia (Mother Earth), the Aztec goddess Coatlicue myth, the ancient Egyptian god Atum story, and the Judeo-Christian Genesis creation narrative in which the Abrahamic God created the universe. In another type of story, the universe is created from the union of male and female deities, as in the Māori story of Rangi and Papa. In other stories, the universe is created by crafting it from pre-existing materials, such as the corpse of a dead god—as from Tiamat in the Babylonian epic Enuma Elish or from the giant Ymir in Norse mythology—or from chaotic materials, as in Izanagi and Izanami in Japanese mythology. In other stories, the universe emanates from fundamental principles, such as Brahman and Prakrti, and the creation myth of the Serers.
=
The pre-Socratic Greek philosophers and Indian philosophers developed some of the earliest philosophical concepts of the universe. The earliest Greek philosophers noted that appearances can be deceiving, and sought to understand the underlying reality behind the appearances. In particular, they noted the ability of matter to change forms (e.g., ice to water to steam) and several philosophers proposed that all the physical materials in the world are different forms of a single primordial material, or arche. The first to do so was Thales, who proposed this material to be water. Thales' student, Anaximander, proposed that everything came from the limitless apeiron. Anaximenes proposed the primordial material to be air on account of its perceived attractive and repulsive qualities that cause the arche to condense or dissociate into different forms. Anaxagoras proposed the principle of Nous (Mind), while Heraclitus proposed fire (and spoke of logos). Empedocles proposed the elements to be earth, water, air and fire. His four-element model became very popular. Like Pythagoras, Plato believed that all things were composed of number, with Empedocles' elements taking the form of the Platonic solids. Democritus, and later philosophers—most notably Leucippus—proposed that the universe is composed of indivisible atoms moving through a void (vacuum), although Aristotle did not believe that to be feasible because air, like water, offers resistance to motion. Air will immediately rush in to fill a void, and moreover, without resistance, it would do so indefinitely fast.
Although Heraclitus argued for eternal change, his contemporary Parmenides emphasized changelessness. Parmenides' poem On Nature has been read as saying that all change is an illusion, that the true underlying reality is eternally unchanging and of a single nature, or at least that the essential feature of each thing that exists must exist eternally, without origin, change, or end. His student Zeno of Elea challenged everyday ideas about motion with several famous paradoxes. Aristotle responded to these paradoxes by developing the notion of a potential countable infinity, as well as the infinitely divisible continuum.
The Indian philosopher Kanada, founder of the Vaisheshika school, developed a notion of atomism and proposed that light and heat were varieties of the same substance. In the 5th century AD, the Buddhist atomist philosopher Dignāga proposed atoms to be point-sized, durationless, and made of energy. They denied the existence of substantial matter and proposed that movement consisted of momentary flashes of a stream of energy.
The notion of temporal finitism was inspired by the doctrine of creation shared by the three Abrahamic religions: Judaism, Christianity and Islam. The Christian philosopher, John Philoponus, presented the philosophical arguments against the ancient Greek notion of an infinite past and future. Philoponus' arguments against an infinite past were used by the early Muslim philosopher, Al-Kindi (Alkindus); the Jewish philosopher, Saadia Gaon (Saadia ben Joseph); and the Muslim theologian, Al-Ghazali (Algazel).
Pantheism is the philosophical religious belief that the universe itself is identical to divinity and a supreme being or entity. The physical universe is thus understood as an all-encompassing, immanent deity. The term 'pantheist' designates one who holds both that everything constitutes a unity and that this unity is divine, consisting of an all-encompassing, manifested god or goddess.
=
The earliest written records of identifiable predecessors to modern astronomy come from Ancient Egypt and Mesopotamia from around 3000 to 1200 BCE. Babylonian astronomers of the 7th century BCE viewed the world as a flat disk surrounded by the ocean.
Later Greek philosophers, observing the motions of the heavenly bodies, were concerned with developing models of the universe based more profoundly on empirical evidence. Some of the earliest cosmological models of the universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the center. The first coherent model was proposed by Eudoxus of Cnidos, a student of Plato who followed Plato's idea that heavenly motions had to be circular. In order to account for the known complications of the planets' motions, particularly retrograde movement, Eudoxus' model included 27 different celestial spheres: four for each of the planets visible to the naked eye, three each for the Sun and the Moon, and one for the stars. All of these spheres were centered on the Earth, which remained motionless while they rotated eternally. Aristotle elaborated upon this model, increasing the number of spheres to 55 in order to account for further details of planetary motion. For Aristotle, normal matter was entirely contained within the terrestrial sphere, and it obeyed fundamentally different rules from heavenly material.
The post-Aristotle treatise De Mundo (of uncertain authorship and date) stated, "Five elements, situated in spheres in five regions, the less being in each case surrounded by the greater—namely, earth surrounded by water, water by air, air by fire, and fire by ether—make up the whole universe". This model was also refined by Callippus and after concentric spheres were abandoned, it was brought into nearly perfect agreement with astronomical observations by Ptolemy. The success of such a model is largely due to the mathematical fact that any function (such as the position of a planet) can be decomposed into a set of circular functions (the Fourier modes). Other Greek scientists, such as the Pythagorean philosopher Philolaus, postulated (according to Stobaeus' account) that at the center of the universe was a "central fire" around which the Earth, Sun, Moon and planets revolved in uniform circular motion.
The Greek astronomer Aristarchus of Samos was the first known individual to propose a heliocentric model of the universe. Though the original text has been lost, a reference in Archimedes' book The Sand Reckoner describes Aristarchus's heliocentric model. Archimedes wrote:
You, King Gelon, are aware the universe is the name given by most astronomers to the sphere the center of which is the center of the Earth, while its radius is equal to the straight line between the center of the Sun and the center of the Earth. This is the common account as you have heard from astronomers. But Aristarchus has brought out a book consisting of certain hypotheses, wherein it appears, as a consequence of the assumptions made, that the universe is many times greater than the universe just mentioned. His hypotheses are that the fixed stars and the Sun remain unmoved, that the Earth revolves about the Sun on the circumference of a circle, the Sun lying in the middle of the orbit, and that the sphere of fixed stars, situated about the same center as the Sun, is so great that the circle in which he supposes the Earth to revolve bears such a proportion to the distance of the fixed stars as the center of the sphere bears to its surface.
Aristarchus thus believed the stars to be very far away, and saw this as the reason why stellar parallax had not been observed, that is, the stars had not been observed to move relative each other as the Earth moved around the Sun. The stars are in fact much farther away than the distance that was generally assumed in ancient times, which is why stellar parallax is only detectable with precision instruments. The geocentric model, consistent with planetary parallax, was assumed to be the explanation for the unobservability of stellar parallax.
The only other astronomer from antiquity known by name who supported Aristarchus's heliocentric model was Seleucus of Seleucia, a Hellenistic astronomer who lived a century after Aristarchus. According to Plutarch, Seleucus was the first to prove the heliocentric system through reasoning, but it is not known what arguments he used. Seleucus' arguments for a heliocentric cosmology were probably related to the phenomenon of tides. According to Strabo (1.1.9), Seleucus was the first to state that the tides are due to the attraction of the Moon, and that the height of the tides depends on the Moon's position relative to the Sun. Alternatively, he may have proved heliocentricity by determining the constants of a geometric model for it, and by developing methods to compute planetary positions using this model, similar to Nicolaus Copernicus in the 16th century. During the Middle Ages, heliocentric models were also proposed by the Persian astronomers Albumasar and Al-Sijzi.
The Aristotelian model was accepted in the Western world for roughly two millennia, until Copernicus revived Aristarchus's perspective that the astronomical data could be explained more plausibly if the Earth rotated on its axis and if the Sun were placed at the center of the universe.
In the center rests the Sun. For who would place this lamp of a very beautiful temple in another or better place than this wherefrom it can illuminate everything at the same time?
As noted by Copernicus, the notion that the Earth rotates is very old, dating at least to Philolaus (c. 450 BC), Heraclides Ponticus (c. 350 BC) and Ecphantus the Pythagorean. Roughly a century before Copernicus, the Christian scholar Nicholas of Cusa also proposed that the Earth rotates on its axis in his book, On Learned Ignorance (1440). Al-Sijzi also proposed that the Earth rotates on its axis. Empirical evidence for the Earth's rotation on its axis, using the phenomenon of comets, was given by Tusi (1201–1274) and Ali Qushji (1403–1474).
This cosmology was accepted by Isaac Newton, Christiaan Huygens and later scientists. Newton demonstrated that the same laws of motion and gravity apply to earthly and to celestial matter, making Aristotle's division between the two obsolete. Edmund Halley (1720) and Jean-Philippe de Chéseaux (1744) noted independently that the assumption of an infinite space filled uniformly with stars would lead to the prediction that the nighttime sky would be as bright as the Sun itself; this became known as Olbers' paradox in the 19th century. Newton believed that an infinite space uniformly filled with matter would cause infinite forces and instabilities causing the matter to be crushed inwards under its own gravity. This instability was clarified in 1902 by the Jeans instability criterion. One solution to these paradoxes is the Charlier universe, in which the matter is arranged hierarchically (systems of orbiting bodies that are themselves orbiting in a larger system, ad infinitum) in a fractal way such that the universe has a negligibly small overall density; such a cosmological model had also been proposed earlier in 1761 by Johann Heinrich Lambert.
=
During the 18th century, Immanuel Kant speculated that nebulae could be entire galaxies separate from the Milky Way, and in 1850, Alexander von Humboldt called these separate galaxies Weltinseln, or "world islands", a term that later developed into "island universes". In 1919, when the Hooker Telescope was completed, the prevailing view was that the universe consisted entirely of the Milky Way Galaxy. Using the Hooker Telescope, Edwin Hubble identified Cepheid variables in several spiral nebulae and in 1922–1923 proved conclusively that Andromeda Nebula and Triangulum among others, were entire galaxies outside our own, thus proving that the universe consists of a multitude of galaxies. With this Hubble formulated the Hubble constant, which allowed for the first time a calculation of the age of the universe and size of the Observable Universe, which became increasingly precise with better meassurements, starting at 2 billion years and 280 million light-years, until 2006 when data of the Hubble Space Telescope allowed a very accurate calculation of the age of the universe and size of the Observable Universe.
The modern era of physical cosmology began in 1917, when Albert Einstein first applied his general theory of relativity to model the structure and dynamics of the universe. The discoveries of this era, and the questions that remain unanswered, are outlined in the sections above.
Footnotes
Citations
=
Van Der Waerden, B. L. (June 1987). "The Heliocentric System in Greek, Persian and Hindu Astronomy". Annals of the New York Academy of Sciences. 500 (1): 525–545. Bibcode:1987NYASA.500..525V. doi:10.1111/j.1749-6632.1987.tb37224.x. ISSN 0077-8923. S2CID 222087224.
Landau LD, Lifshitz EM (1975). The classical theory of fields. Course of theoretical physics. Vol. 2 (4th rev. English ed.). Oxford; New York: Pergamon Press. pp. 358–397. ISBN 978-0-08-018176-9.
Liddell, Henry George & Scott, Robert (1994). A Greek-English lexicon. Oxford: Clarendon Pr. ISBN 978-0-19-864214-5.
Misner, Charles W.; Thorne, Kip S.; Wheeler, John Archibald; Kip; Wheeler; J.A. (2008). Gravitation (27. printing ed.). New York, NY: Freeman. pp. 703–816. ISBN 978-0-7167-0344-0.
Raine, Derek; Thomas, Edwin G. (2001). An introduction to the science of cosmology. Series in astronomy and astrophysics. Bristol: Institute of Physics Publ. ISBN 978-0-7503-0405-4.
Rindler, Wolfgang (1986). Essential relativity: special, general, and cosmological. Texts and monographs in physics. New York Heidelberg: Springer. pp. 193–244. ISBN 978-0-387-10090-6.
Rees, Martin J.; DK Publishing, Inc; Smithsonian Institution, eds. (2012). Universe (Rev. ed.). New York: DK Pub. ISBN 978-0-7566-9841-6. OCLC 809932784.
NASA/IPAC Extragalactic Database (NED) / (NED-Distances).
There are about 1082 atoms in the observable universe – LiveScience, July 2021.
This is why we will never know everything about our universe – Forbes, May 2019.

--- TOPIC: Computer_science ---
Computer science is the study of computation, information, and automation. Included broadly in the sciences, computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). An expert in the field is known as a computer scientist. 
Algorithms and data structures are central to computer science.
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.
The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.
Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and "in less than two years, he had sketched out many of the salient features of the modern computer". "A crucial step was the adoption of a punched card system derived from the Jacquard loom" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used punched cards and a central processing unit. When the machine was finished, some hailed it as "Babbage's dream come true".
During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.
Although first proposed in 1956, the term "computer science" appears in a 1959 article in Communications of the ACM,
in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline. This effort, and those of others such as numerical analyst George Forsythe, were successful, and universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.
In the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression "automatic information" (e.g. "informazione automatica" in Italian) or "information and mathematics" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). "In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain."
A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes." The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.
Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter, Stephen Kleene, and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.
The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software engineering" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.
The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
=
Despite the word science in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.
Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.
=
A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).
Computer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.
As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.
CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.
=
Theoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.
 Theory of computation 
According to Peter Denning, the fundamental question underlying computer science is, "What can be automated?" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
The famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.
 Information and coding theory 
Information theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.
Coding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.
 Data structures and algorithms 
Data structures and algorithms are the studies of commonly used computational methods and their computational efficiency.
 Programming language theory and formal methods 
Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
=
 Computer graphics and visualization 
Computer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
 Image and sound processing 
Information can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.
 Computational science, finance and engineering 
Scientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, societies and social situations (notably war games) along with their habitats, and interactions among biological cells. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.
 Human–computer interaction 
Human–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.
 Software engineering 
Software engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.
 Artificial intelligence 
Artificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
=
 Computer architecture and microarchitecture 
Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term "architecture" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.
 Concurrent, parallel and distributed computing 
Concurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.
 Computer networks 
This branch of computer science aims studies the construction and behavior of computer networks. It addresses their performance, resilience, security, scalability, and cost-effectiveness, along with the variety of services they can provide.
 Computer security and cryptography 
Computer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.
Historical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.
 Databases and data mining 
A database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.
The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:
Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent "anything".
All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as "on/off", "magnetized/de-magnetized", "high-voltage/low-voltage", etc.).
Alan Turing's insight: there are only five actions that a computer has to perform in order to do "anything".
Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:
move left one location;
move right one location;
read symbol at current location;
print 0 at current location;
print 1 at current location.
Corrado Böhm and Giuseppe Jacopini's insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do "anything".
Only three rules are needed to combine any set of basic instructions into more complex ones:
sequence: first do this, then do that;
 selection: IF such-and-such is the case, THEN do this, ELSE do that;
repetition: WHILE such-and-such is the case, DO this.
The three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).
Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:
Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.
Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
Object-oriented programming, a programming paradigm based on the concept of "objects", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.
Service-oriented programming, a programming paradigm that uses "services" as the unit of computer work, to design and implement integrated business applications and mission critical software programs.
Many languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.
Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.
DBLP Computer Science Bibliography
Association for Computing Machinery
Institute of Electrical and Electronics Engineers

--- TOPIC: Algorithm ---
In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning).
In contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results. For example, although social media recommender systems are commonly called "algorithms", they actually rely on heuristics as there is no truly "correct" recommendation.
As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.
Around 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī ("Book of Indian computation") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī ("Addition and subtraction in Indian arithmetic"). In the early 12th century, Latin translations of these texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Here, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or "Thus spoke Al-Khwarizmi".
The word algorism in English came to mean the use of place-value notation in calculations; it occurs in the Ancrene Wisse from circa 1225. By the time Geoffrey Chaucer wrote The Canterbury Tales in the late 14th century, he used a variant of the same word in describing augrym stones, stones used for place-value calculation. In the 15th century, under the influence of the Greek word ἀριθμός (arithmos, "number"; cf. "arithmetic"), the Latin word was altered to algorithmus. By 1596, this form of the word was used in English, as algorithm, by Thomas Hood.
One informal definition is "a set of rules that precisely defines a sequence of operations", which would include all computer programs (including programs that do not perform numeric calculations), and any prescribed bureaucratic procedure
or cook-book recipe. In general, a program is an algorithm only if it stops eventually—even though infinite loops may sometimes prove desirable. Boolos, Jeffrey & 1974, 1999 define an algorithm to be an explicit set of instructions for determining an output, that can be followed by a computing machine or a human who could only carry out specific elementary operations on symbols.
Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain performing arithmetic or an insect looking for food), in an electrical circuit, or a mechanical device.
=
Step-by-step procedures for solving mathematical problems have been recorded since antiquity. This includes in Babylonian mathematics (around 2500 BC), Egyptian mathematics (around 1550 BC), Indian mathematics (around 800 BC and later), the Ifa Oracle (around 500 BC), Greek mathematics (around 240 BC), Chinese mathematics (around 200 BC and later), and Arabic mathematics (around 800 AD).
The earliest evidence of algorithms is found in ancient Mesopotamian mathematics. A Sumerian clay tablet found in Shuruppak near Baghdad and dated to c. 2500 BC describes the earliest division algorithm. During the Hammurabi dynasty c. 1800 – c. 1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.
Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus c. 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC).Examples of ancient Indian mathematics included the Shulba Sutras, the Kerala School, and the Brāhmasphuṭasiddhānta.
The first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.
=
 Weight-driven clocks 
David Bolter credits the invention of the weight-driven clock as "the key invention [of Europe in the Middle Ages]," specifically the verge escapement mechanism producing the tick and tock of a mechanical clock. "The accurate automatic machine" led immediately to "mechanical automata" in the 13th century and "computational machines"—the difference and analytical engines of Charles Babbage and Ada Lovelace in the mid-19th century. Lovelace designed the first algorithm intended for processing on a computer, Babbage's analytical engine, which is the first device considered a real Turing-complete computer instead of just a calculator. Although the full implementation of Babbage's second device was not realized for decades after her lifetime, Lovelace has been called "history's first programmer".
 Electromechanical relay 
Bell and Newell (1971) write that the Jacquard loom, a precursor to Hollerith cards (punch cards), and "telephone switching technologies" led to the development of the first computers. By the mid-19th century, the telegraph, the precursor of the telephone, was in use throughout the world. By the late 19th century, the ticker tape (c. 1870s) was in use, as were Hollerith cards (c. 1890). Then came the teleprinter (c. 1910) with its punched-paper use of Baudot code on tape.
Telephone-switching networks of electromechanical relays were invented in 1835. These led to the invention of the digital adding device by George Stibitz in 1937. While working in Bell Laboratories, he observed the "burdensome" use of mechanical calculators with gears. "He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device".
=
In 1928, a partial formalization of the modern concept of algorithms began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert. Later formalizations were framed as attempts to define "effective calculability" or "effective method". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
=
Algorithms have evolved and improved in many ways as time goes on. Common uses of algorithms today include social media apps like Instagram and YouTube. Algorithms are used as a way to analyze what people like and push more of those things to the people who interact with them. Quantum computing uses quantum algorithm procedures to solve problems faster. More recently, in 2024, NIST updated their post-quantum encryption standards, which includes new encryption algorithms to enhance defenses against attacks using quantum computing.
Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts, and control tables are structured expressions of algorithms that avoid common ambiguities of natural language. Programming languages are primarily for expressing algorithms in a computer-executable form but are also used to define or document algorithms.
=
There are many possible representations and Turing machine programs can be expressed as a sequence of machine tables (see finite-state machine, state-transition table, and control table for more), as flowcharts and drakon-charts (see state diagram for more), as a form of rudimentary machine code or assembly code called "sets of quadruples", and more. Algorithm representations can also be classified into three accepted levels of Turing machine description: high-level description, implementation description, and formal description. A high-level description describes the qualities of the algorithm itself, ignoring how it is implemented on the Turing machine. An implementation description describes the general manner in which the machine moves its head and stores data to carry out the algorithm, but does not give exact states. In the most detail, a formal description gives the exact state table and list of transitions of the Turing machine.
=
The graphical aid called a flowchart offers a way to describe and document an algorithm (and a computer program corresponding to it). It has four primary symbols: arrows showing program flow, rectangles (SEQUENCE, GOTO), diamonds (IF-THEN-ELSE), and dots (OR-tie). Sub-structures can "nest" in rectangles, but only if a single exit occurs from the superstructure.
It is often important to know how much time, storage, or other cost an algorithm may require. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm that adds up the elements of a list of n numbers would have a time requirement of ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠, using big O notation. The algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. If the space required to store the input numbers is not counted, it has a space requirement of ⁠
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
⁠, otherwise ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ is required.
Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost ⁠
  
    
      
        O
        (
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(\log n)}
  
⁠) outperforms a sequential search (cost ⁠
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  
⁠ ) when used for table lookups on sorted lists or arrays.
=
The analysis, and study of algorithms is a discipline of computer science. Algorithms are often studied abstractly, without referencing any specific programming language or implementation. Algorithm analysis resembles other mathematical disciplines as it focuses on the algorithm's properties, not implementation. Pseudocode is typical for analysis as it is a simple and general representation. Most algorithms are implemented on particular hardware/software platforms and their algorithmic efficiency is tested using real code. The efficiency of a particular algorithm may be insignificant for many "one-off" problems but it may be critical for algorithms designed for fast interactive, commercial, or long-life scientific usage. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
Empirical testing is useful for uncovering unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.
Empirical tests cannot replace formal analysis, though, and are non-trivial to perform fairly.
=
To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.
=
The best case of an algorithm refers to the scenario or input for which the algorithm or data structure takes the least time and resources to complete its tasks.  The worst case of an algorithm is the case that causes the algorithm or data structure to consume the maximum period of time and computational resources.
Algorithm design is a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories, such as divide-and-conquer or dynamic programming within operation research. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern. One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g., an algorithm's run-time growth as the size of its input increases.
=
Per the Church–Turing thesis, any algorithm can be computed by any Turing complete model. Turing completeness only requires four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. However, Kemeny and Kurtz observe that, while "undisciplined" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in "spaghetti code", a programmer can write structured programs using only these instructions; on the other hand "it is also possible, and not too hard, to write badly structured programs in a structured language". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.
By themselves, algorithms are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute "processes" (USPTO 2006), so algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is controversial, and there are criticized patents involving algorithms, especially data compression algorithms, such as Unisys's LZW patent. Additionally, some cryptographic algorithms have export restrictions (see export of cryptography).
=
Recursion
A recursive algorithm invokes itself repeatedly until meeting a termination condition and is a common functional programming method. Iterative algorithms use repetitions such as loops or data structures like stacks to solve problems. Problems may be suited for one implementation or the other. The Tower of Hanoi is a puzzle commonly solved using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.
Serial, parallel or distributed
Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time on serial computers. Serial algorithms are designed for these environments, unlike parallel or distributed algorithms. Parallel algorithms take advantage of computer architectures where multiple processors can work on a problem at the same time. Distributed algorithms use multiple machines connected via a computer network. Parallel and distributed algorithms divide the problem into subproblems and collect the results back together. Resource consumption in these algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable, but some problems have no parallel algorithms and are called inherently serial problems.
Deterministic or non-deterministic
Deterministic algorithms solve the problem with exact decisions at every step; whereas non-deterministic algorithms solve problems via guessing. Guesses are typically made more accurate through the use of heuristics.
Exact or approximate
While many algorithms reach an exact solution, approximation algorithms seek an approximation that is close to the true solution. Such algorithms have practical value for many hard problems. For example, the Knapsack problem, where there is a set of items, and the goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. The total weight that can be carried is no more than some fixed number X. So, the solution must consider the weights of items as well as their value.
Quantum algorithm
Quantum algorithms run on a realistic model of quantum computation. The term is usually used for those algorithms that seem inherently quantum or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.
=
Another way of classifying algorithms is by their design methodology or paradigm. Some common paradigms are:
Brute-force or exhaustive search
Brute force is a problem-solving method of systematically trying every possible option until the optimal solution is found. This approach can be very time-consuming, testing every possible combination of variables. It is often used when other methods are unavailable or too complex. Brute force can solve a variety of problems, including finding the shortest path between two points and cracking passwords.
Divide and conquer
A divide-and-conquer algorithm repeatedly reduces a problem to one or more smaller instances of itself (usually recursively) until the instances are small enough to solve easily. Merge sorting is an example of divide and conquer, where an unordered list is repeatedly split into smaller lists, which are sorted in the same way and then merged. In a simpler variant of divide and conquer called prune and search or decrease-and-conquer algorithm, which solves one smaller instance of itself, and does not require a merge step. An example of a prune and search algorithm is the binary search algorithm.
Search and enumeration
Many problems (such as playing chess) can be modelled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration, and backtracking.
Randomized algorithm
Such algorithms make some choices randomly (or pseudo-randomly). They find approximate solutions when finding exact solutions may be impractical (see heuristic method below). For some problems, the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithm for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:
Monte Carlo algorithms return a correct answer with high probability. E.g. RP is the subclass of these that run in polynomial time.
Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
Reduction of complexity
This technique transforms difficult problems into better-known problems solvable with (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithms. For example, one selection algorithm finds the median of an unsorted list by first sorting the list (the expensive portion), and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.
Back tracking
In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.
=
For optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:
Linear programming
When searching for optimal solutions to a linear function bound by linear equality and inequality constraints, the constraints can be used directly to produce optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem also requires that any of the unknowns be integers, then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
Dynamic programming
When a problem shows optimal substructures—meaning the optimal solution can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions. For example, Floyd–Warshall algorithm, the shortest path between a start and goal vertex in a weighted graph can be found using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. Unlike divide and conquer, dynamic programming subproblems often overlap. The difference between dynamic programming and simple recursion is the caching or memoization of recursive calls. When subproblems are independent and do not repeat, memoization does not help; hence dynamic programming is not applicable to all complex problems. Using memoization dynamic programming reduces the complexity of many problems from exponential to polynomial.
The greedy method
Greedy algorithms, similarly to a dynamic programming, work by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution and improve it by making small modifications. For some problems, they always find the optimal solution but for others they may stop at local optima. The most popular use of greedy algorithms is finding minimal spanning trees of graphs without negative cycles. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
The heuristic method
In optimization problems, heuristic algorithms find solutions close to the optimal solution when finding the optimal solution is impractical. These algorithms get closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. They can ideally find a solution very close to the optimal solution in a relatively short time. These algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.
One of the simplest algorithms finds the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be described in plain English as:
High-level description:
If a set of numbers is empty, then there is no highest number.
Assume the first number in the set is the largest.
For each remaining number in the set: if this number is greater than the current largest, it becomes the new largest.
When there are no unchecked numbers left in the set, consider the current largest number to be the largest in the set.
(Quasi-)formal description:
Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:
Zaslavsky, C. (1970). Mathematics of the Yoruba People and of Their Neighbors in Southern Nigeria. The Two-Year College Mathematics Journal, 1(2), 76–99. https://doi.org/10.2307/3027363
NIST Releases First 3 Finalized Post-Quantum Encryption Standards. https://www.nist.gov/news-events/news/2024/08/nist-releases-first-3-finalized-post-quantum-encryption-standards
"Algorithm". Encyclopedia of Mathematics. EMS Press. 2001 [1994].
Weisstein, Eric W. "Algorithm". MathWorld.
Dictionary of Algorithms and Data Structures – National Institute of Standards and Technology
Algorithm repositories
The Stony Brook Algorithm Repository – State University of New York at Stony Brook
Collected Algorithms of the ACM – Associations for Computing Machinery
The Stanford GraphBase Archived December 6, 2015, at the Wayback Machine – Stanford University

--- TOPIC: Software_engineering ---
Software engineering is a branch of both computer science and engineering focused on designing, developing, testing, and maintaining software applications. It involves applying engineering principles and computer programming expertise to develop software systems that meet user needs.
In the tech industry, the title software engineer is often used aspirationally, even though many such roles are fundamentally programming positions and lack the formal regulation associated with traditional engineering.
A software engineer applies a software development process, that involves defining, implementing, testing, managing, and maintaining software systems, as well as developing the software development process itself.
Beginning in the 1960s, software engineering was recognized as a separate field of engineering.
The development of software engineering was seen as a struggle. Problems included software that was over budget, exceeded deadlines, required extensive debugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed.
In 1968, NATO organized the first conference on software engineering, which addressed emerging challenges in software development. The event played a key role in formalizing guidelines and best practices for creating reliable and maintainable software.
The origins of the term software engineering have been attributed to various sources. The term appeared in a list of services offered by companies in the June 1965 issue of "Computers and Automation" and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) in "President's Letter to the ACM Membership" by Anthony A. Oettinger. It is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer. Margaret Hamilton described the discipline of "software engineering" during the Apollo missions to give what they were doing legitimacy. At the time, there was perceived to be a "software crisis". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of "Software Engineering" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton.
In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States.
Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team.
Modern, generally accepted best practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of the major computing disciplines.
In modern systems, where concepts such as Edge Computing, Internet of Things and Cyber-physical Systems are prevalent, software is a critical factor. Thus, software engineering is closely related to the Systems Engineering discipline. The Systems Engineering Body of Knowledge claims:
Software is prominent in most modern systems architectures and is often the primary means for integrating complex system components. Software engineering and systems engineering are not merely related disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering.
=
Notable definitions of software engineering include:
"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software."—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary
"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software."—IEEE Standard Glossary of Software Engineering Terminology
"An engineering discipline concerned with all aspects of software production." — Ian Sommerville
"The establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines."—Fritz Bauer
"A branch of computer science that deals with the design, implementation, and maintenance of complex computer programs."—Merriam-Webster
"'Software engineering' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as 'programming integrated over time.'"—Software Engineering at Google
The term has also been used less formally:
As the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis
As the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science
As the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices
=
Individual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused in the United States.
=
Requirements engineering is about elicitation, analysis, specification, and validation of requirements for software. Software requirements can be functional, non-functional or domain.
Functional requirements describe expected behaviors (i.e. outputs). Non-functional requirements specify issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interface constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.
=
Software design is the process of making high-level plans for the software. Design is sometimes divided into levels:
Interface design plans the interaction between a system and its environment as well as the inner workings of the system.
Architectural design plans the major components of a system, including their responsibilities, properties, and interfaces between them.
Detailed design plans internal elements, including their properties, relationships, algorithms and data structures.
=
Software construction typically involves programming (a.k.a. coding), unit testing, integration testing, and debugging so as to implement the design."Software testing is related to, but different from, ... debugging".
=
Software testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the software under test. Software testing can be viewed as a risk based activity.
When described separately from construction, testing typically is performed by test engineers or quality assurance instead of the programmers who wrote it. It is performed at the system level and is considered an aspect of software quality. The testers' goals during the testing process are to minimize the overall number of tests to a manageable set and make well-informed decisions regarding which risks should be prioritized for testing and which can wait.
=
Program analysis is the process of analyzing computer programs with respect to an aspect such as performance, robustness, and security.
=
Software maintenance refers to supporting the software after release. It may include but is not limited to: error correction, optimization, deletion of unused and discarded features, and enhancement of existing features.
Usually, maintenance takes up 40% to 80% of project cost.
Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004, the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.
In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.
=
A small but growing number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering bachelor's degree in the world; in the following year, the University of Sheffield established a similar program. In 1996, the Rochester Institute of Technology established the first software engineering bachelor's degree program in the United States; however, it did not obtain ABET accreditation until 2003, the same year as Rice University, Clarkson University, Milwaukee School of Engineering, and Mississippi State University.
Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004, about 50 universities in the U.S. offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering master's degree was established at Seattle University in 1979. Since then, graduate software engineering degrees have been made available from many more universities. Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.
Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.
Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title. Software Engineers can also become professionally qualified as a Chartered Engineer through the British Computer Society.
In the United States, the NCEES began offering a Professional Engineer exam for Software Engineering in 2013, thereby allowing Software Engineers to be licensed and recognized. NCEES ended the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial.
The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current version is SWEBOK v4. The IEEE also promulgates a "Software Engineering Code of Ethics".
=
There are an estimated 26.9 million professional software engineers in the world as of 2022, up from 21 million in 2016.
Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, software product managers, educators, and researchers.
Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, Thrombosis, Obesity, and hand and wrist problems such as carpal tunnel syndrome.
 United States 
The U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. The BLS estimates 2024 to 2034 the growth for software engineers is 15% which is lesser than their prediction from 2023 to 2033 that computer software engineering would increase by 17%. This is down from the 2022 to 2032 BLS estimate of 25% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. In addition, the BLS Job Outlook for Computer Programmers, the U.S. Bureau of Labor Statistics (BLS) Occupational Outlook predicts a decline of -7 percent from 2016 to 2026, a further decline of -9 percent from 2019 to 2029, a decline of -10 percent from 2021 to 2031. and then a decline of -11 percent from 2022 to 2032. Currently their prediction for 2024 to 2034 is a decline of -6 percent. Since computer programming can be done from anywhere in the world, companies sometimes hire programmers in countries where wages are lower. Furthermore, the ratio of women in many software fields has also been declining over the years as compared to other engineering fields. Then there is the additional concern that recent advances in Artificial Intelligence might impact the demand for future generations of Software Engineers. However, this trend may change or slow in the future as many current software engineers in the U.S. market flee the profession or age out of the market in the next few decades.
=
The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.
Broader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM and the IEEE Computer Society together examined the possibility of licensing of software engineers as Professional Engineers in the 1990s,
but eventually decided that such licensing was inappropriate for the professional industrial practice of software engineering. John C. Knight and Nancy G. Leveson presented a more balanced analysis of the licensing issue in 2002.
In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the British Computer Society or Institution of Engineering and Technology and so qualify to be considered for Chartered Engineer status through either of those institutions. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.
=
The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Additionally, the glut of high-tech workers has led to a wider adoption of the 996 working hour system and ‘007’ schedules as the expected work load. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans leave work, Asians are just arriving to work. When Asians are leaving work, Europeans arrive to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.
While global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.
=
There are various prizes in the field of software engineering:
ACM-AAAI Allen Newell Award- USA. Awarded to career contributions that have breadth within computer science, or that bridge computer science and other disciplines.
BCS Lovelace Medal. Awarded to individuals who have made outstanding contributions to the understanding or advancement of computing.
ACM SIGSOFT Outstanding Research Award, selected for individual(s) who have made "significant and lasting research contributions to the theory or practice of software engineering."
More ACM SIGSOFT Awards.
The Codie award, a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.
Harlan Mills Award for "contributions to the theory and practice of the information sciences, focused on software engineering".
ICSE Most Influential Paper Award.
Jolt Award, also for the software industry.
Stevens Award given in memory of Wayne Stevens.
Some call for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.
Some claim that the concept of software engineering is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.
Some claim that a core issue with software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."
Edsger Dijkstra, a founder of many of the concepts in software development today, rejected the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what he called the "radical novelty" of computer science:
A number of these phenomena have been bundled under the name "Software Engineering". As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter "How to program if you cannot."
=
Computer science
Computer engineering
Data engineering
Software craftsmanship
Software development
Release engineering
=
Programmer
Systems analyst
Systems architect
=
Bachelor of Science in Information Technology
Bachelor of Software Engineering
List of software engineering conferences
List of computer science journals (including software engineering journals)
List of software programming journals
Lists of programming software development tools
Software Engineering Institute
=
=
Pierre Bourque; Richard E. (Dick) Fairley, eds. (2014). Guide to the Software Engineering Body of Knowledge Version 3.0 (SWEBOK). IEEE Computer Society.
Roger S. Pressman; Bruce Maxim (January 23, 2014). Software Engineering: A Practitioner's Approach (8th ed.). McGraw-Hill. ISBN 978-0-07-802212-8.
Ian Sommerville (March 24, 2015). Software Engineering (10th ed.). Pearson Education Limited. ISBN 978-0-13-394303-0.
Jalote, Pankaj (2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7.
Bruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0-13-606125-0.
Oshana, Robert (2019-06-21). Software engineering for embedded systems : methods, practical techniques, and applications (Second ed.). Kidlington, Oxford, United Kingdom. ISBN 978-0-12-809433-4.
Pierre Bourque; Richard E. Fairley, eds. (2004). Guide to the Software Engineering Body of Knowledge Version 3.0 (SWEBOK), https://www.computer.org/web/swebok/v3. IEEE Computer Society.
The Open Systems Engineering and Software Development Life Cycle Framework Archived 2010-07-18 at the Wayback Machine OpenSDLC.org the integrated Creative Commons SDLC
Software Engineering Institute Carnegie Mellon

--- TOPIC: Internet ---
The Internet (or internet) is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that comprises private, public, academic, business, and government networks of local to global scope, linked by electronic, wireless, and optical networking technologies. The Internet carries a vast range of information services and resources, such as the interlinked hypertext documents and applications of the World Wide Web (WWW), electronic mail, discussion groups, internet telephony, streaming media and file sharing.
Most traditional communication media, including telephone, radio, television, paper mail, newspapers, and print publishing, have been transformed by the Internet, giving rise to new media such as email, online music, digital newspapers, news aggregators, and audio and video streaming websites. The Internet has enabled and accelerated new forms of personal interaction through instant messaging, Internet forums, and social networking services. Online shopping has also grown to occupy a significant market across industries, enabling firms to extend brick and mortar presences to serve larger markets. Business-to-business and financial services on the Internet affect supply chains across entire industries. 
The origins of the Internet date back to research that enabled the time-sharing of computer resources, the development of packet switching, and the design of computer networks for data communication. The set of communication protocols to enable internetworking on the Internet arose from research and development commissioned in the 1970s by the Defense Advanced Research Projects Agency (DARPA) of the United States Department of Defense in collaboration with universities and researchers across the United States and in the United Kingdom and France. 
The Internet has no single centralized governance in either technological implementation or policies for access and usage. Each constituent network sets its own policies. The overarching definitions of the two principal name spaces on the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the non-profit Internet Engineering Task Force (IETF).
The word internetted was used as early as 1849, meaning interconnected or interwoven. The word Internet was used in 1945 by the United States War Department in a radio operator's manual, and in 1974 as the shorthand form of Internetwork. Today, the term Internet most commonly refers to the global system of interconnected computer networks, though it may also refer to any group of smaller networks.
The word Internet may be capitalized as a proper noun, although this is becoming less common. This reflects the tendency in English to capitalize new terms and move them to lowercase as they become familiar. The word is sometimes still capitalized to distinguish the global internet from smaller networks, though many publications, including the AP Stylebook since 2016, recommend the lowercase form in every case. In 2016, the Oxford English Dictionary found that, based on a study of around 2.5 billion printed and online sources, "Internet" was capitalized in 54% of cases.
The terms Internet and World Wide Web are often used interchangeably; it is common to speak of "going on the Internet" when using a web browser to view web pages. However, the World Wide Web, or the Web, is only one of a large number of Internet services. It is the global collection of web pages, documents and other web resources linked by hyperlinks and URLs.
=
In the 1960s, computer scientists began developing systems for time-sharing of computer resources. J. C. R. Licklider proposed the idea of a universal network while working at Bolt Beranek & Newman and, later, leading the Information Processing Techniques Office at the Advanced Research Projects Agency (ARPA) of the United States Department of Defense. Research into packet switching, one of the fundamental Internet technologies, started in the work of Paul Baran at RAND in the early 1960s and, independently, Donald Davies at the United Kingdom's National Physical Laboratory in 1965.
After the Symposium on Operating Systems Principles in 1967, packet switching from the proposed NPL network was incorporated into the design of the ARPANET, an experimental resource sharing network proposed by ARPA. ARPANET development began with two network nodes which were interconnected between the University of California, Los Angeles and the Stanford Research Institute on 29 October 1969. The third site was at the University of California, Santa Barbara, followed by the University of Utah. 
=
By the end of 1971, 15 sites were connected to the young ARPANET. Thereafter, the ARPANET gradually developed into a decentralized communications network, connecting remote centers and military bases in the United States. Other user networks and research networks, such as the Merit Network and CYCLADES, were developed in the late 1960s and early 1970s. Early international collaborations for the ARPANET were rare. Connections were made in 1973 to Norway (NORSAR and, later, NDRE) and to Peter Kirstein's research group at University College London, which provided a gateway to British academic networks, the first internetwork for resource sharing. 
ARPA projects, the International Network Working Group and commercial initiatives led to the development of various protocols and standards by which multiple separate networks could become a single network, or a network of networks. In 1974, Vint Cerf at Stanford University and Bob Kahn at DARPA published a proposal for "A Protocol for Packet Network Intercommunication". Cerf and his graduate students used the term internet as a shorthand for internetwork in RFC 675. The Internet Experiment Notes and later RFCs repeated this use. The work of Louis Pouzin and Robert Metcalfe had important influences on the resulting TCP/IP design. National PTTs and commercial providers developed the X.25 standard and deployed it on public data networks.
=
The ARPANET initially served as a backbone for the interconnection of regional academic and military networks in the United States to enable resource sharing. Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). 
In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which facilitated worldwide proliferation of interconnected networks. TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s. 
The NSFNet expanded into academic and research organizations in Europe, Australia, New Zealand and Japan in 1988–89. Although other network protocols such as UUCP and PTT public data networks had global reach well before this time, this marked the beginning of the Internet as an intercontinental network. Commercial Internet service providers emerged in 1989 in the United States and Australia. The ARPANET was decommissioned in 1990.
=
The linking of commercial networks and enterprises by the early 1990s, as well as the advent of the World Wide Web, marked the beginning of the transition to the modern Internet. Steady advances in semiconductor technology and optical networking created new economic opportunities for commercial involvement in the expansion of the network in its core and for delivering services to the public. In mid-1989, MCI Mail and Compuserve established connections to the Internet, delivering email and public access products to the half million users of the Internet. 
Just months later, on 1 January 1990, PSInet launched an alternate Internet backbone for commercial use; one of the networks that added to the core of the commercial Internet of later years. In March 1990, the first high-speed T1 (1.5 Mbit/s) link between the NSFNET and Europe was installed between Cornell University and CERN, allowing much more robust communications than were capable with satellites.
Later in 1990, Tim Berners-Lee began writing WorldWideWeb, the first web browser, after two years of lobbying CERN management. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web: the HyperText Transfer Protocol (HTTP) 0.9, the HyperText Markup Language (HTML), the first Web browser (which was also an HTML editor and could access Usenet newsgroups and FTP files), the first HTTP server software (later known as CERN httpd), the first web server, and the first Web pages that described the project itself. 
In 1991 the Commercial Internet eXchange was founded, allowing PSInet to communicate with the other commercial networks CERFnet and Alternet. Stanford Federal Credit Union was the first financial institution to offer online Internet banking services to all of its members in October 1994. In 1996, OP Financial Group, also a cooperative bank, became the second online bank in the world and the first in Europe. By 1995, the Internet was fully commercialized in the U.S. when the NSFNet was decommissioned, removing the last restrictions on use of the Internet to carry commercial traffic.
As technology advanced and commercial opportunities fueled reciprocal growth, the volume of Internet traffic started experiencing similar characteristics as that of the scaling of MOS transistors, exemplified by Moore's law, doubling every 18 months. This growth, formalized as Edholm's law, was catalyzed by advances in MOS technology, laser light wave systems, and noise performance.
=
Since 1995, the Internet has tremendously impacted culture and commerce, including the rise of near-instant communication by email, instant messaging, telephony (Voice over Internet Protocol or VoIP), two-way interactive video calls, and the World Wide Web. Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1 Gbit/s, 10 Gbit/s, or more. The Internet continues to grow, driven by ever-greater amounts of online information and knowledge, commerce, entertainment and social networking services. 
During the late 1990s, it was estimated that traffic on the public Internet grew by 100 percent per year, while the mean annual growth in the number of Internet users was thought to be between 20% and 50%. This growth is often attributed to the lack of central administration, which allows organic growth of the network, as well as the non-proprietary nature of the Internet protocols, which encourages vendor interoperability and prevents any one company from exerting too much control over the network.
In November 2006, the Internet was included on USA Today's list of the New Seven Wonders. As of 31 March 2011, the estimated total number of Internet users was 2.095 billion (30% of world population). It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication. By 2000 this figure had grown to 51%, and by 2007 more than 97% of all telecommunicated information was carried over the Internet. Modern smartphones can access the Internet through cellular carrier networks, and internet usage by mobile and tablet devices exceeded desktop worldwide for the first time in October 2016. As of 2018, 80% of the world's population were covered by a 4G network. 
 Mobile communication 
 The International Telecommunication Union (ITU) estimated that, by the end of 2017, 48% of individual users regularly connect to the Internet, up from 34% in 2012. Mobile Internet connectivity has played an important role in expanding access in recent years, especially in Asia and the Pacific and in Africa. The number of unique mobile cellular subscriptions increased from 3.9 billion in 2012 to 4.8 billion in 2016, two-thirds of the world's population, with more than half of subscriptions located in Asia and the Pacific. 
The limits that users face on accessing information via mobile applications coincide with a broader process of fragmentation of the Internet. Fragmentation restricts access to media content and tends to affect the poorest users the most. One solution, zero-rating, is the practice of Internet service providers allowing users free connectivity to access specific content or applications without cost.
The Internet has enabled new forms of social interaction, activities, and social associations, giving rise to the scholarly study of the sociology of the Internet.
=
Between 2000 and 2009, the number of Internet users globally rose from 390 million to 1.9 billion. By 2010, 22% of the world's population had access to computers with 1 billion Google searches every day, 300 million Internet users reading blogs, and 2 billion videos viewed daily on YouTube. In 2014 the world's Internet users surpassed 3 billion or 44 percent of world population, but two-thirds came from the richest countries, with 78 percent of Europeans using the Internet, followed by 57 percent of the Americas. However, by 2018, Asia alone accounted for 51% of all Internet users, with 2.2 billion out of the 4.3 billion Internet users in the world. China's Internet users surpassed a major milestone in 2018, when the country's Internet regulatory authority, China Internet Network Information Centre, announced that China had 802 million users. China was followed by India, with some 700 million users, with the United States third with 275 million users. 
However, in terms of penetration, in 2022, China had a 70% penetration rate compared to India's 60% and the United States's 90%. In 2022, 54% of the world's Internet users were based in Asia, 14% in Europe, 7% in North America, 10% in Latin America and the Caribbean, 11% in Africa, 4% in the Middle East and 1% in Oceania. In 2019, Kuwait, Qatar, the Falkland Islands, Bermuda and Iceland had the highest Internet penetration by the number of users, with 93% or more of the population with access. As of 2022, it was estimated that 5.4 billion people use the Internet, more than two-thirds of the world's population.
Early computer systems were limited to the characters in the American Standard Code for Information Interchange (ASCII), a subset of the Latin alphabet. After English (27%), the most requested languages on the World Wide Web are Chinese (25%), Spanish (8%), Japanese (5%), Portuguese and German (4% each), Arabic, French and Russian (3% each), and Korean (2%). Modern character encoding standards, such as Unicode, allow for development and communication in the world's widely used languages. However, some glitches such as mojibake (incorrect display of some languages' characters) still remain.
Several neologisms exist that refer to Internet users: Netizen (as in "citizen of the net") refers to those actively involved in improving online communities, the Internet in general or surrounding political affairs and rights such as free speech, Internaut refers to operators or technically highly capable users of the Internet, digital citizen refers to a person using the Internet in order to engage in society, politics, and government participation.
=
The Internet allows greater flexibility in working hours and location, especially with the spread of unmetered high-speed connections. The Internet can be accessed almost anywhere by numerous means, including through mobile Internet devices. Mobile phones, datacards, handheld game consoles and cellular routers allow users to connect to the Internet wirelessly.
 Information and education 
Educational material at all levels from pre-school (e.g. CBeebies) to post-doctoral (e.g. scholarly literature through Google Scholar) is available on websites. The internet has facilitated the development of virtual universities and distance education, enabling both formal and informal education. The Internet allows researchers to conduct research remotely via virtual laboratories, with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results. By the late 2010s the Internet had been described as "the main source of scientific information "for the majority of the global North population".
Wikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries. In those settings, they have been found useful for collaboration on grant writing, strategic planning, departmental documentation, and committee work. The United States Patent and Trademark Office uses a wiki to allow the public to collaborate on finding prior art relevant to examination of pending patent applications. Queens, New York has used a wiki to allow citizens to collaborate on the design and planning of a local park. The English Wikipedia has the largest user base among wikis on the World Wide Web and ranks in the top 10 among all sites in terms of traffic.
 Entertainment 
The Internet has been a major outlet for leisure activity since its inception, with entertaining social experiments such as MUDs and MOOs being conducted on university servers, and humor-related Usenet groups receiving much traffic. Many Internet forums have sections devoted to games and funny videos. 
Another area of leisure activity on the Internet is multiplayer gaming. This form of recreation creates communities, where people of all ages and origins enjoy the fast-paced world of multiplayer games. These range from MMORPG to first-person shooters, from role-playing video games to online gambling. While online gaming has been around since the 1970s, modern modes of online gaming began with subscription services such as GameSpy and MPlayer.
Streaming media is the real-time delivery of digital media for immediate consumption or enjoyment by end users. Streaming companies (such as Netflix, Disney+, Amazon's Prime Video, Mubi, Hulu, and Apple TV+) now dominate the entertainment industry, eclipsing traditional broadcasters. Audio streamers such as Spotify and Apple Music also have significant market share in the audio entertainment market.
Video sharing websites are also a major factor in the entertainment ecosystem. YouTube was founded on 15 February 2005 and is now the leading website for free streaming video with more than two billion users. It uses a web player to stream and show video files. YouTube users watch hundreds of millions, and upload hundreds of thousands, of videos daily. Other video sharing websites include Vimeo, Instagram and TikTok.
 Pornography 
Although many governments have attempted to restrict both Internet pornography and online gambling, this has generally failed to stop their widespread popularity.
A number of advertising-funded ostensible video sharing websites known as "tube sites" have been created to host shared pornographic video content. Due to laws requiring the documentation of the origin of pornography, these websites now largely operate in conjunction with pornographic movie studios and their own independent creator networks, acting as de-facto video streaming services. Major players in this field include the market leader Aylo, the operator of PornHub and numerous other branded sites, as well as other independent operators such as xHamster and Xvideos. As of 2023, Internet traffic to pornographic video sites rivalled that of mainstream video streaming and sharing services.
 Remote work 
Remote work is facilitated by tools such as groupware, virtual private networks, conference calling, videotelephony, and VoIP so that work may be performed from any location, such as the worker's home.
 Philanthropy 
The spread of low-cost Internet access in developing countries has opened up new possibilities for peer-to-peer charities, which allow individuals to contribute small amounts to charitable projects for other individuals. Websites, such as DonorsChoose and GlobalGiving, allow small-scale donors to direct funds to individual projects of their choice. A popular twist on Internet-based philanthropy is the use of peer-to-peer lending for charitable purposes. Kiva pioneered this concept in 2005, offering the first web-based service to publish individual loan profiles for funding.
=
The low cost and nearly instantaneous sharing of ideas, knowledge, and skills have made collaborative work dramatically easier, with the help of collaborative software, which allow groups to easily form, cheaply communicate, and share ideas. An example of collaborative software is the free software movement, which has produced, among other things, Linux, Mozilla Firefox, and OpenOffice.org (later forked into LibreOffice). Content management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work.
The internet also allows for cloud computing, virtual private networks, remote desktops, and remote work.
=
The online disinhibition effect describes the tendency of many individuals to behave more stridently or offensively online than they would in person. A significant number of feminist women have been the target of various forms of harassment, including insults and hate speech, to, in extreme cases, rape and death threats, in response to posts they have made on social media. Social media companies have been criticized in the past for not doing enough to aid victims of online abuse.
Children also face dangers online such as cyberbullying and approaches by sexual predators, who sometimes pose as children themselves. Due to naivety, they may also post personal information about themselves online, which could put them or their families at risk unless warned not to do so. Many parents choose to enable Internet filtering or supervise their children's online activities in an attempt to protect their children from pornography or violent content on the Internet. The most popular social networking services commonly forbid users under the age of 13. However, these policies can be circumvented by registering an account with a false birth date, and a significant number of children aged under 13 join such sites. Social networking services for younger children, which claim to provide better levels of protection for children, also exist.
Internet usage has been correlated to users' loneliness. Lonely people tend to use the Internet as an outlet for their feelings and to share their stories with others, such as in the "I am lonely will anyone speak to me" thread.
Cyberslacking can become a drain on corporate resources; employees spend a significant amount of time surfing the Web while at work. Internet addiction disorder is excessive computer use that interferes with daily life. Nicholas G. Carr believes that Internet use has other effects on individuals, for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity.
=
Electronic business encompasses business processes spanning the entire value chain: purchasing, supply chain management, marketing, sales, customer service, and business relationship. E-commerce seeks to add revenue streams using the Internet to build and enhance relationships with clients and partners. According to International Data Corporation, the size of worldwide e-commerce, when global business-to-business and -consumer transactions are combined, equate to $16 trillion in 2013. A report by Oxford Economics added those two together to estimate the total size of the digital economy at $20.4 trillion, equivalent to roughly 13.8% of global sales.
While much has been written of the economic advantages of Internet-enabled commerce, there is also evidence that some aspects of the Internet such as maps and location-aware services may serve to reinforce economic inequality and the digital divide. Electronic commerce may be responsible for consolidation and the decline of mom-and-pop, brick and mortar businesses resulting in increases in income inequality.
A 2013 Institute for Local Self-Reliance report states that brick-and-mortar retailers employ 47 people for every $10 million in sales, while Amazon employs only 14. Similarly, the 700-employee room rental start-up Airbnb was valued at $10 billion in 2014, about half as much as Hilton Worldwide, which employs 152,000 people. At that time, Uber employed 1,000 full-time employees and was valued at $18.2 billion, about the same valuation as Avis Rent a Car and The Hertz Corporation combined, which together employed almost 60,000 people.
Advertising on popular web pages can be lucrative, and e-commerce. Online advertising is a form of marketing and advertising which uses the Internet to deliver promotional marketing messages to consumers. It includes email marketing, search engine marketing (SEM), social media marketing, many types of display advertising (including web banner advertising), and mobile advertising. In 2011, Internet advertising revenues in the United States surpassed those of cable television and nearly exceeded those of broadcast television. Many common online advertising practices are controversial and increasingly subject to regulation.
=
The Internet has achieved new relevance as a political tool. The presidential campaign of Howard Dean in 2004 in the United States was notable for its success in soliciting donation via the Internet. Many political groups use the Internet to achieve a new method of organizing for carrying out their mission, having given rise to Internet activism. Social media websites, such as Facebook and Twitter, helped people organize the Arab Spring, by helping activists organize protests, communicate grievances, and disseminate information.
Many have understood the Internet as an extension of the Habermasian notion of the public sphere, observing how network communication technologies provide something like a global civic forum. However, incidents of politically motivated Internet censorship have now been recorded in many countries, including western democracies.
E-government is the use of technological communications devices, such as the Internet, to provide public services to citizens and other persons in a country or region. E-government offers opportunities for more direct and convenient citizen access to government and for government provision of services directly to citizens.
=
Cybersectarianism is a new organizational form that involves:highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy, while still linked remotely to a larger network of believers who share a set of practices and texts, and often a common devotion to a particular leader. Overseas supporters provide funding and support; domestic practitioners distribute tracts, participate in acts of resistance, and share information on the internal situation with outsiders. Collectively, members and practitioners of such sects construct viable virtual communities of faith, exchanging personal testimonies and engaging in the collective study via email, online chat rooms, and web-based message boards. In particular, the British government has raised concerns about the prospect of young British Muslims being indoctrinated into Islamic extremism by material on the Internet, being persuaded to join terrorist groups such as the so-called "Islamic State", and then potentially committing acts of terrorism on returning to Britain after fighting in Syria or Iraq.
The Internet carries many applications and services, most prominently the World Wide Web, including social media, electronic mail, mobile applications, multiplayer online games, Internet telephony, file sharing, and streaming media services. 
=
The World Wide Web is a global collection of documents, images, multimedia, applications, and other resources, logically interrelated by hyperlinks and referenced with Uniform Resource Identifiers (URIs), which provide a global system of named references. URIs symbolically identify services, web servers, databases, and the documents and resources that they can provide.
HyperText Transfer Protocol (HTTP) is the main access protocol of the World Wide Web. Web services also use HTTP for communication between software systems for information transfer, sharing and exchanging business data and logistics and is one of many languages or protocols that can be used for communication on the Internet.
World Wide Web browser software, such as Microsoft Edge, Mozilla Firefox, Opera, Apple's Safari, and Google Chrome, enable users to navigate from one web page to another via the hyperlinks embedded in the documents. These documents may also contain computer data, including graphics, sounds, text, video, multimedia and interactive content. Client-side scripts can include animations, games, office applications and scientific demonstrations.
=
Email is an important communications service available via the Internet. The concept of sending electronic text messages between parties, analogous to mailing letters or memos, predates the creation of the Internet.
Internet telephony is a common communications service realized with the Internet. The name of the principal internetworking protocol, the Internet Protocol, lends its name to voice over Internet Protocol (VoIP). VoIP systems now dominate many markets, being as easy and convenient as a traditional telephone, while having substantial cost savings, especially over long distances.
=
File sharing is the practice of transferring large amounts of data in the form of computer files across the Internet, for example via file servers. The load of bulk downloads to many users can be eased by the use of "mirror" servers or peer-to-peer networks.  
Access to the file may be controlled by user authentication, the transit of the file over the Internet may be obscured by encryption, and money may change hands for access to the file. The price can be paid by the remote charging of funds from, for example, a credit card whose details are also passed—usually fully encrypted—across the Internet. The origin and authenticity of the file received may be checked by a digital signature.
The Internet is a global network that comprises many voluntarily interconnected autonomous networks. It operates without a central governing body. 
=
The technical underpinning and standardization of the core protocols (IPv4 and IPv6) is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.
While the hardware components in the Internet infrastructure can often be used to support other software systems, it is the design and the standardization process of the software that characterizes the Internet and provides the foundation for its scalability and success. The responsibility for the architectural design of the Internet software systems has been assumed by the IETF. 
The IETF conducts standard-setting work groups, open to any individual, about the various aspects of Internet architecture. The resulting contributions and standards are published as Request for Comments (RFC) documents on the IETF web site. The principal methods of networking that enable the Internet are contained in specially designated RFCs that constitute the Internet Standards. Other less rigorous documents are simply informative, experimental, or historical, or document the best current practices when implementing Internet technologies.
=
To maintain interoperability, the principal name spaces of the Internet are administered by the Internet Corporation for Assigned Names and Numbers (ICANN). ICANN is governed by an international board of directors drawn from across the Internet technical, business, academic, and other non-commercial communities. The organization coordinates the assignment of unique identifiers for use on the Internet, including domain names, IP addresses, application port numbers in the transport protocols, and many other parameters. Globally unified name spaces are essential for maintaining the global reach of the Internet. This role of ICANN distinguishes it as perhaps the only central coordinating body for the global Internet.
The National Telecommunications and Information Administration, an agency of the United States Department of Commerce, had final approval over changes to the DNS root zone until the IANA stewardship transition on 1 October 2016. 
=
Regional Internet registries (RIRs) were established for five regions of the world to assign IP address blocks and other Internet parameters to local registries, such as Internet service providers, from a designated pool of addresses set aside for each region:
The African Network Information Center (AfriNIC) for Africa
The American Registry for Internet Numbers (ARIN) for North America
The Asia–Pacific Network Information Centre (APNIC) for Asia and the Pacific region
The Latin American and Caribbean Internet Addresses Registry (LACNIC) for Latin America and the Caribbean region
The Réseaux IP Européens – Network Coordination Centre (RIPE NCC) for Europe, the Middle East, and Central Asia.
=
The Internet Society (ISOC) was founded in 1992 with a mission to "assure the open development, evolution and use of the Internet for the benefit of all people throughout the world". Its members include individuals as well as corporations, organizations, governments, and universities. Among other activities ISOC provides an administrative home for a number of less formally organized groups that are involved in developing and managing the Internet, including: the Internet Engineering Task Force (IETF), Internet Architecture Board (IAB), Internet Engineering Steering Group (IESG), Internet Research Task Force (IRTF), and Internet Research Steering Group (IRSG). 
On 16 November 2005, the United Nations-sponsored World Summit on the Information Society in Tunis established the Internet Governance Forum (IGF) to discuss Internet-related issues.
The communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture. As with any computer network, the Internet physically consists of routers, media (such as cabling and radio links), repeaters, and modems. However, as an example of internetworking, many of the network nodes are not necessarily Internet equipment per se. Internet packets are carried by other full-fledged networking protocols, with the Internet acting as a homogeneous networking standard, running across heterogeneous hardware, with the packets guided to their destinations by IP routers.
=
Internet service providers (ISPs) establish worldwide connectivity between individual networks at various levels of scope. At the top of the routing hierarchy are the tier 1 networks, large telecommunication companies that exchange traffic directly with each other via very high speed fiber-optic cables and governed by peering agreements. Tier 2 and lower-level networks buy Internet transit from other providers to reach at least some parties on the global Internet, though they may also engage in peering. End-users who only access the Internet when needed to perform a function or obtain information, represent the bottom of the routing hierarchy.
An ISP may use a single upstream provider for connectivity, or implement multihoming to achieve redundancy and load balancing. Internet exchange points are major traffic exchanges with physical connections to multiple ISPs. Large organizations, such as academic institutions, large enterprises, and governments, may perform the same function as ISPs, engaging in peering and purchasing transit on behalf of their internal networks. Research networks tend to interconnect with large subnetworks such as GEANT, GLORIAD, Internet2, and the UK's national research and education network, JANET.
=
Common methods of Internet access by users include broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite, and cellular telephone technology. Grassroots efforts have led to wireless community networks. Commercial Wi-Fi services that cover large areas are available in many cities, such as New York, London, Vienna, Toronto, San Francisco, Philadelphia, Chicago and Pittsburgh. 
=
Most servers that provide internet services are today hosted in data centers, and content is often accessed through high-performance content delivery networks.
Colocation centers often host private peering connections between their customers, internet transit providers, cloud providers, meet-me rooms for connecting customers together, Internet exchange points, and landing points and terminal equipment for fiber optic submarine communication cables, connecting the internet.
The Internet standards describe a framework known as the Internet protocol suite (also called TCP/IP, based on the first two components.) This is a suite of protocols that are ordered into a set of four conceptional layers by the scope of their operation, originally documented in RFC 1122 and RFC 1123:
At the top is the application layer, where communication is described in terms of the objects or data structures most appropriate for each application. For example, a web browser operates in a client–server application model and exchanges information with the HyperText Transfer Protocol (HTTP) and an application-germane data structure, such as the HyperText Markup Language (HTML).
Below this top layer, the transport layer connects applications on different hosts with a logical channel through the network. It provides this service with a variety of possible characteristics, such as ordered, reliable delivery (TCP), and an unreliable datagram service (UDP).
Underlying these layers are the networking technologies that interconnect networks at their borders and exchange traffic across them. The Internet layer implements the Internet Protocol (IP) which enables computers to identify and locate each other by IP address and route their traffic via intermediate transit networks. The Internet Protocol layer code is independent of the type of network that it is physically running over.
At the bottom of the architecture is the link layer, which connects nodes on the same physical link, and contains protocols that do not require routers for traversal to other links. The protocol suite does not explicitly specify hardware methods to transfer bits, or protocols to manage such hardware, but assumes that appropriate technology is available. Examples of that technology include Wi-Fi, Ethernet, and DSL.
=
The most prominent component of the Internet model is the Internet Protocol. IP enables internetworking, essentially establishing the Internet itself. Two versions of the Internet Protocol exist, IPv4 and IPv6. Aside from the complex array of physical connections that make up its infrastructure, the Internet is facilitated by bi- or multi-lateral commercial contracts (e.g., peering agreements), and by technical specifications or protocols that describe the exchange of data over the network.
 IP Addresses 
For locating individual computers on the network, the Internet provides IP addresses. IP addresses are used by the Internet infrastructure to direct internet packets to their destinations. They consist of fixed-length numbers, which are found within the packet. IP addresses are generally assigned to equipment either automatically via Dynamic Host Configuration Protocol, or are configured.
Domain Name Systems convert user-inputted domain names (e.g. "en.wikipedia.org") into IP addresses.
 IPv4 
Internet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. IPv4 is the initial version used on the first generation of the Internet and is still in dominant use. It was designed in 1981 to address up to ≈4.3 billion (109) hosts. However, the explosive growth of the Internet has led to IPv4 address exhaustion, which entered its final stage in 2011, when the global IPv4 address allocation pool was exhausted.
 IPv6 
Because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP IPv6, was developed in the mid-1990s, which provides vastly larger addressing capabilities and more efficient routing of Internet traffic. IPv6 uses 128 bits for the IP address and was standardized in 1998. IPv6 deployment has been ongoing since the mid-2000s and is currently in growing deployment around the world, since Internet address registries began to urge all resource managers to plan rapid adoption and conversion.
By design, IPv6 is not directly interoperable with IPv4. Instead, it establishes a parallel version of the Internet not directly accessible with IPv4 software. Thus, translation facilities exist for internetworking, and some nodes have duplicate networking software for both networks. Essentially all modern computer operating systems support both versions of the Internet Protocol. Network infrastructure, however, has been lagging in this development.
 Subnets 
A subnet or subnetwork is a logical subdivision of an IP network. Computers that belong to a subnet are addressed with an identical most-significant bit-group in their IP addresses. This results in the logical division of an IP address into two fields, the network number or routing prefix and the rest field or host identifier. The rest field is an identifier for a specific host or network interface.
The routing prefix may be expressed in Classless Inter-Domain Routing (CIDR) notation written as the first address of a network, followed by a slash character (/), and ending with the bit-length of the prefix. For example, 198.51.100.0/24 is the prefix of the Internet Protocol version 4 network starting at the given address, having 24 bits allocated for the network prefix, and the remaining 8 bits reserved for host addressing. Addresses in the range 198.51.100.0 to 198.51.100.255 belong to this network. The IPv6 address specification 2001:db8::/32 is a large address block with 296 addresses, having a 32-bit routing prefix.
For IPv4, a network may also be characterized by its subnet mask or netmask, which is the bitmask that when applied by a bitwise AND operation to any IP address in the network, yields the routing prefix. Subnet masks are also expressed in dot-decimal notation like an address. For example, 255.255.255.0 is the subnet mask for the prefix 198.51.100.0/24.
 Routing 
Computers and routers use routing tables in their operating system to forward IP packets to reach a node on a different subnetwork. Routing tables are maintained by manual configuration or automatically by routing protocols. End-nodes typically use a default route that points toward an ISP providing transit, while ISP routers use the Border Gateway Protocol to establish the most efficient routing across the complex connections of the global Internet.
The default gateway is the node that serves as the forwarding host (router) to other networks when no other route specification matches the destination IP address of a packet.
Internet resources, hardware, and software components are the target of criminal or malicious attempts to gain unauthorized control to cause interruptions, commit fraud, engage in blackmail or access private information.
=
Malware is malicious software used and distributed via the Internet. It includes computer viruses which are copied with the help of humans, computer worms which copy themselves automatically, software for denial of service attacks, ransomware, botnets, and spyware that reports on the activity and typing of users. Usually, these activities constitute cybercrime. Defense theorists have also speculated about the possibilities of hackers using cyber warfare using similar methods on a large scale.
Malware poses serious problems to individuals and businesses on the Internet. According to Symantec's 2018 Internet Security Threat Report (ISTR), malware variants number has increased to 669,947,865 in 2017, which is twice as many malware variants as in 2016. Cybercrime, which includes malware attacks as well as other crimes committed by computer, was predicted to cost the world economy US$6 trillion in 2021, and is increasing at a rate of 15% per year. Since 2021, malware has been designed to target computer systems that run critical infrastructure such as the electricity distribution network. Malware can be designed to evade antivirus software detection algorithms.
=
The vast majority of computer surveillance involves the monitoring of data and traffic on the Internet. In the United States for example, under the Communications Assistance For Law Enforcement Act, all phone calls and broadband Internet traffic (emails, web traffic, instant messaging, etc.) are required to be available for unimpeded real-time monitoring by Federal law enforcement agencies. Under the Act, all U.S. telecommunications providers are required to install packet sniffing technology to allow Federal law enforcement and intelligence agencies to intercept all of their customers' broadband Internet and VoIP traffic.
The large amount of data gathered from packet capture requires surveillance software that filters and reports relevant information, such as the use of certain words or phrases, the access to certain types of web sites, or communicating via email or chat with certain parties. Agencies, such as the Information Awareness Office, NSA, GCHQ and the FBI, spend billions of dollars per year to develop, purchase, implement, and operate systems for interception and analysis of data. Similar systems are operated by Iranian secret police to identify and suppress dissidents. The required hardware and software were allegedly installed by German Siemens AG and Finnish Nokia.
=
Some governments, such as those of Myanmar, Iran, North Korea, Mainland China, Saudi Arabia and the United Arab Emirates, restrict access to content on the Internet within their territories, especially to political and religious content, with domain name and keyword filters.
In Norway, Denmark, Finland, and Sweden, major Internet service providers have voluntarily agreed to restrict access to sites listed by authorities. While this list of forbidden resources is supposed to contain only known child pornography sites, the content of the list is secret. 
Many countries, including the United States, have enacted laws against the possession or distribution of certain material, such as child pornography, via the Internet but do not mandate filter software. Many free or commercially available software programs, called content-control software are available to users to block offensive specific on individual computers or networks in order to limit access by children to pornographic material or depiction of violence.
As the Internet is a heterogeneous network, its physical characteristics, including, for example the data transfer rates of connections, vary widely. It exhibits emergent phenomena that depend on its large-scale organization.
=
PB per monthYear020,00040,00060,00080,000100,000120,000140,000199019952000200520102015Petabytes per monthGlobal Internet Traffic Volume.mw-chart-e446570db5b3cee2dcadd7f78969d5ae85fac1e84065d6b4703d751d8d7e633b6dcf6e8370438a8209183e08f460e28491cd38e604767c68df840cdf1ff116ad__zr399640-cls-2023702:hover{pointer-events:none}.mw-chart-e446570db5b3cee2dcadd7f78969d5ae85fac1e84065d6b4703d751d8d7e633b6dcf6e8370438a8209183e08f460e28491cd38e604767c68df840cdf1ff116ad__zr399640-cls-2023703:hover{cursor:pointer;fill:rgba(0,0,0,1)}.mw-chart-e446570db5b3cee2dcadd7f78969d5ae85fac1e84065d6b4703d751d8d7e633b6dcf6e8370438a8209183e08f460e28491cd38e604767c68df840cdf1ff116ad__zr399640-cls-2023704:hover{cursor:pointer}.mw-chart-e446570db5b3cee2dcadd7f78969d5ae85fac1e84065d6b4703d751d8d7e633b6dcf6e8370438a8209183e08f460e28491cd38e604767c68df840cdf1ff116ad__zr399640-cls-2023705:hover{cursor:pointer;fill:rgba(82,130,235,1)}.mw-chart-e446570db5b3cee2dcadd7f78969d5ae85fac1e84065d6b4703d751d8d7e633b6dcf6e8370438a8209183e08f460e28491cd38e604767c68df840cdf1ff116ad__zr399640-cls-2023706:hover{cursor:pointer;fill:rgba(0,0,0,0)}
The volume of Internet traffic is difficult to measure because no single point of measurement exists in the multi-tiered, non-hierarchical topology. Traffic data may be estimated from the aggregate volume through the peering points of the Tier 1 network providers, but traffic that stays local in large provider networks may not be accounted for.
=
An Internet blackout or outage can be caused by local signaling interruptions. Disruptions of submarine communications cables may cause blackouts or slowdowns to large areas, such as in the 2008 submarine cable disruption. Less-developed countries are more vulnerable due to the small number of high-capacity links. Land cables are also vulnerable, as in 2011 when a woman digging for scrap metal severed most connectivity for the nation of Armenia. Internet blackouts affecting almost entire countries can be achieved by governments as a form of Internet censorship, as in the blockage of the Internet in Egypt, whereby approximately 93% of networks were without access in 2011 in an attempt to stop mobilization for anti-government protests.
=
Estimates of the Internet's electricity usage have been the subject of controversy, according to a 2014 peer-reviewed research paper that found claims differing by a factor of 20,000 published in the literature during the preceding decade, ranging from 0.0064 kilowatt hours per gigabyte transferred (kWh/GB) to 136 kWh/GB. The researchers attributed these discrepancies mainly to the year of reference (i.e. whether efficiency gains over time had been taken into account) and to whether "end devices such as personal computers and servers are included" in the analysis.
In 2011, academic researchers estimated the overall energy used by the Internet to be between 170 and 307 GW, less than two percent of the energy used by humanity. This estimate included the energy needed to build, operate, and periodically replace the estimated 750 million laptops, a billion smart phones and 100 million servers worldwide as well as the energy that routers, cell towers, optical switches, Wi-Fi transmitters and cloud storage devices use when transmitting Internet traffic. According to a non-peer-reviewed study published in 2018 by The Shift Project (a French think tank funded by corporate sponsors), nearly 4% of global CO2 emissions could be attributed to global data transfer and the necessary infrastructure. The study also said that online video streaming alone accounted for 60% of this data transfer and therefore contributed to over 300 million tons of CO2 emission per year, and argued for new "digital sobriety" regulations restricting the use and size of video files.
 This article incorporates text from a free content work. Text taken from World Trends in Freedom of Expression and Media Development Global Report 2017/2018​, 202, UNESCO.
Abbate, Janet (1999). Inventing the Internet. Cambridge, Massachusetts: MIT Press. ISBN 978-0-262-01172-3.
First Monday, a peer-reviewed journal on the Internet by the University Library of the University of Illinois at Chicago, ISSN 1396-0466
James Gleick, "How the Web Was Lost" (review of Tim Berners-Lee with Stephen Witt, This Is for Everyone: The Unfinished Story of the World Wide Web, Farrar, Straus and Giroux, 2025, 389 pp.; Joanna Walsh, Amateurs! How We Built Internet Culture, and Why It Matters, Verso, 2025, 262 pp.; and Cory Doctorow, Enshittification: Why Everything Suddenly Got Worse and What to Do About It, MCD, 338 pp.), The New York Review of Books, vol. LXXII, no. 19 (4 December 2025), pp. 6, 8, 10.
The Internet Explained, Vincent Zegna & Mike Pepper, Sonet Digital, November 2005, pp. 1–7.
Castells, Manuel (2010). The Rise of the Network Society. Wiley. ISBN 978-1-4051-9686-4.
Yeo, ShinJoung (2023), Behind the Search Box: Google and the Global Internet Industry, U of Illinois Press, ISBN 978-0-252-04499-1, JSTOR 10.5406/jj.4116455
The Internet Society
Living Internet, Internet history and related information, including information from many creators of the Internet

--- TOPIC: Python_(programming_language) ---
Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.
Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabilities and keywords for typing were added to the language, allowing optional static typing. As of 2026, the Python Software Foundation supports Python 3.10, 3.11, 3.12, 3.13, and 3.14, following the project's annual release cycle and five-year support policy. Python 3.15 is currently in the alpha development phase, and the stable release is expected to come out in October 2026."Earlier versions in the 3.x series have reached end-of-life and no longer receive security updates. 
Python has gained widespread use in the machine learning community. It is widely taught as an introductory programming language. Since 2003, Python has consistently ranked in the top ten of the most popular programming languages in the TIOBE Programming Community Index, which ranks based on searches in 24 platforms.
Python was conceived in the late 1980s by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands. It was designed as a successor to the ABC programming language, which was inspired by SETL, capable of exception handling and interfacing with the Amoeba operating system. Python implementation began in December 1989. Van Rossum first released it in 1991 as Python 0.9.0. Van Rossum assumed sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his "permanent vacation" from responsibilities as Python's "benevolent dictator for life" (BDFL); this title was bestowed on him by the Python community to reflect his long-term commitment as the project's chief decision-maker. (He has since come out of retirement and is self-titled "BDFL-emeritus".) In January 2019, active Python core developers elected a five-member Steering Council to lead the project.
The name Python derives from the British comedy series Monty Python's Flying Circus. (See § Naming.)
Python 2.0 was released on 16 October 2000, featuring many new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 2.7's end-of-life was initially set for 2015, and then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python 3. It no longer receives security patches or updates. While Python 2.7 and older versions are officially unsupported, a different unofficial Python implementation, PyPy, continues to support Python 2, i.e., "2.7.18+" (plus 3.11), with the plus signifying (at least some) "backported security updates".
Python 3.0 was released on 3 December 2008, and was a major revision and not completely backward-compatible with earlier versions, with some new semantics and changed syntax. Python 2.7.18, released in 2020, was the last release of Python 2. Several releases in the Python 3.x series have added new syntax to the language, and made a few (considered very minor) backward-incompatible changes.
As of January 2026, Python 3.14.2 is the latest stable release. All older 3.x versions had a security update down to Python 3.9.24 then again with 3.9.25, the final version in 3.9 series. Python 3.10 is, since November 2025, the oldest supported branch. Python 3.15 has an alpha released, and Android has an official downloadable executable available for Python 3.14. Releases receive two years of full support followed by three years of security support.
Python is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming – including metaprogramming and metaobjects. Many other paradigms are supported via extensions, including design by contract and logic programming. Python is often referred to as a 'glue language' because it is purposely designed to be able to integrate components written in other languages.
Python uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management. It uses dynamic name resolution (late binding), which binds method and variable names during program execution.
Python's design offers some support for functional programming in the "Lisp tradition". It has filter, map, and reduce functions; list comprehensions, dictionaries, sets, and generator expressions. The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.
Python's core philosophy is summarized in the Zen of Python (PEP 20) written by Tim Peters, which includes aphorisms such as these:
Explicit is better than implicit.
Simple is better than complex.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity,  errors should never pass silently, unless explicitly silenced.
There should be one-- and preferably only one --obvious way to do it.
However, Python has received criticism for violating these principles and adding unnecessary language bloat. Responses to these criticisms note that the Zen of Python is a guideline rather than a rule. The addition of some new features had been controversial: Guido van Rossum resigned as Benevolent Dictator for Life after conflict about adding the assignment expression operator in Python 3.8.
Nevertheless, rather than building all functionality into its core, Python was designed to be highly extensible via modules. This compact modularity has made it particularly popular as a means of adding programmable interfaces to existing applications. Van Rossum's vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which represented the opposite approach.
Python claims to strive for a simpler, less-cluttered syntax and grammar, while giving developers a choice in their coding methodology. Python lacks do .. while loops, which Rossum considered harmful. In contrast to Perl's motto "there is more than one way to do it", Python advocates an approach where "there should be one – and preferably only one – obvious way to do it". In practice, however, Python provides many ways to achieve a given goal. There are at least three ways to format a string literal, with no certainty as to which one a programmer should use. Alex Martelli is a Fellow at the Python Software Foundation and Python book author; he wrote that "To describe something as 'clever' is not considered a compliment in the Python culture."
Python's developers typically prioritize readability over performance. For example, they reject patches to non-critical parts of the CPython reference implementation that would offer increases in speed that do not justify the cost of clarity and readability. Execution speed can be improved by moving speed-critical functions to extension modules written in languages such as C, or by using a just-in-time compiler like PyPy. Also, it is possible to transpile to other languages. However, this approach either fails to achieve the expected speed-up, since Python is a very dynamic language, or only a restricted subset of Python is compiled (with potential minor semantic changes).
Python is meant to be a fun language to use. This goal is reflected in the name – a tribute to the British comedy group Monty Python – and in playful approaches to some tutorials and reference materials. For instance, some code examples use the terms "spam" and "eggs" (in reference to a Monty Python sketch), rather than the typical terms "foo" and "bar".
A common neologism in the Python community is pythonic, which has a broad range of meanings related to program style: Pythonic code may use Python idioms well; be natural or show fluency in the language; or conform with Python's minimalist philosophy and emphasis on readability.
Python is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal.
=
Python uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block. Thus, the program's visual structure accurately represents its semantic structure. This feature is sometimes termed the off-side rule. Some other languages use indentation this way; but in most, indentation has no semantic meaning. The recommended indent size is four spaces.
=
Python's statements include the following:
The assignment statement, using a single equals sign =
The if statement, which conditionally executes a block of code, along with else and elif (a contraction of else if)
The for statement, which iterates over an iterable object, capturing each element to a variable for use by the attached block; the variable is not deleted when the loop finishes
The while statement, which executes a block of code as long as boolean condition is true
The try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups); the try statement also ensures that clean-up code in a finally block is always run regardless of how the block exits
The raise statement, used to raise a specified exception or re-raise a caught exception
The class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming
The def statement, which defines a function or method
The with statement, which encloses a code block within a context manager, allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try/finally idiom Examples of a context include acquiring a lock before some code is run, and then releasing the lock; or opening and then closing a file
The break statement, which exits a loop
The continue statement, which skips the rest of the current iteration and continues with the next
The del statement, which removes a variable—deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined 
The pass statement, serving as a NOP (i.e., no operation), which is syntactically needed to create an empty code block
The assert statement, used in debugging to check for conditions that should apply
The yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines
The return statement, used to return a value from a function
The import and from statements, used to import modules whose functions or variables can be used in the current program
The match and case statements, analogous to a switch statement construct, which compares an expression against one or more cases as a control-flow measure
The assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing—in contrast to statically-typed languages, where each variable may contain only a value of a certain type.
Python does not support tail call optimization or first-class continuations; according to Van Rossum, the language never will. However, better support for coroutine-like functionality is provided by extending Python's generators. Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python 2.5 on, it is possible to pass data back into a generator function; and from version 3.3, data can be passed through multiple stack levels.
=
Python's expressions include the following:
The +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of division in Python: floor division (or integer division) //, and floating-point division /. Python uses the ** operator for exponentiation.
Python uses the + operator for string concatenation. The language uses the * operator for duplicating a string a specified number of times.
The @ infix operator is intended to be used by libraries such as NumPy for matrix multiplication.
The syntax :=, called the "walrus operator", was introduced in Python 3.8. This operator assigns values to variables as part of a larger expression.
In Python, == compares two objects by value. Python's is operator may be used to compare object identities (i.e., comparison by reference), and comparisons may be chained—for example, a <= b <= c.
Python uses and, or, and not as Boolean operators.
Python has a type of expression called a list comprehension, and a more general expression called a generator expression.
Anonymous functions are implemented using lambda expressions; however, there may be only one expression in each body.
Conditional expressions are written as x if c else y. (This is different in operand order from the c ? x : y operator common to many other languages.)
Python makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (since dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as the keys of dictionaries, provided that all of the tuple's elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. For example, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5); this result is then assigned back to t—thereby effectively "modifying the contents" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.
Python features sequence unpacking where multiple expressions, each evaluating to something assignable (e.g., a variable or a writable property) are associated just as in forming tuple literal; as a whole, the results are then put on the left-hand side of the equal sign in an assignment statement. This statement expects an iterable object on the right-hand side of the equal sign to produce the same number of values as the writable expressions on the left-hand side; while iterating, the statement assigns each of the values produced on the right to the corresponding expression on the left.
Python has a "string format" operator % that functions analogously to printf format strings in the C language—e.g. "spam=%s eggs=%d" % ("blah", 2) evaluates to "spam=blah eggs=2". In Python 2.6+ and 3+, this operator was supplemented by the format() method of the str class, e.g., "spam={0} eggs={1}".format("blah", 2). Python 3.6 added "f-strings": spam = "blah"; eggs = 2; f'spam={spam} eggs={eggs}'.
Strings in Python can be concatenated by "adding" them (using the same operator as for adding integers and floats); e.g., "spam" + "eggs" returns "spameggs". If strings contain numbers, they are concatenated as strings rather than as integers, e.g. "2" + "2" returns "22".
Python supports string literals in several ways:
Delimited by single or double quotation marks; single and double quotation marks have equivalent functionality (unlike in Unix shells, Perl, and Perl-influenced languages). Both marks use the backslash (\) as an escape character. String interpolation became available in Python 3.6 as "formatted string literals".
Triple-quoted, i.e., starting and ending with three single or double quotation marks; this may span multiple lines and function like here documents in shells, Perl, and Ruby.
Raw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as in regular expressions and Windows-style paths. (Compare "@-quoting" in C#.)
Python has array index and array slicing expressions in lists, which are written as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The (optional) third slice parameter, called step or stride, allows elements to be skipped or reversed. Slice indexes may be omitted—for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.
In Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This distinction leads to duplicating some functionality, for example:
List comprehensions vs. for-loops
Conditional expressions vs. if blocks
The eval() vs. exec() built-in functions (in Python 2, exec is a statement); the former function is for expressions, while the latter is for statements
A statement cannot be part of an expression; because of this restriction, expressions such as list and dict comprehensions (and lambda expressions) cannot contain statements. As a particular case, an assignment statement such as a = 1 cannot be part of the conditional expression of a conditional statement.
=
Python uses duck typing, and it has typed objects but untyped variable names. Type constraints are not checked at definition time; rather, operations on an object may fail at usage time, indicating that the object is not of an appropriate type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are poorly defined (e.g., adding a number and a string) rather than quietly attempting to interpret them.
Python allows programmers to define their own types using classes, most often for object-oriented programming. New instances of classes are constructed by calling the class, for example, SpamClass() or EggsClass()); the classes are instances of the metaclass type (which is an instance of itself), thereby allowing metaprogramming and reflection.
Before version 3.0, Python had two kinds of classes, both using the same syntax: old-style and new-style. Current Python versions support the semantics of only the new style.
Python supports optional type annotations. These annotations are not enforced by the language, but may be used by external tools such as mypy to catch errors. Python includes a module typing including several type names for type annotations. Also, mypy supports a Python compiler called mypyc, which leverages type annotations for optimization.
=
Python includes conventional symbols for arithmetic operators (+, -, *, /), the floor-division operator //, and the modulo operator %. (With the modulo operator, a remainder can be negative, e.g., 4 % -3  125 and 9**0.5 == 3.0. Also, it offers the matrix‑multiplication operator @ . These operators work as in traditional mathematics; with the same precedence rules, the infix operators + and - can also be unary, to represent positive and negative numbers respectively.
Division between integers produces floating-point results. The behavior of division has changed significantly over time:
The current version of Python (i.e., since 3.0) changed the / operator to always represent floating-point division, e.g., 5/2 == 2.5.
The floor division // operator was introduced, meaning that 7//3  -3, 7.5//3  -3.0. For Python 2.7, adding the from __future__ import division statement allows a module in Python 2.7 to use Python 3.x rules for division (see above).
In Python terms, the / operator represents true division (or simply division), while the // operator represents floor division. Before version 3.0, the / operator represents classic division.
Rounding towards negative infinity, though a different method than in most languages, adds consistency to Python. For instance, this rounding implies that the equation (a + b)//b  a is valid for both positive and negative values of a. As expected, the result of a%b lies in the half-open interval [0, b), where b is a positive integer; however, maintaining the validity of the equation requires that the result must lie in the interval (b, 0] when b is negative.
Python provides a round function for rounding a float to the nearest integer. For tie-breaking, Python 3 uses the round to even method: round(1.5) and round(2.5) both produce 2. Python versions before 3 used the round-away-from-zero method: round(0.5) is 1.0, and round(-0.5) is −1.0.
Python allows Boolean expressions that contain multiple equality relations to be consistent with general usage in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c. C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.
Python uses arbitrary-precision arithmetic for all integer operations. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision with several rounding modes. The Fraction class in the fractions module provides arbitrary precision for rational numbers.
Due to Python's extensive mathematics library and the third-party library NumPy, the language is frequently used for scientific scripting in tasks such as numerical data processing and manipulation.
=
Functions are created in Python by using the def keyword. A function is defined similarly to how it is called, by first providing the function name and then the required parameters. Here is an example of a function that prints its inputs:
To assign a default value to a function parameter in case no actual value is provided at run time, variable-definition syntax can be used inside the function header.
"Hello, World!" program:
Program to calculate the factorial of a non-negative integer:
Python's large standard library is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. The language includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals, manipulating regular expressions, and unit testing.
Some parts of the standard library are covered by specifications—for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333—but most parts are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules must be altered or rewritten for variant implementations.
As of 13 March 2025, the Python Package Index (PyPI), the official repository for third-party Python software, contains over 614,339 packages.
Most Python implementations (including CPython) include a read–eval–print loop (REPL); this permits the environment to function as a command line interpreter, with which users enter statements sequentially and receive results immediately.
Also, CPython is bundled with an integrated development environment (IDE) called IDLE, which is oriented toward beginners.
Other shells, including IDLE and IPython, add additional capabilities such as improved auto-completion, session-state retention, and syntax highlighting.
Standard desktop IDEs include PyCharm, Spyder, and Visual Studio Code; there are web browser-based IDEs, such as the following environments:
Jupyter Notebooks, an open-source interactive computing platform;
PythonAnywhere, a browser-based IDE and hosting environment; and
Canopy, a commercial IDE from Enthought that emphasizes scientific computing.
=
CPython is the reference implementation of Python. This implementation is written in C, meeting the C11 standard since version 3.11. Older versions use the C89 standard with several select C99 features, but third-party extensions are not limited to older C versions—e.g., they can be implemented using C11 or C++. CPython compiles Python programs into an intermediate bytecode, which is then executed by a virtual machine. CPython is distributed with a large standard library written in a combination of C and native Python.
CPython is available for many platforms, including Windows and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python 3.9.1, using an experimental installer). Starting with Python 3.9, the Python installer intentionally fails to install on Windows 7 and 8; Windows XP was supported until Python 3.5, with unofficial support for VMS. Platform portability was one of Python's earliest priorities. During development of Python 1 and 2, even OS/2 and Solaris were supported; since that time, support has been dropped for many platforms.
All current Python versions (since 3.7) support only operating systems that feature multithreading, by now supporting not nearly as many operating systems (dropping many outdated) than in the past.
=
The energy usage of Python with CPython for typically written code is much worse than C by a factor of 75.88.
The throughput of Python with CPython for typically written code is worse than C by a factor of 71.9.
The average memory usage of CPython for typically written code is worse than C by a factor of 2.4.
=
All alternative implementations have at least slightly different semantics. For example, an alternative may include unordered dictionaries, in contrast to other current Python versions. As another example in the larger Python ecosystem, PyPy does not support the full C Python API.
Creating an executable with Python often is done by bundling an entire Python interpreter into the executable, which causes binary sizes to be massive for small programs, yet there exist implementations that are capable of truly compiling Python. Alternative implementations include the following:
PyPy is a faster, compliant interpreter of Python 2.7 and  3.10. PyPy's just-in-time compiler often improves speed significantly relative to CPython, but PyPy does not support some libraries written in C. PyPy offers support for the RISC-V instruction-set architecture.
Codon is an implementation with an ahead-of-time (AOT) compiler, which compiles a statically-typed Python-like language whose "syntax and semantics are nearly identical to Python's, there are some notable differences" For example, Codon uses 64-bit machine integers for speed, not arbitrarily as with Python; Codon developers claim that speedups over CPython are usually on the order of ten to a hundred times. Codon compiles to machine code (via LLVM) and supports native multithreading.  Codon can also compile to Python extension modules that can be imported and used from Python.
MicroPython and CircuitPython are Python 3 variants that are optimized for microcontrollers, including the Lego Mindstorms EV3.
Pyston is a variant of the Python runtime that uses just-in-time compilation to speed up execution of Python programs.
Cinder is a performance-oriented fork of CPython 3.8 that features a number of optimizations, including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.
The Snek embedded computing language "is Python-inspired, but it is not Python. It is possible to write Snek programs that run under a full Python system, but most Python programs will not run under Snek." Snek is compatible with 8-bit AVR microcontrollers such as ATmega 328P-based Arduino, as well as larger microcontrollers that are compatible with MicroPython. Snek is an imperative language that (unlike Python) omits object-oriented programming. Snek supports only one numeric data type, which features 32-bit single precision (resembling JavaScript numbers, though smaller).
=
Stackless Python is a significant fork of CPython that implements microthreads. This implementation uses the call stack differently, thus allowing massively concurrent programs. PyPy also offers a stackless version.
Just-in-time Python compilers have been developed, but are now unsupported:
Google began a project named Unladen Swallow in 2009: this project aimed to speed up the Python interpreter five-fold by using LLVM, and improve multithreading capability for scaling to thousands of cores, while typical implementations are limited by the global interpreter lock.
Psyco is a discontinued just-in-time specializing compiler, which integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialized for certain data types and is faster than standard Python code. Psyco does not support Python 2.7 or later.
PyS60 was a Python 2 interpreter for Series 60 mobile phones, which was released by Nokia in 2005. The interpreter implemented many modules from Python's standard library, as well as additional modules for integration with the Symbian operating system. The Nokia N900 also supports Python through the GTK widget library, allowing programs to be written and run on the target device.
=
There are several compilers/transpilers to high-level object languages; the source language is unrestricted Python, a subset of Python, or a language similar to Python:
Brython and Transcrypt compile Python to JavaScript.
Cython compiles a superset of Python to C. The resulting code can be used with Python via direct C-level API calls into the Python interpreter.
PyJL compiles/transpiles a subset of Python to "human-readable, maintainable, and high-performance Julia source code". Despite the developers' performance claims, this is not possible for arbitrary Python code; that is, compiling to a faster language or machine code is known to be impossible in the general case. The semantics of Python might potentially be changed, but in many cases speedup is possible with few or no changes in the Python code. The faster Julia source code can then be used from Python or compiled to machine code.
Nuitka compiles Python into C. This compiler works with Python 3.4 to 3.13 (and 2.6 and 2.7) for Python's main supported platforms (and Windows 7 or even Windows XP) and for Android. The compiler developers claim full support for Python 3.10, partial support for Python 3.11 and 3.12,  and experimental support for Python 3.13. Nuitka supports macOS including Apple Silicon-based versions.  The compiler is free of cost, though it has commercial add-ons (e.g., for hiding source code).
Numba is a JIT compiler that is used from Python; the compiler translates a subset of Python and NumPy code into fast machine code. This tool is enabled by adding a decorator to the relevant Python code.
Pythran compiles a subset of Python 3 to C++ (C++11).
RPython can be compiled to C, and it is used to build the PyPy interpreter for Python.
The Python → 11l → C++ transpiler compiles a subset of Python 3 to C++ (C++17).
There are also specialized compilers:
MyHDL is a Python-based hardware description language (HDL) that converts MyHDL code to Verilog or VHDL code.
Some older projects existed, as well as compilers not designed for use with Python 3.x and related syntax:
Google's Grumpy transpiles Python 2 to Go. The latest release was in 2017.
IronPython allows running Python 2.7 programs with the .NET Common Language Runtime. An alpha version (released in 2021), is available for "Python 3.4, although features and behaviors from later versions may be included."
Jython compiles Python 2.7 to Java bytecode, allowing the use of Java libraries from a Python program.
Pyrex (last released in 2010) and Shed Skin (last released in 2013) compile to C and C++ respectively.
=
A performance comparison among various Python implementations, using a non-numerical (combinatorial) workload, was presented at EuroSciPy '13. In addition, Python's performance relative to other programming languages is benchmarked by The Computer Language Benchmarks Game.
There are several approaches to optimizing Python performance, despite the inherent slowness of an interpreted language. These approaches include the following strategies or tools:
Just-in-time compilation: Dynamically compiling parts of a Python program during the execution of the program. This technique is used in libraries such as Numba and PyPy.
Static compilation: Sometimes, Python code can be compiled into machine code sometime before execution. An example of this approach is Cython, which compiles Python into C.
Concurrency and parallelism: Multiple tasks can be run simultaneously. Python contains modules such as `multiprocessing` to support this form of parallelism. Moreover, this approach helps to overcome limitations of the Global Interpreter Lock (GIL) in CPU tasks.
Efficient data structures: Performance can also be improved by using data types such as Set for membership tests, or deque from collections for queue operations.
Performance gains can be observed by utilizing libraries such as NumPy. Most high performance Python libraries use C or Fortran under the hood instead of the Python interpreter.
Python's development is conducted mostly through the Python Enhancement Proposal (PEP) process; this process is the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions. Python coding style is covered in PEP 8. Outstanding PEPs are reviewed and commented on by the Python community and the steering council.
Enhancement of the language corresponds with development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language's development. Specific issues were originally discussed in the Roundup bug tracker hosted by the foundation. In 2022, all issues and discussions were migrated to GitHub. Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017.
CPython's public releases have three types, distinguished by which part of the version number is incremented:
Backward-incompatible versions, where code is expected to break and must be manually ported. The first part of the version number is incremented. These releases happen infrequently—version 3.0 was released 8 years after 2.0. According to Guido van Rossum, a version 4.0 will probably never exist.
Major or "feature" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python 3.9, these releases are expected to occur annually. Each major version is supported by bug fixes for several years after its release.
Bug fix releases, which introduce no new features, occur approximately every three months; these releases are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented.
Many alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for releases, they are often delayed if the code is not ready yet. Python's development team monitors the state of the code by running a large unit test suite during development.
The major academic conference on Python is PyCon. Also, there are special Python mentoring programs, such as PyLadies.
Python's name is inspired by the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture; for example, the metasyntactic variables often used in Python literature are spam and eggs, rather than the traditional foo and bar. Also, the official Python documentation contains various references to Monty Python routines. Python users are sometimes referred to as "Pythonistas".
Cobra has an Acknowledgements document that lists Python first among influencing languages.
ECMAScript and JavaScript borrowed iterators and generators from Python.
Go is designed for "speed of working in a dynamic language like Python".
Julia was designed to be "as usable for general programming as Python".
Mojo is almost a superset of Python.
GDScript is strongly influenced by Python.
 Groovy, Boo, CoffeeScript, F#, Nim, Ruby, Swift, and V have been influenced, as well.
List of Python programming books
pip (package manager) (see also uv)
Pydoc
NumPy
SciPy
Jupyter
PyTorch
Cython
CPython
Mojo
Pygame
PyQt
PyGTK
PyPy
PyCon
Google Colab –  zero setup online IDE that runs Python
Ren'Py
=
"Python for Artificial Intelligence". Python Wiki. 19 July 2012. Archived from the original on 1 November 2012. Retrieved 3 December 2012.
Paine, Jocelyn, ed. (August 2005). "AI in Python". AI Expert Newsletter. Amzi!. Archived from the original on 26 March 2012. Retrieved 11 February 2012.
"PyAIML 0.8.5: Python Package Index". Pypi.python.org. Retrieved 17 July 2013.
Russell, Stuart J. & Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd ed.). Upper Saddle River, NJ: Prentice Hall. ISBN 978-0-13-604259-4.
Downey, Allen (July 2024). Think Python: How to Think Like a Computer Scientist (3rd ed.). O'Reilly Media. ISBN 978-1-0981-5543-8.
Lutz, Mark (2013). Learning Python (5th ed.). O'Reilly Media. ISBN 978-0-596-15806-4.
Summerfield, Mark (2009). Programming in Python 3 (2nd ed.). Addison-Wesley Professional. ISBN 978-0-321-68056-3.
Ramalho, Luciano (May 2022). Fluent Python. O'Reilly Media. ISBN 978-1-4920-5632-4.
Official website 
Python documentation
The Python Tutorial

--- TOPIC: JavaScript ---
JavaScript (JS) is a programming language and core technology of the Web, alongside HTML and CSS. It was created by Brendan Eich in 1995. As of 2025, the overwhelming majority of websites (98.9%) uses JavaScript for client side webpage behavior.
Web browsers have a dedicated JavaScript engine that executes the client code. These engines are also utilized in some servers and a variety of apps. The most popular runtime system for non-browser usage is Node.js.
JavaScript is a high-level, often just-in-time–compiled language that conforms to the ECMAScript standard. It has dynamic typing, prototype-based object-orientation, and first-class functions. It is multi-paradigm, supporting event-driven, functional, and imperative programming styles. It has application programming interfaces (APIs) for working with text, dates, regular expressions, standard data structures, and the Document Object Model (DOM).
The ECMAScript standard does not include any input/output (I/O), such as networking, storage, or graphics facilities. In practice, the web browser or other runtime system provides JavaScript APIs for I/O.
Although Java and JavaScript are similar in name and syntax, the two languages are distinct and differ greatly in design.
=
The first popular web browser with a graphical user interface, Mosaic, was released in 1993. The lead developers of Mosaic then founded the Netscape corporation, which released a more polished browser, Netscape Navigator, in 1994. This quickly became the most-used.
During these formative years of the Web, web pages could only be static, lacking the capability for dynamic behavior after the page was loaded in the browser. There was a desire in the flourishing web development scene to remove this limitation, so in 1995, Netscape decided to add a programming language to Navigator. They pursued two routes to achieve this: collaborating with Sun Microsystems to embed the Java language, while also hiring Brendan Eich to embed the Scheme language.
The goal was a "language for the masses", "to help nonprogrammers create dynamic, interactive Web sites". Netscape management soon decided that the best option was for Eich to devise a new language, with syntax similar to Java and less like Scheme or other extant scripting languages. Although the new language and its interpreter implementation were called LiveScript when first shipped as part of a Navigator beta in September 1995, the name was changed to JavaScript for the official release in December.
The choice of the JavaScript name has caused confusion, implying that it is directly related to Java. At the time, the dot-com boom had begun and Java was a popular new language, so Eich considered the JavaScript name a marketing ploy by Netscape.
=
Microsoft debuted Internet Explorer in 1995, leading to a browser war with Netscape. On the JavaScript front, Microsoft created its own interpreter called JScript.
Microsoft first released JScript in 1996, alongside initial support for CSS and extensions to HTML. Each of these implementations was noticeably different from their counterparts in Netscape Navigator. These differences made it difficult for developers to make their websites work well in both browsers, leading to widespread use of "best viewed in Netscape" and "best viewed in Internet Explorer" logos for several years.
=
In November 1996, Netscape submitted JavaScript to Ecma International, as the starting point for a standard specification that all browser vendors could conform to. This led to the official release of the first ECMAScript language specification in June 1997.
The standards process continued for a few years, with the release of ECMAScript 2 in June 1998 and ECMAScript 3 in December 1999. Work on ECMAScript 4 began in 2000.
However, the effort to fully standardize the language was undermined by Microsoft gaining an increasingly dominant position in the browser market. By the early 2000s, Internet Explorer's market share reached 95%. This meant that JScript became the de facto standard for client-side scripting on the Web.
Microsoft initially participated in the standards process and implemented some proposals in its JScript language, but eventually it stopped collaborating on ECMA work. Thus ECMAScript 4 was mothballed.
=
 
During the period of Internet Explorer dominance in the early 2000s, client-side scripting was stagnant. This started to change in 2004, when the successor of Netscape, Mozilla, released the Firefox browser. Firefox was well received by many, taking significant market share from Internet Explorer.
In 2005, Mozilla joined ECMA International, and work started on the ECMAScript for XML (E4X) standard. This led to Mozilla working jointly with Macromedia (later acquired by Adobe Systems), who were implementing E4X in their ActionScript 3 language, which was based on an ECMAScript 4 draft. The goal became standardizing ActionScript 3 as the new ECMAScript 4. To this end, Adobe Systems released the Tamarin implementation as an open source project. However, Tamarin and ActionScript 3 were too different from established client-side scripting, and without cooperation from Microsoft, ECMAScript 4 never reached fruition.
Meanwhile, very important developments were occurring in open-source communities not affiliated with ECMA work. In 2005, Jesse James Garrett released a white paper in which he coined the term Ajax and described a set of technologies, of which JavaScript was the backbone, to create web applications where data can be loaded in the background, avoiding the need for full page reloads. This sparked a renaissance period of JavaScript, spearheaded by open-source libraries and the communities that formed around them. Many new libraries were created, including jQuery, Prototype, Dojo Toolkit, and MooTools.
Google debuted its Chrome browser in 2008, with the V8 JavaScript engine that was faster than its competition. The key innovation was just-in-time compilation (JIT), so other browser vendors needed to overhaul their engines for JIT.
In July 2008, these disparate parties came together for a conference in Oslo. This led to the eventual agreement in early 2009 to combine all relevant work and drive the language forward. The result was the ECMAScript 5 standard, released in December 2009.
=
Ambitious work on the language continued for several years, culminating in an extensive collection of additions and refinements being formalized with the publication of ECMAScript 6 in 2015.
The creation of Node.js in 2009 by Ryan Dahl sparked a significant increase in the usage of JavaScript outside of web browsers. Node combines the V8 engine, an event loop, and I/O APIs, thereby providing a stand-alone JavaScript runtime system. As of 2018, Node had been used by millions of developers, and npm had the most modules of any package manager in the world.
The ECMAScript draft specification is currently maintained openly on GitHub, and editions are produced via regular annual snapshots. Potential revisions to the language are vetted through a comprehensive proposal process. Now, instead of edition numbers, developers check the status of upcoming features individually.
The current JavaScript ecosystem has many libraries and frameworks, established programming practices, and substantial usage of JavaScript outside of web browsers. Plus, with the rise of single-page applications and other JavaScript-heavy websites, several transpilers have been created to aid the development process.
"JavaScript" is a trademark of Oracle Corporation in the United States. The trademark was originally issued to Sun Microsystems on 6 May 1997, and was transferred to Oracle when they acquired Sun in 2009.
A letter was circulated in September 2024, spearheaded by Ryan Dahl, calling on Oracle to free the JavaScript trademark. Brendan Eich, the original creator of JavaScript, was among the over 14,000 signatories who supported the initiative.
JavaScript is the dominant client-side scripting language of the Web, with 99% of all websites using it for this purpose. Scripts are embedded in or included from HTML documents and interact with the DOM.
All major web browsers have a built-in JavaScript engine that executes the code on the user's device.
=
Loading new web page content without reloading the page, via Ajax or a WebSocket. For example, users of social media can send and receive messages without leaving the current page.
Web page animations, such as fading objects in and out, resizing, and moving them.
Playing browser games.
Controlling the playback of streaming media.
Generating pop-up ads or alert boxes.
Validating input values of a web form before the data is sent to a web server.
Logging data about the user's behavior then sending it to a server. The website owner can use this data for analytics, ad tracking, and personalization.
Redirecting a user to another page.
Storing and retrieving data on the user's device, via the storage or IndexedDB standards.
=
Over 80% of websites use a third-party JavaScript library or web framework as part of their client-side scripting.
jQuery is by far the most-used. Other notable ones include Angular, Bootstrap, Lodash, Modernizr, React, Underscore, and Vue. Multiple options can be used in conjunction, such as jQuery and Bootstrap.
However, the term "Vanilla JS" was coined for websites not using any libraries or frameworks at all, instead relying entirely on standard JavaScript functionality.
The use of JavaScript has expanded beyond its web browser roots. JavaScript engines are now embedded in a variety of other software systems, both for server-side website deployments and non-browser applications.
Initial attempts at promoting server-side JavaScript usage were Netscape Enterprise Server and Microsoft's Internet Information Services, but they were small niches. Server-side usage eventually started to grow in the late 2000s, with the creation of Node.js and other approaches.
Electron, Cordova, React Native, and other application frameworks have been used to create many applications with behavior implemented in JavaScript. Other non-browser applications include Adobe Acrobat support for scripting PDF documents and GNOME Shell extensions written in JavaScript.
Oracle used to provide  Nashorn, a JavaScript interpreter, as part of their Java Development Kit (JDK) API library along with jjs a command line interpreter as of JDK version 8.  It was removed in JDK 15. As a replacement Oracle offered GraalJS which can also be used with the OpenJDK  which allows one to create and reference Java objects in JavaScript code and add runtime scripting in JavaScript to applications written in Java. 
JavaScript has been used in some embedded systems, usually by leveraging Node.js.
=
=
A JavaScript engine must be embedded within a runtime system (such as a web browser or a standalone system) to enable scripts to interact with the broader environment. The runtime system includes the necessary APIs for input/output operations, such as networking, storage, and graphics, and provides the ability to import scripts.
JavaScript is a single-threaded language. The runtime processes messages from a queue one at a time, and it calls a function associated with each new message, creating a call stack frame with the function's arguments and local variables. The call stack shrinks and grows based on the function's needs. When the call stack is empty upon function completion, JavaScript proceeds to the next message in the queue. This is called the event loop, described as "run to completion" because each message is fully processed before the next message is considered. However, the language's concurrency model describes the event loop as non-blocking: program I/O is performed using events and callback functions. This means, for example, that JavaScript can process a mouse click while waiting for a database query to return information.
The notable standalone runtimes are Node.js, Deno, and Bun.
The following features are common to all conforming ECMAScript implementations unless explicitly specified otherwise. The number of cited reserved words including keywords is 50–60 and varies depending on the implementation.
=
JavaScript supports much of the structured programming syntax from C (e.g., if statements, while loops, switch statements, do while loops, etc.). One partial exception is scoping: originally JavaScript only had function scoping with var; block scoping was added in ECMAScript 2015 with the keywords let and const. Like C, JavaScript makes a distinction between expressions and statements. One syntactic difference from C is automatic semicolon insertion, which allow semicolons (which terminate statements) to be omitted.
=
JavaScript is weakly typed, which means certain types are implicitly cast depending on the operation used.
The binary + operator casts both operands to a string unless both operands are numbers. This is because the addition operator doubles as a concatenation operator
The binary - operator always casts both operands to a number
Both unary operators (+, -) always cast the operand to a number. However, + always casts to Number (binary64) while - preserves BigInt (integer)
Values are cast to strings as follows:
Strings are left as-is
Numbers are converted to their string representation
Arrays have their elements cast to strings after which they are joined by commas (,)
Other objects are converted to the string [object Object] where Object is the name of the constructor of the object
Values are cast to numbers by casting to strings and then casting the strings to numbers. These processes can be modified by defining toString and valueOf functions on the prototype for string and number casting respectively.
JavaScript has received criticism for the way it implements these conversions as the complexity of the rules can be mistaken for inconsistency. For example, when adding a number to a string, the number will be cast to a string before performing concatenation, but when subtracting a number from a string, the string is cast to a number before performing subtraction.
Often also mentioned is {} + [] resulting in 0 (number). This is misleading: the {} is interpreted as an empty code block instead of an empty object, and the empty array is cast to a number by the remaining unary + operator. If the expression is wrapped in parentheses - ({} + []) – the curly brackets are interpreted as an empty object and the result of the expression is "[object Object]" as expected.
=
 Typing 
JavaScript is dynamically typed like most other scripting languages. A type is associated with a value rather than an expression. For example, a variable initially bound to a number may be reassigned to a string. JavaScript supports various ways to test the type of objects, including duck typing.
 Run-time evaluation 
JavaScript includes an eval function that can execute statements provided as strings at run-time.
=
Prototypal inheritance in JavaScript is described by Douglas Crockford as:
You make prototype objects, and then ... make new instances. Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. We don't need classes to make lots of similar objects... Objects inherit from objects. What could be more object oriented than that?
In JavaScript, an object is an associative array, augmented with a prototype (see below); each key provides the name for an object property, and there are two syntactical ways to specify such a name: dot notation (obj.x = 10) and bracket notation (obj["x"] = 10). A property may be added, rebound, or deleted at run-time. Most properties of an object (and any property that belongs to an object's prototype inheritance chain) can be enumerated using a for...in loop.
 Prototypes 
JavaScript uses prototypes where many other object-oriented languages use classes for inheritance, but it's still possible to simulate most class-based features with the prototype system. Additionally, ECMAScript version 6 (released June 2015) introduced the keywords class, extends and super, which serve as syntactic sugar to abstract the underlying prototypal inheritance system with a more conventional interface. Constructors are declared by specifying a method named constructor, and all classes are automatically subclasses of the base class Object, similarly to Java. Though the underlying object mechanism is still based on prototypes, the newer syntax is similar to other object oriented languages. Private variables are declared by prefixing the field name with a number sign (#), and polymorphism is not directly supported, although it can be emulated by manually calling different functions depending on the number and type of arguments provided.
 Functions as object constructors 
Functions double as object constructors, along with their typical role. Prefixing a function call with new will create an instance of a prototype, inheriting properties and methods from the constructor (including properties from the Object prototype). ECMAScript 5 offers the Object.create method, allowing explicit creation of an instance without automatically inheriting from the Object prototype (older environments can assign the prototype to null). The constructor's prototype property determines the object used for the new object's internal prototype. New methods can be added by modifying the prototype of the function used as a constructor.JavaScript's built-in classes, such as Array and Object, also have prototypes that can be modified. However, it's generally considered bad practice to modify built-in objects, because third-party code may use or inherit methods and properties from these objects, and may not expect the prototype to be modified.
 Functions as methods 
Unlike in many object-oriented languages, in JavaScript there is no distinction between a function definition and a method definition. Rather, the distinction occurs during function calling. When a function is called as a method of an object, the function's local this keyword is bound to that object for that invocation.
=
JavaScript functions are first-class; a function is considered to be an object. As such, a function may have properties and methods, such as .call() and .bind().
 Lexical closure 
A nested function is a function defined within another function. It is created each time the outer function is invoked.
In addition, each nested function forms a lexical closure: the lexical scope of the outer function (including any constant, local variable, or argument value) becomes part of the internal state of each inner function object, even after execution of the outer function concludes.
 Anonymous function 
JavaScript also supports anonymous functions.
=
JavaScript supports implicit and explicit delegation.
 Functions as roles (Traits and Mixins) 
JavaScript natively supports various function-based implementations of Role patterns like Traits and Mixins. Such a function defines additional behavior by at least one method bound to the this keyword within its function body. A Role then has to be delegated explicitly via call or apply to objects that need to feature additional behavior that is not shared via the prototype chain.
 Object composition and inheritance 
Whereas explicit function-based delegation does cover composition in JavaScript, implicit delegation already happens every time the prototype chain is walked in order to, e.g., find a method that might be related to but is not directly owned by an object. Once the method is found it gets called within this object's context. Thus inheritance in JavaScript is covered by a delegation automatism that is bound to the prototype property of constructor functions.
=
 Zero-based numbering 
JavaScript is a zero-index language.
 Variadic functions 
An indefinite number of parameters can be passed to a function. The function can access them through formal parameters and also through the local arguments object. Variadic functions can also be created by using the bind method.
 Array and object literals 
Like in many scripting languages, arrays and objects (associative arrays in other languages) can each be created with a succinct shortcut syntax. In fact, these literals form the basis of the JSON data format.
 Regular expressions 
JavaScript supports regular expressions for text searches and manipulation.
= Promises =
A built-in Promise object provides functionality for handling promises and associating handlers with an asynchronous action's eventual result. JavaScript supplies combinator methods, which allow developers to combine multiple JavaScript promises and do operations based on different scenarios. The methods introduced are: Promise.race, Promise.all, Promise.allSettled and Promise.any.
= Async/await =
Async/await allows an asynchronous, non-blocking function to be structured in a way similar to an ordinary synchronous function. Asynchronous, non-blocking code can be written, with minimal overhead, structured similarly to traditional synchronous, blocking code.
=
Historically, some JavaScript engines supported these non-standard features:
array comprehensions and generator expressions (like Python)
concise function expressions (function(args) expr; this experimental syntax predated arrow functions)
ECMAScript for XML (E4X), an extension that adds native XML support to ECMAScript (unsupported in Firefox since version 21)
Variables in JavaScript can be defined using either the var, let or const keywords.  Variables defined without keywords will be defined at the global scope.
Arrow functions were first introduced in 6th Edition – ECMAScript 2015. They shorten the syntax for writing functions in JavaScript. Arrow functions are anonymous, so a variable is needed to refer to them in order to invoke them after their creation, unless surrounded by parenthesis and executed immediately.
Here is an example of JavaScript syntax.
Note the comments in the examples above, all of which were preceded with two forward slashes.
More examples can be found at the Wikibooks page on JavaScript syntax examples.
JavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors minimize this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same-origin policy: scripts from one website do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.
There are subsets of general JavaScript—ADsafe, Secure ECMAScript (SES)—that provide greater levels of security, especially on code created by third parties (such as advertisements). Closure Toolkit is another project for safe embedding and isolation of third-party JavaScript and HTML.
Content Security Policy is the main intended method of ensuring that only trusted code is executed on a Web page.
=
A common JavaScript-related security problem is cross-site scripting (XSS), a violation of the same-origin policy. XSS vulnerabilities occur when an attacker can cause a target Website, such as an online banking website, to include a malicious script in the webpage presented to a victim. The script in this example can then access the banking application with the privileges of the victim, potentially disclosing secret information or transferring money without the victim's authorization. One important solution to XSS vulnerabilities is HTML sanitization.
Some browsers include partial protection against reflected XSS attacks, in which the attacker provides a URL including malicious script. However, even users of those browsers are vulnerable to other XSS attacks, such as those where the malicious code is stored in a database. Only correct design of Web applications on the server-side can fully prevent XSS.
XSS vulnerabilities can also occur because of implementation mistakes by browser authors.
=
Another cross-site vulnerability is cross-site request forgery (CSRF). In CSRF, code on an attacker's site tricks the victim's browser into taking actions the user did not intend at a target site (like transferring money at a bank). When target sites rely solely on cookies for request authentication, requests originating from code on the attacker's site can carry the same valid login credentials of the initiating user. In general, the solution to CSRF is to require an authentication value in a hidden form field, and not only in the cookies, to authenticate any request that might have lasting effects. Checking the HTTP Referrer header can also help.
"JavaScript hijacking" is a type of CSRF attack in which a <script> tag on an attacker's site exploits a page on the victim's site that returns private information such as JSON or JavaScript. Possible solutions include:
requiring an authentication token in the POST and GET parameters for any response that returns private information.
=
Developers of client-server applications must recognize that untrusted clients may be under the control of attackers. The author of an application should not assume that their JavaScript code will run as intended (or at all) because any secret embedded in the code could be extracted by a determined adversary. Some implications are:
Website authors cannot perfectly conceal how their JavaScript operates because the raw source code must be sent to the client. The code can be obfuscated, but obfuscation can be reverse-engineered.
JavaScript form validation only provides convenience for users, not security. If a site verifies that the user agreed to its terms of service, or filters invalid characters out of fields that should only contain numbers, it must do so on the server, not only the client.
Scripts can be selectively disabled, so JavaScript cannot be relied on to prevent operations such as right-clicking on an image to save it.
It is considered very bad practice to embed sensitive information such as passwords in JavaScript because it can be extracted by an attacker.
Prototype pollution is a runtime vulnerability in which attackers can overwrite arbitrary properties in an object's prototype.
=
Package management systems such as npm and Bower are popular with JavaScript developers. Such systems allow a developer to easily manage their program's dependencies upon other developers' program libraries. Developers trust that the maintainers of the libraries will keep them secure and up to date, but that is not always the case. A vulnerability has emerged because of this blind trust. Relied-upon libraries can have new releases that cause bugs or vulnerabilities to appear in all programs that rely upon the libraries. Inversely, a library can go unpatched with known vulnerabilities out in the wild. In a study done looking over a sample of 133,000 websites, researchers found 37% of the websites included a library with at least one known vulnerability. "The median lag between the oldest library version used on each website and the newest available version of that library is 1,177 days in ALEXA, and development of some libraries still in active use ceased years ago." Another possibility is that the maintainer of a library may remove the library entirely. This occurred in March 2016 when Azer Koçulu removed his repository from npm. This caused tens of thousands of programs and websites depending upon his libraries to break.
=
JavaScript provides an interface to a wide range of browser capabilities, some of which may have flaws such as buffer overflows. These flaws can allow attackers to write scripts that would run any code they wish on the user's system. This code is not by any means limited to another JavaScript application. For example, a buffer overrun exploit can allow an attacker to gain access to the operating system's API with superuser privileges.
These flaws have affected major browsers including Firefox, Internet Explorer, and Safari.
Plugins, such as video players, Adobe Flash, and the wide range of ActiveX controls enabled by default in Microsoft Internet Explorer, may also have flaws exploitable via JavaScript (such flaws have been exploited in the past).
In Windows Vista, Microsoft has attempted to contain the risks of bugs such as buffer overflows by running the Internet Explorer process with limited privileges. Google Chrome similarly confines its page renderers to their own "sandbox".
=
Web browsers are capable of running JavaScript outside the sandbox, with the privileges necessary to, for example, create or delete files. Such privileges are not intended to be granted to code from the Web.
Incorrectly granting privileges to JavaScript from the Web has played a role in vulnerabilities in both Internet Explorer and Firefox. In Windows XP Service Pack 2, Microsoft demoted JScript's privileges in Internet Explorer.
Microsoft Windows allows JavaScript source files on a computer's hard drive to be launched as general-purpose, non-sandboxed programs (see: Windows Script Host). This makes JavaScript (like VBScript) a theoretically viable vector for a Trojan horse, although JavaScript Trojan horses are uncommon in practice.
=
In 2015, a JavaScript-based proof-of-concept implementation of a rowhammer attack was described in a paper by security researchers.
In 2017, a JavaScript-based attack via browser was demonstrated that could bypass ASLR. It is called "ASLR⊕Cache" or AnC.
In 2018, the paper that announced the Spectre attacks against Speculative Execution in Intel and other processors included a JavaScript implementation.
Important tools have evolved with the language.
Every major web browser has built-in web development tools, including a JavaScript debugger.
Static program analysis tools, such as ESLint and JSLint, scan JavaScript code for conformance to a set of standards and guidelines.
Some browsers have built-in profilers. Stand-alone profiling libraries have also been created, such as benchmark.js and jsbench.
Many text editors have syntax highlighting support for JavaScript code.
=
A common misconception is that JavaScript is directly related to Java. Both indeed have a C-like syntax (the C language being their most immediate common ancestor language). They are also typically sandboxed, and JavaScript was designed with Java's syntax and standard library in mind. In particular, all Java keywords were reserved in original JavaScript, JavaScript's standard library follows Java's naming conventions, and JavaScript's Math and Date objects are based on classes from Java 1.0.
Both languages first appeared in 1995, but Java was developed by James Gosling of Sun Microsystems and JavaScript by Brendan Eich of Netscape Communications.
The differences between the two languages are more prominent than their similarities. Java has static typing, while JavaScript's typing is dynamic. Java is loaded from compiled bytecode, while JavaScript is loaded as human-readable source code. Java's objects are class-based, while JavaScript's are prototype-based. Finally, Java did not support functional programming until Java 8, while JavaScript has done so from the beginning, being influenced by Scheme.
=
JSON is a data format derived from JavaScript; hence the name JavaScript Object Notation. It is a widely used format supported by many other programming languages.
=
Many websites are JavaScript-heavy, so transpilers have been created to convert code written in other languages, which can aid the development process.
TypeScript and CoffeeScript are two notable languages that transpile to JavaScript.
=
WebAssembly is a newer language with a bytecode format designed to complement JavaScript, especially the performance-critical portions of web page scripts. All of the major JavaScript engines support WebAssembly, which runs in the same sandbox as regular JavaScript code.
asm.js is a subset of JavaScript that served as the forerunner of WebAssembly.
Flanagan, David (2020). JavaScript: The Definitive Guide (7th ed.). Sebastopol, California: O'Reilly. ISBN 978-1-491-95202-3.
Haverbeke, Marijn (2024). Eloquent JavaScript (PDF) (4th ed.). San Francisco: No Starch Press. ISBN 978-1-71850-411-0. Archived (PDF) from the original on 12 March 2025.
Zakas, Nicholas (2014). Principles of Object-Oriented JavaScript (1st ed.). No Starch Press. ISBN 978-1-59327-540-2.
The Modern JavaScript Tutorial. A community maintained continuously updated collection of tutorials on the entirety of the language.
"JavaScript: The First 20 Years". Retrieved 6 February 2022.

--- TOPIC: Blockchain ---
A blockchain is a distributed ledger with growing lists of records (blocks) that are securely linked together via cryptographic hashes. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leaves). Since each block contains information about the previous block, they effectively form a chain (viz. linked list data structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes.
Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.
A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need for a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.
Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model "snake oil"; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.
Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation "Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups". Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block. Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995.
The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.
In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020.
The words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016.
According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.
In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term "planning or [looking at] active experimentation with blockchain". For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.
A blockchain is a decentralized, distributed, and often public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol.
Logically, a blockchain can be seen as consisting of several layers:
infrastructure (hardware)
networking (node discovery, information propagation and verification)
consensus (proof of work, proof of stake)
data (blocks, transactions)
application (smart contracts/decentralized applications, if applicable)
=
Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree. Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain. This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block (Block 0). To assure the integrity of a block and the data contained in it, the block is usually digitally signed.
Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks. Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially as more blocks are built on top of it, eventually becoming very low. For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.
 Block time 
The block time is the average time it takes for the network to generate one extra block in the blockchain. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.
 Hard forks 
=
By storing data across its peer-to-peer network, the blockchain eliminates some risks that come with data being held centrally. The decentralized blockchain may use ad hoc message passing and distributed networking.
In a so-called "51% attack" a central entity gains control of more than half of a network and can then manipulate that specific blockchain record at will, allowing double-spending.
Blockchain security methods include the use of public-key cryptography. A public key (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A private key is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.
Every node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication and computational trust. No centralized "official" copy exists and no user is "trusted" more than any other. Transactions are broadcast to the network using the software. Messages are delivered on a best-effort basis. Early blockchains rely on energy-intensive mining nodes to validate transactions, add them to the block they are building, and then broadcast the completed block to other nodes. Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes. Later consensus methods include proof of stake. The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.
 Finality 
Finality is the level of confidence that the well-formed block recently appended to the blockchain will not be revoked in the future (is "finalized") and thus can be trusted. Most distributed blockchain protocols, whether proof of work or proof of stake, cannot guarantee the finality of a freshly committed block, and instead rely on "probabilistic finality": as the block goes deeper into a blockchain, it is less likely to be altered or reverted by a newly found consensus.
Byzantine fault tolerance-based proof-of-stake protocols purport to provide so called "absolute finality": a randomly chosen validator proposes a block, the rest of validators vote on it, and, if a supermajority decision approves it, the block is irreversibly committed into the blockchain. A modification of this method, an "economic finality", is used in practical protocols, like the Casper protocol used in Ethereum: validators which sign two different blocks at the same position in the blockchain are subject to "slashing", where their leveraged stake is forfeited.
=
Open blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain. Proponents of permissioned or private chains argue that the term "blockchain" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases. Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain. Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision. Nikolai Hampton of Computerworld said that "many in-house blockchain solutions will be nothing more than cumbersome databases," and "without a clear security model, proprietary blockchains should be eyed with suspicion."
 Permissionless (public) blockchain 
An advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed. This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.
Bitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper "Pricing via Processing or Combatting Junk Mail".
In 2016, venture capital investment for blockchain-related projects was weakening in the US but increasing in China. Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018, bitcoin has the highest market capitalization.
 Permissioned (private) blockchain 
Permissioned blockchains use an access control layer to govern who has access to the network. It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.
= Disadvantages of permissioned blockchain =
Nikolai Hampton argued in Computerworld that "There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished." This has a set of particularly profound adverse implications during a financial crisis or debt crisis such as the 2008 financial crisis, where politically powerful actors may make decisions that favor some groups at the expense of others, and "the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power — it's time-consuming and expensive." He also said, "Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases."
 Blockchain analysis 
The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies. A blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how.  The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.  The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit dark market trading of drugs, weapons, money laundering, etc. A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing now that specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and fiat-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as Monero.
=
In April 2016, Standards Australia submitted a proposal to the International Organization for Standardization to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies. The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements. More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union (ITU) and the United Nations Economic Commission for Europe (UNECE).
Many other national standards bodies and open standards bodies are also working on blockchain standards. These include the National Institute of Standards and Technology (NIST), the European Committee for Electrotechnical Standardization (CENELEC), the Institute of Electrical and Electronics Engineers (IEEE), the Organization for the Advancement of Structured Information Standards (OASIS), and some individual participants in the Internet Engineering Task Force (IETF).
=
Although most of blockchain implementation are decentralized and distributed, Oracle launched a centralized blockchain table feature in Oracle 21c database. The Blockchain Table in Oracle 21c database is a centralized blockchain which provide immutable feature. Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains.
Currently, there are at least four types of blockchain networks — public blockchains, private blockchains, consortium blockchains and hybrid blockchains.
=
A public blockchain has no access restrictions. Anyone with an Internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol). Usually, such networks offer economic incentives for those who secure them and utilize some type of a proof-of-stake or proof-of-work algorithm.
Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain.
=
A private blockchain is permissioned. One cannot join it unless invited by the network administrators. Participant and validator access is restricted. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology Distributed Ledger (DLT) is normally used for private blockchains.
=
A hybrid blockchain has a combination of centralized and decentralized features. The exact workings of the chain can vary based on which portions of centralization and decentralization are used.
=
A sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain. Entries from the primary blockchain (where said entries typically represent digital assets) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate consensus algorithm, etc.).
=
A consortium blockchain is a type of blockchain that combines elements of both public and private blockchains. In a consortium blockchain, a group of organizations come together to create and operate the blockchain, rather than a single entity. The consortium members jointly manage the blockchain network and are responsible for validating transactions. Consortium blockchains are permissioned, meaning that only certain individuals or organizations are allowed to participate in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential.
Consortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One advantage of consortium blockchains is that they can be more efficient and scalable than public blockchains, as the number of nodes required to validate transactions is typically smaller. Additionally, consortium blockchains can provide greater security and reliability than private blockchains, as the consortium members work together to maintain the network. Some examples of consortium blockchains include Quorum and Hyperledger.
Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products that had matured from proof of concept by late 2016. As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office.
Blockchain is seen as a pivotal technological advancement of the 21st century, with the ability to impact organizations at strategic, operational, and market levels. In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp estimated that corporate investment into blockchain technology would reach $12.4 billion by 2022. Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicates a significant demand and interest in blockchain technology.
In 2019, the BBC World Service radio and podcast series Fifty Things That Made the Modern Economy identified blockchain as a technology that would have far-reaching consequences for economics and society. The economist and Financial Times journalist and broadcaster Tim Harford discussed why the underlying technology might have much wider applications and the challenges that needed to be overcome. His first broadcast was on 29 June 2019.
The number of blockchain wallets quadrupled to 40 million between 2016 and 2020.
A paper published in 2022 discussed the potential use of blockchain technology in sustainable management.
Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation. Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.
The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography. Each block typically contains a hash pointer as a link to a previous block, a timestamp and transaction data. By design, blockchains are inherently resistant to modification of the data. It is "an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.
Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.
In the context of cryptocurrencies, the blockchain serves as a public ledger for all transactions. Cryptocurrencies use various timestamping schemes to "prove the validity of transactions added to the blockchain ledger without the need for a trusted third party".
The first cryptocurrency was Bitcoin, which was first released as open-source software in 2009.
As cryptocurrencies have gained prominence, several countries have made advancements in their private and commercial law treatment to address legal uncertainties. In the United States, for example, the 2022 amendments to the Uniform Commercial Code (UCC) introduced Article 12, which establishes "controllable electronic records" (CERs) as a new category of personal property. This framework provides legal clarity for the ownership, transfer, and use of cryptocurrencies as CERs, with the concept of "control" serving as a functional equivalent to possession for digital assets. These reforms aim to align legal standards with market practices, reducing title disputes and supporting the integration of cryptocurrencies into commercial transactions.
=
Blockchain-based smart contracts are contracts that can be partially or fully executed or enforced without human interaction. One of the main objectives of a smart contract is automated escrow. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities — the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation. An IMF staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general, but "no viable smart contract systems have yet emerged." Due to the lack of widespread use, their legal status was unclear.
=
According to Reason, many banks have expressed interest in implementing distributed ledgers for use in banking and are cooperating with companies creating private blockchains; according to a September 2016 IBM study, it is occurring faster than expected. It was estimated by the World Economic Forum that by 2025, 10% of the world's GDP would be stored on blockchain related technology. 
Banks are interested in this technology not least because it has the potential to speed up back office settlement systems. Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails. This technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration. 
Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.
Berenberg, a German bank, believes that blockchain is an "overhyped technology" that has had a large number of "proofs of concept", but still has major challenges, and very few success stories.
The blockchain has also given rise to initial coin offerings (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs). STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate, art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs.
=
Blockchain technology, such as cryptocurrencies and non-fungible tokens (NFTs), has been used in video games for monetization. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to gray market issues such as skin gambling, and thus publishers typically have shied away from allowing players to earn real-world funds from games. Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.
The first known game to use blockchain technologies was CryptoKitties, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a virtual pet that the player could breed with others to create offspring with combined traits as new NFTs. The game made headlines in December 2017 when one virtual pet sold for more than US$100,000. CryptoKitties also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions being for the game.
By the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict. However, limited successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021. Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.
In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with gambling, specifically skin gambling, was speculated to be a factor in the decision to ban blockchain games. Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers, and Epic Games, which runs the Epic Games Store in competition to Steam, said that they would be open to accepted blockchain games in the wake of Valve's refusal.
=
There have been several different efforts to employ blockchains in supply chain management.
Precious commodities mining — Blockchain technology has been used for tracking the origins of gemstones and other precious commodities. In 2016, The Wall Street Journal reported that the blockchain technology company Everledger was partnering with IBM's blockchain-based tracking service to trace the origin of diamonds to ensure that they were ethically mined. As of 2019, the Diamond Trading Company (DTC) has been involved in building a diamond trading supply chain product called Tracer.
Food supply — As of 2018, Walmart and IBM were running a trial to use a blockchain-backed system for supply chain monitoring for lettuce and spinach – all nodes of the blockchain were administered by Walmart and located on the IBM cloud.
Fashion industry — There is an opaque relationship between brands, distributors, and customers in the fashion industry, which prevents the sustainable and stable development of the fashion industry. Blockchain could make this information transparent, assisting sustainable development of the industry.
Motor vehicles — Mercedes-Benz and partner Icertis developed a blockchain prototype used to facilitate consistent documentation of contracts along the supply chain so that the ethical standards and contractual obligations required of its direct suppliers can be passed on to second tier suppliers and beyond. In another project, the company uses blockchain technology to track the emissions of climate-relevant gases and the amount of secondary material along the supply chain for its battery cell manufacturers.
=
There are several different efforts to offer domain name services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.
Namecoin is a cryptocurrency that supports the ".bit" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root. As of 2015, .bit was used by 28 websites, out of 120,000 registered names. Namecoin was dropped by OpenNIC in 2019, due to malware and potential other legal issues. Other blockchain alternatives to ICANN include The Handshake Network, EmerDNS, and Unstoppable Domains.
Specific TLDs include ".eth", ".luxe", and ".kred", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional cryptocurrency wallet addresses as a convenience for transferring cryptocurrency.
=
Blockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users or musicians. The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months. In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution. Imogen Heap's Mycelia service has also been proposed as a blockchain-based alternative "that gives artists more control over how their songs and associated data circulate among fans and other musicians."
New distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain. The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers. The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.
Other blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM). Another is Quorum, a permissioned private blockchain by JPMorgan Chase with private storage, used for contract applications.
Oracle introduced a blockchain table feature in its Oracle 21c database.
Blockchain is also being used in peer-to-peer energy trading.
Lightweight blockchains, or simplified blockchains, are more suitable for internet of things (IoT) applications than conventional blockchains. One experiment suggested that a lightweight blockchain-based network could accommodate up to 1.34 million authentication processes every second, which could be sufficient for resource-constrained IoT networks.
Blockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered. It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems, as well as provisions for content creator verification ala KYC standards. The EUIPO established an Anti-Counterfeiting Blockathon Forum, with the objective of "defining, piloting and implementing" an anti-counterfeiting infrastructure at the European level. The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates.
Beijing and Shanghai are among the cities designated by China to trial blockchain applications as January 30, 2022. In Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the Hangzhou Internet Court in 2019 and has since been accepted by other Chinese courts.
With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner stated that "interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.
There are already several blockchain interoperability solutions available. They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.
Several individual IETF participants produced the draft of a blockchain interoperability architecture.
Some cryptocurrencies use blockchain mining, namely the peer-to-peer computer computations by which transactions are validated and verified. This requires a large amount of energy. In June 2018, the Bank for International Settlements criticized the use of public proof-of-work blockchains for their high energy consumption.
Early concern over the high energy consumption was a factor in later blockchains such as Cardano (2017), Solana (2020) and Polkadot (2020) adopting the less energy-intensive proof-of-stake model. Researchers have estimated that bitcoin consumes 100,000 times as much energy as proof-of-stake networks.
In 2021, a study by Cambridge University determined that bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh). According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.
In February 2021, U.S. Treasury secretary Janet Yellen called bitcoin "an extremely inefficient way to conduct transactions", saying "the amount of energy consumed in processing those transactions is staggering". In March 2021, Bill Gates stated that "Bitcoin uses more electricity per transaction than any other method known to mankind", adding "It's not a great climate thing."
Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate. The 31TWh-45TWh of electricity used for bitcoin in 2018 produced 17–23 million tonnes of CO2. By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year.
Some cryptocurrency developers are considering moving from the proof-of-work model to the proof-of-stake model. In Sept, 2022, Ethereum converted from proof-of-work to proof-of-stake.
In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology. Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became "one of the first big European universities to launch a blockchain course", according to the Financial Times.
=
Motivations for adopting blockchain technology (an aspect of innovation adoption) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis, and Koens & Poll pointed out that adoption could be heavily driven by non-technical factors. Based on behavioral models, Li has discussed the differences between adoption at the individual level and organizational levels.
=
Scholars in business and management have started studying the role of blockchains to support collaboration. It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements. In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators.
=
The need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats. Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address this transformational technology. New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, Blockchain and Internal Audit, assesses these factors. The American Institute of Certified Public Accountants has outlined new roles for auditors as a result of blockchain.
=
In blockchain technology, a testnet is an instance of a blockchain powered by the same or a newer version of the underlying software, to be used for testing and experimentation without risk to real funds or the main chain. Testnet coins are separate and distinct from the official mainnet coins, don't have value, and can be obtained freely from faucets.
Testnets allow for the development of blockchain applications without the risk of losing funds.
Using testnets, a bug was discovered in the Bitcoin Core software that gave miners the ability to take down essential parts of the Bitcoin infrastructure (nodes) by sending a 'bad' block to the blockchain.
=
A mainnet (short for main network) is the fully operational version of a blockchain where real transactions occur, as opposed to a testnet. It is secured through consensus mechanisms like Proof of Work or Proof of Stake and supports smart contracts, token transfers, and decentralized applications.
A mainnet launch marks the transition from a testnet to a live blockchain, involving security audits, network deployment, and token migration.
=
In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural issue was published in December 2016. The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies. The journal encourages authors to digitally sign a file hash of submitted papers, which are then timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.
Changelog – a record of all notable changes made to a project
Checklist – an informational aid used to reduce failure
Economics of digitization
Git – a version control system where the commit hash value depends on the previous commit hash value, also with Merkle tree inside
List of blockchains
Privacy and blockchain
Real-World Assets
Version control – a record of all changes (mostly of software project) in a form of a graph
 Media related to Blockchain at Wikimedia Commons

--- TOPIC: Cryptography ---
Cryptography, or cryptology, is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, information security, electrical engineering, digital signal processing, physics, and others. Core concepts related to information security (data confidentiality, data integrity, authentication and non-repudiation) are also central to cryptography. Practical applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords and military communications.
Cryptography prior to the modern age was effectively synonymous with encryption, converting readable information (plaintext) to unintelligible nonsense text (ciphertext), which can only be read by reversing the process (decryption). The sender of an encrypted (coded) message shares the decryption (decoding) technique only with the intended recipients to preclude access from adversaries. The cryptography literature often uses the names "Alice" (or "A") for the sender, "Bob" (or "B") for the intended recipient, and "Eve" (or "E") for the eavesdropping adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and their applications more varied.
Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed "computationally secure". Theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated and, if necessary, adapted. Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable but computationally secure schemes.
The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes with regard to digital media.
The first use of the term "cryptograph" (as opposed to "cryptogram") dates back to the 19th century – originating from "The Gold-Bug", a story by Edgar Allan Poe.
Until modern times, cryptography referred almost exclusively to "encryption", which is the process of converting ordinary information (called plaintext) into an unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a "key". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext. In formal mathematical terms, a "cryptosystem" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms that correspond to each key. Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks.
There are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message. Data manipulation in symmetric systems is significantly faster than in asymmetric systems. Asymmetric systems use a "public key" to encrypt a message and a related "private key" to decrypt it. The advantage of asymmetric systems is that the public key can be freely published, allowing parties to establish secure communication without having a shared secret key. In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie–Hellman key exchange, RSA (Rivest–Shamir–Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard). Insecure symmetric algorithms include children's language tangling schemes such as Pig Latin or other cant, and all historical cryptographic schemes, however seriously intended, prior to the invention of the one-time pad early in the 20th century.
In colloquial use, the term "code" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, "wallaby" replaces "attack at dawn"). A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, a syllable, or a pair of letters, etc.) to produce a cyphertext.
Cryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to "crack" encryption algorithms or their implementations.
Some use the terms "cryptography" and "cryptology" interchangeably in English, while others (including US military practice generally) use "cryptography" to refer specifically to the use and practice of cryptographic techniques and "cryptology" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which "cryptology" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.
The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics. Cryptolingusitics is especially used in military intelligence applications for deciphering foreign communications.
Before the modern era, cryptography focused on message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communication, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.
=
The main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., 'hello world' becomes 'ehlol owrdl' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., 'fly at once' becomes 'gmz bu podf' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter three positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (c. 1900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.
The Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military). Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave's shaved head and concealed under the regrown hair. Other steganography methods involve 'hiding in plain sight,' such as using a music cipher to disguise an encrypted message within a regular piece of sheet music. More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.
In India, the 2000-year-old Kama Sutra of Vātsyāyana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.
In Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the šāh-dabīrīya (literally "King's script") which was used for official correspondence, and the rāz-saharīya which was used to communicate secret messages with other countries.
David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.
Ciphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). The Arab mathematician and polymath Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.
Language letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.
Essentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti's innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel that implemented a partial realization of his invention. In the Vigenère cipher, a polyalphabetic cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigenère cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.
Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher's algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs's Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon's Maxim—'the enemy knows the system'.
Different physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher. In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti's own cipher disk, Johannes Trithemius' tabula recta scheme, and Thomas Jefferson's wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines—famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.
=
Cryptanalysis of the new mechanical ciphering devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitive tasks, such as military code breaking (decryption). This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.
Extensive open academic research into cryptography is relatively recent, beginning in the mid-1970s. In the early 1970s IBM personnel designed the Data Encryption Standard (DES) algorithm that became the first federal government cryptography standard in the United States. In 1976 Whitfield Diffie and Martin Hellman published the Diffie–Hellman key exchange algorithm. In 1977 the RSA algorithm was published in Martin Gardner's Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally.
Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so, proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers. There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring n = pq is impossible; it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem.
As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing. The potential impact of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography. The announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative.
Claude Shannon's two papers, his 1948 paper on information theory, and especially his 1949 paper on cryptography, laid the foundations of modern cryptography and provided a mathematical basis for future cryptography. His 1949 paper has been noted as having provided a "solid theoretical basis for cryptography and for cryptanalysis", and as having turned cryptography from an "art to a science". As a result of his contributions and work, he has been described as the "founding father of modern cryptography".
Prior to the early 20th century, cryptography was mainly concerned with linguistic and lexicographic patterns. Since then cryptography has broadened in scope, and now makes extensive use of mathematical subdisciplines, including information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics.
Just as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.
Research into post-quantum cryptography (PQC) has intensified because practical quantum computers would break widely deployed public-key systems such as RSA, Diffie–Hellman and ECC. A 2017 review in Nature surveys the leading PQC families—lattice-based, code-based, multivariate-quadratic and hash-based schemes—and stresses that standardisation and deployment should proceed well before large-scale quantum machines become available.
=
Symmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.
Symmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.
The Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES's designation was finally withdrawn after the AES was adopted). Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption to e-mail privacy and secure remote access. Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL.
Stream ciphers, in contrast to the 'block' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher. Block ciphers can be used as stream ciphers by generating blocks of a keystream (in place of a Pseudorandom number generator) and applying an XOR operation to each bit of the plaintext with each bit of the keystream.
Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort. Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it "prudent" from a security perspective to develop a new standard to "significantly improve the robustness of NIST's overall hash algorithm toolkit." Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.
=
Symmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret.
In a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used—a public key and a private key. A public key system is so constructed that calculation of one key (the 'private key') is computationally infeasible from the other (the 'public key'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as "the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance".
In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. The public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie–Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.
The X.509 standard defines the most commonly used format for public key certificates.
Diffie and Hellman's publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.
The Diffie–Hellman and RSA algorithms, in addition to being the first publicly known examples of high-quality public-key algorithms, have been among the most widely used. Other asymmetric-key algorithms include the Cramer–Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques.
A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that was very similar in design rationale to RSA. In 1974, Malcolm J. Williamson is claimed to have developed the Diffie–Hellman key exchange.
Public-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be 'moved' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).
Public-key algorithms are most often based on the computational complexity of "hard" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie–Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.
=
Cryptographic hash functions are functions that take a variable-length input and return a fixed-length output, which can be used in, for example, a digital signature. For a hash function to be secure, it must be difficult to compute two inputs that hash to the same value (collision resistance) and to compute an input that hashes to a given output (preimage resistance). MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it "prudent" from a security perspective to develop a new standard to "significantly improve the robustness of NIST's overall hash algorithm toolkit." Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.
=
The goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.
It is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., "work factor", in Shannon's terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible.
There are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forward it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved).
Cryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations. This is a considerable improvement over brute force attacks.
Public-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time (P) using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, to achieve an equivalent strength of encryption, techniques that depend upon the difficulty of factoring large composite numbers, such as the RSA cryptosystem, require larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.
While pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, they may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, rubber-hose cryptanalysis or torture) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin.
=
Much of the theoretical work in cryptography concerns cryptographic primitives—algorithms with basic cryptographic properties—and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.
=
One or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system's security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem's structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols.
Some widely known cryptosystems include RSA, Schnorr signature, ElGamal encryption, and Pretty Good Privacy (PGP). More complex cryptosystems include electronic cash systems, signcryption systems, etc. Some more 'theoretical' cryptosystems include interactive proof systems, (like zero-knowledge proofs) and systems for secret sharing.
=
Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. The growth of Internet of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security. Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed to achieve the standard set by the National Institute of Standards and Technology.
Cryptography is widely used on the internet to help protect user-data and prevent eavesdropping. To ensure secrecy during transmission, many systems use private key cryptography to protect transmitted information. With public-key systems, one can maintain secrecy without a master key or a large number of keys. But, some algorithms like BitLocker and VeraCrypt are generally not private-public key cryptography. For example, Veracrypt uses a password hash to generate the single private key. However, it can be configured to run in public-private key systems. The C++ opensource encryption library OpenSSL provides free and opensource encryption software and tools. The most commonly used encryption cipher suit is AES, as it has hardware acceleration for all x86 based processors that has AES-NI. A close contender is ChaCha20-Poly1305, which is a stream cipher, however it is commonly used for mobile devices as they are ARM based which does not feature AES-NI instruction set extension.
=
Cryptography can be used to secure communications by encrypting them. Websites use encryption via HTTPS. "End-to-end" encryption, where only sender and receiver can read messages, is implemented for email in Pretty Good Privacy and for secure messaging in general in WhatsApp, Signal and Telegram.
Operating systems use encryption to keep passwords secret, conceal parts of the system, and ensure that software updates are truly from the system maker. Instead of storing plaintext passwords, computer systems store hashes thereof; then, when a user logs in, the system passes the given password through a cryptographic hash function and compares it to the hashed value on file. In this manner, neither the system nor an attacker has at any point access to the password in plaintext.
Encryption is sometimes used to encrypt one's entire drive. For example, University College London has implemented BitLocker (a program by Microsoft) to render drive data opaque without users logging in.
=
Cryptographic techniques enable cryptocurrency technologies, such as distributed ledger technologies (e.g., blockchains), which finance cryptoeconomics applications such as decentralized finance (DeFi). Key cryptographic techniques that enable cryptocurrencies and cryptoeconomics include, but are not limited to: cryptographic keys, cryptographic hash function, asymmetric (public key) encryption, Multi-Factor Authentication (MFA), End-to-End Encryption (E2EE), and Zero Knowledge Proofs (ZKP).
=
Estimates suggest that a quantum computer could reduce the effort required to break today’s strongest RSA or elliptic-curve keys from millennia to mere seconds, rendering current protocols (such as the versions of TLS that rely on those keys) insecure.
To mitigate this "quantum threat", researchers are developing quantum-resistant algorithms whose security rests on problems believed to remain hard for both classical and quantum computers.
=
Cryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible.
In some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.
In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.
=
In the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann's Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.
In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and "dual-use" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users do not realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally do not find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.
=
Another contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA's request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.
Another instance of the NSA's involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs's Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).
=
Cryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.
The United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech.
=
In the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.
In the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.
In many jurisdictions, the legal status of forced disclosure remains unclear.
The 2016 FBI–Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers' assistance in unlocking cell phones whose contents are cryptographically protected.
As a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).
Collision attack
Comparison of cryptography libraries
Cryptovirology – Securing and encrypting virology
Crypto Wars – Attempts to limit access to strong cryptography
Encyclopedia of Cryptography and Security – Book by Technische Universiteit Eindhoven
Global surveillance – Mass surveillance across national borders
Indistinguishability obfuscation – Type of cryptographic software obfuscation
Information theory – Scientific study of digital information
Outline of cryptography
List of cryptographers
List of multiple discoveries
List of cryptography books
List of open-source Cypherpunk software
List of unsolved problems in computer science – List of unsolved computational problems
Pre-shared key – Method to set encryption keys
Secure cryptoprocessor
Strong cryptography – Term applied to cryptographic systems that are highly resistant to cryptanalysis
Syllabical and Steganographical Table – Eighteenth-century work believed to be the first cryptography chart – first cryptography chart
World Wide Web Consortium's Web Cryptography API – World Wide Web Consortium cryptography standard
 The dictionary definition of cryptography at Wiktionary
 Media related to Cryptography at Wikimedia Commons
Cryptography on In Our Time at the BBC
Crypto Glossary and Dictionary of Technical Cryptography Archived 4 July 2022 at the Wayback Machine
A Course in Cryptography by Raphael Pass & Abhi Shelat – offered at Cornell in the form of lecture notes.
For more on the use of cryptographic elements in fiction, see: Dooley, John F. (23 August 2012). "Cryptology in Fiction". Archived from the original on 29 July 2020. Retrieved 20 February 2015.
The George Fabyan Collection at the Library of Congress has early editions of works of seventeenth-century English literature, publications relating to cryptography.

--- TOPIC: Cloud_computing ---
Cloud computing is defined by the ISO as "a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on demand". It is commonly referred to as "the cloud".
In 2011, the National Institute of Standards and Technology (NIST) identified five "essential characteristics" for cloud systems. Below are the exact definitions according to NIST:
On-demand self-service: "A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider."
Broad network access: "Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations)."
Resource pooling: " The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand."
Rapid elasticity: "Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time."
Measured service: "Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.
By 2023, the International Organization for Standardization (ISO) had expanded and refined the list.
The history of cloud computing extends to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The "data center" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This period saw broad experimentation with making large-scale computing power more accessible through time-sharing, while optimizing infrastructure, platforms, and applications to improve efficiency for end users.
The "cloud" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of "places" that mobile agents in the Telescript environment could "go". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom. The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with "cloud computing-enabled applications". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.
In the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.
The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.
Cloud computing can shorten time to market by offering pre-configured tools, scalable resources, and managed services, allowing users to focus on core business value rather than maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment.
While cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection.
Cloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees.
Cloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model—Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)—with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services.
The decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization.
Organizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups.
On the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarly, tech giants like Google, Meta, and Amazon build their own data centers due to economies of scale, predictable workloads, and the ability to customize hardware and network infrastructure for optimal efficiency. However, these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs.
In practice, many organizations are increasingly adopting hybrid cloud architectures, combining on-premises infrastructure with cloud services. This approach allows businesses to balance scalability, cost-effectiveness, and control, offering the benefits of both deployment models while mitigating their respective limitations.
One of the primary challenges of cloud computing, compared with traditional on-premises systems, is maintaining data security and privacy. Cloud users entrust their sensitive data to third-party providers, who may not have adequate measures to protect it from unauthorized access, breaches, or leaks. Cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection, such as GDPR or HIPAA.
Another challenge of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences. Complete understanding of all technology may be impossible, especially given the scale, complexity, and deliberate opacity of contemporary systems; however, there is a need for understanding complex technologies and their interconnections to have power and agency within them. The metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous; it is something experienced without precisely understanding what it is or how it works.
Additionally, cloud migration is a significant challenge. This process involves transferring data, applications, or workloads from one cloud environment to another, or from on-premises infrastructure to the cloud. Cloud migration can be complicated, time-consuming, and expensive, particularly when there are compatibility issues between different cloud platforms or architectures. If not carefully planned and executed, cloud migration can lead to downtime, reduced performance, or even data loss.
=
According to the 2024 State of the Cloud Report by Flexera, approximately 50% of respondents identified the following top challenges when migrating workloads to public clouds:
"Understanding application dependencies"
"Comparing on-premise and cloud costs"
"Assessing technical feasibility."
=
Applications hosted in the cloud are susceptible to the fallacies of distributed computing, a series of misconceptions that can lead to significant issues in software development and deployment.
=
In a report by Gartner, a survey of 200 IT leaders revealed that 69% experienced budget overruns in their organizations' cloud expenditures during 2023. Conversely, 31% of IT leaders whose organizations stayed within budget attributed their success to accurate forecasting and budgeting, proactive monitoring of spending, and effective optimization.
The 2024 Flexera State of Cloud Report identifies the top cloud challenges as managing cloud spend, followed by security concerns and lack of expertise. Public cloud expenditures exceeded budgeted amounts by an average of 15%. The report also reveals that cost savings is the top cloud initiative for 60% of respondents. Furthermore, 65% measure cloud progress through cost savings, while 42% prioritize shorter time-to-market, indicating that cloud's promise of accelerated deployment is often overshadowed by cost concerns.
=
Typically, cloud providers' Service Level Agreements (SLAs) do not encompass all forms of service interruptions. Exclusions typically include planned maintenance, downtime resulting from external factors such as network issues, human errors, like misconfigurations, natural disasters, force majeure events, or security breaches. Typically, customers bear the responsibility of monitoring SLA compliance and must file claims for any unmet SLAs within a designated timeframe. Customers should be aware of how deviations from SLAs are calculated, as these parameters may vary by service. These requirements can place a considerable burden on customers. Additionally, SLA percentages and conditions can differ across various services within the same provider, with some services lacking any SLA altogether. In cases of service interruptions due to hardware failures in the cloud provider, the company typically does not offer monetary compensation. Instead, eligible users may receive credits as outlined in the corresponding SLA.
=
Cloud computing abstractions aim to simplify resource management, but leaky abstractions can expose underlying complexities. These variations in abstraction quality depend on the cloud vendor, service and architecture. Mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize.
=
Service lock-in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor, making it challenging to switch to alternative services within the same vendor when their needs change.
=
Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information. Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored. Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access. Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities.
According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. "There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called "hyperjacking". Some examples of this include the Dropbox security breach, and iCloud 2014 leak. Dropbox had been breached in October 2014, having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).
There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership. Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services. Some small businesses that do not have expertise in IT security could find that it is more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes do not read the many pages of the terms of service agreement, and just click "Accept" without reading). This is important now that cloud computing is common and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Assistant). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.
The attacks that can be made on cloud computing systems include man-in-the middle attacks, phishing attacks, authentication attacks, and malware attacks. One of the largest threats is considered to be malware attacks, such as Trojan horses. Recent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems.
=
The CLOUD Act allows United States authorities to request data from cloud providers, and courts can impose nondisclosure requirements preventing providers from notifying affected users. This framework is in legal tension with Article 48 of the European General Data Protection Regulation (GDPR), which restricts the transfer of personal data in response to foreign court or administrative orders unless based on an international agreement. As a result, cloud service providers operating in both Europe and the U.S. may face competing legal obligations.
According to Laura K. Donohue writing for the Harvard Journal of Law and Public Policy, cloud service providers also fall within the broader category of service providers subject to Section 702 of the Foreign Intelligence Surveillance Act (FISA), which has had documented effects on cloud providers and their customers.
The National Institute of Standards and Technology recognized three cloud service models in 2011: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). The International Organization for Standardization (ISO) later identified additional models in 2023, including "Network as a Service", "Communications as a Service", "Compute as a Service", and "Data Storage as a Service".
=
Infrastructure as a service (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.
The NIST's definition of cloud computing describes IaaS as "where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls)."
IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the number of resources allocated and consumed.
=
The NIST's definition of cloud computing defines Platform as a Service as:
The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment.
PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including an operating system, programming-language execution environment, database, and the web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.
Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows. Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware. dPaaS delivers integration—and data-management—products as a fully managed service. Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.
=
The NIST's definition of cloud computing defines Software as a Service as:
The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings.
In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as "on-demand software" and is usually priced on a pay-per-use basis or using a subscription fee. In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand. Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.
The pricing model for SaaS applications is typically a monthly or yearly flat fee per user, so prices become scalable and adjustable if users are added or removed at any point. It may also be free. Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result, there could be unauthorized access to the data. Examples of applications offered as SaaS are games and productivity software like Google Docs and Office Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive, and Office Online being integrated with OneDrive.
=
Serverless computing allows customers to use various cloud capabilities without the need to provision, deploy, or manage hardware or software resources, apart from providing their application code or data. ISO/IEC 22123-2:2023 classifies serverless alongside Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) under the broader category of cloud service categories. Notably, while ISO refers to these classifications as cloud service categories, the National Institute of Standards and Technology (NIST) refers to them as service models.
"A cloud deployment model represents the way in which cloud computing can be organized based on the control and sharing of physical or virtual resources." Cloud deployment models define the fundamental patterns of interaction between cloud customers and cloud providers. They do not detail implementation specifics or the configuration of resources.
=
Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally. Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users "still have to buy, build, and manage them" and thus do not benefit from less hands-on management, essentially "[lacking] the economic model that makes cloud computing such an intriguing concept".
=
Cloud services are considered "public" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge. Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.
Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.
=
Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources, that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed or dedicated services with cloud resources. Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers. A hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.
Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service. This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.
Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud. This capability enables hybrid clouds to employ cloud bursting for scaling across clouds. Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and "bursts" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed. Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.
=
Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether it is managed internally or by a third-party, and hosted internally or externally, the costs are distributed among fewer users compared to a public cloud (but more than a private cloud). As a result, only a portion of the potential cost savings of cloud computing is achieved.
=
According to ISO/IEC 22123-1: "multi-cloud is a cloud deployment model in which a customer uses public cloud services provided by two or more cloud service providers".   Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more than could be done with a single provider.
According to International Data Corporation (IDC), global spending on cloud computing services has reached $706 billion and is expected to reach $1.3 trillion by 2025. Gartner estimated that global public cloud services end-user spending would reach $600 billion by 2023. According to a McKinsey & Company report, cloud cost-optimization levers and value-oriented business use cases foresee more than $1 trillion in run-rate EBITDA across Fortune 500 companies as up for grabs in 2030. In 2022, more than $1.3 trillion in enterprise IT spending was at stake from the shift to the cloud, growing to almost $1.8 trillion in 2025, according to Gartner.
The European Commission's 2012 Communication identified several issues which were impeding the development of the cloud computing market:
fragmentation of the digital single market across the EU
concerns about contracts including reservations about data access and ownership, data portability, and change control
variations in standards applicable to cloud computing
The Communication set out a series of "digital agenda actions" which the Commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services.
As of 2025, the three largest cloud computing providers by market share, commonly referred to as hyperscalers, are Amazon Web Services (AWS), Microsoft Azure, and Google Cloud. These companies dominate the global cloud market due to their extensive infrastructure, broad service offerings, and scalability. 
In recent years, organizations have increasingly adopted alternative cloud providers, which offer specialized services that distinguish them from hyperscalers. These providers may offer advantages such as lower costs, improved cost transparency and predictability, enhanced data sovereignty (particularly within regions such as the European Union to comply with regulations like the General Data Protection Regulation (GDPR)), stronger alignment with local regulatory requirements, or industry-specific services.
Alternative cloud providers are often part of multi-cloud strategies, where organizations use multiple cloud services—both from hyperscalers and specialized providers—to optimize performance, compliance, and cost efficiency. However, they do not necessarily serve as direct replacements for hyperscalers, as their offerings are typically more specialized.
The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more "virtual" devices, each of which can be easily used and managed to perform computing tasks. With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.
Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.
Cloud computing shares characteristics with:
Client–server model – Client–server computing refers broadly to any distributed application that distinguishes between service providers (servers) and service requestors (clients).
Computer bureau – A service bureau providing computer services, particularly from the 1960s to 1980s.
Grid computing – A form of distributed and parallel computing, whereby a 'super and virtual computer' is composed of a cluster of networked, loosely coupled computers acting in concert to perform very large tasks.
Fog computing – Distributed computing paradigm that provides data, compute, storage and application services closer to the client or near-user edge devices, such as network routers. Furthermore, fog computing handles data at the network level, on smart devices and on the end-user client-side (e.g. mobile devices), instead of sending data to a remote location for processing.
Utility computing – The "packaging of computing resources, such as computation and storage, as a metered service similar to a traditional public utility, such as electricity."
Peer-to-peer – A distributed architecture without the need for central coordination. Participants are both suppliers and consumers of resources (in contrast to the traditional client-server model).
Cloud sandbox – A live, isolated computer environment in which a program, code or file can run without affecting the application in which it runs.
Millard, Christopher (2013). Cloud Computing Law. Oxford University Press. ISBN 978-0-19-967168-7.
Weisser, Alexander (2020). International Taxation of Cloud Computing. Editions Juridiques Libres, ISBN 978-2-88954-030-3.
Singh, Jatinder; Powles, Julia; Pasquier, Thomas; Bacon, Jean (July 2015). "Data Flow Management and Compliance in Cloud Computing". IEEE Cloud Computing. 2 (4): 24–32. doi:10.1109/MCC.2015.69. S2CID 9812531.
Armbrust, Michael; Stoica, Ion; Zaharia, Matei; Fox, Armando; Griffith, Rean; Joseph, Anthony D.; Katz, Randy; Konwinski, Andy; Lee, Gunho; Patterson, David; Rabkin, Ariel (1 April 2010). "A view of cloud computing". Communications of the ACM. 53 (4): 50. doi:10.1145/1721654.1721672. S2CID 1673644.
Hu, Tung-Hui (2015). A Prehistory of the Cloud. MIT Press. ISBN 978-0-262-02951-3.
Mell, P. (2011, September). The NIST Definition of Cloud Computing. Retrieved November 1, 2015, from National Institute of Standards and Technology website
 Media related to Cloud computing at Wikimedia Commons

--- TOPIC: Cybersecurity ---


--- TOPIC: History_of_the_world ---


--- TOPIC: Philosophy ---
Philosophy (from Ancient Greek philosophía lit. 'love of wisdom') is a systematic study of general and fundamental questions concerning topics like existence, knowledge, mind, reason, language, and value. It is a rational and critical inquiry that reflects on its methods and assumptions.
Historically, many of the individual sciences, such as physics and psychology, formed part of philosophy. However, they are considered separate academic disciplines in the modern sense of the term. Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Western philosophy originated in Ancient Greece and covers a wide area of philosophical subfields. A central topic in Arabic–Persian philosophy is the relation between reason and revelation. Indian philosophy combines the spiritual problem of how to reach enlightenment with the exploration of the nature of reality and the ways of arriving at knowledge. Chinese philosophy focuses principally on practical issues about right social conduct, government, and self-cultivation.
Major branches of philosophy are epistemology, ethics, logic, and metaphysics. Epistemology studies what knowledge is and how to acquire it. Ethics investigates moral principles and what constitutes right conduct. Logic is the study of correct reasoning and explores how good arguments can be distinguished from bad ones. Metaphysics examines the most general features of reality, existence, objects, and properties. Other subfields are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, philosophy of mathematics, philosophy of history, and political philosophy. Within each branch, there are competing schools of philosophy that promote different principles, theories, or methods.
Philosophers use a great variety of methods to arrive at philosophical knowledge. They include conceptual analysis, reliance on common sense and intuitions, use of thought experiments, analysis of ordinary language, description of experience, and critical questioning. Philosophy is related to many other fields, such as the natural and social sciences, mathematics, business, law, and journalism. It provides an interdisciplinary perspective and studies the scope and fundamental concepts of these fields. It also investigates their methods and ethical implications.
The word philosophy comes from the Ancient Greek words φίλος (philos) 'love' and σοφία (sophia) 'wisdom'. Some sources say that the term was coined by the pre-Socratic philosopher Pythagoras, but this is not certain.
The word entered the English language primarily from Old French and Anglo-Norman starting around 1175 CE. The French philosophie is itself a borrowing from the Latin philosophia. The term philosophy acquired the meanings of "advanced study of the speculative subjects (logic, ethics, physics, and metaphysics)", "deep wisdom consisting of love of truth and virtuous living", "profound learning as transmitted by the ancient writers", and "the study of the fundamental nature of knowledge, reality, and existence, and the basic limits of human understanding".
Before the modern age, the term philosophy was used in a wide sense. It included most forms of rational inquiry, such as the individual sciences, as its subdisciplines. For instance, natural philosophy was a major branch of philosophy. This branch of philosophy encompassed a wide range of fields, including disciplines like physics, chemistry, and biology. An example of this usage is the 1687 book Philosophiæ Naturalis Principia Mathematica by Isaac Newton. This book referred to natural philosophy in its title, but it is today considered a book of physics.
The meaning of philosophy changed toward the end of the modern period when it acquired the more narrow meaning common today. In this new sense, the term is mainly associated with disciplines like metaphysics, epistemology, and ethics. Among other topics, it covers the rational study of reality, knowledge, and values. It is distinguished from other disciplines of rational inquiry such as the empirical sciences and mathematics.
=
The practice of philosophy is characterized by several general features: it is a form of rational inquiry, it aims to be systematic, and it tends to critically reflect on its own methods and presuppositions. It requires attentively thinking long and carefully about the provocative, vexing, and enduring problems central to the human condition.
The philosophical pursuit of wisdom involves asking general and fundamental questions. It often does not result in straightforward answers but may help a person to better understand the topic, examine their life, dispel confusion, and overcome prejudices and self-deceptive ideas associated with common sense. For example, Socrates stated that "the unexamined life is not worth living" to highlight the role of philosophical inquiry in understanding one's own existence. And according to Bertrand Russell, "the man who has no tincture of philosophy goes through life imprisoned in the prejudices derived from common sense, from the habitual beliefs of his age or his nation, and from convictions which have grown up in his mind without the cooperation or consent of his deliberate reason."
=
Attempts to provide more precise definitions of philosophy are controversial and are studied in metaphilosophy. Some approaches argue that there is a set of essential features shared by all parts of philosophy. Others see only weaker family resemblances or contend that it is merely an empty blanket term. Precise definitions are often only accepted by theorists belonging to a certain philosophical movement and are revisionistic according to Søren Overgaard et al. in that many presumed parts of philosophy would not deserve the title "philosophy" if they were true.
Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: "What can I know?"; "What should I do?"; "What may I hope?"; and "What is the human being?" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines.
Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as "the midwife of the sciences".
Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language.
Phenomenologists, such as Edmund Husserl, characterize philosophy as a "rigorous science" investigating essences. They practice a radical suspension of theoretical assumptions about reality to get back to the "things themselves", that is, as originally given in experience. They contend that this base-level of experience provides the foundation for higher-order theoretical knowledge, and that one needs to understand the former to understand the latter.
An early approach found in ancient Greek and Roman philosophy is that philosophy is the spiritual practice of developing one's rational capacities. This practice is an expression of the philosopher's love of wisdom and has the aim of improving one's well-being by leading a reflective life. For example, the Stoics saw philosophy as an exercise to train the mind and thereby achieve eudaimonia and flourish in life.
As a discipline, the history of philosophy aims to provide a systematic and chronological exposition of philosophical concepts and doctrines. Some theorists see it as a part of intellectual history, but it also investigates questions not covered by intellectual history such as whether the theories of past philosophers are true and have remained philosophically relevant. The history of philosophy is primarily concerned with theories based on rational inquiry and argumentation; some historians understand it in a looser sense that includes myths, religious teachings, and proverbial lore.
Influential traditions in the history of philosophy include Western, Arabic–Persian, Indian, and Chinese philosophy. Other philosophical traditions are Japanese philosophy, Latin American philosophy, and African philosophy.
=
Western philosophy originated in Ancient Greece in the 6th century BCE with the pre-Socratics. They attempted to provide rational explanations of the cosmos as a whole. The philosophy following them was shaped by Socrates (469–399 BCE), Plato (427–347 BCE), and Aristotle (384–322 BCE). They expanded the range of topics to questions like how people should act, how to arrive at knowledge, and what the nature of reality and mind is. The later part of the ancient period was marked by the emergence of philosophical movements, for example, Epicureanism, Stoicism, Skepticism, and Neoplatonism. The medieval period started in the 5th century CE. Its focus was on religious topics and many thinkers used ancient philosophy to explain and further elaborate Christian doctrines.
The Renaissance period started in the 14th century and saw a renewed interest in schools of ancient philosophy, in particular Platonism. Humanism also emerged in this period. The modern period started in the 17th century. One of its central concerns was how philosophical and scientific knowledge are created. Specific importance was given to the role of reason and sensory experience. Many of these innovations were used in the Enlightenment movement to challenge traditional authorities. Several attempts to develop comprehensive systems of philosophy were made in the 19th century, for instance, by German idealism and Marxism. Influential developments in 20th-century philosophy were the emergence and application of formal logic, the focus on the role of language as well as pragmatism, and movements in continental philosophy like phenomenology, existentialism, and post-structuralism. The 20th century saw a rapid expansion of academic philosophy in terms of the number of philosophical publications and philosophers working at academic institutions. There was also a noticeable growth in the number of female philosophers, but they still remained underrepresented.
=
Arabic–Persian philosophy arose in the early 9th century CE as a response to discussions in the Islamic theological tradition. Its classical period lasted until the 12th century CE and was strongly influenced by ancient Greek philosophers. It employed their ideas to elaborate and interpret the teachings of the Quran.
Al-Kindi (801–873 CE) is usually regarded as the first philosopher of this tradition. He translated and interpreted many works of Aristotle and Neoplatonists in his attempt to show that there is a harmony between reason and faith. Avicenna (980–1037 CE) also followed this goal and developed a comprehensive philosophical system to provide a rational understanding of reality encompassing science, religion, and mysticism. Al-Ghazali (1058–1111 CE) was a strong critic of the idea that reason can arrive at a true understanding of reality and God. He formulated a detailed critique of philosophy and tried to assign philosophy a more limited place besides the teachings of the Quran and mystical insight. Following Al-Ghazali and the end of the classical period, the influence of philosophical inquiry waned. Mulla Sadra (1571–1636 CE) is often regarded as one of the most influential philosophers of the subsequent period. The increasing influence of Western thought and institutions in the 19th and 20th centuries gave rise to the intellectual movement of Islamic modernism, which aims to understand the relation between traditional Islamic beliefs and modernity.
=
One of the distinguishing features of Indian philosophy is that it integrates the exploration of the nature of reality, the ways of arriving at knowledge, and the spiritual question of how to reach enlightenment. It started around 900 BCE when the Vedas were written. They are the foundational scriptures of Hinduism and contemplate issues concerning the relation between the self and ultimate reality as well as the question of how souls are reborn based on their past actions. This period also saw the emergence of non-Vedic teachings, like Buddhism and Jainism. Buddhism was founded by Gautama Siddhartha (563–483 BCE), who challenged the Vedic idea of a permanent self and proposed a path to liberate oneself from suffering. Jainism was founded by Mahavira (599–527 BCE), who emphasized non-violence as well as respect toward all forms of life.
The subsequent classical period started roughly 200 BCE and was characterized by the emergence of the six orthodox schools of Hinduism: Nyāyá, Vaiśeṣika, Sāṃkhya, Yoga, Mīmāṃsā, and Vedanta. The school of Advaita Vedanta developed later in this period. It was systematized by Adi Shankara (c. 700–750 CE), who held that everything is one and that the impression of a universe consisting of many distinct entities is an illusion. A slightly different perspective was defended by Ramanuja (1017–1137 CE), who founded the school of Vishishtadvaita Vedanta and argued that individual entities are real as aspects or parts of the underlying unity. He also helped to popularize the Bhakti movement, which taught devotion toward the divine as a spiritual path and lasted until the 17th to 18th centuries CE. The modern period began roughly 1800 CE and was shaped by encounters with Western thought. Philosophers tried to formulate comprehensive systems to harmonize diverse philosophical and religious teachings. For example, Swami Vivekananda (1863–1902 CE) used the teachings of Advaita Vedanta to argue that all the different religions are valid paths toward the one divine.
=
Chinese philosophy is particularly interested in practical questions associated with right social conduct, government, and self-cultivation. Many schools of thought emerged in the 6th century BCE in competing attempts to resolve the political turbulence of that period. The most prominent among them were Confucianism and Daoism. Confucianism was founded by Confucius (551–479 BCE). It focused on different forms of moral virtues and explored how they lead to harmony in society. Daoism was founded by Laozi (6th century BCE) and examined how humans can live in harmony with nature by following the Dao or the natural order of the universe. Other influential early schools of thought were Mohism, which developed an early form of altruistic consequentialism, and Legalism, which emphasized the importance of a strong state and strict laws.
Buddhism was introduced to China in the 1st century CE and diversified into new forms of Buddhism. Starting in the 3rd century CE, the school of Xuanxue emerged. It interpreted earlier Daoist works with a specific emphasis on metaphysical explanations. Neo-Confucianism developed in the 11th century CE. It systematized previous Confucian teachings and sought a metaphysical foundation of ethics. The modern period in Chinese philosophy began in the early 20th century and was shaped by the influence of and reactions to Western philosophy. The emergence of Chinese Marxism—which focused on class struggle, socialism, and communism—resulted in a significant transformation of the political landscape. Another development was the emergence of New Confucianism, which aims to modernize and rethink Confucian teachings to explore their compatibility with democratic ideals and modern science.
=
Traditional Japanese philosophy assimilated and synthesized ideas from different traditions, including the indigenous Shinto religion and Chinese and Indian thought in the forms of Confucianism and Buddhism, both of which entered Japan in the 6th and 7th centuries. Its practice is characterized by active interaction with reality rather than disengaged examination. Neo-Confucianism became an influential school of thought in the 16th century and the following Edo period and prompted a greater focus on language and the natural world. The Kyoto School emerged in the 20th century and integrated Eastern spirituality with Western philosophy in its exploration of concepts like absolute nothingness (zettai-mu), place (basho), and the self.
Latin American philosophy in the pre-colonial period was practiced by indigenous civilizations and explored questions concerning the nature of reality and the role of humans. It has similarities to indigenous North American philosophy, which covered themes such as the interconnectedness of all things. Latin American philosophy during the colonial period, starting around 1550, was dominated by religious philosophy in the form of scholasticism. Influential topics in the post-colonial period were positivism, the philosophy of liberation, and the exploration of identity and culture.
Early African philosophy was primarily conducted and transmitted orally. It focused on community, morality, and ancestral ideas, encompassing folklore, wise sayings, religious ideas, and philosophical concepts like Ubuntu. Systematic African philosophy emerged at the beginning of the 20th century. It discusses topics such as ethnophilosophy, négritude, pan-Africanism, Marxism, postcolonialism, the role of cultural identity, relativism, African epistemology, and the critique of Eurocentrism.
Philosophical questions can be grouped into several branches. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. Epistemology, ethics, logic, and metaphysics are sometimes listed as the main branches. There are many other subfields besides them and the different divisions are neither exhaustive nor mutually exclusive. For example, political philosophy, ethics, and aesthetics are sometimes linked under the general heading of value theory as they investigate normative or evaluative aspects. Furthermore, philosophical inquiry sometimes overlaps with other disciplines in the natural and social sciences, religion, and mathematics.
=
Epistemology is the branch of philosophy that studies knowledge. It is also known as theory of knowledge and aims to understand what knowledge is, how it arises, what its limits are, and what value it has. It further examines the nature of truth, belief, justification, and rationality. Some of the questions addressed by epistemologists include "By what method(s) can one acquire knowledge?"; "How is truth established?"; and "Can we prove causal relations?"
Epistemology is primarily interested in declarative knowledge or knowledge of facts, like knowing that Princess Diana died in 1997. But it also investigates practical knowledge, such as knowing how to ride a bicycle, and knowledge by acquaintance, for example, knowing a celebrity personally.
One area in epistemology is the analysis of knowledge. It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: it is a belief that is justified and true. This theory is controversial and the difficulties associated with it are known as the Gettier problem. Alternative views state that knowledge requires additional components, like the absence of luck; different components, like the manifestation of cognitive virtues instead of justification; or they deny that knowledge can be analyzed in terms of other phenomena.
Another area in epistemology asks how people acquire knowledge. Often-discussed sources of knowledge are perception, introspection, memory, inference, and testimony. According to empiricists, all knowledge is based on some form of experience. Rationalists reject this view and hold that some forms of knowledge, like innate knowledge, are not acquired through experience. The regress problem is a common issue in relation to the sources of knowledge and the justification they offer. It is based on the idea that beliefs require some kind of reason or evidence to be justified. The problem is that the source of justification may itself be in need of another source of justification. This leads to an infinite regress or circular reasoning. Foundationalists avoid this conclusion by arguing that some sources can provide justification without requiring justification themselves. Another solution is presented by coherentists, who state that a belief is justified if it coheres with other beliefs of the person.
Many discussions in epistemology touch on the topic of philosophical skepticism, which raises doubts about some or all claims to knowledge. These doubts are often based on the idea that knowledge requires absolute certainty and that humans are unable to acquire it.
=
Ethics, also known as moral philosophy, studies what constitutes right conduct. It is also concerned with the moral evaluation of character traits and institutions. It explores what the standards of morality are and how to live a good life. Philosophical ethics addresses such basic questions as "Are moral obligations relative?"; "Which has priority: well-being or obligation?"; and "What gives life meaning?"
The main branches of ethics are meta-ethics, normative ethics, and applied ethics. Meta-ethics asks abstract questions about the nature and sources of morality. It analyzes the meaning of ethical concepts, like right action and obligation. It also investigates whether ethical theories can be true in an absolute sense and how to acquire knowledge of them. Normative ethics encompasses general theories of how to distinguish between right and wrong conduct. It helps guide moral decisions by examining what moral obligations and rights people have. Applied ethics studies the consequences of the general theories developed by normative ethics in specific situations, for example, in the workplace or for medical treatments.
Within contemporary normative ethics, consequentialism, deontology, and virtue ethics are influential schools of thought. Consequentialists judge actions based on their consequences. One such view is utilitarianism, which argues that actions should increase overall happiness while minimizing suffering. Deontologists judge actions based on whether they follow moral duties, such as abstaining from lying or killing. According to them, what matters is that actions are in tune with those duties and not what consequences they have. Virtue theorists judge actions based on how the moral character of the agent is expressed. According to this view, actions should conform to what an ideally virtuous agent would do by manifesting virtues like generosity and honesty.
=
Logic is the study of correct reasoning. It aims to understand how to distinguish good from bad arguments. It is usually divided into formal and informal logic. Formal logic uses artificial languages with a precise symbolic representation to investigate arguments. In its search for exact criteria, it examines the structure of arguments to determine whether they are correct or incorrect. Informal logic uses non-formal criteria and standards to assess the correctness of arguments. It relies on additional factors such as content and context.
Logic examines a variety of arguments. Deductive arguments are mainly studied by formal logic. An argument is deductively valid if the truth of its premises ensures the truth of its conclusion. Deductively valid arguments follow a rule of inference, like modus ponens, which has the following logical form: "p; if p then q; therefore q". An example is the argument "today is Sunday; if today is Sunday then I don't have to go to work today; therefore I don't have to go to work today".
The premises of non-deductive arguments also support their conclusion, although this support does not guarantee that the conclusion is true. One form is inductive reasoning. It starts from a set of individual cases and uses generalization to arrive at a universal law governing all cases. An example is the inference that "all ravens are black" based on observations of many individual black ravens. Another form is abductive reasoning. It starts from an observation and concludes that the best explanation of this observation must be true. This happens, for example, when a doctor diagnoses a disease based on the observed symptoms.
Logic also investigates incorrect forms of reasoning. They are called fallacies and are divided into formal and informal fallacies based on whether the source of the error lies only in the form of the argument or also in its content and context.
=
Metaphysics is the study of the most general features of reality, such as existence, objects and their properties, wholes and their parts, space and time, events, and causation. There are disagreements about the precise definition of the term and its meaning has changed throughout the ages. Metaphysicians attempt to answer basic questions including "Why is there something rather than nothing?"; "Of what does reality ultimately consist?"; and "Are humans free?"
Metaphysics is sometimes divided into general metaphysics and specific or special metaphysics. General metaphysics investigates being as such. It examines the features that all entities have in common. Specific metaphysics is interested in different kinds of being, the features they have, and how they differ from one another.
An important area in metaphysics is ontology. Some theorists identify it with general metaphysics. Ontology investigates concepts like being, becoming, and reality. It studies the categories of being and asks what exists on the most fundamental level. Another subfield of metaphysics is philosophical cosmology. It is interested in the essence of the world as a whole. It asks questions including whether the universe has a beginning and an end and whether it was created by something else.
A key topic in metaphysics concerns the question of whether reality only consists of physical things like matter and energy. Alternative suggestions are that mental entities (such as souls and experiences) and abstract entities (such as numbers) exist apart from physical things. Another topic in metaphysics concerns the problem of identity. One question is how much an entity can change while still remaining the same entity. According to one view, entities have essential and accidental features. They can change their accidental features but they cease to be the same entity if they lose an essential feature. A central distinction in metaphysics is between particulars and universals. Universals, like the color red, can exist at different locations at the same time. This is not the case for particulars including individual persons or specific objects. Other metaphysical questions are whether the past fully determines the present and what implications this would have for the existence of free will.
=
There are many other subfields of philosophy besides its core branches. Some of the most prominent are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, and political philosophy.
Aesthetics in the philosophical sense is the field that studies the nature and appreciation of beauty and other aesthetic properties, like the sublime. Although it is often treated together with the philosophy of art, aesthetics is a broader category that encompasses other aspects of experience, such as natural beauty. In a more general sense, aesthetics is "critical reflection on art, culture, and nature". A key question in aesthetics is whether beauty is an objective feature of entities or a subjective aspect of experience. Aesthetic philosophers also investigate the nature of aesthetic experiences and judgments. Further topics include the essence of works of art and the processes involved in creating them.
The philosophy of language studies the nature and function of language. It examines the concepts of meaning, reference, and truth. It aims to answer questions such as how words are related to things and how language affects human thought and understanding. It is closely related to the disciplines of logic and linguistics. The philosophy of language rose to particular prominence in the early 20th century in analytic philosophy due to the works of Frege and Russell. One of its central topics is to understand how sentences get their meaning. There are two broad theoretical camps: those emphasizing the formal truth conditions of sentences and those investigating circumstances that determine when it is suitable to use a sentence, the latter of which is associated with speech act theory.
The philosophy of mind studies the nature of mental phenomena and how they are related to the physical world. It aims to understand different types of conscious and unconscious mental states, like beliefs, desires, intentions, feelings, sensations, and free will. An influential intuition in the philosophy of mind is that there is a distinction between the inner experience of objects and their existence in the external world. The mind-body problem is the problem of explaining how these two types of thing—mind and matter—are related. The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter are distinct types of entities. In contemporary philosophy, another common view is functionalism, which understands mental states in terms of the functional or causal roles they play. The mind-body problem is closely related to the hard problem of consciousness, which asks how the physical brain can produce qualitatively subjective experiences.
The philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that reject religious doctrines. Further questions addressed by the philosophy of religion are: "How are we to interpret religious language, if not literally?"; "Is divine omniscience compatible with free will?"; and, "Are the great variety of world religions in some way compatible in spite of their apparently contradictory theological claims?" It includes topics from nearly all branches of philosophy. It differs from theology since theological debates typically take place within one religious tradition, whereas debates in the philosophy of religion transcend any particular set of theological assumptions.
The philosophy of science examines the fundamental concepts, assumptions, and problems associated with science. It reflects on what science is and how to distinguish it from pseudoscience. It investigates the methods employed by scientists, how their application can result in knowledge, and on what assumptions they are based. It also studies the purpose and implications of science. Some of its questions are "What counts as an adequate explanation?"; "Is a scientific law anything more than a description of a regularity?"; and "Can some special sciences be explained entirely in the terms of a more general science?" It is a vast field that is commonly divided into the philosophy of the natural sciences and the philosophy of the social sciences, with further subdivisions for each of the individual sciences under these headings. How these branches are related to one another is also a question in the philosophy of science. Many of its philosophical issues overlap with the fields of metaphysics or epistemology.
Political philosophy is the philosophical inquiry into the fundamental principles and ideas governing political systems and societies. It examines the basic concepts, assumptions, and arguments in the field of politics. It investigates the nature and purpose of government and compares its different forms. It further asks under what circumstances the use of political power is legitimate, rather than a form of simple violence. In this regard, it is concerned with the distribution of political power, social and material goods, and legal rights. Other topics are justice, liberty, equality, sovereignty, and nationalism. Political philosophy involves a general inquiry into normative matters and differs in this respect from political science, which aims to provide empirical descriptions of actually existing states. Political philosophy is often treated as a subfield of ethics. Influential schools of thought in political philosophy are liberalism, conservativism, socialism, and anarchism.
Methods of philosophy are ways of conducting philosophical inquiry. They include techniques for arriving at philosophical knowledge and justifying philosophical claims as well as principles used for choosing between competing theories. A great variety of methods have been employed throughout the history of philosophy. Many of them differ significantly from the methods used in the natural sciences in that they do not use experimental data obtained through measuring equipment. The choice of one's method usually has important implications both for how philosophical theories are constructed and for the arguments cited for or against them. This choice is often guided by epistemological considerations about what constitutes philosophical evidence.
Methodological disagreements can cause conflicts among philosophical theories or about the answers to philosophical questions. The discovery of new methods has often had important consequences both for how philosophers conduct their research and for what claims they defend. Some philosophers engage in most of their theorizing using one particular method while others employ a wider range of methods based on which one fits the specific problem investigated best.
Conceptual analysis is a common method in analytic philosophy. It aims to clarify the meaning of concepts by analyzing them into their component parts. Another method often employed in analytic philosophy is based on common sense. It starts with commonly accepted beliefs and tries to draw unexpected conclusions from them, which it often employs in a negative sense to criticize philosophical theories that are too far removed from how the average person sees the issue. It is similar to how ordinary language philosophy approaches philosophical questions by investigating how ordinary language is used.
Various methods in philosophy give particular importance to intuitions, that is, non-inferential impressions about the correctness of specific claims or general principles. For example, they play an important role in thought experiments, which employ counterfactual thinking to evaluate the possible consequences of an imagined situation. These anticipated consequences can then be used to confirm or refute philosophical theories. The method of reflective equilibrium also employs intuitions. It seeks to form a coherent position on a certain issue by examining all the relevant beliefs and intuitions, some of which often have to be deemphasized or reformulated to arrive at a coherent perspective.
Pragmatists stress the significance of concrete practical consequences for assessing whether a philosophical theory is true. According to the pragmatic maxim as formulated by Charles Sanders Peirce, the idea a person has of an object is nothing more than the totality of practical consequences they associate with this object. Pragmatists have also used this method to expose disagreements as merely verbal, that is, to show they make no genuine difference on the level of consequences.
Phenomenologists seek knowledge of the realm of appearance and the structure of human experience. They insist upon the first-personal character of all experience and proceed by suspending theoretical judgments about the external world. This technique of phenomenological reduction is known as "bracketing" or epoché. The goal is to give an unbiased description of the appearance of things.
Methodological naturalism places great emphasis on the empirical approach and the resulting theories found in the natural sciences. In this way, it contrasts with methodologies that give more weight to pure reasoning and introspection.
Philosophy is closely related to many other fields. It is sometimes understood as a meta-discipline that clarifies their nature and limits. It does this by critically examining their basic concepts, background assumptions, and methods. In this regard, it plays a key role in providing an interdisciplinary perspective. It bridges the gap between different disciplines by analyzing which concepts and problems they have in common. It shows how they overlap while also delimiting their scope. Historically, most of the individual sciences originated from philosophy.
The influence of philosophy is felt in several fields that require difficult practical decisions. In medicine, philosophical considerations related to bioethics affect issues like whether an embryo is already a person and under what conditions abortion is morally permissible. A closely related philosophical problem is how humans should treat other animals, for instance, whether it is acceptable to use non-human animals as food or for research experiments. In relation to business and professional life, philosophy has contributed by providing ethical frameworks. They contain guidelines on which business practices are morally acceptable and cover the issue of corporate social responsibility.
Philosophical inquiry is relevant to many fields that are concerned with what to believe and how to arrive at evidence for one's beliefs. This is a key issue for the sciences, which have as one of their prime objectives the creation of scientific knowledge. Scientific knowledge is based on empirical evidence but it is often not clear whether empirical observations are neutral or already include theoretical assumptions. A closely connected problem is whether the available evidence is sufficient to decide between competing theories. Epistemological problems in relation to the law include what counts as evidence and how much evidence is required to find a person guilty of a crime. A related issue in journalism is how to ensure truth and objectivity when reporting on events.
In the fields of theology and religion, there are many doctrines associated with the existence and nature of God as well as rules governing correct behavior. A key issue is whether a rational person should believe these doctrines, for example, whether revelation in the form of holy books and religious experiences of the divine are sufficient evidence for these beliefs.
Philosophy in the form of logic has been influential in the fields of mathematics and computer science. Further fields influenced by philosophy include psychology, sociology, linguistics, education, and the arts. The close relation between philosophy and other fields in the contemporary period is reflected in the fact that many philosophy graduates go on to work in related fields rather than in philosophy itself.
In the field of politics, philosophy addresses issues such as how to assess whether a government policy is just. Philosophical ideas have prepared and shaped various political developments. For example, ideals formulated in Enlightenment philosophy laid the foundation for constitutional democracy and played a role in the American Revolution and the French Revolution. Marxist philosophy and its exposition of communism was one of the factors in the Russian Revolution and the Chinese Communist Revolution. In India, Mahatma Gandhi's philosophy of non-violence shaped the Indian independence movement.
An example of the cultural and critical role of philosophy is found in its influence on the feminist movement through philosophers such as Mary Wollstonecraft, Simone de Beauvoir, and Judith Butler. It has shaped the understanding of key concepts in feminism, for instance, the meaning of gender, how it differs from biological sex, and what role it plays in the formation of personal identity. Philosophers have also investigated the concepts of justice and equality and their implications with respect to the prejudicial treatment of women in male-dominated societies.
The idea that philosophy is useful for many aspects of life and society is sometimes rejected. According to one such view, philosophy is mainly undertaken for its own sake and does not make significant contributions to existing practices or external goals.
=
=
=
Internet Encyclopedia of Philosophy – a peer-reviewed online encyclopedia of philosophy
Stanford Encyclopedia of Philosophy – an online encyclopedia of philosophy maintained by Stanford University
PhilPapers – a comprehensive directory of online philosophical articles and books by academic philosophers
Internet Philosophy Ontology Project – a model of relationships between philosophical ideas, thinkers, and journals

--- TOPIC: Logic ---
Logic is the study of correct reasoning. It includes both formal and informal logic. Formal logic is the study of deductively valid inferences or logical truths. It examines how conclusions follow from premises based on the structure of arguments alone, independent of their topic and content. Informal logic is associated with informal fallacies, critical thinking, and argumentation theory. Informal logic examines arguments expressed in natural language whereas formal logic uses formal language. When used as a countable noun, the term "a logic" refers to a specific logical formal system that articulates a proof system. Logic plays a central role in many fields, such as philosophy, mathematics, computer science, and linguistics.
Logic studies arguments, which consist of a set of premises that leads to a conclusion. An example is the argument from the premises "it's Sunday" and "if it's Sunday then I don't have to work" leading to the conclusion "I don't have to work." Premises and conclusions express propositions or claims that can be true or false. An important feature of propositions is their internal structure. For example, complex propositions are made up of simpler propositions linked by logical vocabulary like 
  
    
      
        ∧
      
    
    {\displaystyle \land }
  
 (and) or 
  
    
      
        →
      
    
    {\displaystyle \to }
  
 (if...then). Simple propositions also have parts, like "Sunday" or "work" in the example. The truth of a proposition usually depends on the meanings of all of its parts. However, this is not the case for logically true propositions. They are true only because of their logical structure independent of the specific meanings of the individual parts.
Arguments can be either correct or incorrect. An argument is correct if its premises support its conclusion. Deductive arguments have the strongest form of support: if their premises are true then their conclusion must also be true. This is not the case for ampliative arguments, which arrive at genuinely new information not found in the premises. Many arguments in everyday discourse and the sciences are ampliative arguments. They are divided into inductive and abductive arguments. Inductive arguments are statistical generalizations, such as inferring that all ravens are black based on many individual observations of black ravens. Abductive arguments are inferences to the best explanation, for example, when a doctor concludes that a patient has a certain disease which explains the symptoms they suffer. Arguments that fall short of the standards of correct reasoning often embody fallacies. Systems of logic are theoretical frameworks for assessing the correctness of arguments.
Logic has been studied since antiquity. Early approaches include Aristotelian logic, Stoic logic, Nyaya, and Mohism. Aristotelian logic focuses on reasoning in the form of syllogisms. It was considered the main system of logic in the Western world until it was replaced by modern formal logic, which has its roots in the work of late 19th-century mathematicians such as Gottlob Frege. Today, the most commonly used system is classical logic. It consists of propositional logic and first-order logic. Propositional logic only considers logical relations between full propositions. First-order logic also takes the internal parts of propositions into account, like predicates and quantifiers. Extended logics accept the basic intuitions behind classical logic and apply it to other fields, such as metaphysics, ethics, and epistemology. Deviant logics, on the other hand, reject certain classical intuitions and provide alternative explanations of the basic laws of logic.
The word "logic" originates from the Greek word logos, which has a variety of translations, such as reason, discourse, or language. Logic is traditionally defined as the study of the laws of thought or correct reasoning, and is usually understood in terms of inferences or arguments. Reasoning is the activity of drawing inferences. Arguments are the outward expression of inferences. An argument is a set of premises together with a conclusion. Logic is interested in whether arguments are correct, i.e. whether their premises support the conclusion. These general characterizations apply to logic in the widest sense, i.e., to both formal and informal logic since they are both concerned with assessing the correctness of arguments. Formal logic is the traditionally dominant field, and some logicians restrict logic to formal logic.
=
Formal logic (also known as symbolic logic) is widely used in mathematical logic. It uses a formal approach to study reasoning: it replaces concrete expressions with abstract symbols to examine the logical form of arguments independent of their concrete content. In this sense, it is topic-neutral since it is only concerned with the abstract structure of arguments and not with their concrete content.
Formal logic is interested in deductively valid arguments, for which the truth of their premises ensures the truth of their conclusion. This means that it is impossible for the premises to be true and the conclusion to be false. For valid arguments, the logical structure that leads from the premises to the conclusion follows a pattern called a rule of inference. For example, modus ponens is a rule of inference according to which all arguments of the form "(1) p, (2) if p then q, (3) therefore q" are valid, independent of what the terms p and q stand for. In this sense, formal logic can be defined as the science of valid inferences. An alternative definition sees logic as the study of logical truths. A proposition is logically true if its truth depends only on the logical vocabulary used in it. This means that it is true in all possible worlds and under all interpretations of its non-logical terms, like the claim "either it is raining, or it is not". These two definitions of formal logic are not identical, but they are closely related. For example, if the inference from p to q is deductively valid then the claim "if p then q" is a logical truth.
Formal logic uses formal languages to express, analyze, and clarify arguments. They normally have a very limited vocabulary and exact syntactic rules. These rules specify how their symbols can be combined to construct sentences, so-called well-formed formulas. This simplicity and exactness of formal logic make it capable of formulating precise rules of inference. They determine whether a given argument is valid. Because of the reliance on formal language, natural language arguments cannot be studied directly. Instead, they need to be translated into formal language before their validity can be assessed.
The term "logic" can also be used in a slightly different sense as a countable noun. In this sense, a logic is a logical formal system. Distinct logics differ from each other concerning the rules of inference they accept as valid and the formal languages used to express them. Starting in the late 19th century, many new formal systems have been proposed. There are disagreements about what makes a formal system a logic. For example, it has been suggested that only logically complete systems, like first-order logic, qualify as logics. For such reasons, some theorists deny that higher-order logics are logics in the strict sense.
=
When understood in a wide sense, logic encompasses both formal and informal logic. Informal logic uses non-formal criteria and standards to analyze and assess the correctness of arguments. Its main focus is on everyday discourse. Its development was prompted by difficulties in applying the insights of formal logic to natural language arguments. In this regard, it considers problems that formal logic on its own is unable to address. Both provide criteria for assessing the correctness of arguments and distinguishing them from fallacies.
Many characterizations of informal logic have been suggested but there is no general agreement on its precise definition. The most literal approach sees the terms "formal" and "informal" as applying to the language used to express arguments. On this view, informal logic studies arguments that are in informal or natural language. Formal logic can only examine them indirectly by translating them first into a formal language while informal logic investigates them in their original form. On this view, the argument "Birds fly. Tweety is a bird. Therefore, Tweety flies." belongs to natural language and is examined by informal logic. But the formal translation "(1) 
  
    
      
        ∀
        x
        (
        B
        i
        r
        d
        (
        x
        )
        →
        F
        l
        i
        e
        s
        (
        x
        )
        )
      
    
    {\displaystyle \forall x(Bird(x)\to Flies(x))}
  
; (2) 
  
    
      
        B
        i
        r
        d
        (
        T
        w
        e
        e
        t
        y
        )
      
    
    {\displaystyle Bird(Tweety)}
  
; (3) 
  
    
      
        F
        l
        i
        e
        s
        (
        T
        w
        e
        e
        t
        y
        )
      
    
    {\displaystyle Flies(Tweety)}
  
" is studied by formal logic. The study of natural language arguments comes with various difficulties. For example, natural language expressions are often ambiguous, vague, and context-dependent. Another approach defines informal logic in a wide sense as the normative study of the standards, criteria, and procedures of argumentation. In this sense, it includes questions about the role of rationality, critical thinking, and the psychology of argumentation.
Another characterization identifies informal logic with the study of non-deductive arguments. In this way, it contrasts with deductive reasoning examined by formal logic. Non-deductive arguments make their conclusion probable but do not ensure that it is true. An example is the inductive argument from the empirical observation that "all ravens I have seen so far are black" to the conclusion "all ravens are black".
A further approach is to define informal logic as the study of informal fallacies. Informal fallacies are incorrect arguments in which errors are present in the content and the context of the argument. A false dilemma, for example, involves an error of content by excluding viable options. This is the case in the fallacy "you are either with us or against us; you are not with us; therefore, you are against us". Some theorists state that formal logic studies the general form of arguments while informal logic studies particular instances of arguments. Another approach is to hold that formal logic only considers the role of logical constants for correct inferences while informal logic also takes the meaning of substantive concepts into account. Further approaches focus on the discussion of logical topics with or without formal devices and on the role of epistemology for the assessment of arguments.
=
 Premises and conclusions 
Premises and conclusions are the basic parts of inferences or arguments and therefore play a central role in logic. In the case of a valid inference or a correct argument, the conclusion follows from the premises, or in other words, the premises support the conclusion. For instance, the premises "Mars is red" and "Mars is a planet" support the conclusion "Mars is a red planet". For most types of logic, it is accepted that premises and conclusions have to be truth-bearers. This means that they have a truth value: they are either true or false. Contemporary philosophy generally sees them either as propositions or as sentences. Propositions are the denotations of sentences and are usually seen as abstract objects. For example, the English sentence "the tree is green" is different from the German sentence "der Baum ist grün" but both express the same proposition.
Propositional theories of premises and conclusions are often criticized because they rely on abstract objects. For instance, philosophical naturalists usually reject the existence of abstract objects. Other arguments concern the challenges involved in specifying the identity criteria of propositions. These objections are avoided by seeing premises and conclusions not as propositions but as sentences, i.e. as concrete linguistic objects like the symbols displayed on a page of a book. But this approach comes with new problems of its own: sentences are often context-dependent and ambiguous, meaning an argument's validity would not only depend on its parts but also on its context and on how it is interpreted. Another approach is to understand premises and conclusions in psychological terms as thoughts or judgments. This position is known as psychologism. It was discussed at length around the turn of the 20th century but it is not widely accepted today.
 Internal structure 
Premises and conclusions have an internal structure. As propositions or sentences, they can be either simple or complex. A complex proposition has other propositions as its constituents, which are linked to each other through propositional connectives like "and" or "if...then". Simple propositions, on the other hand, do not have propositional parts. But they can also be conceived as having an internal structure: they are made up of subpropositional parts, like singular terms and predicates. For example, the simple proposition "Mars is red" can be formed by applying the predicate "red" to the singular term "Mars". In contrast, the complex proposition "Mars is red and Venus is white" is made up of two simple propositions connected by the propositional connective "and".
Whether a proposition is true depends, at least in part, on its constituents. For complex propositions formed using truth-functional propositional connectives, their truth only depends on the truth values of their parts. But this relation is more complicated in the case of simple propositions and their subpropositional parts. These subpropositional parts have meanings of their own, like referring to objects or classes of objects. Whether the simple proposition they form is true depends on their relation to reality, i.e. what the objects they refer to are like. This topic is studied by theories of reference.
 Logical truth 
Some complex propositions are true independently of the substantive meanings of their parts. In classical logic, for example, the complex proposition "either Mars is red or Mars is not red" is true independent of whether its parts, like the simple proposition "Mars is red", are true or false. In such cases, the truth is called a logical truth: a proposition is logically true if its truth depends only on the logical vocabulary used in it. This means that it is true under all interpretations of its non-logical terms. In some modal logics, this means that the proposition is true in all possible worlds. Some theorists define logic as the study of logical truths.
 Truth tables 
Truth tables can be used to show how logical connectives work or how the truth values of complex propositions depends on their parts. They have a column for each input variable. Each row corresponds to one possible combination of the truth values these variables can take; for truth tables presented in the English literature, the symbols "T" and "F" or "1" and "0" are commonly used as abbreviations for the truth values "true" and "false". The first columns present all the possible truth-value combinations for the input variables. Entries in the other columns present the truth values of the corresponding expressions as determined by the input values. For example, the expression "
  
    
      
        p
        ∧
        q
      
    
    {\displaystyle p\land q}
  
" uses the logical connective 
  
    
      
        ∧
      
    
    {\displaystyle \land }
  
 (and). It could be used to express a sentence like "yesterday was Sunday and the weather was good". It is only true if both of its input variables, 
  
    
      
        p
      
    
    {\displaystyle p}
  
 ("yesterday was Sunday") and 
  
    
      
        q
      
    
    {\displaystyle q}
  
 ("the weather was good"), are true. In all other cases, the expression as a whole is false. Other important logical connectives are 
  
    
      
        ¬
      
    
    {\displaystyle \lnot }
  
 (not), 
  
    
      
        ∨
      
    
    {\displaystyle \lor }
  
 (or), 
  
    
      
        →
      
    
    {\displaystyle \to }
  
 (if...then), and 
  
    
      
        ↑
      
    
    {\displaystyle \uparrow }
  
 (Sheffer stroke). Given the conditional proposition 
  
    
      
        p
        →
        q
      
    
    {\displaystyle p\to q}
  
, one can form truth tables of its converse 
  
    
      
        q
        →
        p
      
    
    {\displaystyle q\to p}
  
, its inverse (
  
    
      
        ¬
        p
        →
        ¬
        q
      
    
    {\displaystyle \lnot p\to \lnot q}
  
), and its contrapositive (
  
    
      
        ¬
        q
        →
        ¬
        p
      
    
    {\displaystyle \lnot q\to \lnot p}
  
). Truth tables can also be defined for more complex expressions that use several propositional connectives.
=
Logic is commonly defined in terms of arguments or inferences as the study of their correctness. An argument is a set of premises together with a conclusion. An inference is the process of reasoning from these premises to the conclusion. But these terms are often used interchangeably in logic. Arguments are correct or incorrect depending on whether their premises support their conclusion. Premises and conclusions, on the other hand, are true or false depending on whether they are in accord with reality. In formal logic, a sound argument is an argument that is both correct and has only true premises. Sometimes a distinction is made between simple and complex arguments. A complex argument is made up of a chain of simple arguments. This means that the conclusion of one argument acts as a premise of later arguments. For a complex argument to be successful, each link of the chain has to be successful.
Arguments and inferences are either correct or incorrect. If they are correct then their premises support their conclusion. In the incorrect case, this support is missing. It can take different forms corresponding to the different types of reasoning. The strongest form of support corresponds to deductive reasoning. But even arguments that are not deductively valid may still be good arguments because their premises offer non-deductive support to their conclusions. For such cases, the term ampliative or inductive reasoning is used. Deductive arguments are associated with formal logic in contrast to the relation between ampliative arguments and informal logic.
 Deductive 
A deductively valid argument is one whose premises guarantee the truth of its conclusion. For instance, the argument "(1) all frogs are amphibians; (2) no cats are amphibians; (3) therefore no cats are frogs" is deductively valid. For deductive validity, it does not matter whether the premises or the conclusion are actually true. So the argument "(1) all frogs are mammals; (2) no cats are mammals; (3) therefore no cats are frogs" is also valid because the conclusion follows necessarily from the premises.
According to an influential view by Alfred Tarski, deductive arguments have three essential features: (1) they are formal, i.e. they depend only on the form of the premises and the conclusion; (2) they are a priori, i.e. no sense experience is needed to determine whether they obtain; (3) they are modal, i.e. that they hold by logical necessity for the given propositions, independent of any other circumstances.
Because of the first feature, the focus on formality, deductive inference is usually identified with rules of inference. Rules of inference specify the form of the premises and the conclusion: how they have to be structured for the inference to be valid. Arguments that do not follow any rule of inference are deductively invalid. The modus ponens is a prominent rule of inference. It has the form "p; if p, then q; therefore q". Knowing that it has just rained (
  
    
      
        p
      
    
    {\displaystyle p}
  
) and that after rain the streets are wet (
  
    
      
        p
        →
        q
      
    
    {\displaystyle p\to q}
  
), one can use modus ponens to deduce that the streets are wet (
  
    
      
        q
      
    
    {\displaystyle q}
  
).
The third feature can be expressed by stating that deductively valid inferences are truth-preserving: it is impossible for the premises to be true and the conclusion to be false. Because of this feature, it is often asserted that deductive inferences are uninformative since the conclusion cannot arrive at new information not already present in the premises. But this point is not always accepted since it would mean, for example, that most of mathematics is uninformative. A different characterization distinguishes between surface and depth information. The surface information of a sentence is the information it presents explicitly. Depth information is the totality of the information contained in the sentence, both explicitly and implicitly. According to this view, deductive inferences are uninformative on the depth level. But they can be highly informative on the surface level by making implicit information explicit. This happens, for example, in mathematical proofs.
 Ampliative 
Ampliative arguments are arguments whose conclusions contain additional information not found in their premises. In this regard, they are more interesting since they contain information on the depth level and the thinker may learn something genuinely new. But this feature comes with a certain cost: the premises support the conclusion in the sense that they make its truth more likely but they do not ensure its truth. This means that the conclusion of an ampliative argument may be false even though all its premises are true. This characteristic is closely related to non-monotonicity and defeasibility: it may be necessary to retract an earlier conclusion upon receiving new information or in light of new inferences drawn. Ampliative reasoning plays a central role in many arguments found in everyday discourse and the sciences. Ampliative arguments are not automatically incorrect. Instead, they just follow different standards of correctness. The support they provide for their conclusion usually comes in degrees. This means that strong ampliative arguments make their conclusion very likely while weak ones are less certain. As a consequence, the line between correct and incorrect arguments is blurry in some cases, such as when the premises offer weak but non-negligible support. This contrasts with deductive arguments, which are either valid or invalid with nothing in-between.
The terminology used to categorize ampliative arguments is inconsistent. Some authors, like James Hawthorne, use the term "induction" to cover all forms of non-deductive arguments. But in a more narrow sense, induction is only one type of ampliative argument alongside abductive arguments. Some philosophers, like Leo Groarke, also allow conductive arguments as another type. In this narrow sense, induction is often defined as a form of statistical generalization. In this case, the premises of an inductive argument are many individual observations that all show a certain pattern. The conclusion then is a general law that this pattern always obtains. In this sense, one may infer that "all elephants are gray" based on one's past observations of the color of elephants. A closely related form of inductive inference has as its conclusion not a general law but one more specific instance, as when it is inferred that an elephant one has not seen yet is also gray. Some theorists, like Igor Douven, stipulate that inductive inferences rest only on statistical considerations. This way, they can be distinguished from abductive inference.
Abductive inference may or may not take statistical observations into consideration. In either case, the premises offer support for the conclusion because the conclusion is the best explanation of why the premises are true. In this sense, abduction is also called the inference to the best explanation. For example, given the premise that there is a plate with breadcrumbs in the kitchen in the early morning, one may infer the conclusion that one's house-mate had a midnight snack and was too tired to clean the table. This conclusion is justified because it is the best explanation of the current state of the kitchen. For abduction, it is not sufficient that the conclusion explains the premises. For example, the conclusion that a burglar broke into the house last night, got hungry on the job, and had a midnight snack, would also explain the state of the kitchen. But this conclusion is not justified because it is not the best or most likely explanation.
=
Not all arguments live up to the standards of correct reasoning. When they do not, they are usually referred to as fallacies. Their central aspect is not that their conclusion is false but that there is some flaw with the reasoning leading to this conclusion. So the argument "it is sunny today; therefore spiders have eight legs" is fallacious even though the conclusion is true. Some theorists, like John Stuart Mill, give a more restrictive definition of fallacies by additionally requiring that they appear to be correct. This way, genuine fallacies can be distinguished from mere mistakes of reasoning due to carelessness. This explains why people tend to commit fallacies: because they have an alluring element that seduces people into committing and accepting them. However, this reference to appearances is controversial because it belongs to the field of psychology, not logic, and because appearances may be different for different people.
Fallacies are usually divided into formal and informal fallacies. For formal fallacies, the source of the error is found in the form of the argument. For example, denying the antecedent is one type of formal fallacy, as in "if Othello is a bachelor, then he is male; Othello is not a bachelor; therefore Othello is not male". But most fallacies fall into the category of informal fallacies, of which a great variety is discussed in the academic literature. The source of their error is usually found in the content or the context of the argument. Informal fallacies are sometimes categorized as fallacies of ambiguity, fallacies of presumption, or fallacies of relevance. For fallacies of ambiguity, the ambiguity and vagueness of natural language are responsible for their flaw, as in "feathers are light; what is light cannot be dark; therefore feathers cannot be dark". Fallacies of presumption have a wrong or unjustified premise but may be valid otherwise. In the case of fallacies of relevance, the premises do not support the conclusion because they are not relevant to it.
=
The main focus of most logicians is to study the criteria according to which an argument is correct or incorrect. A fallacy is committed if these criteria are violated. In the case of formal logic, they are known as rules of inference. They are definitory rules, which determine whether an inference is correct or which inferences are allowed. Definitory rules contrast with strategic rules. Strategic rules specify which inferential moves are necessary to reach a given conclusion based on a set of premises. This distinction does not just apply to logic but also to games. In chess, for example, the definitory rules dictate that bishops may only move diagonally. The strategic rules, on the other hand, describe how the allowed moves may be used to win a game, for instance, by controlling the center and by defending one's king. It has been argued that logicians should give more emphasis to strategic rules since they are highly relevant for effective reasoning.
=
A formal system of logic consists of a formal language together with a set of axioms and a proof system used to draw inferences from these axioms. In logic, axioms are statements that are accepted without proof. They are used to justify other statements. Some theorists also include a semantics that specifies how the expressions of the formal language relate to real objects. Starting in the late 19th century, many new formal systems have been proposed.
A formal language consists of an alphabet and syntactic rules. The alphabet is the set of basic symbols used in expressions. The syntactic rules determine how these symbols may be arranged to result in well-formed formulas. For instance, the syntactic rules of propositional logic determine that "
  
    
      
        P
        ∧
        Q
      
    
    {\displaystyle P\land Q}
  
" is a well-formed formula but "
  
    
      
        ∧
        Q
      
    
    {\displaystyle \land Q}
  
" is not since the logical conjunction 
  
    
      
        ∧
      
    
    {\displaystyle \land }
  
 requires terms on both sides.
A proof system is a collection of rules to construct formal proofs. It is a tool to arrive at conclusions from a set of axioms. Rules in a proof system are defined in terms of the syntactic form of formulas independent of their specific content. For instance, the classical rule of conjunction introduction states that 
  
    
      
        P
        ∧
        Q
      
    
    {\displaystyle P\land Q}
  
 follows from the premises 
  
    
      
        P
      
    
    {\displaystyle P}
  
 and 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
. Such rules can be applied sequentially, giving a mechanical procedure for generating conclusions from premises. There are different types of proof systems including natural deduction and sequent calculi.
A semantics is a system for mapping expressions of a formal language to their denotations. In many systems of logic, denotations are truth values. For instance, the semantics for classical propositional logic assigns the formula 
  
    
      
        P
        ∧
        Q
      
    
    {\displaystyle P\land Q}
  
 the denotation "true" whenever 
  
    
      
        P
      
    
    {\displaystyle P}
  
 and 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 are true. From the semantic point of view, a premise entails a conclusion if the conclusion is true whenever the premise is true.
A system of logic is sound when its proof system cannot derive a conclusion from a set of premises unless it is semantically entailed by them. In other words, its proof system cannot lead to false conclusions, as defined by the semantics. A system is complete when its proof system can derive every conclusion that is semantically entailed by its premises. In other words, its proof system can lead to any true conclusion, as defined by the semantics. Thus, soundness and completeness together describe a system whose notions of validity and entailment line up perfectly.
Systems of logic are theoretical frameworks for assessing the correctness of reasoning and arguments. For over two thousand years, Aristotelian logic was treated as the canon of logic in the Western world, but modern developments in this field have led to a vast proliferation of logical systems. One prominent categorization divides modern formal logical systems into classical logic, extended logics, and deviant logics.
=
Aristotelian logic encompasses a great variety of topics. They include metaphysical theses about ontological categories and problems of scientific explanation. But in a more narrow sense, it is identical to term logic or syllogistics. A syllogism is a form of argument involving three propositions: two premises and a conclusion. Each proposition has three essential parts: a subject, a predicate, and a copula connecting the subject to the predicate. For example, the proposition "Socrates is wise" is made up of the subject "Socrates", the predicate "wise", and the copula "is". The subject and the predicate are the terms of the proposition. Aristotelian logic does not contain complex propositions made up of simple propositions. It differs in this aspect from propositional logic, in which any two propositions can be linked using a logical connective like "and" to form a new complex proposition.
In Aristotelian logic, the subject can be universal, particular, indefinite, or singular. For example, the term "all humans" is a universal subject in the proposition "all humans are mortal". A similar proposition could be formed by replacing it with the particular term "some humans", the indefinite term "a human", or the singular term "Socrates".
Aristotelian logic only includes predicates for simple properties of entities. But it lacks predicates corresponding to relations between entities. The predicate can be linked to the subject in two ways: either by affirming it or by denying it. For example, the proposition "Socrates is not a cat" involves the denial of the predicate "cat" to the subject "Socrates". Using combinations of subjects and predicates, a great variety of propositions and syllogisms can be formed. Syllogisms are characterized by the fact that the premises are linked to each other and to the conclusion by sharing one term in each case. Thus, these three propositions contain three terms, referred to as major term, minor term, and middle term. The central aspect of Aristotelian logic involves classifying all possible syllogisms into valid and invalid arguments according to how the propositions are formed. For example, the syllogism "all men are mortal; Socrates is a man; therefore Socrates is mortal" is valid. The syllogism "all cats are mortal; Socrates is mortal; therefore Socrates is a cat", on the other hand, is invalid.
=
Classical logic is distinct from traditional or Aristotelian logic. It encompasses propositional logic and first-order logic. It is "classical" in the sense that it is based on basic logical intuitions shared by most logicians. These intuitions include the law of excluded middle, the double negation elimination, the principle of explosion, and the bivalence of truth. It was originally developed to analyze mathematical arguments and was only later applied to other fields as well. Because of this focus on mathematics, it does not include logical vocabulary relevant to many other topics of philosophical importance. Examples of concepts it overlooks are the contrast between necessity and possibility and the problem of ethical obligation and permission. Similarly, it does not address the relations between past, present, and future. Such issues are addressed by extended logics. They build on the basic intuitions of classical logic and expand it by introducing new logical vocabulary. This way, the exact logical approach is applied to fields like ethics or epistemology that lie beyond the scope of mathematics.
 Propositional logic 
Propositional logic comprises formal systems in which formulae are built from atomic propositions using logical connectives. For instance, propositional logic represents the conjunction of two atomic propositions 
  
    
      
        P
      
    
    {\displaystyle P}
  
 and 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 as the complex formula 
  
    
      
        P
        ∧
        Q
      
    
    {\displaystyle P\land Q}
  
. Unlike predicate logic where terms and predicates are the smallest units, propositional logic takes full propositions with truth values as its most basic component. Thus, propositional logics can only represent logical relationships that arise from the way complex propositions are built from simpler ones. But it cannot represent inferences that result from the inner structure of a proposition.
 First-order logic 
First-order logic includes the same propositional connectives as propositional logic but differs from it because it articulates the internal structure of propositions. This happens through devices such as singular terms, which refer to particular objects, predicates, which refer to properties and relations, and quantifiers, which treat notions like "some" and "all". For example, to express the proposition "this raven is black", one may use the predicate 
  
    
      
        B
      
    
    {\displaystyle B}
  
 for the property "black" and the singular term 
  
    
      
        r
      
    
    {\displaystyle r}
  
 referring to the raven to form the expression 
  
    
      
        B
        (
        r
        )
      
    
    {\displaystyle B(r)}
  
. To express that some objects are black, the existential quantifier 
  
    
      
        ∃
      
    
    {\displaystyle \exists }
  
 is combined with the variable 
  
    
      
        x
      
    
    {\displaystyle x}
  
 to form the proposition 
  
    
      
        ∃
        x
        B
        (
        x
        )
      
    
    {\displaystyle \exists xB(x)}
  
. First-order logic contains various rules of inference that determine how expressions articulated this way can form valid arguments, for example, that one may infer 
  
    
      
        ∃
        x
        B
        (
        x
        )
      
    
    {\displaystyle \exists xB(x)}
  
 from 
  
    
      
        B
        (
        r
        )
      
    
    {\displaystyle B(r)}
  
.
=
Extended logics are logical systems that accept the basic principles of classical logic. They introduce additional symbols and principles to apply it to fields like metaphysics, ethics, and epistemology.
 Modal logic 
Modal logic is an extension of classical logic. In its original form, sometimes called "alethic modal logic", it introduces two new symbols: 
  
    
      
        ◊
      
    
    {\displaystyle \Diamond }
  
 expresses that something is possible while 
  
    
      
        ◻
      
    
    {\displaystyle \Box }
  
 expresses that something is necessary. For example, if the formula 
  
    
      
        B
        (
        s
        )
      
    
    {\displaystyle B(s)}
  
 stands for the sentence "Socrates is a banker" then the formula 
  
    
      
        ◊
        B
        (
        s
        )
      
    
    {\displaystyle \Diamond B(s)}
  
 articulates the sentence "It is possible that Socrates is a banker". To include these symbols in the logical formalism, modal logic introduces new rules of inference that govern what role they play in inferences. One rule of inference states that, if something is necessary, then it is also possible. This means that 
  
    
      
        ◊
        A
      
    
    {\displaystyle \Diamond A}
  
 follows from 
  
    
      
        ◻
        A
      
    
    {\displaystyle \Box A}
  
. Another principle states that if a proposition is necessary then its negation is impossible and vice versa. This means that 
  
    
      
        ◻
        A
      
    
    {\displaystyle \Box A}
  
 is equivalent to 
  
    
      
        ¬
        ◊
        ¬
        A
      
    
    {\displaystyle \lnot \Diamond \lnot A}
  
.
Other forms of modal logic introduce similar symbols but associate different meanings with them to apply modal logic to other fields. For example, deontic logic concerns the field of ethics and introduces symbols to express the ideas of obligation and permission, i.e. to describe whether an agent has to perform a certain action or is allowed to perform it. The modal operators in temporal modal logic articulate temporal relations. They can be used to express, for example, that something happened at one time or that something is happening all the time. In epistemology, epistemic modal logic is used to represent the ideas of knowing something in contrast to merely believing it to be the case.
 Higher order logic 
Higher-order logics extend classical logic not by using modal operators but by introducing new forms of quantification. Quantifiers correspond to terms like "all" or "some". In classical first-order logic, quantifiers are only applied to individuals. The formula "
  
    
      
        ∃
        x
        (
        A
        p
        p
        l
        e
        (
        x
        )
        ∧
        S
        w
        e
        e
        t
        (
        x
        )
        )
      
    
    {\displaystyle \exists x(Apple(x)\land Sweet(x))}
  
" (some apples are sweet) is an example of the existential quantifier "
  
    
      
        ∃
      
    
    {\displaystyle \exists }
  
" applied to the individual variable "
  
    
      
        x
      
    
    {\displaystyle x}
  
". In higher-order logics, quantification is also allowed over predicates. This increases its expressive power. For example, to express the idea that Mary and John share some qualities, one could use the formula "
  
    
      
        ∃
        Q
        (
        Q
        (
        M
        a
        r
        y
        )
        ∧
        Q
        (
        J
        o
        h
        n
        )
        )
      
    
    {\displaystyle \exists Q(Q(Mary)\land Q(John))}
  
". In this case, the existential quantifier is applied to the predicate variable "
  
    
      
        Q
      
    
    {\displaystyle Q}
  
". The added expressive power is especially useful for mathematics since it allows for more succinct formulations of mathematical theories. But it has drawbacks in regard to its meta-logical properties and ontological implications, which is why first-order logic is still more commonly used.
=
Deviant logics are logical systems that reject some of the basic intuitions of classical logic. Because of this, they are usually seen not as its supplements but as its rivals. Deviant logical systems differ from each other either because they reject different classical intuitions or because they propose different alternatives to the same issue.
Intuitionistic logic is a restricted version of classical logic. It uses the same symbols but excludes some rules of inference. For example, according to the law of double negation elimination, if a sentence is not not true, then it is true. This means that 
  
    
      
        A
      
    
    {\displaystyle A}
  
 follows from 
  
    
      
        ¬
        ¬
        A
      
    
    {\displaystyle \lnot \lnot A}
  
. This is a valid rule of inference in classical logic but it is invalid in intuitionistic logic. Another classical principle not part of intuitionistic logic is the law of excluded middle. It states that for every sentence, either it or its negation is true. This means that every proposition of the form 
  
    
      
        A
        ∨
        ¬
        A
      
    
    {\displaystyle A\lor \lnot A}
  
 is true. These deviations from classical logic are based on the idea that truth is established by verification using a proof. Intuitionistic logic is especially prominent in the field of constructive mathematics, which emphasizes the need to find or construct a specific example to prove its existence.
Multi-valued logics depart from classicality by rejecting the principle of bivalence, which requires all propositions to be either true or false. For instance, Jan Łukasiewicz and Stephen Cole Kleene both proposed ternary logics which have a third truth value representing that a statement's truth value is indeterminate. These logics have been applied in the field of linguistics. Fuzzy logics are multivalued logics that have an infinite number of "degrees of truth", represented by a real number between 0 and 1.
Paraconsistent logics are logical systems that can deal with contradictions. They are formulated to avoid the principle of explosion: for them, it is not the case that anything follows from a contradiction. They are often motivated by dialetheism, the view that contradictions are real or that reality itself is contradictory. Graham Priest is an influential contemporary proponent of this position and similar views have been ascribed to Georg Wilhelm Friedrich Hegel.
=
Informal logic is usually carried out in a less systematic way. It often focuses on more specific issues, like investigating a particular type of fallacy or studying a certain aspect of argumentation. Nonetheless, some frameworks of informal logic have also been presented that try to provide a systematic characterization of the correctness of arguments.
The pragmatic or dialogical approach to informal logic sees arguments as speech acts and not merely as a set of premises together with a conclusion. As speech acts, they occur in a certain context, like a dialogue, which affects the standards of right and wrong arguments. A prominent version by Douglas N. Walton understands a dialogue as a game between two players. The initial position of each player is characterized by the propositions to which they are committed and the conclusion they intend to prove. Dialogues are games of persuasion: each player has the goal of convincing the opponent of their own conclusion. This is achieved by making arguments: arguments are the moves of the game. They affect to which propositions the players are committed. A winning move is a successful argument that takes the opponent's commitments as premises and shows how one's own conclusion follows from them. This is usually not possible straight away. For this reason, it is normally necessary to formulate a sequence of arguments as intermediary steps, each of which brings the opponent a little closer to one's intended conclusion. Besides these positive arguments leading one closer to victory, there are also negative arguments preventing the opponent's victory by denying their conclusion. Whether an argument is correct depends on whether it promotes the progress of the dialogue. Fallacies, on the other hand, are violations of the standards of proper argumentative rules. These standards also depend on the type of dialogue. For example, the standards governing the scientific discourse differ from the standards in business negotiations.
The epistemic approach to informal logic, on the other hand, focuses on the epistemic role of arguments. It is based on the idea that arguments aim to increase our knowledge. They achieve this by linking justified beliefs to beliefs that are not yet justified. Correct arguments succeed at expanding knowledge while fallacies are epistemic failures: they do not justify the belief in their conclusion. For example, the fallacy of begging the question is a fallacy because it fails to provide independent justification for its conclusion, even though it is deductively valid. In this sense, logical normativity consists in epistemic success or rationality. The Bayesian approach is one example of an epistemic approach. Central to Bayesianism is not just whether the agent believes something but the degree to which they believe it, the so-called credence. Degrees of belief are seen as subjective probabilities in the believed proposition, i.e. how certain the agent is that the proposition is true. On this view, reasoning can be interpreted as a process of changing one's credences, often in reaction to new incoming information. Correct reasoning and the arguments it is based on follow the laws of probability, for example, the principle of conditionalization. Bad or irrational reasoning, on the other hand, violates these laws.
Logic is studied in various fields. In many cases, this is done by applying its formal method to specific topics outside its scope, like to ethics or computer science. In other cases, logic itself is made the subject of research in another discipline. This can happen in diverse ways. For instance, it can involve investigating the philosophical assumptions linked to the basic concepts used by logicians. Other ways include interpreting and analyzing logic through mathematical structures as well as studying and comparing abstract properties of formal logical systems.
=
Philosophy of logic is the philosophical discipline studying the scope and nature of logic. It examines many presuppositions implicit in logic, like how to define its basic concepts or the metaphysical assumptions associated with them. It is also concerned with how to classify logical systems and considers the ontological commitments they incur. Philosophical logic is one of the areas within the philosophy of logic. It studies the application of logical methods to philosophical problems in fields like metaphysics, ethics, and epistemology. This application usually happens in the form of extended or deviant logical systems.
=
Metalogic is the field of inquiry studying the properties of formal logical systems. For example, when a new formal system is developed, metalogicians may study it to determine which formulas can be proven in it. They may also study whether an algorithm could be developed to find a proof for each formula and whether every provable formula in it is a tautology. Finally, they may compare it to other logical systems to understand its distinctive features. A key issue in metalogic concerns the relation between syntax and semantics. The syntactic rules of a formal system determine how to deduce conclusions from premises, i.e. how to formulate proofs. The semantics of a formal system governs which sentences are true and which ones are false. This determines the validity of arguments since, for valid arguments, it is impossible for the premises to be true and the conclusion to be false. The relation between syntax and semantics concerns issues like whether every valid argument is provable and whether every provable argument is valid. Metalogicians also study whether logical systems are complete, sound, and consistent. They are interested in whether the systems are decidable and what expressive power they have. Metalogicians usually rely heavily on abstract mathematical reasoning when examining and formulating metalogical proofs. This way, they aim to arrive at precise and general conclusions on these topics.
=
The term "mathematical logic" is sometimes used as a synonym of "formal logic". But in a more restricted sense, it refers to the study of logic within mathematics. Major subareas include model theory, proof theory, set theory, and computability theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic. However, it can also include attempts to use logic to analyze mathematical reasoning or to establish logic-based foundations of mathematics. The latter was a major concern in early 20th-century mathematical logic, which pursued the program of logicism pioneered by philosopher-logicians such as Gottlob Frege, Alfred North Whitehead, and Bertrand Russell. Mathematical theories were supposed to be logical tautologies, and their program was to show this by means of a reduction of mathematics to logic. Many attempts to realize this program failed, from the crippling of Frege's project in his Grundgesetze by Russell's paradox, to the defeat of Hilbert's program by Gödel's incompleteness theorems.
Set theory originated in the study of the infinite by Georg Cantor, and it has been the source of many of the most challenging and important issues in mathematical logic. They include Cantor's theorem, the status of the Axiom of Choice, the question of the independence of the continuum hypothesis, and the modern debate on large cardinal axioms.
Computability theory is the branch of mathematical logic that studies effective procedures to solve calculation problems. One of its main goals is to understand whether it is possible to solve a given problem using an algorithm. For instance, given a certain claim about the positive integers, it examines whether an algorithm can be found to determine if this claim is true. Computability theory uses various theoretical tools and models, such as Turing machines, to explore this type of issue.
=
Computational logic is the branch of logic and computer science that studies how to implement mathematical reasoning and logical formalisms using computers. This includes, for example, automatic theorem provers, which employ rules of inference to construct a proof step by step from a set of premises to the intended conclusion without human intervention. Logic programming languages are designed specifically to express facts using logical formulas and to draw inferences from these facts. For example, Prolog is a logic programming language based on predicate logic. Computer scientists also apply concepts from logic to problems in computing. The works of Claude Shannon were influential in this regard. He showed how Boolean logic can be used to understand and implement computer circuits. This can be achieved using electronic logic gates, i.e. electronic circuits with one or more inputs and usually one output. The truth values of propositions are represented by voltage levels. In this way, logic functions can be simulated by applying the corresponding voltages to the inputs of the circuit and determining the value of the function by measuring the voltage of the output.
=
Formal semantics is a subfield of logic, linguistics, and the philosophy of language. The discipline of semantics studies the meaning of language. Formal semantics uses formal tools from the fields of symbolic logic and mathematics to give precise theories of the meaning of natural language expressions. It understands meaning usually in relation to truth conditions, i.e. it examines in which situations a sentence would be true or false. One of its central methodological assumptions is the principle of compositionality. It states that the meaning of a complex expression is determined by the meanings of its parts and how they are combined. For example, the meaning of the verb phrase "walk and sing" depends on the meanings of the individual expressions "walk" and "sing". Many theories in formal semantics rely on model theory. This means that they employ set theory to construct a model and then interpret the meanings of expression in relation to the elements in this model. For example, the term "walk" may be interpreted as the set of all individuals in the model that share the property of walking. Early influential theorists in this field were Richard Montague and Barbara Partee, who focused their analysis on the English language.
=
The epistemology of logic studies how one knows that an argument is valid or that a proposition is logically true. This includes questions like how to justify that modus ponens is a valid rule of inference or that contradictions are false. The traditionally dominant view is that this form of logical understanding belongs to knowledge a priori. In this regard, it is often argued that the mind has a special faculty to examine relations between pure ideas and that this faculty is also responsible for apprehending logical truths. A similar approach understands the rules of logic in terms of linguistic conventions. On this view, the laws of logic are trivial since they are true by definition: they just express the meanings of the logical vocabulary.
Some theorists, like Hilary Putnam and Penelope Maddy, object to the view that logic is knowable a priori. They hold instead that logical truths depend on the empirical world. This is usually combined with the claim that the laws of logic express universal regularities found in the structural features of the world. According to this view, they may be explored by studying general patterns of the fundamental sciences. For example, it has been argued that certain insights of quantum mechanics refute the principle of distributivity in classical logic, which states that the formula 
  
    
      
        A
        ∧
        (
        B
        ∨
        C
        )
      
    
    {\displaystyle A\land (B\lor C)}
  
 is equivalent to 
  
    
      
        (
        A
        ∧
        B
        )
        ∨
        (
        A
        ∧
        C
        )
      
    
    {\displaystyle (A\land B)\lor (A\land C)}
  
. This claim can be used as an empirical argument for the thesis that quantum logic is the correct logical system and should replace classical logic.
Logic was developed independently in several cultures during antiquity. One major early contributor was Aristotle, who developed term logic in his Organon and Prior Analytics. He was responsible for the introduction of the hypothetical syllogism and temporal modal logic. Further innovations include inductive logic as well as the discussion of new logical concepts such as terms, predicables, syllogisms, and propositions. Aristotelian logic was highly regarded in classical and medieval times, both in Europe and the Middle East. It remained in wide use in the West until the early 19th century. It has now been superseded by later work, though many of its key insights are still present in modern systems of logic.
Ibn Sina (Avicenna) was the founder of Avicennian logic, which replaced Aristotelian logic as the dominant system of logic in the Islamic world. It influenced Western medieval writers such as Albertus Magnus and William of Ockham. Ibn Sina wrote on the hypothetical syllogism and on the propositional calculus. He developed an original "temporally modalized" syllogistic theory, involving temporal logic and modal logic. He also made use of inductive logic, such as his methods of agreement, difference, and concomitant variation, which are critical to the scientific method. Fakhr al-Din al-Razi was another influential Muslim logician. He criticized Aristotelian syllogistics and formulated an early system of inductive logic, foreshadowing the system of inductive logic developed by John Stuart Mill.
During the Middle Ages, many translations and interpretations of Aristotelian logic were made. The works of Boethius were particularly influential. Besides translating Aristotle's work into Latin, he also produced textbooks on logic. Later, the works of Islamic philosophers such as Ibn Sina and Ibn Rushd (Averroes) were drawn on. This expanded the range of ancient works available to medieval Christian scholars since more Greek work was available to Muslim scholars that had been preserved in Latin commentaries. In 1323, William of Ockham's influential Summa Logicae was released. It is a comprehensive treatise on logic that discusses many basic concepts of logic and provides a systematic exposition of types of propositions and their truth conditions.
In Chinese philosophy, the School of Names and Mohism were particularly influential. The School of Names focused on the use of language and on paradoxes. For example, Gongsun Long proposed the white horse paradox, which defends the thesis that a white horse is not a horse. The school of Mohism also acknowledged the importance of language for logic and tried to relate the ideas in these fields to the realm of ethics.
In India, the study of logic was primarily pursued by the schools of Nyaya, Buddhism, and Jainism. It was not treated as a separate academic discipline and discussions of its topics usually happened in the context of epistemology and theories of dialogue or argumentation. In Nyaya, inference is understood as a source of knowledge (pramāṇa). It follows the perception of an object and tries to arrive at conclusions, for example, about the cause of this object. A similar emphasis on the relation to epistemology is also found in Buddhist and Jainist schools of logic, where inference is used to expand the knowledge gained through other sources. Some of the later theories of Nyaya, belonging to the Navya-Nyāya school, resemble modern forms of logic, such as Gottlob Frege's distinction between sense and reference and his definition of number.
The syllogistic logic developed by Aristotle predominated in the West until the mid-19th century, when interest in the foundations of mathematics stimulated the development of modern symbolic logic. Many see Gottlob Frege's Begriffsschrift as the birthplace of modern logic. Gottfried Wilhelm Leibniz's idea of a universal formal language is often considered a forerunner. Other pioneers were George Boole, who invented Boolean algebra as a mathematical system of logic, and Charles Peirce, who developed the logic of relatives. Alfred North Whitehead and Bertrand Russell, in turn, condensed many of these insights in their work Principia Mathematica. Modern logic introduced novel concepts, such as functions, quantifiers, and relational predicates. A hallmark of modern symbolic logic is its use of formal language to precisely codify its insights. In this regard, it departs from earlier logicians, who relied mainly on natural language. Of particular influence was the development of first-order logic, which is usually treated as the standard system of modern logic. Its analytical generality allowed the formalization of mathematics and drove the investigation of set theory. It also made Alfred Tarski's approach to model theory possible and provided the foundation of modern mathematical logic.
=
=
=

--- TOPIC: Ethics ---
Ethics is the philosophical study of moral phenomena. Also called moral philosophy, it investigates normative questions about what people ought to do or which behavior is morally right. Its main branches include normative ethics, applied ethics, and metaethics.
Normative ethics aims to find general principles that govern how people should act. Applied ethics examines concrete ethical problems in real-life situations, such as abortion, treatment of animals, and business practices. Metaethics explores the underlying assumptions and concepts of ethics. It asks whether there are objective moral facts, how moral knowledge is possible, and how moral judgments motivate people. Influential normative theories are consequentialism, deontology, and virtue ethics. According to consequentialists, an act is right if it leads to the best consequences. Deontologists focus on acts themselves, saying that they must adhere to duties, like telling the truth and keeping promises. Virtue ethics sees the manifestation of virtues, like courage and compassion, as the fundamental principle of morality.
Ethics is closely connected to value theory, which studies the nature and types of value, like the contrast between intrinsic and instrumental value. Moral psychology is a related empirical field and investigates psychological processes involved in morality, such as reasoning and the formation of character. Descriptive ethics describes the dominant moral codes and beliefs in different societies and considers their historical dimension.
The history of ethics started in the ancient period with the development of ethical principles and theories in ancient Egypt, India, China, and Greece. This period saw the emergence of ethical teachings associated with Hinduism, Buddhism, Confucianism, Daoism, and contributions of philosophers like Socrates and Aristotle. During the medieval period, ethical thought was strongly influenced by religious teachings. In the modern period, this focus shifted to a more secular approach concerned with moral experience, reasons for acting, and the consequences of actions. An influential development in the 20th century was the emergence of metaethics.
Ethics, also called moral philosophy, is the study of moral phenomena. It is one of the main branches of philosophy and investigates the nature of morality and the principles that govern the moral evaluation of conduct, character traits, and institutions. It examines what obligations people have, what behavior is right and wrong, and how to lead a good life. Some of its key questions are "How should one live?" and "What gives meaning to life?". In contemporary philosophy, ethics is usually divided into normative ethics, applied ethics, and metaethics.
Morality concerns what people ought to do rather than what they actually do, what they want to do, or what social conventions require. As a rational and systematic field of inquiry, ethics studies practical reasons why people should act one way rather than another. Most ethical theories seek universal principles that express a general standpoint of what is objectively right and wrong. In a slightly different sense, the term ethics can also refer to individual ethical theories in the form of a rational system of moral principles, such as Aristotelian ethics, and to a moral code that certain societies, social groups, or professions follow, as in Protestant work ethic and medical ethics.
The English word ethics has its roots in the Ancient Greek word êthos (ἦθος), meaning 'character' and 'personal disposition'. This word gave rise to the Ancient Greek word ēthikós (ἠθικός), which was translated into Latin as ethica and entered the English language in the 15th century through the Old French term éthique. The term morality originates in the Latin word moralis, meaning 'manners' and 'character'. It was introduced into the English language during the Middle English period through the Old French term moralité.
The terms ethics and morality are usually used interchangeably but some philosophers distinguish between the two. According to one view, morality focuses on what moral obligations people have while ethics is broader and includes ideas about what is good and how to lead a meaningful life. Another difference is that codes of conduct in specific areas, such as business and environment, are usually termed ethics rather than morality, as in business ethics and environmental ethics.
Normative ethics is the philosophical study of ethical conduct and investigates the fundamental principles of morality. It aims to discover and justify general answers to questions like "How should one live?" and "How should people act?", usually in the form of universal or domain-independent principles that determine whether an act is right or wrong. For example, given the particular impression that it is wrong to set a child on fire for fun, normative ethics aims to find more general principles that explain why this is the case, like the principle that one should not cause extreme suffering to the innocent, which may itself be explained in terms of a more general principle. Many theories of normative ethics also aim to guide behavior by helping people make moral decisions.
Theories in normative ethics state how people should act or what kind of behavior is correct. They do not aim to describe how people normally act, what moral beliefs ordinary people have, how these beliefs change over time, or what ethical codes are upheld in certain social groups. These topics belong to descriptive ethics and are studied in fields like anthropology, sociology, and history rather than normative ethics.
Some systems of normative ethics arrive at a single principle covering all possible cases. Others encompass a small set of basic rules that address all or at least the most important moral considerations. One difficulty for systems with several basic principles is that these principles may conflict with each other in some cases and lead to ethical dilemmas.
Distinct theories in normative ethics suggest different principles as the foundation of morality. The three most influential schools of thought are consequentialism, deontology, and virtue ethics. These schools are usually presented as exclusive alternatives, but depending on how they are defined, they can overlap and do not necessarily exclude one another. In some cases, they differ in which acts they see as right or wrong. In other cases, they recommend the same course of action but provide different justifications for why it is right.
=
Consequentialism, also called teleological ethics, says that morality depends on consequences. According to the most common view, an act is right if it brings the best future. This means that there is no alternative course of action that has better consequences. A key aspect of consequentialist theories is that they provide a characterization of what is good and then define what is right in terms of what is good. For example, classical utilitarianism says that pleasure is good and that the action leading to the most overall pleasure is right. Consequentialism has been discussed indirectly since the formulation of classical utilitarianism in the late 18th century. A more explicit analysis of this view happened in the 20th century, when the term was coined by G. E. M. Anscombe.
Consequentialists usually understand the consequences of an action in a very wide sense that includes the totality of its effects. This is based on the idea that actions make a difference in the world by bringing about a causal chain of events that would not have existed otherwise. A core intuition behind consequentialism is that the future should be shaped to achieve the best possible outcome.
The act itself is usually not seen as part of the consequences. This means that if an act has intrinsic value or disvalue, it is not included as a factor. Some consequentialists see this as a flaw, saying that all value-relevant factors need to be considered. They try to avoid this complication by including the act itself as part of the consequences. A related approach is to characterize consequentialism not in terms of consequences but in terms of outcome, with the outcome being defined as the act together with its consequences.
Most forms of consequentialism are agent-neutral. This means that the value of consequences is assessed from a neutral perspective, that is, acts should have consequences that are good in general and not just good for the agent. It is controversial whether agent-relative moral theories, like ethical egoism, should be considered as types of consequentialism.
 Types 
There are many different types of consequentialism. They differ based on what type of entity they evaluate, what consequences they take into consideration, and how they determine the value of consequences. Most theories assess the moral value of acts. However, consequentialism can also be used to evaluate motives, character traits, rules, and policies.
Many types assess the value of consequences based on whether they promote happiness or suffering. But there are also alternative evaluative principles, such as desire satisfaction, autonomy, freedom, knowledge, friendship, beauty, and self-perfection. Some forms of consequentialism hold that there is only a single source of value. The most prominent among them is classical utilitarianism, which states that the moral value of acts only depends on the pleasure and suffering they cause. An alternative approach says that there are many different sources of value, which all contribute to one overall value. Before the 20th century, consequentialists were only concerned with the total of value or the aggregate good. In the 20th century, alternative views were developed that additionally consider the distribution of value. One of them states that an equal distribution of goods is better than an unequal distribution even if the aggregate good is the same.
There are disagreements about which consequences should be assessed. An important distinction is between act consequentialism and rule consequentialism. According to act consequentialism, the consequences of an act determine its moral value. This means that there is a direct relation between the consequences of an act and its moral value. Rule consequentialism, by contrast, holds that an act is right if it follows a certain set of rules. Rule consequentialism determines the best rules by considering their outcomes at a community level. People should follow the rules that lead to the best consequences when everyone in the community follows them. This implies that the relation between an act and its consequences is indirect. For example, if telling the truth is one of the best rules, then according to rule consequentialism, a person should tell the truth even in specific cases where lying would lead to better consequences.
Another disagreement is between actual and expected consequentialism. According to the traditional view, only the actual consequences of an act affect its moral value. One difficulty of this view is that many consequences cannot be known in advance. This means that in some cases, even well-planned and intentioned acts are morally wrong if they inadvertently lead to negative outcomes. An alternative perspective states that what matters are not the actual consequences but the expected consequences. This view takes into account that when deciding what to do, people have to rely on their limited knowledge of the total consequences of their actions. According to this view, a course of action has positive moral value despite leading to an overall negative outcome if it had the highest expected value, for example, because the negative outcome could not be anticipated or was unlikely.
A further difference is between maximizing and satisficing consequentialism. According to maximizing consequentialism, only the best possible act is morally permitted. This means that acts with positive consequences are wrong if there are alternatives with even better consequences. One criticism of maximizing consequentialism is that it demands too much by requiring that people do significantly more than they are socially expected to. For example, if the best action for someone with a good salary would be to donate 70% of their income to charity, it would be morally wrong for them to only donate 65%. Satisficing consequentialism, by contrast, only requires that an act is "good enough" even if it is not the best possible alternative. According to this view, it is possible to do more than one is morally required to do.
Mohism in ancient Chinese philosophy is one of the earliest forms of consequentialism. It arose in the 5th century BCE and argued that political action should promote justice as a means to increase the welfare of the people.
 Utilitarianism 
The most well-known form of consequentialism is utilitarianism. In its classical form, it is an act consequentialism that sees happiness as the only source of intrinsic value. This means that an act is morally right if it produces "the greatest good for the greatest number" by increasing happiness and reducing suffering. Utilitarians do not deny that other things also have value, like health, friendship, and knowledge. However, they deny that these things have intrinsic value. Instead, they say that they have extrinsic value because they affect happiness and suffering. In this regard, they are desirable as a means but, unlike happiness, not as an end. The view that pleasure is the only thing with intrinsic value is called ethical or evaluative hedonism.
Classical utilitarianism was initially formulated by Jeremy Bentham at the end of the 18th century and further developed by John Stuart Mill. Bentham introduced the hedonic calculus to assess the value of consequences. Two key aspects of the hedonic calculus are the intensity and the duration of pleasure. According to this view, a pleasurable experience has a high value if it has a high intensity and lasts for a long time. A common criticism of Bentham's utilitarianism argued that its focus on the intensity of pleasure promotes an immoral lifestyle centered around indulgence in sensory gratification. Mill responded to this criticism by distinguishing between higher and lower pleasures. He stated that higher pleasures, like the intellectual satisfaction of reading a book, are more valuable than lower pleasures, like the sensory enjoyment of food and drink, even if their intensity and duration are the same. Since its original formulation, many variations of utilitarianism have developed, including the difference between act and rule utilitarianism and between maximizing and satisficing utilitarianism.
=
Deontology assesses the moral rightness of actions based on a set of norms or principles. These norms describe the requirements that all actions need to follow. They may include principles like telling the truth, keeping promises, and not intentionally harming others. Unlike consequentialists, deontologists hold that the validity of general moral principles does not directly depend on their consequences. They state that these principles should be followed in every case since they express how actions are inherently right or wrong. According to moral philosopher David Ross, it is wrong to break a promise even if no harm comes from it. Deontologists are interested in which actions are right and often allow that there is a gap between what is right and what is good. Many focus on prohibitions and describe which acts are forbidden under any circumstances.
 Agent-centered and patient-centered 
Agent-centered deontological theories focus on the person who acts and the duties they have. Agent-centered theories often focus on the motives and intentions behind people's actions, highlighting the importance of acting for the right reasons. They tend to be agent-relative, meaning that the reasons for which people should act depend on personal circumstances. For example, a parent has a special obligation to their child, while a stranger does not have this kind of obligation toward a child they do not know. Patient-centered theories, by contrast, focus on the people affected by actions and the rights they have. An example is the requirement to treat other people as ends and not merely as a means to an end. This requirement can be used to argue, for example, that it is wrong to kill a person against their will even if this act would save the lives of several others. Patient-centered deontological theories are usually agent-neutral, meaning that they apply equally to everyone in a situation, regardless of their specific role or position.
 Kantianism 
Immanuel Kant (1724–1804) is one of the most well-known deontologists. He states that reaching outcomes that people desire, such as being happy, is not the main purpose of moral actions. Instead, he argues that there are universal principles that apply to everyone independent of their desires. He uses the term categorical imperative for these principles, saying that they have their source in the structure of practical reason and are true for all rational agents. According to Kant, to act morally is to act in agreement with reason as expressed by these principles while violating them is both immoral and irrational.
Kant provided several formulations of the categorical imperative. One formulation says that a person should only follow maxims that can be universalized. This means that the person would want everyone to follow the same maxim as a universal law applicable to everyone. Another formulation states that one should treat other people always as ends in themselves and never as mere means to an end. This formulation focuses on respecting and valuing other people for their own sake rather than using them in the pursuit of personal goals.
In either case, Kant says that what matters is to have a good will. A person has a good will if they respect the moral law and form their intentions and motives in agreement with it. Kant states that actions motivated in such a way are unconditionally good, meaning that they are good even in cases where they result in undesirable consequences.
 Others 
Divine command theory says that God is the source of morality. It states that moral laws are divine commands and that to act morally is to obey and follow God's will. While all divine command theorists agree that morality depends on God, there are disagreements about the precise content of the divine commands, and theorists belonging to different religions tend to propose different moral laws. For example, Christian and Jewish divine command theorists may argue that the Ten Commandments express God's will while Muslims may reserve this role for the teachings of the Quran.
Contractualists reject the reference to God as the source of morality and argue instead that morality is based on an explicit or implicit social contract between humans. They state that actual or hypothetical consent to this contract is the source of moral norms and duties. To determine which duties people have, contractualists often rely on a thought experiment about what rational people under ideal circumstances would agree on. For example, if they would agree that people should not lie then there is a moral obligation to refrain from lying. Because it relies on consent, contractualism is often understood as a patient-centered form of deontology. Famous social contract theorists include Thomas Hobbes, John Locke, Jean-Jacques Rousseau, and John Rawls.
Discourse ethics also focuses on social agreement on moral norms but says that this agreement is based on communicative rationality. It aims to arrive at moral norms for pluralistic modern societies that encompass a diversity of viewpoints. A universal moral norm is seen as valid if all rational discourse participants do or would approve. This way, morality is not imposed by a single moral authority but arises from the moral discourse within society. This discourse should aim to establish an ideal speech situation to ensure fairness and inclusivity. In particular, this means that discourse participants are free to voice their different opinions without coercion but are at the same time required to justify them using rational argumentation.
=
The main concern of virtue ethics is how virtues are expressed in actions. As such, it is neither directly interested in the consequences of actions nor in universal moral duties. Virtues are positive character traits like honesty, courage, kindness, and compassion. They are usually understood as dispositions to feel, decide, and act in a certain manner by being wholeheartedly committed to this manner. Virtues contrast with vices, which are their harmful counterparts.
Virtue theorists usually say that the mere possession of virtues by itself is not sufficient. Instead, people should manifest virtues in their actions. An important factor is the practical wisdom, also called phronesis, of knowing when, how, and which virtue to express. For example, a lack of practical wisdom may lead courageous people to perform morally wrong actions by taking unnecessary risks that should better be avoided.
Different types of virtue ethics differ on how they understand virtues and their role in practical life. Eudaimonism is the original form of virtue theory developed in Ancient Greek philosophy and draws a close relation between virtuous behavior and happiness. It states that people flourish by living a virtuous life. Eudaimonist theories often hold that virtues are positive potentials residing in human nature and that actualizing these potentials results in leading a good and happy life. Agent-based theories, by contrast, see happiness only as a side effect and focus instead on the admirable traits and motivational characteristics expressed while acting. This is often combined with the idea that one can learn from exceptional individuals what those characteristics are. Feminist ethics of care are another form of virtue ethics. They emphasize the importance of interpersonal relationships and say that benevolence by caring for the well-being of others is one of the key virtues.
Influential schools of virtue ethics in ancient philosophy were Aristotelianism and Stoicism. According to Aristotle (384–322 BCE), each virtue is a golden mean between two types of vices: excess and deficiency. For example, courage is a virtue that lies between the deficient state of cowardice and the excessive state of recklessness. Aristotle held that virtuous action leads to happiness and makes people flourish in life. Stoicism emerged about 300 BCE and taught that, through virtue alone, people can achieve happiness characterized by a peaceful state of mind free from emotional disturbances. The Stoics advocated rationality and self-mastery to achieve this state. In the latter half of the 20th century, virtue ethics experienced a resurgence thanks to philosophers such as Elizabeth Anscombe, Philippa Foot, Alasdair MacIntyre, and Martha Nussbaum.
=
There are many other schools of normative ethics in addition to the three main traditions. Pragmatist ethics focuses on the role of practice and holds that one of the key tasks of ethics is to solve practical problems in concrete situations. It has certain similarities to utilitarianism and its focus on consequences but concentrates more on how morality is embedded in and relative to social and cultural contexts. Pragmatists tend to give more importance to habits than to conscious deliberation and understand morality as a habit that should be shaped in the right way.
Postmodern ethics agrees with pragmatist ethics about the cultural relativity of morality. It rejects the idea that there are objective moral principles that apply universally to all cultures and traditions. It asserts that there is no one coherent ethical code since morality itself is irrational and humans are morally ambivalent beings. Postmodern ethics instead focuses on how moral demands arise in specific situations as one encounters other people.
Ethical egoism is the view that people should act in their self-interest or that an action is morally right if the person acts for their own benefit. It differs from psychological egoism, which states that people actually follow their self-interest without claiming that they should do so. Ethical egoists may act in agreement with commonly accepted moral expectations and benefit other people, for example, by keeping promises, helping friends, and cooperating with others. However, they do so only as a means to promote their self-interest. Ethical egoism is often criticized as an immoral and contradictory position.
Normative ethics has a central place in most religions. Key aspects of Jewish ethics are to follow the 613 commandments of God according to the Mitzvah duty found in the Torah and to take responsibility for societal welfare. Christian ethics puts less emphasis on following precise laws and teaches instead the practice of selfless love, such as the Great Commandment to "Love your neighbor as yourself". The Five Pillars of Islam constitute a basic framework of Muslim ethics and focus on the practice of faith, prayer, charity, fasting during Ramadan, and pilgrimage to Mecca.
Buddhists emphasize the importance of compassion and loving-kindness towards all sentient entities. A similar outlook is found in Jainism, which has non-violence as its principal virtue. Duty is a central aspect of Hindu ethics and is about fulfilling social obligations, which may vary depending on a person's social class and stage of life. Confucianism places great emphasis on harmony in society and sees benevolence as a key virtue. Taoism extends the importance of living in harmony to the whole world and teaches that people should practice effortless action by following the natural flow of the universe. Indigenous belief systems, like Native American philosophy and the African Ubuntu philosophy, often emphasize the interconnectedness of all living beings and the environment while stressing the importance of living in harmony with nature.
Metaethics is the branch of ethics that examines the nature, foundations, and scope of moral judgments, concepts, and values. It is not interested in which actions are right but in what it means for an action to be right and whether moral judgments are objective and can be true at all. It further examines the meaning of morality and other moral terms. Metaethics is a metatheory that operates on a higher level of abstraction than normative ethics by investigating its underlying assumptions. Metaethical theories typically do not directly judge which normative ethical theories are correct. However, metaethical theories can still influence normative theories by examining their foundational principles.
Metaethics overlaps with various branches of philosophy. On the level of ontology, it examines whether there are objective moral facts. Concerning semantics, it asks what the meaning of moral terms are and whether moral statements have a truth value. The epistemological side of metaethics discusses whether and how people can acquire moral knowledge. Metaethics overlaps with psychology because of its interest in how moral judgments motivate people to act. It also overlaps with anthropology since it aims to explain how cross-cultural differences affect moral assessments.
=
Metaethics examines basic ethical concepts and their relations. Ethics is primarily concerned with normative statements about what ought to be the case, in contrast to descriptive statements, which are about what is the case. Duties and obligations express requirements of what people ought to do. Duties are sometimes defined as counterparts of the rights that always accompany them. According to this view, someone has a duty to benefit another person if this other person has the right to receive that benefit.
Obligation and permission are contrasting terms that can be defined through each other: to be obligated to do something means that one is not permitted not to do it and to be permitted to do something means that one is not obligated not to do it. Some theorists define obligations in terms of values or what is good. When used in a general sense, good contrasts with bad. When describing people and their intentions, the term evil rather than bad is often employed.
Obligations are used to assess the moral status of actions, motives, and character traits. An action is morally right if it is in tune with a person's obligations and morally wrong if it violates them. Supererogation is a special moral status that applies to cases in which the agent does more than is morally required of them. To be morally responsible for an action usually means that the person possesses and exercises certain capacities or some form of control. If a person is morally responsible then it is appropriate to respond to them in certain ways, for example, by praising or blaming them.
=
A major debate in metaethics is about the ontological status of morality, questioning whether ethical values and principles are real. It examines whether moral properties exist as objective features independent of the human mind and culture rather than as subjective constructs or expressions of personal preferences and cultural norms.
Moral realists accept the claim that there are objective moral facts. This view implies that moral values are mind-independent aspects of reality and that there is an absolute fact about whether a given action is right or wrong. A consequence of this view is that moral requirements have the same ontological status as non-moral facts: it is an objective fact whether there is an obligation to keep a promise just as it is an objective fact whether a thing is rectangular. Moral realism is often associated with the claim that there are universal ethical principles that apply equally to everyone. It implies that if two people disagree about a moral evaluation then at least one of them is wrong. This observation is sometimes taken as an argument against moral realism since moral disagreement is widespread in most fields.
Moral relativists reject the idea that morality is an objective feature of reality. They argue instead that moral principles are human inventions. This means that a behavior is not objectively right or wrong but only subjectively right or wrong relative to a certain standpoint. Moral standpoints may differ between persons, cultures, and historical periods. For example, moral statements like "Slavery is wrong" or "Suicide is permissible" may be true in one culture and false in another. Some moral relativists say that moral systems are constructed to serve certain goals such as social coordination. According to this view, different societies and different social groups within a society construct different moral systems based on their diverging purposes. Emotivism provides a different explanation, stating that morality arises from moral emotions, which are not the same for everyone.
Moral nihilists deny the existence of moral facts. They reject the existence of both objective moral facts defended by moral realism and subjective moral facts defended by moral relativism. They believe that the basic assumptions underlying moral claims are misguided. Some moral nihilists conclude from this that anything is allowed. A slightly different view emphasizes that moral nihilism is not itself a moral position about what is allowed and prohibited but the rejection of any moral position. Moral nihilism, like moral relativism, recognizes that people judge actions as right or wrong from different perspectives. However, it disagrees that this practice involves morality and sees it as just one type of human behavior.
 Naturalism and non-naturalism 
A central disagreement among moral realists is between naturalism and non-naturalism. Naturalism states that moral properties are natural properties accessible to empirical observation. They are similar to the natural properties investigated by the natural sciences, like color and shape. Some moral naturalists hold that moral properties are a unique and basic type of natural property. Another view states that moral properties are real but not a fundamental part of reality and can be reduced to other natural properties, such as properties describing the causes of pleasure and pain.
Non-naturalism argues that moral properties form part of reality and that moral features are not identical or reducible to natural properties. This view is usually motivated by the idea that moral properties are unique because they express what should be the case. Proponents of this position often emphasize this uniqueness by claiming that it is a fallacy to define ethics in terms of natural entities or to infer prescriptive from descriptive statements.
=
The metaethical debate between cognitivism and non-cognitivism is about the meaning of moral statements and is a part of the study of semantics. According to cognitivism, moral statements like "Abortion is morally wrong" and "Going to war is never morally justified" are truth-apt, meaning that they all have a truth value: they are either true or false. Cognitivism claims that moral statements have a truth value but is not interested in which truth value they have. It is often seen as the default position since moral statements resemble other statements, like "Abortion is a medical procedure" or "Going to war is a political decision", which have a truth value.
There is a close relation between the semantic theory of cognitivism and the ontological theory of moral realism. Moral realists assert that moral facts exist. This can be used to explain why moral statements are true or false: a statement is true if it is consistent with the facts and false otherwise. As a result, philosophers who accept one theory often accept the other as well. An exception is error theory, which combines cognitivism with moral nihilism by claiming that all moral statements are false because there are no moral facts.
Non-cognitivism is the view that moral statements lack a truth value. According to this view, the statement "Murder is wrong" is neither true nor false. Some non-cognitivists claim that moral statements have no meaning at all. A different interpretation is that they have another type of meaning. Emotivism says that they articulate emotional attitudes. According to this view, the statement "Murder is wrong" expresses that the speaker has a negative moral attitude towards murder or disapproves of it. Prescriptivism, by contrast, understands moral statements as commands. According to this view, stating that "Murder is wrong" expresses a command like "Do not commit murder".
=
The epistemology of ethics studies whether or how one can know moral truths. Foundationalist views state that some moral beliefs are basic and do not require further justification. Ethical intuitionism is one such view that says that humans have a special cognitive faculty through which they can know right from wrong. Intuitionists often argue that general moral truths, like "Lying is wrong", are self-evident and that it is possible to know them without relying on empirical experience. A different foundationalist position focuses on particular observations rather than general intuitions. It says that if people are confronted with a concrete moral situation, they can perceive whether right or wrong conduct was involved.
In contrast to foundationalists, coherentists say that there are no basic moral beliefs. They argue that beliefs form a complex network and mutually support and justify one another. According to this view, a moral belief can only amount to knowledge if it coheres with the rest of the beliefs in the network. Moral skeptics say that people are unable to distinguish between right and wrong behavior, thereby rejecting the idea that moral knowledge is possible. A common objection by critics of moral skepticism asserts that it leads to immoral behavior.
Thought experiments are used as a method in ethics to decide between competing theories. They usually present an imagined situation involving an ethical dilemma and explore how people's intuitions of right and wrong change based on specific details in that situation. For example, in Philippa Foot's trolley problem, a person can flip a switch to redirect a trolley from one track to another, thereby sacrificing the life of one person to save five. This scenario explores how the difference between doing and allowing harm affects moral obligations. Another thought experiment, proposed by Judith Jarvis Thomson, examines the moral implications of abortion by imagining a situation in which a person gets connected without their consent to an ill violinist. In this scenario, the violinist dies if the connection is severed, similar to how a fetus dies in the case of abortion. The thought experiment explores whether it would be morally permissible to sever the connection within the next nine months.
=
On the level of psychology, metaethics is interested in how moral beliefs and experiences affect behavior. According to motivational internalists, there is a direct link between moral judgments and action. This means that every judgment about what is right motivates the person to act accordingly. For example, Socrates defends a strong form of motivational internalism by holding that a person can only perform an evil deed if they are unaware that it is evil. Weaker forms of motivational internalism say that people can act against their own moral judgments, for example, because of the weakness of the will. Motivational externalists accept that people can judge an act to be morally required without feeling a reason to engage in it. This means that moral judgments do not always provide motivational force. A closely related question is whether moral judgments can provide motivation on their own or need to be accompanied by other mental states, such as a desire to act morally.
Applied ethics, also known as practical ethics, is the branch of ethics and applied philosophy that examines concrete moral problems encountered in real-life situations. Unlike normative ethics, it is not concerned with discovering or justifying universal ethical principles. Instead, it studies how those principles can be applied to specific domains of practical life, what consequences they have in these fields, and whether additional domain-specific factors need to be considered.
One of the main challenges of applied ethics is to breach the gap between abstract universal theories and their application to concrete situations. For example, an in-depth understanding of Kantianism or utilitarianism is usually not sufficient to decide how to analyze the moral implications of a medical procedure like abortion. One reason is that it may not be clear how the Kantian requirement of respecting everyone's personhood applies to a fetus or, from a utilitarian perspective, what the long-term consequences are in terms of the greatest good for the greatest number. This difficulty is particularly relevant to applied ethicists who employ a top-down methodology by starting from universal ethical principles and applying them to particular cases within a specific domain. A different approach is to use a bottom-up methodology, known as casuistry. This method does not start from universal principles but from moral intuitions about particular cases. It seeks to arrive at moral principles relevant to a specific domain, which may not be applicable to other domains. In either case, inquiry into applied ethics is often triggered by ethical dilemmas in which a person is subject to conflicting moral requirements.
Applied ethics covers issues belonging to both the private sphere, like right conduct in the family and close relationships, and the public sphere, like moral problems posed by new technologies and duties toward future generations. Major branches include bioethics, business ethics, and professional ethics. There are many other branches, and their domains of inquiry often overlap.
=
Bioethics covers moral problems associated with living organisms and biological disciplines. A key problem in bioethics is how features such as consciousness, being able to feel pleasure and pain, rationality, and personhood affect the moral status of entities. These differences concern, for example, how to treat non-living entities like rocks and non-sentient entities like plants in contrast to animals, and whether humans have a different moral status than other animals. According to anthropocentrism, only humans have a basic moral status. This suggests that all other entities possess a derivative moral status only insofar as they impact human life. Sentientism, by contrast, extends an inherent moral status to all sentient beings. Further positions include biocentrism, which also covers non-sentient lifeforms, and ecocentrism, which states that all of nature has a basic moral status.
Bioethics is relevant to various aspects of life and many professions. It covers a wide range of moral problems associated with topics like abortion, cloning, stem cell research, euthanasia, suicide, animal testing, intensive animal farming, nuclear waste, and air pollution.
Bioethics can be divided into medical ethics, animal ethics, and environmental ethics based on whether the ethical problems relate to humans, other animals, or nature in general. Medical ethics is the oldest branch of bioethics. The Hippocratic Oath is one of the earliest texts to engage in medical ethics by establishing ethical guidelines for medical practitioners like a prohibition to harm the patient. Medical ethics often addresses issues related to the start and end of life. It examines the moral status of fetuses, for example, whether they are full-fledged persons and whether abortion is a form of murder. Ethical issues also arise about whether a person has the right to end their life in cases of terminal illness or chronic suffering and if doctors may help them do so. Other topics in medical ethics include medical confidentiality, informed consent, research on human beings, organ transplantation, and access to healthcare.
Animal ethics examines how humans should treat other animals. This field often emphasizes the importance of animal welfare while arguing that humans should avoid or minimize the harm done to animals. There is wide agreement that it is wrong to torture animals for fun. The situation is more complicated in cases where harm is inflicted on animals as a side effect of the pursuit of human interests. This happens, for example, during factory farming, when using animals as food, and for research experiments on animals. A key topic in animal ethics is the formulation of animal rights. Animal rights theorists assert that animals have a certain moral status and that humans should respect this status when interacting with them. Examples of suggested animal rights include the right to life, the right to be free from unnecessary suffering, and the right to natural behavior in a suitable environment.
Environmental ethics deals with moral problems relating to the natural environment including animals, plants, natural resources, and ecosystems. In its widest sense, it covers the whole cosmos. In the domain of agriculture, this concerns the circumstances under which the vegetation of an area may be cleared to use it for farming and the implications of planting genetically modified crops. On a wider scale, environmental ethics addresses the problem of global warming and people's responsibility on the individual and collective levels, including topics like climate justice and duties towards future generations. Environmental ethicists often promote sustainable practices and policies directed at protecting and conserving ecosystems and biodiversity.
=
Business ethics examines the moral implications of business conduct and how ethical principles apply to corporations and organizations. A key topic is corporate social responsibility, which is the responsibility of corporations to act in a manner that benefits society at large. Corporate social responsibility is a complex issue since many stakeholders are directly and indirectly involved in corporate decisions, such as the CEO, the board of directors, and the shareholders. A closely related topic is the question of whether corporations themselves, and not just their stakeholders, have moral agency. Business ethics further examines the role of honesty and fairness in business practices as well as the moral implications of bribery, conflict of interest, protection of investors and consumers, worker's rights, ethical leadership, and corporate philanthropy.
Professional ethics is a closely related field that studies ethical principles applying to members of a specific profession, like engineers, medical doctors, lawyers, and teachers. It is a diverse field since different professions often have different responsibilities. Principles applying to many professions include that the professional has the required expertise for the intended work and that they have personal integrity and are trustworthy. Further principles are to serve the interest of their target group, follow client confidentiality, and respect and uphold the client's rights, such as informed consent. More precise requirements often vary between professions. A cornerstone of engineering ethics is to protect public safety, health, and well-being. Legal ethics emphasizes the importance of respect for justice, personal integrity, and confidentiality. Key factors in journalism ethics include accuracy, truthfulness, independence, and impartiality as well as proper attribution to avoid plagiarism.
=
Many other fields of applied ethics are discussed in the academic literature. Communication ethics covers moral principles of communicative conduct. Two key issues in it are freedom of speech and speech responsibility. Freedom of speech concerns the ability to articulate one's opinions and ideas without the threats of punishment and censorship. Speech responsibility is about being accountable for the consequences of communicative action and inaction. A closely related field is information ethics, which focuses on the moral implications of creating, controlling, disseminating, and using information.
The ethics of technology examines the moral issues associated with the creation and use of any artifact, from simple spears to high-tech computers and nanotechnology. Central topics in the ethics of technology include the risks associated with creating new technologies, their responsible use, and questions about human enhancement through technological means, such as performance-enhancing drugs and genetic enhancement. Important subfields include computer ethics, ethics of artificial intelligence, machine ethics, ethics of nanotechnology, and nuclear ethics.
The ethics of war investigates moral problems of war and violent conflicts. According to just war theory, waging war is morally justified if it fulfills certain conditions. These conditions are commonly divided into requirements concerning the cause to initiate violent activities, such as self-defense, and the way those violent activities are conducted, such as avoiding excessive harm to civilians in the pursuit of legitimate military targets. Military ethics is a closely related field that is interested in the conduct of military personnel. It governs questions of the circumstances under which they are permitted to kill enemies, destroy infrastructure, and put the lives of their own troops at risk. Additional topics are the recruitment, training, and discharge of military personnel.
Other fields of applied ethics include political ethics, which examines the moral dimensions of political decisions, educational ethics, which covers ethical issues related to proper teaching practices, and sexual ethics, which addresses the moral implications of sexual behavior.
=
Value theory, also called axiology, is the philosophical study of value. It examines the nature and types of value. A central distinction is between intrinsic and instrumental value. An entity has intrinsic value if it is good in itself or good for its own sake. An entity has instrumental value if it is valuable as a means to something else, for example, by causing something that has intrinsic value. Other topics include what kinds of things have value and how valuable they are. For instance, axiological hedonists say that pleasure is the only source of intrinsic value and that the magnitude of value corresponds to the degree of pleasure. Axiological pluralists, by contrast, hold that there are different sources of intrinsic value, such as happiness, knowledge, and beauty.
There are disagreements about the exact relation between value theory and ethics. Some philosophers characterize value theory as a subdiscipline of ethics while others see value theory as the broader term that encompasses other fields besides ethics, such as aesthetics and political philosophy. A different characterization sees the two disciplines as overlapping but distinct fields. The term axiological ethics is sometimes used for the discipline studying this overlap, that is, the part of ethics that studies values. The two disciplines are sometimes distinguished based on their focus: ethics is about moral behavior or what is right while value theory is about value or what is good. Some ethical theories, like consequentialism, stand very close to value theory by defining what is right in terms of what is good. But this is not true for ethics in general and deontological theories tend to reject the idea that what is good can be used to define what is right.
=
Moral psychology explores the psychological foundations and processes involved in moral behavior. It is an empirical science that studies how humans think and act in moral contexts. It is interested in how moral reasoning and judgments take place, how moral character forms, what sensitivity people have to moral evaluations, and how people attribute and react to moral responsibility.
One of its key topics is moral development or the question of how morality develops on a psychological level from infancy to adulthood. According to Lawrence Kohlberg, children go through different stages of moral development as they understand moral principles first as fixed rules governing reward and punishment, then as conventional social norms, and later as abstract principles of what is objectively right across societies. A closely related question is whether and how people can be taught to act morally.
Evolutionary ethics, a closely related field, explores how evolutionary processes have shaped ethics. One of its key ideas is that natural selection is responsible for moral behavior and moral sensitivity. It interprets morality as an adaptation to evolutionary pressure that augments fitness by offering a selective advantage. Altruism, for example, can provide benefits to group survival by improving cooperation. Some theorists, like Mark Rowlands, argue that morality is not limited to humans, meaning that some non-human animals act based on moral emotions. Others explore evolutionary precursors to morality in non-human animals.
=
Descriptive ethics, also called comparative ethics, studies existing moral codes, practices, and beliefs. It investigates and compares moral phenomena in different societies and different groups within a society. It aims to provide a value-neutral and empirical description without judging or justifying which practices are objectively right. For instance, the question of how nurses think about the ethical implications of abortion belongs to descriptive ethics. Another example is descriptive business ethics, which describes ethical standards in the context of business, including common practices, official policies, and employee opinions. Descriptive ethics also has a historical dimension by exploring how moral practices and beliefs have changed over time.
Descriptive ethics is a multidisciplinary field that is covered by disciplines such as anthropology, sociology, psychology, and history. Its empirical outlook contrasts with the philosophical inquiry into normative questions, such as which ethical principles are correct and how to justify them.
The history of ethics studies how moral philosophy has developed and evolved in the course of history. It has its origin in ancient civilizations. In ancient Egypt, the concept of Maat was used as an ethical principle to guide behavior and maintain order by emphasizing the importance of truth, balance, and harmony. In ancient India starting in the 2nd millennium BCE, the Vedas and later Upanishads were composed as the foundational texts of Hindu philosophy and discussed the role of duty and the consequences of one's actions. Buddhist ethics originated in ancient India between the 6th and 5th centuries BCE and advocated compassion, non-violence, and the pursuit of enlightenment. Ancient China in the 6th century BCE saw the emergence of Confucianism, which focuses on moral conduct and self-cultivation by acting in agreement with virtues, and Daoism, which teaches that human behavior should be in harmony with the natural order of the universe.
In ancient Greece, Socrates (c. 469–399 BCE) emphasized the importance of inquiry into what a good life is by critically questioning established ideas and exploring concepts like virtue, justice, courage, and wisdom. According to Plato (c. 428–347 BCE), to lead a good life means that the different parts of the soul are in harmony with each other. For Aristotle (384–322 BCE), a good life is associated with being happy by cultivating virtues and flourishing. Starting in the 4th century BCE, the close relation between right action and happiness was also explored by the Hellenistic schools of Epicureanism, which recommended a simple lifestyle without indulging in sensory pleasures, and Stoicism, which advocated living in tune with reason and virtue while practicing self-mastery and becoming immune to disturbing emotions.
Ethical thought in the medieval period was strongly influenced by religious teachings. Christian philosophers interpreted moral principles as divine commands originating from God. Thomas Aquinas (1224–1274 CE) developed natural law ethics by claiming that ethical behavior consists in following the laws and order of nature, which he believed were created by God. In the Islamic world, philosophers like Al-Farabi (c. 878–950 CE) and Avicenna (980–1037 CE) synthesized ancient Greek philosophy with the ethical teachings of Islam while emphasizing the harmony between reason and faith. In medieval India, Hindu philosophers like Adi Shankara (c. 700–750 CE) and Ramanuja (1017–1137 CE) saw the practice of spirituality to attain liberation as the highest goal of human behavior.
Moral philosophy in the modern period was characterized by a shift toward a secular approach to ethics. Thomas Hobbes (1588–1679) identified self-interest as the primary drive of humans. He concluded that it would lead to "a war of every man against every man" unless a social contract is established to avoid this outcome. David Hume (1711–1776) thought that only moral sentiments, like empathy, can motivate ethical actions while he saw reason not as a motivating factor but only as what anticipates the consequences of possible actions. Immanuel Kant (1724–1804), by contrast, saw reason as the source of morality. He formulated a deontological theory, according to which the ethical value of actions depends on their conformity with moral laws independent of their outcome. These laws take the form of categorical imperatives, which are universal requirements that apply to every situation.
Georg Wilhelm Friedrich Hegel (1770–1831) saw Kant's categorical imperative on its own as an empty formalism and emphasized the role of social institutions in providing concrete content to moral duties. According to the Christian philosophy of Søren Kierkegaard (1813–1855), the demands of ethical duties are sometimes suspended when doing God's will. Friedrich Nietzsche (1844–1900) formulated criticisms of both Christian and Kantian morality. Another influential development in this period was the formulation of utilitarianism by Jeremy Bentham (1748–1832) and John Stuart Mill (1806–1873). According to the utilitarian doctrine, actions should promote happiness while reducing suffering and the right action is the one that produces the greatest good for the greatest number of people.
An important development in 20th-century ethics in analytic philosophy was the emergence of metaethics. Significant early contributions to this field were made by G. E. Moore (1873–1958), who argued that moral values are essentially different from other properties found in the natural world. R. M. Hare (1919–2002) followed this idea in formulating his prescriptivism, which states that moral statements are commands that, unlike regular judgments, are neither true nor false. J. L. Mackie (1917–1981) suggested that every moral statement is false since there are no moral facts. An influential argument for moral realism was made by Derek Parfit (1942–2017), who argued that morality concerns objective features of reality that give people reasons to act in one way or another. Bernard Williams (1929–2003) agreed with the close relation between reasons and ethics but defended a subjective view instead that sees reasons as internal mental states that may or may not reflect external reality.
Another development in this period was the revival of ancient virtue ethics by philosophers like Philippa Foot (1920–2010). In the field of political philosophy, John Rawls (1921–2002) relied on Kantian ethics to analyze social justice as a form of fairness. In continental philosophy, phenomenologists such as Max Scheler (1874–1928) and Nicolai Hartmann (1882–1950) built ethical systems based on the claim that values have objective reality that can be investigated using the phenomenological method. Existentialists like Jean-Paul Sartre (1905–1980) and Simone de Beauvoir (1908–1986), by contrast, held that values are created by humans and explored the consequences of this view in relation to individual freedom, responsibility, and authenticity. This period also saw the emergence of feminist ethics, which questions traditional ethical assumptions associated with a male perspective and puts alternative concepts, like care, at the center.
=
=
=
Meta-Ethics at PhilPapers
Normative Ethics at PhilPapers
Applied Ethics at PhilPapers
Ethics at the Indiana Philosophy Ontology Project

--- TOPIC: Psychology ---
Psychology is the scientific study of the mind and behavior. Its subject matter includes the behavior of humans and nonhumans, both conscious and unconscious phenomena, and mental processes such as thoughts, feelings, and motives. Psychology is an academic discipline of immense scope, crossing the boundaries between the natural and social sciences. Biological psychologists seek an understanding of the emergent properties of brains, linking the discipline to neuroscience. As social scientists, psychologists aim to understand the behavior of individuals and groups.
A professional practitioner or researcher involved in the discipline is called a psychologist. Some psychologists can also be classified as behavioral or cognitive scientists. Some psychologists attempt to understand the role of mental functions in individual and social behavior. Others explore the physiological and neurobiological processes that underlie cognitive functions and behaviors.
As part of an interdisciplinary field, psychologists are involved in research on perception, cognition, attention, emotion, intelligence, subjective experiences, motivation, brain functioning, and personality. Psychologists' interests extend to interpersonal relationships, psychological resilience, family resilience, and other areas within social psychology. They also consider the unconscious mind. Research psychologists employ empirical methods to infer causal and correlational relationships between psychosocial variables. Some, but not all, clinical and counseling psychologists rely on symbolic interpretation.
While psychological knowledge is often applied to the assessment and treatment of mental health problems, it is also directed towards understanding and solving problems in several spheres of human activity. By many accounts, psychology ultimately aims to benefit society. Many psychologists are involved in some kind of therapeutic role, practicing psychotherapy in clinical, counseling, or school settings. Other psychologists conduct scientific research on a wide range of topics related to mental processes and behavior. Typically the latter group of psychologists work in academic settings (e.g., universities, medical schools, or hospitals). Another group of psychologists is employed in industrial and organizational settings. Yet others are involved in work on human development, aging, sports, health, forensic science, education, and the media.
The word psychology derives from the Greek word psyche, for spirit or soul. The latter part of the word psychology derives from -λογία -logia, which means "study" or "research". The word psychology was first used in the Renaissance. In its Latin form psychiologia, it was first employed by the Croatian humanist and Latinist Marko Marulić in his book Psichiologia de ratione animae humanae (Psychology, on the Nature of the Human Soul) in the decade 1510–1520 The earliest known reference to the word psychology in English was by Steven Blankaart in 1694 in The Physical Dictionary. The dictionary refers to "Anatomy, which treats the Body, and Psychology, which treats of the Soul."
Ψ (psi), the first letter of the Greek word psyche from which the term psychology is derived, is commonly associated with the field of psychology.
In 1890, William James defined psychology as "the science of mental life, both of its phenomena and their conditions." This definition enjoyed widespread currency for decades. However, this meaning was contested, notably by John B. Watson, who in 1913 asserted the methodological behaviorist view of psychology as a purely objective experimental branch of natural science, the theoretical goal of which "is the prediction and control of behavior." Since James defined "psychology", the term more strongly implicates scientific experimentation. Folk psychology is the understanding of the mental states and behaviors of people held by ordinary people, as contrasted with psychology professionals' understanding.
The ancient civilizations of Egypt, Greece, China, India, and Persia all engaged in the philosophical study of psychology. In Ancient Egypt the Ebers Papyrus mentioned depression and thought disorders. Historians note that Greek philosophers, including Thales, Plato, and Aristotle (especially in his De Anima treatise), addressed the workings of the mind. As early as the 4th century BCE, the Greek physician Hippocrates theorized that mental disorders had physical rather than supernatural causes. In 387 BCE, Plato suggested that the brain is where mental processes take place, and in 335 BC Aristotle suggested that it was the heart.
In China, the foundations of psychological thought emerged from the philosophical works of ancient thinkers like Laozi and Confucius, as well as the teachings of Buddhism. This body of knowledge drew insights from introspection, observation, and techniques for focused thinking and behavior. It viewed the universe as comprising physical and mental realms, along with the interplay between the two. Chinese philosophy also emphasized purifying the mind in order to increase virtue and power. An ancient text known as The Yellow Emperor's Classic of Internal Medicine identifies the brain as the nexus of wisdom and sensation, includes theories of personality based on yin–yang balance, and analyzes mental disorder in terms of physiological and social disequilibria. Chinese scholarship that focused on the brain advanced during the Qing dynasty with the work of Western-educated Fang Yizhi (1611–1671), Liu Zhi (1660–1730), and Wang Qingren (1768–1831). Wang Qingren emphasized the importance of the brain as the center of the nervous system, linked mental disorder with brain diseases, investigated the causes of dreams and insomnia, and advanced a theory of hemispheric lateralization in brain function.
Influenced by Hinduism, Indian philosophy explored distinctions in types of awareness. A central idea of the Upanishads and other Vedic texts that formed the foundations of Hinduism was the distinction between a person's transient mundane self and their eternal, unchanging soul. Divergent Hindu doctrines and Buddhism have challenged this hierarchy of selves, but have all emphasized the importance of reaching higher awareness. Yoga encompasses a range of techniques used in pursuit of this goal. Theosophy, a religion established by Russian-American philosopher Helena Blavatsky, drew inspiration from these doctrines during her time in British India.
Psychology was of interest to Enlightenment thinkers in Europe. In Germany, Gottfried Wilhelm Leibniz (1646–1716) applied his principles of calculus to the mind, arguing that mental activity took place on an indivisible continuum. He suggested that the difference between conscious and unconscious awareness is only a matter of degree. Christian Wolff identified psychology as its own science, writing Psychologia Empirica in 1732 and Psychologia Rationalis in 1734. Immanuel Kant advanced the idea of anthropology as a discipline, with psychology an important subdivision. Kant, however, explicitly rejected the idea of an experimental psychology, writing that "the empirical doctrine of the soul can also never approach chemistry even as a systematic art of analysis or experimental doctrine, for in it the manifold of inner observation can be separated only by mere division in thought, and cannot then be held separate and recombined at will (but still less does another thinking subject suffer himself to be experimented upon to suit our purpose), and even observation by itself already changes and displaces the state of the observed object."
In 1783, Ferdinand Ueberwasser (1752–1812) designated himself Professor of Empirical Psychology and Logic and gave lectures on scientific psychology, though these developments were soon overshadowed by the Napoleonic Wars. At the end of the Napoleonic era, Prussian authorities discontinued the Old University of Münster. Having consulted philosophers Hegel and Herbart, however, in 1825 the Prussian state established psychology as a mandatory discipline in its rapidly expanding and highly influential educational system. However, this discipline did not yet embrace experimentation. In England, early psychology involved phrenology and the response to social problems including alcoholism, violence, and the country's crowded "lunatic" asylums.
=
Philosopher John Stuart Mill believed that the human mind was open to scientific investigation, even if the science is in some ways inexact. Mill proposed a "mental chemistry" in which elementary thoughts could combine into ideas of greater complexity. Gustav Fechner began conducting psychophysics research in Leipzig in the 1830s. He articulated the principle that human perception of a stimulus varies logarithmically according to its intensity. The principle became known as the Weber–Fechner law. Fechner's 1860 Elements of Psychophysics challenged Kant's negative view with regard to conducting quantitative research on the mind. Fechner's achievement was to show that "mental processes could not only be given numerical magnitudes, but also that these could be measured by experimental methods." In Heidelberg, Hermann von Helmholtz conducted parallel research on sensory perception, and trained physiologist Wilhelm Wundt. Wundt, in turn, came to Leipzig University, where he established the psychological laboratory that brought experimental psychology to the world. Wundt focused on breaking down mental processes into the most basic components, motivated in part by an analogy to recent advances in chemistry, and its successful investigation of the elements and structure of materials. Paul Flechsig and Emil Kraepelin soon created another influential laboratory at Leipzig, a psychology-related lab, that focused more on experimental psychiatry.
James McKeen Cattell, a professor of psychology at the University of Pennsylvania and Columbia University and the co-founder of Psychological Review, was the first professor of psychology in the United States.
The German psychologist Hermann Ebbinghaus, a researcher at the University of Berlin, was a 19th-century contributor to the field. He pioneered the experimental study of memory and developed quantitative models of learning and forgetting. In the early 20th century, Wolfgang Kohler, Max Wertheimer, and Kurt Koffka co-founded the school of Gestalt psychology of Fritz Perls. The approach of Gestalt psychology is based upon the idea that individuals experience things as unified wholes. Rather than reducing thoughts and behavior into smaller component elements, as in structuralism, the Gestaltists maintain that whole of experience is important, "and is something else than the sum of its parts, because summing is a meaningless procedure, whereas the whole-part relationship is meaningful."
Psychologists in Germany, Denmark, Austria, England, and the United States soon followed Wundt in setting up laboratories. G. Stanley Hall, an American who studied with Wundt, founded a psychology lab that became internationally influential. The lab was located at Johns Hopkins University. Hall, in turn, trained Yujiro Motora, who brought experimental psychology, emphasizing psychophysics, to the Imperial University of Tokyo. Wundt's assistant, Hugo Münsterberg, taught psychology at Harvard to students such as Narendra Nath Sen Gupta—who, in 1905, founded a psychology department and laboratory at the University of Calcutta. Wundt's students Walter Dill Scott, Lightner Witmer, and James McKeen Cattell worked on developing tests of mental ability. Cattell, who also studied with eugenicist Francis Galton, went on to found the Psychological Corporation. Witmer focused on the mental testing of children; Scott, on employee selection.
Another student of Wundt, the Englishman Edward Titchener, created the psychology program at Cornell University and advanced "structuralist" psychology. The idea behind structuralism was to analyze and classify different aspects of the mind, primarily through the method of introspection. William James, John Dewey, and Harvey Carr advanced the idea of functionalism, an expansive approach to psychology that underlined the Darwinian idea of a behavior's usefulness to the individual. In 1890, James wrote an influential book, The Principles of Psychology, which expanded on the structuralism. He memorably described "stream of consciousness." James's ideas interested many American students in the emerging discipline. Dewey integrated psychology with societal concerns, most notably by promoting progressive education, inculcating moral values in children, and assimilating immigrants.
A different strain of experimentalism, with a greater connection to physiology, emerged in South America, under the leadership of Horacio G. Piñero at the University of Buenos Aires. In Russia, too, researchers placed greater emphasis on the biological basis for psychology, beginning with Ivan Sechenov's 1873 essay, "Who Is to Develop Psychology and How?" Sechenov advanced the idea of brain reflexes and aggressively promoted a deterministic view of human behavior. The Russian-Soviet physiologist Ivan Pavlov discovered in dogs a learning process that was later termed "classical conditioning" and applied the process to human beings.
=
One of the earliest psychology societies was La Société de Psychologie Physiologique in France, which lasted from 1885 to 1893. The first meeting of the International Congress of Psychology sponsored by the International Union of Psychological Science took place in Paris, in August 1889, amidst the World's Fair celebrating the centennial of the French Revolution. William James was one of three Americans among the 400 attendees. The American Psychological Association (APA) was founded soon after, in 1892. The International Congress continued to be held at different locations in Europe and with wide international participation. The Sixth Congress, held in Geneva in 1909, included presentations in Russian, Chinese, and Japanese, as well as Esperanto. After a hiatus for World War I, the Seventh Congress met in Oxford, with substantially greater participation from the war-victorious Anglo-Americans. In 1929, the Congress took place at Yale University in New Haven, Connecticut, attended by hundreds of members of the APA. Tokyo Imperial University led the way in bringing new psychology to the East. New ideas about psychology diffused from Japan into China.
American psychology gained status upon the U.S.'s entry into World War I. A standing committee headed by Robert Yerkes administered mental tests ("Army Alpha" and "Army Beta") to almost 1.8 million soldiers. Subsequently, the Rockefeller family, via the Social Science Research Council, began to provide funding for behavioral research. Rockefeller charities funded the National Committee on Mental Hygiene, which disseminated the concept of mental illness and lobbied for applying ideas from psychology to child rearing. Through the Bureau of Social Hygiene and later funding of Alfred Kinsey, Rockefeller foundations helped establish research on sexuality in the U.S. Under the influence of the Carnegie-funded Eugenics Record Office, the Draper-funded Pioneer Fund, and other institutions, the eugenics movement also influenced American psychology. In the 1910s and 1920s, eugenics became a standard topic in psychology classes. In contrast to the US, in the UK psychology was met with antagonism by the scientific and medical establishments, and up until 1939, there were only six psychology chairs in universities in England.
During World War II and the Cold War, the U.S. military and intelligence agencies established themselves as leading funders of psychology by way of the armed forces and in the new Office of Strategic Services intelligence agency. University of Michigan psychologist Dorwin Cartwright reported that university researchers began large-scale propaganda research in 1939–1941. He observed that "the last few months of the war saw a social psychologist become chiefly responsible for determining the week-by-week-propaganda policy for the United States Government." Cartwright also wrote that psychologists had significant roles in managing the domestic economy. The Army rolled out its new General Classification Test to assess the ability of millions of soldiers. The Army also engaged in large-scale psychological research of troop morale and mental health. In the 1950s, the Rockefeller Foundation and Ford Foundation collaborated with the Central Intelligence Agency (CIA) to fund research on psychological warfare. In 1965, public controversy called attention to the Army's Project Camelot, the "Manhattan Project" of social science, an effort which enlisted psychologists and anthropologists to analyze the plans and policies of foreign countries for strategic purposes.
In Germany after World War I, psychology held institutional power through the military, which was subsequently expanded along with the rest of the military during Nazi Germany. Under the direction of Hermann Göring's cousin Matthias Göring, the Berlin Psychoanalytic Institute was renamed the Göring Institute. Freudian psychoanalysts were expelled and persecuted under the anti-Jewish policies of the Nazi Party, and all psychologists had to distance themselves from Freud and Adler, founders of psychoanalysis who were also Jewish. The Göring Institute was well-financed throughout the war with a mandate to create a "New German Psychotherapy." This psychotherapy aimed to align suitable Germans with the overall goals of the Reich. As described by one physician, "Despite the importance of analysis, spiritual guidance and the active cooperation of the patient represent the best way to overcome individual mental problems and to subordinate them to the requirements of the Volk and the Gemeinschaft." Psychologists were to provide Seelenführung [lit., soul guidance], the leadership of the mind, to integrate people into the new vision of a German community. Harald Schultz-Hencke melded psychology with the Nazi theory of biology and racial origins, criticizing psychoanalysis as a study of the weak and deformed. Johannes Heinrich Schultz, a German psychologist recognized for developing the technique of autogenic training, prominently advocated sterilization and euthanasia of men considered genetically undesirable, and devised techniques for facilitating this process.
After the war, new institutions were created although some psychologists, because of their Nazi affiliation, were discredited. Alexander Mitscherlich founded a prominent applied psychoanalysis journal called Psyche. With funding from the Rockefeller Foundation, Mitscherlich established the first clinical psychosomatic medicine division at Heidelberg University. In 1970, psychology was integrated into the required studies of medical students.
After the Russian Revolution, the Bolsheviks promoted psychology as a way to engineer the "New Man" of socialism. Consequently, university psychology departments trained large numbers of students in psychology. At the completion of training, positions were made available for those students at schools, workplaces, cultural institutions, and in the military. The Russian state emphasized pedology and the study of child development. Lev Vygotsky became prominent in the field of child development. The Bolsheviks also promoted free love and embraced the doctrine of psychoanalysis as an antidote to sexual repression. Although pedology and intelligence testing fell out of favor in 1936, psychology maintained its privileged position as an instrument of the Soviet Union. Stalinist purges took a heavy toll and instilled a climate of fear in the profession, as elsewhere in Soviet society. Following World War II, Jewish psychologists past and present, including Lev Vygotsky, A.R. Luria, and Aron Zalkind, were denounced; Ivan Pavlov (posthumously) and Stalin himself were celebrated as heroes of Soviet psychology. Soviet academics experienced a degree of liberalization during the Khrushchev Thaw. The topics of cybernetics, linguistics, and genetics became acceptable again. The new field of engineering psychology emerged. The field involved the study of the mental aspects of complex jobs (such as pilot and cosmonaut). Interdisciplinary studies became popular and scholars such as Georgy Shchedrovitsky developed systems theory approaches to human behavior.
Twentieth-century Chinese psychology originally modeled itself on U.S. psychology, with translations from American authors like William James, the establishment of university psychology departments and journals, and the establishment of groups including the Chinese Association of Psychological Testing (1930) and the Chinese Psychological Society (1937). Chinese psychologists were encouraged to focus on education and language learning. Chinese psychologists were drawn to the idea that education would enable modernization. John Dewey, who lectured to Chinese audiences between 1919 and 1921, had a significant influence on psychology in China. Chancellor T'sai Yuan-p'ei introduced him at Peking University as a greater thinker than Confucius. Kuo Zing-yang who received a PhD at the University of California, Berkeley, became President of Zhejiang University and popularized behaviorism. After the Chinese Communist Party gained control of the country, the Stalinist Soviet Union became the major influence, with Marxism–Leninism the leading social doctrine and Pavlovian conditioning the approved means of behavior change. Chinese psychologists elaborated on Lenin's model of a "reflective" consciousness, envisioning an "active consciousness" (pinyin: tzu-chueh neng-tung-li) able to transcend material conditions through hard work and ideological struggle. They developed a concept of "recognition" (pinyin: jen-shih) which referred to the interface between individual perceptions and the socially accepted worldview; failure to correspond with party doctrine was "incorrect recognition." Psychology education was centralized under the Chinese Academy of Sciences, supervised by the State Council. In 1951, the academy created a Psychology Research Office, which in 1956 became the Institute of Psychology. Because most leading psychologists were educated in the United States, the first concern of the academy was the re-education of these psychologists in the Soviet doctrines. Child psychology and pedagogy for the purpose of a nationally cohesive education remained a central goal of the discipline.
=
 1900–1949 
Women in the early 1900s started to make key findings within the world of psychology. In 1923, Anna Freud, the daughter of Sigmund Freud, built on her father's work using different defense mechanisms (denial, repression, and suppression) to psychoanalyze children. She believed that once a child reached the latency period, child analysis could be used as a mode of therapy. She stated it is important focus on the child's environment, support their development, and prevent neurosis. She believed a child should be recognized as their own person with their own right and have each session catered to the child's specific needs. She encouraged drawing, moving freely, and expressing themselves in any way. This helped build a strong therapeutic alliance with child patients, which allows psychologists to observe their normal behavior. She continued her research on the impact of children after family separation, children with socio-economically disadvantaged backgrounds, and all stages of child development from infancy to adolescence.
Functional periodicity, the belief women are mentally and physically impaired during menstruation, impacted women's rights because employers were less likely to hire them due to the belief they would be incapable of working for 1 week a month. Leta Stetter Hollingworth wanted to prove this hypothesis and Edward L. Thorndike's theory, that women have lesser psychological and physical traits than men and were simply mediocre, incorrect. Hollingworth worked to prove differences were not from male genetic superiority, but from culture. She also included the concept of women's impairment during menstruation in her research. She recorded both women and men performances on tasks (cognitive, perceptual, and motor) for three months. No evidence was found of decreased performance due to a woman's menstrual cycle. She also challenged the belief intelligence is inherited and women here are intellectually inferior to men. She stated that women do not reach positions of power due to the societal norms and roles they are assigned. As she states in her article, "Variability as related to sex differences in achievement: A Critique", the largest problem women have is the social order that was built due to the assumption women have less interests and abilities than men. To further prove her point, she completed another experiment with infants who have not been influenced by the environment of social norms, like the adult male getting more opportunities than women. She found no difference between infants besides size. After this research proved the original hypothesis wrong, Hollingworth was able to show there is no difference between the physiological and psychological traits of men and women, and women are not impaired during menstruation.
The first half of the 1900s was filled with new theories and it was a turning point for women's recognition within the field of psychology. In addition to the contributions made by Leta Stetter Hollingworth and Anna Freud, Mary Whiton Calkins invented the paired associates technique of studying memory and developed self-psychology. Karen Horney developed the concept of "womb envy" and neurotic needs. Psychoanalyst Melanie Klein impacted developmental psychology with her research of play therapy. These great discoveries and contributions were made during struggles of sexism, discrimination, and little recognition for their work.
 1950–1999 
Women in the second half of the 20th century continued to do research that had large-scale impacts on the field of psychology. Mary Ainsworth's work centered around attachment theory. Building off fellow psychologist John Bowlby, Ainsworth spent years doing fieldwork to understand the development of mother-infant relationships. In doing this field research, Ainsworth developed the Strange Situation Procedure, a laboratory procedure meant to study attachment style by separating and uniting a child with their mother several different times under different circumstances. These field studies are also where she developed her attachment theory and the order of attachment styles, which was a landmark for developmental psychology. Because of her work, Ainsworth became one of the most cited psychologists of all time. Mamie Phipps Clark was another woman in psychology that changed the field with her research. She was one of the first African-Americans to receive a doctoral degree in psychology from Columbia University, along with her husband, Kenneth Clark. Her master's thesis, "The Development of Consciousness in Negro Pre-School Children," argued that black children's self-esteem was negatively impacted by racial discrimination. She and her husband conduced research building off her thesis throughout the 1940s. These tests, called the doll tests, asked young children to choose between identical dolls whose only difference was race, and they found that the majority of the children preferred the white dolls and attributed positive traits to them. Repeated over and over again, these tests helped to determine the negative effects of racial discrimination and segregation on black children's self-image and development. In 1954, this research would help decide the landmark Brown v. Board of Education decision, leading to the end of legal segregation across the nation. Clark went on to be an influential figure in psychology, her work continuing to focus on minority youth.
As the field of psychology developed throughout the latter half of the 20th century, women in the field advocated for their voices to be heard and their perspectives to be valued. Second-wave feminism did not miss psychology. An outspoken feminist in psychology was Naomi Weisstein, who was an accomplished researcher in psychology and neuroscience, and is perhaps best known for her paper, "Kirche, Kuche, Kinder as Scientific Law: Psychology Constructs the Female." Psychology Constructs the Female criticized the field of psychology for centering men and using biology too much to explain gender differences without taking into account social factors. Her work set the stage for further research to be done in social psychology, especially in gender construction. Other women in the field also continued advocating for women in psychology, creating the Association for Women in Psychology to criticize how the field treated women. E. Kitsch Child, Phyllis Chesler, and Dorothy Riddle were some of the founding members of the organization in 1969.
The latter half of the 20th century further diversified the field of psychology, with women of color reaching new milestones. In 1962, Martha Bernal became the first Latina woman to get a Ph.D. in psychology. In 1969, Marigold Linton, the first Native American woman to get a Ph.D. in psychology, founded the National Indian Education Association. She was also a founding member of the Society for Advancement of Chicanos and Native Americans in Science. In 1971, The Network of Indian Psychologists was established by Carolyn Attneave. Harriet McAdoo was appointed to the White House Conference on Families in 1979.
 21st century 
In the 21st century, women have gained greater prominence in psychology, contributing significantly to a wide range of subfields. Many have taken on leadership roles, directed influential research labs, and guided the next generation of psychologists. However, gender disparities remain, especially when it comes to equal pay and representation in senior academic positions. The number of women pursuing education and training in psychological science has reached a record high. In the United States, estimates suggest that women make up about 78% of undergraduate students and 71% of graduate students in psychology.
=
In 1920, Édouard Claparède and Pierre Bovet created a new applied psychology organization called the International Congress of Psychotechnics Applied to Vocational Guidance, later called the International Congress of Psychotechnics and then the International Association of Applied Psychology. The IAAP is considered the oldest international psychology association. Today, at least 65 international groups deal with specialized aspects of psychology. In response to male predominance in the field, female psychologists in the U.S. formed the National Council of Women Psychologists in 1941. This organization became the International Council of Women Psychologists after World War II and the International Council of Psychologists in 1959. Several associations including the Association of Black Psychologists and the Asian American Psychological Association have arisen to promote the inclusion of non-European racial groups in the profession.
The International Union of Psychological Science (IUPsyS) is the world federation of national psychological societies. The IUPsyS was founded in 1951 under the auspices of the United Nations Educational, Cultural and Scientific Organization (UNESCO). Psychology departments have since proliferated around the world, based primarily on the Euro-American model. Since 1966, the Union has published the International Journal of Psychology. IAAP and IUPsyS agreed in 1976 each to hold a congress every four years, on a staggered basis.
IUPsyS recognizes 66 national psychology associations and at least 15 others exist. The American Psychological Association is the oldest and largest. Its membership has increased from 5,000 in 1945 to 100,000 in the present day. The APA includes 54 divisions, which since 1960 have steadily proliferated to include more specialties. Some of these divisions, such as the Society for the Psychological Study of Social Issues and the American Psychology–Law Society, began as autonomous groups.
The Interamerican Psychological Society, founded in 1951, aspires to promote psychology across the Western Hemisphere. It holds the Interamerican Congress of Psychology and had 1,000 members in year 2000. The European Federation of Professional Psychology Associations, founded in 1981, represents 30 national associations with a total of 100,000 individual members. At least 30 other international organizations represent psychologists in different regions.
In some places, governments legally regulate who can provide psychological services or represent themselves as a "psychologist." The APA defines a psychologist as someone with a doctoral degree in psychology.
=
Early practitioners of experimental psychology distinguished themselves from parapsychology, which in the late nineteenth century enjoyed popularity (including the interest of scholars such as William James). Some people considered parapsychology to be part of "psychology". Parapsychology, hypnotism, and psychism were major topics at the early International Congresses. But students of these fields were eventually ostracized, and more or less banished from the Congress in 1900–1905. Parapsychology persisted for a time at Imperial University in Japan, with publications such as Clairvoyance and Thoughtography by Tomokichi Fukurai, but it was mostly shunned by 1913.
As a discipline, psychology has long sought to fend off accusations that it is a "soft" science. Philosopher of science Thomas Kuhn's 1962 critique implied psychology overall was in a pre-paradigm state, lacking agreement on the type of overarching theory found in mature hard sciences such as chemistry and physics. Because some areas of psychology rely on research methods such as self-reports in surveys and questionnaires, critics asserted that psychology is not an objective science. Skeptics have suggested that personality, thinking, and emotion cannot be directly measured and are often inferred from subjective self-reports, which may be problematic. Experimental psychologists have devised a variety of ways to indirectly measure these elusive phenomenological entities.
Divisions still exist within the field, with some psychologists more oriented towards the unique experiences of individual humans, which cannot be understood only as data points within a larger population. Critics inside and outside the field have argued that mainstream psychology has become increasingly dominated by a "cult of empiricism", which limits the scope of research because investigators restrict themselves to methods derived from the physical sciences. Feminist critiques have argued that claims to scientific objectivity obscure the values and agenda of (historically) mostly male researchers. Jean Grimshaw, for example, argues that mainstream psychological research has advanced a patriarchal agenda through its efforts to control behavior.
=
Psychologists generally consider biology the substrate of thought and feeling, and therefore an important area of study. Behaviorial neuroscience, also known as biological psychology, involves the application of biological principles to the study of physiological and genetic mechanisms underlying behavior in humans and other animals. The allied field of comparative psychology is the scientific study of the behavior and mental processes of non-human animals. A leading question in behavioral neuroscience has been whether and how mental functions are localized in the brain. From Phineas Gage to H.M. and Clive Wearing, individual people with mental deficits traceable to physical brain damage have inspired new discoveries in this area. Modern behavioral neuroscience could be said to originate in the 1870s, when in France Paul Broca traced production of speech to the left frontal gyrus, thereby also demonstrating hemispheric lateralization of brain function. Soon after, Carl Wernicke identified a related area necessary for the understanding of speech.
The contemporary field of behavioral neuroscience focuses on the physical basis of behavior. Behaviorial neuroscientists use animal models, often relying on rats, to study the neural, genetic, and cellular mechanisms that underlie behaviors involved in learning, memory, and fear responses. Cognitive neuroscientists, by using neural imaging tools, investigate the neural correlates of psychological processes in humans. Neuropsychologists conduct psychological assessments to determine how an individual's behavior and cognition are related to the brain. The biopsychosocial model is a cross-disciplinary, holistic model that concerns the ways in which interrelationships of biological, psychological, and socio-environmental factors affect health and behavior.
Evolutionary psychology approaches thought and behavior from a modern evolutionary perspective. This perspective suggests that psychological adaptations evolved to solve recurrent problems in human ancestral environments. Evolutionary psychologists attempt to find out how human psychological traits are evolved adaptations, the results of natural selection or sexual selection over the course of human evolution.
The history of the biological foundations of psychology includes evidence of racism. The idea of white supremacy and indeed the modern concept of race itself arose during the process of world conquest by Europeans. Carl von Linnaeus's four-fold classification of humans classifies Europeans as intelligent and severe, Americans as contented and free, Asians as ritualistic, and Africans as lazy and capricious. Race was also used to justify the construction of socially specific mental disorders such as drapetomania and dysaesthesia aethiopica—the behavior of uncooperative African slaves. After the creation of experimental psychology, "ethnical psychology" emerged as a subdiscipline, based on the assumption that studying primitive races would provide an important link between animal behavior and the psychology of more evolved humans.
=
A tenet of behavioral research is that a large part of both human and lower-animal behavior is learned. A principle associated with behavioral research is that the mechanisms involved in learning apply to humans and non-human animals. Behavioral researchers have developed a treatment known as behavior modification, which is used to help individuals replace undesirable behaviors with desirable ones.
Early behavioral researchers studied stimulus–response pairings, now known as classical conditioning. They demonstrated that when a biologically potent stimulus (e.g., food that elicits salivation) is paired with a previously neutral stimulus (e.g., a bell) over several learning trials, the neutral stimulus by itself can come to elicit the response the biologically potent stimulus elicits. Ivan Pavlov—known best for inducing dogs to salivate in the presence of a stimulus previously linked with food—became a leading figure in the Soviet Union and inspired followers to use his methods on humans. In the United States, Edward Lee Thorndike initiated "connectionist" studies by trapping animals in "puzzle boxes" and rewarding them for escaping. Thorndike wrote in 1911, "There can be no moral warrant for studying man's nature unless the study will enable us to control his acts." From 1910 to 1913 the American Psychological Association went through a sea change of opinion, away from mentalism and towards "behavioralism." In 1913, John B. Watson coined the term behaviorism for this school of thought. Watson's famous Little Albert experiment in 1920 was at first thought to demonstrate that repeated use of upsetting loud noises could instill phobias (aversions to other stimuli) in an infant human, although such a conclusion was likely an exaggeration. Karl Lashley, a close collaborator with Watson, examined biological manifestations of learning in the brain.
Clark L. Hull, Edwin Guthrie, and others did much to help behaviorism become a widely used paradigm. A new method of "instrumental" or "operant" conditioning added the concepts of reinforcement and punishment to the model of behavior change. Radical behaviorists avoided discussing the inner workings of the mind, especially the unconscious mind, which they considered impossible to assess scientifically. Operant conditioning was first described by Miller and Kanorski and popularized in the U.S. by B.F. Skinner, who emerged as a leading intellectual of the behaviorist movement.
Noam Chomsky published an influential critique of radical behaviorism on the grounds that behaviorist principles could not adequately explain the complex mental process of language acquisition and language use. The review, which was scathing, did much to reduce the status of behaviorism within psychology. Martin Seligman and his colleagues discovered that they could condition in dogs a state of "learned helplessness", which was not predicted by the behaviorist approach to psychology. Edward C. Tolman advanced a hybrid "cognitive behavioral" model, most notably with his 1948 publication discussing the cognitive maps used by rats to guess at the location of food at the end of a maze. Skinner's behaviorism did not die, in part because it generated successful practical applications.
The Association for Behavior Analysis International was founded in 1974 and by 2003 had members from 42 countries. The field has gained a foothold in Latin America and Japan. Applied behavior analysis is the term used for the application of the principles of operant conditioning to change socially significant behavior (it supersedes the term, "behavior modification").
=
Cognitive psychology involves the study of mental processes, including perception, attention, language comprehension and production, memory, and problem solving. Researchers in the field of cognitive psychology are sometimes called cognitivists. They rely on an information processing model of mental functioning. Cognitivist research is informed by functionalism and experimental psychology.
Starting in the 1950s, the experimental techniques developed by Wundt, James, Ebbinghaus, and others re-emerged as experimental psychology became increasingly cognitivist and, eventually, constituted a part of the wider, interdisciplinary cognitive science. Some called this development the cognitive revolution because it rejected the anti-mentalist dogma of behaviorism as well as the strictures of psychoanalysis.
Albert Bandura helped along the transition in psychology from behaviorism to cognitive psychology. Bandura and other social learning theorists advanced the idea of vicarious learning. In other words, they advanced the view that a child can learn by observing the immediate social environment and not necessarily from having been reinforced for enacting a behavior, although they did not rule out the influence of reinforcement on learning a behavior.
Technological advances also renewed interest in mental states and mental representations. English neuroscientist Charles Sherrington and Canadian psychologist Donald O. Hebb used experimental methods to link psychological phenomena to the structure and function of the brain. The rise of computer science, cybernetics, and artificial intelligence underlined the value of comparing information processing in humans and machines.
A popular and representative topic in this area is cognitive bias, or irrational thought. Psychologists (and economists) have classified and described a sizeable catalog of biases which recur frequently in human thought. The availability heuristic, for example, is the tendency to overestimate the importance of something which happens to come readily to mind.
Elements of behaviorism and cognitive psychology were synthesized to form cognitive behavioral therapy, a form of psychotherapy modified from techniques developed by American psychologist Albert Ellis and American psychiatrist Aaron T. Beck.
On a broader level, cognitive science is an interdisciplinary enterprise involving cognitive psychologists, cognitive neuroscientists, linguists, and researchers in artificial intelligence, human–computer interaction, and computational neuroscience. The discipline of cognitive science covers cognitive psychology as well as philosophy of mind, computer science, and neuroscience. Computer simulations are sometimes used to model phenomena of interest.
=
Social psychology is concerned with how behaviors, thoughts, feelings, and the social environment influence human interactions. Social psychologists study such topics as the influence of others on an individual's behavior (e.g. conformity, persuasion) and the formation of beliefs, attitudes, and stereotypes about other people. Social cognition fuses elements of social and cognitive psychology for the purpose of understanding how people process, remember, or distort social information. The study of group dynamics involves research on the nature of leadership, organizational communication, and related phenomena. In recent years, social psychologists have become interested in implicit measures, mediational models, and the interaction of person and social factors in accounting for behavior. Some concepts that sociologists have applied to the study of psychiatric disorders, concepts such as the social role, sick role, social class, life events, culture, migration, and total institution, have influenced social psychologists.
=
Psychoanalysis is a collection of theories and therapeutic techniques intended to analyze the unconscious mind and its impact on everyday life. These theories and techniques inform treatments for mental disorders. Psychoanalysis originated in the 1890s, most prominently with the work of Sigmund Freud. Freud's psychoanalytic theory was largely based on interpretive methods, introspection, and clinical observation. It became very well known, largely because it tackled subjects such as sexuality, repression, and the unconscious. Freud pioneered the methods of free association and dream interpretation.
Psychoanalytic theory is not monolithic. Other well-known psychoanalytic thinkers who diverged from Freud include Alfred Adler, Carl Jung, Erik Erikson, Melanie Klein, D. W. Winnicott, Karen Horney, Erich Fromm, John Bowlby, Freud's daughter Anna Freud, and Harry Stack Sullivan. These individuals ensured that psychoanalysis would evolve into diverse schools of thought. Among these schools are ego psychology, object relations, and interpersonal, Lacanian, and relational psychoanalysis.
Psychologists such as Hans Eysenck and philosophers including Karl Popper sharply criticized psychoanalysis. Popper argued that psychoanalysis was not falsifiable (no claim it made could be proven wrong) and therefore inherently not a scientific discipline, whereas Eysenck advanced the view that psychoanalytic tenets had been contradicted by experimental data. By the end of the 20th century, psychology departments in American universities mostly had marginalized Freudian theory, dismissing it as a "desiccated and dead" historical artifact. Researchers such as António Damásio, Oliver Sacks, and Joseph LeDoux; and individuals in the emerging field of neuro-psychoanalysis have defended some of Freud's ideas on scientific grounds.
=
Humanistic psychology, which has been influenced by existentialism and phenomenology, stresses free will and self-actualization. It emerged in the 1950s as a movement within academic psychology, in reaction to both behaviorism and psychoanalysis. The humanistic approach seeks to view the whole person, not just fragmented parts of the personality or isolated cognitions. Humanistic psychology also focuses on personal growth, self-identity, death, aloneness, and freedom. It emphasizes subjective meaning, the rejection of determinism, and concern for positive growth rather than pathology. Some founders of the humanistic school of thought were American psychologists Abraham Maslow, who formulated a hierarchy of human needs, and Carl Rogers, who created and developed client-centered therapy.
Later, positive psychology opened up humanistic themes to scientific study. Positive psychology is the study of factors which contribute to human happiness and well-being, focusing more on people who are currently healthy. In 2010, Clinical Psychological Review published a special issue devoted to positive psychological interventions, such as gratitude journaling and the physical expression of gratitude. It is, however, far from clear that positive psychology is effective in making people happier. Positive psychological interventions have been limited in scope, but their effects are thought to be somewhat better than placebo effects.
The American Association for Humanistic Psychology, formed in 1963, declared:
Humanistic psychology is primarily an orientation toward the whole of psychology rather than a distinct area or school. It stands for respect for the worth of persons, respect for differences of approach, open-mindedness as to acceptable methods, and interest in exploration of new aspects of human behavior. As a "third force" in contemporary psychology, it is concerned with topics having little place in existing theories and systems: e.g., love, creativity, self, growth, organism, basic need-gratification, self-actualization, higher values, being, becoming, spontaneity, play, humor, affection, naturalness, warmth, ego-transcendence, objectivity, autonomy, responsibility, meaning, fair-play, transcendental experience, peak experience, courage, and related concepts.
Existential psychology emphasizes the need to understand a client's total orientation towards the world. Existential psychology is opposed to reductionism, behaviorism, and other methods that objectify the individual. In the 1950s and 1960s, influenced by philosophers Søren Kierkegaard and Martin Heidegger, psychoanalytically trained American psychologist Rollo May helped to develop existential psychology. Existential psychotherapy, which follows from existential psychology, is a therapeutic approach that is based on the idea that a person's inner conflict arises from that individual's confrontation with the givens of existence. Swiss psychoanalyst Ludwig Binswanger and American psychologist George Kelly may also be said to belong to the existential school. Existential psychologists tend to differ from more "humanistic" psychologists in the former's relatively neutral view of human nature and relatively positive assessment of anxiety. Existential psychologists emphasized the humanistic themes of death, free will, and meaning, suggesting that meaning can be shaped by myths and narratives; meaning can be deepened by the acceptance of free will, which is requisite to living an authentic life, albeit often with anxiety with regard to death.
Austrian existential psychiatrist and Holocaust survivor Viktor Frankl drew evidence of meaning's therapeutic power from reflections upon his own internment. He created a variation of existential psychotherapy called logotherapy, a type of existentialist analysis that focuses on a will to meaning (in one's life), as opposed to Adler's Nietzschean doctrine of will to power or Freud's will to pleasure.
=
Personality psychology is concerned with enduring patterns of behavior, thought, and emotion. Theories of personality vary across different psychological schools of thought. Each theory carries different assumptions about such features as the role of the unconscious and the importance of childhood experience. According to Freud, personality is based on the dynamic interactions of the id, ego, and super-ego. By contrast, trait theorists have developed taxonomies of personality constructs in describing personality in terms of key traits. Trait theorists have often employed statistical data-reduction methods, such as factor analysis. Although the number of proposed traits has varied widely, Hans Eysenck's early biologically based model suggests at least three major trait constructs are necessary to describe human personality, extraversion–introversion, neuroticism-stability, and psychoticism-normality. Raymond Cattell empirically derived a theory of 16 personality factors at the primary-factor level and up to eight broader second-stratum factors. Since the 1980s, the Big Five (openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism) emerged as an important trait theory of personality. Dimensional models of personality disorders are receiving increasing support, and a version of dimensional assessment, namely the Alternative DSM-5 Model for Personality Disorders, has been included in the DSM-5. However, despite a plethora of research into the various versions of the "Big Five" personality dimensions, it appears necessary to move on from static conceptualizations of personality structure to a more dynamic orientation, acknowledging that personality constructs are subject to learning and change over the lifespan.
An early example of personality assessment was the Woodworth Personal Data Sheet, constructed during World War I. The popular, although psychometrically inadequate, Myers–Briggs Type Indicator was developed to assess individuals' "personality types" according to the personality theories of Carl Jung. The Minnesota Multiphasic Personality Inventory (MMPI), despite its name, is more a dimensional measure of psychopathology than a personality measure. California Psychological Inventory contains 20 personality scales (e.g., independence, tolerance). The International Personality Item Pool, which is in the public domain, has become a source of scales that can be used personality assessment.
=
Study of the unconscious mind, a part of the psyche outside the individual's awareness but that is believed to influence conscious thought and behavior, was a hallmark of early psychology. In one of the first psychology experiments conducted in the United States, C.S. Peirce and Joseph Jastrow found in 1884 that research subjects could choose the minutely heavier of two weights even if consciously uncertain of the difference. Freud popularized the concept of the unconscious mind, particularly when he referred to an uncensored intrusion of unconscious thought into one's speech (a Freudian slip) or to his efforts to interpret dreams. His 1901 book The Psychopathology of Everyday Life catalogs hundreds of everyday events that Freud explains in terms of unconscious influence. Pierre Janet advanced the idea of a subconscious mind, which could contain autonomous mental elements unavailable to the direct scrutiny of the subject.
The concept of unconscious processes has remained important in psychology. Cognitive psychologists have used a "filter" model of attention. According to the model, much information processing takes place below the threshold of consciousness, and only certain stimuli, limited by their nature and number, make their way through the filter. Much research has shown that subconscious priming of certain ideas can covertly influence thoughts and behavior. Because of the unreliability of self-reporting, a major hurdle in this type of research involves demonstrating that a subject's conscious mind has not perceived a target stimulus. For this reason, some psychologists prefer to distinguish between implicit and explicit memory. In another approach, one can also describe a subliminal stimulus as meeting an objective but not a subjective threshold.
The automaticity model of John Bargh and others involves the ideas of automaticity and unconscious processing in our understanding of social behavior, although there has been dispute with regard to replication.
Some experimental data suggest that the brain begins to consider taking actions before the mind becomes aware of them. The influence of unconscious forces on people's choices bears on the philosophical question of free will. John Bargh, Daniel Wegner, and Ellen Langer describe free will as an illusion.
=
Some psychologists study motivation or the subject of why people or lower animals initiate a behavior at a particular time. It also involves the study of why humans and lower animals continue or terminate a behavior. Psychologists such as William James initially used the term motivation to refer to intention, in a sense similar to the concept of will in European philosophy. With the steady rise of Darwinian and Freudian thinking, instinct also came to be seen as a primary source of motivation. According to drive theory, the forces of instinct combine into a single source of energy which exerts a constant influence. Psychoanalysis, like biology, regarded these forces as demands originating in the nervous system. Psychoanalysts believed that these forces, especially the sexual instincts, could become entangled and transmuted within the psyche. Classical psychoanalysis conceives of a struggle between the pleasure principle and the reality principle, roughly corresponding to id and ego. Later, in Beyond the Pleasure Principle, Freud introduced the concept of the death drive, a compulsion towards aggression, destruction, and psychic repetition of traumatic events. Meanwhile, behaviorist researchers used simple dichotomous models (pleasure/pain, reward/punishment) and well-established principles such as the idea that a thirsty creature will take pleasure in drinking. Clark Hull formalized the latter idea with his drive reduction model.
Hunger, thirst, fear, sexual desire, and thermoregulation constitute fundamental motivations in animals. Humans seem to exhibit a more complex set of motivations—though theoretically these could be explained as resulting from desires for belonging, positive self-image, self-consistency, truth, love, and control.
Motivation can be modulated or manipulated in many different ways. Researchers have found that eating, for example, depends not only on the organism's fundamental need for homeostasis—an important factor causing the experience of hunger—but also on circadian rhythms, food availability, food palatability, and cost. Abstract motivations are also malleable, as evidenced by such phenomena as goal contagion: the adoption of goals, sometimes unconsciously, based on inferences about the goals of others. Vohs and Baumeister suggest that contrary to the need-desire-fulfillment cycle of animal instincts, human motivations sometimes obey a "getting begets wanting" rule: the more you get a reward such as self-esteem, love, drugs, or money, the more you want it. They suggest that this principle can even apply to food, drink, sex, and sleep.
=
Developmental psychology is the scientific study of how and why the thought processes, emotions, and behaviors of humans change over the course of their lives. Some credit Charles Darwin with conducting the first systematic study within the rubric of developmental psychology, having published in 1877 a short paper detailing the development of innate forms of communication based on his observations of his infant son. The main origins of the discipline, however, are found in the work of Jean Piaget. Like Piaget, developmental psychologists originally focused primarily on the development of cognition from infancy to adolescence. Later, developmental psychology extended itself to the study cognition over the life span. In addition to studying cognition, developmental psychologists have also come to focus on affective, behavioral, moral, social, and neural development.
Developmental psychologists who study children use a number of research methods. For example, they make observations of children in natural settings such as preschools and engage them in experimental tasks. Such tasks often resemble specially designed games and activities that are both enjoyable for the child and scientifically useful. Developmental researchers have even devised clever methods to study the mental processes of infants. In addition to studying children, developmental psychologists also study aging and processes throughout the life span, including old age. These psychologists draw on the full range of psychological theories to inform their research.
=
All researched psychological traits are influenced by both genes and environment, to varying degrees. These two sources of influence are often confounded in observational research of individuals and families. An example of this confounding can be shown in the transmission of depression from a depressed mother to her offspring. A theory based on environmental transmission would hold that an offspring, by virtue of their having a problematic rearing environment managed by a depressed mother, is at risk for developing depression. On the other hand, a hereditarian theory would hold that depression risk in an offspring is influenced to some extent by genes passed to the child from the mother. Genes and environment in these simple transmission models are completely confounded. A depressed mother may both carry genes that contribute to depression in her offspring and also create a rearing environment that increases the risk of depression in her child.
Behavioral genetics researchers have employed methodologies that help to disentangle this confound and understand the nature and origins of individual differences in behavior. Traditionally the research has involved twin studies and adoption studies, two designs where genetic and environmental influences can be partially un-confounded. More recently, gene-focused research has contributed to understanding genetic contributions to the development of psychological traits.
The availability of microarray molecular genetic or genome sequencing technologies allows researchers to measure participant DNA variation directly, and test whether individual genetic variants within genes are associated with psychological traits and psychopathology through methods including genome-wide association studies. One goal of such research is similar to that in positional cloning and its success in Huntington's: once a causal gene is discovered biological research can be conducted to understand how that gene influences the phenotype. One major result of genetic association studies is the general finding that psychological traits and psychopathology, as well as complex medical diseases, are highly polygenic, where a large number (on the order of hundreds to thousands) of genetic variants, each of small effect, contribute to individual differences in the behavioral trait or propensity to the disorder. Active research continues to work toward understanding the genetic and environmental bases of behavior and their interaction.
Psychology encompasses many subfields and includes different approaches to the study of mental processes and behavior.
=
Psychological testing has ancient origins, dating as far back as 2200 BCE, in the examinations for the Chinese civil service. Written exams began during the Han dynasty (202 BCE – 220 CE). By 1370, the Chinese system required a stratified series of tests, involving essay writing and knowledge of diverse topics. The system was ended in 1906. In Europe, mental assessment took a different approach, with theories of physiognomy—judgment of character based on the face—described by Aristotle in 4th century BCE Greece. Physiognomy remained current through the Enlightenment, and added the doctrine of phrenology: a study of mind and intelligence based on simple assessment of neuroanatomy.
When experimental psychology came to Britain, Francis Galton was a leading practitioner. By virtue of his procedures for measuring reaction time and sensation, he is considered an inventor of modern mental testing (also known as psychometrics). James McKeen Cattell, a student of Wundt and Galton, brought the idea of psychological testing to the United States, and in fact coined the term "mental test". In 1901, Cattell's student Clark Wissler published discouraging results, suggesting that mental testing of Columbia and Barnard students failed to predict academic performance. In response to 1904 orders from the Minister of Public Instruction, One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them. psychologists Alfred Binet and Théodore Simon developed and elaborated a new test of intelligence in 1905–1911. They used a range of questions diverse in their nature and difficulty. Binet and Simon introduced the concept of mental age and referred to the lowest scorers on their test as idiots. Henry H. Goddard put the Binet-Simon scale to work and introduced classifications of mental level such as imbecile and feebleminded. In 1916, (after Binet's death), Stanford professor Lewis M. Terman modified the Binet-Simon scale (renamed the Stanford–Binet scale) and introduced the intelligence quotient as a score report. Based on his test findings, and reflecting the racism common to that era, Terman concluded that intellectual disability "represents the level of intelligence which is very, very common among Spanish-Indians and Mexican families of the Southwest and also among negroes. Their dullness seems to be racial."
Following the Army Alpha and Army Beta tests, which was developed by psychologist Robert Yerkes in 1917 and then used in World War 1 by industrial and organizational psychologists for large-scale employee testing and selection of military personnel. Mental testing also became popular in the U.S., where it was applied to schoolchildren. The federally created National Intelligence Test was administered to 7 million children in the 1920s. In 1926, the College Entrance Examination Board created the Scholastic Aptitude Test to standardize college admissions. The results of intelligence tests were used to argue for segregated schools and economic functions, including the preferential training of Black Americans for manual labor. These practices were criticized by Black intellectuals such a Horace Mann Bond and Allison Davis. Eugenicists used mental testing to justify and organize compulsory sterilization of individuals classified as mentally retarded (now referred to as intellectual disability). In the United States, tens of thousands of men and women were sterilized. Setting a precedent that has never been overturned, the U.S. Supreme Court affirmed the constitutionality of this practice in the 1927 case Buck v. Bell.
Today mental testing is a routine phenomenon for people of all ages in Western societies. Modern testing aspires to criteria including standardization of procedure, consistency of results, output of an interpretable score, statistical norms describing population outcomes, and, ideally, effective prediction of behavior and life outcomes outside of testing situations. Psychological testing is regularly used in forensic contexts to aid legal judgments and decisions. Developments in psychometrics include work on test and scale reliability and validity. Developments in item-response theory, structural equation modeling, and bifactor analysis have helped in strengthening test and scale construction.
=
The provision of psychological health services is generally called clinical psychology in the U.S. Sometimes, however, members of the school psychology and counseling psychology professions engage in practices that resemble that of clinical psychologists. Clinical psychologists typically include people who have graduated from doctoral programs in clinical psychology. In Canada, some of the members of the abovementioned groups usually fall within the larger category of professional psychology. In Canada and the U.S., practitioners get bachelor's degrees and doctorates; doctoral students in clinical psychology usually spend one year in a predoctoral internship and one year in postdoctoral internship. In Mexico and most other Latin American and European countries, psychologists do not get bachelor's and doctoral degrees; instead, they take a three-year professional course following high school. Clinical psychology is at present the largest specialization within psychology. It includes the study and application of psychology for the purpose of understanding, preventing, and relieving psychological distress, dysfunction, and/or mental illness. Clinical psychologists also try to promote subjective well-being and personal growth. Central to the practice of clinical psychology are psychological assessment and psychotherapy although clinical psychologists may also engage in research, teaching, consultation, forensic testimony, and program development and administration.
Credit for the first psychology clinic in the United States typically goes to Lightner Witmer, who established his practice in Philadelphia in 1896. Another modern psychotherapist was Morton Prince, an early advocate for the establishment of psychology as a clinical and academic discipline. In the first part of the twentieth century, most mental health care in the United States was performed by psychiatrists, who are medical doctors. Psychology entered the field with its refinements of mental testing, which promised to improve the diagnosis of mental problems. For their part, some psychiatrists became interested in using psychoanalysis and other forms of psychodynamic psychotherapy to understand and treat the mentally ill.
Psychotherapy as conducted by psychiatrists blurred the distinction between psychiatry and psychology, and this trend continued with the rise of community mental health facilities. Some in the clinical psychology community adopted behavioral therapy, a thoroughly non-psychodynamic model that used behaviorist learning theory to change the actions of patients. A key aspect of behavior therapy is empirical evaluation of the treatment's effectiveness. In the 1970s, cognitive-behavior therapy emerged with the work of Albert Ellis and Aaron Beck. Although there are similarities between behavior therapy and cognitive-behavior therapy, cognitive-behavior therapy required the application of cognitive constructs. Since the 1970s, the popularity of cognitive-behavior therapy among clinical psychologists increased. A key practice in behavioral and cognitive-behavioral therapy is exposing patients to things they fear, based on the premise that their responses (fear, panic, anxiety) can be deconditioned.
Mental health care today involves psychologists and social workers in increasing numbers. In 1977, National Institute of Mental Health director Bertram Brown described this shift as a source of "intense competition and role confusion." Graduate programs issuing doctorates in clinical psychology emerged in the 1950s and underwent rapid increase through the 1980s. The PhD degree is intended to train practitioners who could also conduct scientific research. The PsyD degree is more exclusively designed to train practitioners.
Some clinical psychologists focus on the clinical management of patients with brain injury. This subspecialty is known as clinical neuropsychology. In many countries, clinical psychology is a regulated mental health profession. The emerging field of disaster psychology (see crisis intervention) involves professionals who respond to large-scale traumatic events.
The work performed by clinical psychologists tends to be influenced by various therapeutic approaches, all of which involve a formal relationship between professional and client (usually an individual, couple, family, or small group). Typically, these approaches encourage new ways of thinking, feeling, or behaving. Four major theoretical perspectives are psychodynamic, cognitive behavioral, existential–humanistic, and systems or family therapy. There has been a growing movement to integrate the various therapeutic approaches, especially with an increased understanding of issues regarding culture, gender, spirituality, and sexual orientation. With the advent of more robust research findings regarding psychotherapy, there is evidence that most of the major therapies have equal effectiveness, with the key common element being a strong therapeutic alliance. Because of this, more training programs and psychologists are now adopting an eclectic therapeutic orientation.
Diagnosis in clinical psychology usually follows the Diagnostic and Statistical Manual of Mental Disorders (DSM). The study of mental illnesses is called abnormal psychology.
=
Educational psychology is the study of how humans learn in educational settings, the effectiveness of educational interventions, the psychology of teaching, and the social psychology of schools as organizations. Educational psychologists can be found in preschools, schools of all levels including post secondary institutions, community organizations and learning centers, Government or private research firms, and independent or private consultant. The work of developmental psychologists such as Lev Vygotsky, Jean Piaget, and Jerome Bruner has been influential in creating teaching methods and educational practices. Educational psychology is often included in teacher education programs in places such as North America, Australia, and New Zealand.
School psychology combines principles from educational psychology and clinical psychology to understand and treat students with learning disabilities; to foster the intellectual growth of gifted students; to facilitate prosocial behaviors in adolescents; and otherwise to promote safe, supportive, and effective learning environments. School psychologists are trained in educational and behavioral assessment, intervention, prevention, and consultation, and many have extensive training in research.
=
Industrial and organizational (I/O) psychology involves research and practices that apply psychological theories and principles to organizations and individuals' work-lives. In the field's beginnings, industrialists brought the nascent field of psychology to bear on the study of scientific management techniques for improving workplace efficiency. The field was at first called economic psychology or business psychology; later, industrial psychology, employment psychology, or psychotechnology. An influential early study examined workers at Western Electric's Hawthorne plant in Cicero, Illinois from 1924 to 1932. Western Electric experimented on factory workers to assess their responses to changes in illumination, breaks, food, and wages. The researchers came to focus on workers' responses to observation itself, and the term Hawthorne effect is now used to describe the fact that people's behavior can change when they think they are being observed. Although the Hawthorne research can be found in psychology textbooks, the research and its findings were weak at best.
The name industrial and organizational psychology emerged in the 1960s. In 1973, it became enshrined in the name of the Society for Industrial and Organizational Psychology, Division 14 of the American Psychological Association. One goal of the discipline is to optimize human potential in the workplace. Personnel psychology is a subfield of I/O psychology. Personnel psychologists apply the methods and principles of psychology in selecting and evaluating workers. Another subfield, organizational psychology, examines the effects of work environments and management styles on worker motivation, job satisfaction, and productivity. Most I/O psychologists work outside of academia, for private and public organizations and as consultants. A psychology consultant working in business today might expect to provide executives with information and ideas about their industry, their target markets, and the organization of their company.
Organizational behavior (OB) is an allied field involved in the study of human behavior within organizations. One way to differentiate I/O psychology from OB is that I/O psychologists train in university psychology departments and OB specialists, in business schools.
=
One role for psychologists in the military has been to evaluate and counsel soldiers and other personnel. In the U.S., this function began during World War I, when Robert Yerkes established the School of Military Psychology at Fort Oglethorpe in Georgia. The school provided psychological training for military staff. Today, U.S. Army psychologists perform psychological screening, clinical psychotherapy, suicide prevention, and treatment for post-traumatic stress, as well as provide prevention-related services, for example, smoking cessation. The United States Army's Mental Health Advisory Teams implement psychological interventions to help combat troops experiencing mental problems.
Psychologists may also work on a diverse set of campaigns known broadly as psychological warfare. Psychological warfare chiefly involves the use of propaganda to influence enemy soldiers and civilians. This so-called black propaganda is designed to seem as if it originates from a source other than the Army. The CIA's MKULTRA program involved more individualized efforts at mind control, involving techniques such as hypnosis, torture, and covert involuntary administration of LSD. The U.S. military used the name Psychological Operations (PSYOP) until 2010, when these activities were reclassified as Military Information Support Operations (MISO), part of Information Operations (IO). Psychologists have sometimes been involved in assisting the interrogation and torture of suspects, staining the records of the psychologists involved.
=
 Social change 
An example of the contribution of psychologists to social change involves the research of Kenneth and Mamie Phipps Clark. These two African American psychologists studied segregation's adverse psychological impact on Black children. Their research findings played a role in the desegregation case Brown v. Board of Education (1954).
The impact of psychology on social change includes the discipline's broad influence on teaching and learning. Research has shown that compared to the "whole word" or "whole language" approach, the phonics approach to reading instruction is more efficacious.
 Medical applications 
Medical facilities increasingly employ psychologists to perform various roles. One aspect of health psychology is the psychoeducation of patients: instructing them in how to follow a medical regimen. Health psychologists can also educate doctors and conduct research on patient compliance. Psychologists in the field of public health use a wide variety of interventions to influence human behavior. These range from public relations campaigns and outreach to governmental laws and policies. Psychologists study the composite influence of all these different tools in an effort to influence whole populations of people.
 Worker health, safety and wellbeing 
Psychologists work with organizations to apply findings from psychological research to improve the health and well-being of employees. Some work as external consultants hired by organizations to solve specific problems, whereas others are full-time employees of the organization. Applications include conducting surveys to identify issues and designing interventions to make work healthier. Some of the specific health areas include:
Accidents and injuries: A major contribution is the concept of safety climate, which is employee shared perceptions of the behaviors that are encouraged (e.g., wearing safety gear) and discouraged (not following safety rules) at work. Organizations with strong safety climates have fewer work accidents and injuries.
Cardiovascular disease: Cardiovascular disease has been related to lack of job control.
Mental health: Exposure to occupational stress is associated with mental health disorder.
Musculoskeletal disorder: These are injuries in bones, nerves and tendons due to overexertion and repetitive strain. They have been linked to job satisfaction and workplace stress.
Physical health symptoms: Occupational stress has been linked to physical symptoms such as digestive distress and headache.
Workplace violence: Violence prevention climate is related to being physically assaulted and psychologically mistreated at work.
Interventions that improve climates are a way to address accidents and violence. Interventions that reduce stress at work or provide employees with tools to better manage it can help in areas where stress is an important component.
Industrial psychology became interested in worker fatigue during World War I, when government ministers in Britain were concerned about the impact of fatigue on workers in munitions factories but not other types of factories. In the U. K. some interest in worker well-being emerged with the efforts of Charles Samuel Myers and his National Institute of Industrial Psychology (NIIP) during the inter-War years. In the U. S. during the mid-twentieth century industrial psychologist Arthur Kornhauser pioneered the study of occupational mental health, linking industrial working conditions to mental health as well as the spillover of an unsatisfying job into a worker's personal life. Zickar accumulated evidence to show that "no other industrial psychologist of his era was as devoted to advocating management and labor practices that would improve the lives of working people."
 Occupational health psychology 
As interest in the worker health expanded toward the end of the twentieth century, the field of occupational health psychology (OHP) emerged. OHP is a branch of psychology that is interdisciplinary. OHP is concerned with the health and safety of workers. OHP addresses topic areas such as the impact of occupational stressors on physical and mental health, mistreatment of workers (e.g., bullying and violence), work-family balance, the impact of involuntary unemployment on physical and mental health, the influence of psychosocial factors on safety and accidents, and interventions designed to improve/protect worker health. OHP grew out of health psychology, industrial and organizational psychology, and occupational medicine. OHP has also been informed by disciplines outside psychology, including industrial engineering, sociology, and economics.
Quantitative psychological research lends itself to the statistical testing of hypotheses. Although the field makes abundant use of randomized and controlled experiments in laboratory settings, such research can only assess a limited range of short-term phenomena. Some psychologists rely on less rigorously controlled, but more ecologically valid, field experiments as well. Other research psychologists rely on statistical methods to glean knowledge from population data. The statistical methods research psychologists employ include the Pearson product–moment correlation coefficient, the analysis of variance, multiple linear regression, logistic regression, structural equation modeling, and hierarchical linear modeling. The measurement and operationalization of important constructs is an essential part of these research designs.
Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation. While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed.
=
A true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment.
A quasi-experiment is a situation in which different conditions are being studied, but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level.
Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works.
=
Surveys are used in psychology for the purpose of measuring attitudes and traits, monitoring changes in mood, and checking the validity of experimental manipulations (checking research participants' perception of the condition they were assigned to). Psychologists have commonly used paper-and-pencil surveys. However, surveys are also conducted over the phone or through e-mail. Web-based surveys are increasingly used to conveniently reach many subjects.
Observational studies are commonly conducted in psychology. In cross-sectional observational studies, psychologists collect data at a single point in time. The goal of many cross-sectional studies is the assess the extent factors are correlated with each other. By contrast, in longitudinal studies psychologists collect data on the same sample at two or more points in time. Sometimes the purpose of longitudinal research is to study trends across time such as the stability of traits or age-related changes in behavior. Because some studies involve endpoints that psychologists cannot ethically study from an experimental standpoint, such as identifying the causes of depression, they conduct longitudinal studies a large group of depression-free people, periodically assessing what is happening in the individuals' lives. In this way psychologists have an opportunity to test causal hypotheses regarding conditions that commonly arise in people's lives that put them at risk for depression. Problems that affect longitudinal studies include selective attrition, the type of problem in which bias is introduced when a certain type of research participant disproportionately leaves a study.
One example of an observational study was run by Arthur Bandura. This observational study focused on children who were exposed to an adult exhibiting aggressive behaviors and their reaction to toys versus other children who were not exposed to these stimuli. The result shows that children who had seen the adult acting aggressively towards a toy, in turn, were aggressive towards their own toy when put in a situation that frustrated them.
Exploratory data analysis includes a variety of practices that researchers use to reduce a great many variables to a small number overarching factors. In Peirce's three modes of inference, exploratory data analysis corresponds to abduction. Meta-analysis is the technique research psychologists use to integrate results from many studies of the same variables and arriving at a grand average of the findings.
=
A classic and popular tool used to relate mental and neural activity is the electroencephalogram (EEG), a technique using amplified electrodes on a person's scalp to measure voltage changes in different parts of the brain. Hans Berger, the first researcher to use EEG on an unopened skull, quickly found that brains exhibit signature "brain waves": electric oscillations which correspond to different states of consciousness. Researchers subsequently refined statistical methods for synthesizing the electrode data, and identified unique brain wave patterns such as the delta wave observed during non-REM sleep.
Newer functional neuroimaging techniques include functional magnetic resonance imaging and positron emission tomography, both of which track the flow of blood through the brain. These technologies provide more localized information about activity in the brain and create representations of the brain with widespread appeal. They also provide insight which avoids the classic problems of subjective self-reporting. It remains challenging to draw hard conclusions about where in the brain specific thoughts originate—or even how usefully such localization corresponds with reality. However, neuroimaging has delivered unmistakable results showing the existence of correlations between mind and brain. Some of these draw on a systemic neural network model rather than a localized function model.
Interventions such as transcranial magnetic stimulation and drugs also provide information about brain–mind interactions. Psychopharmacology is the study of drug-induced mental effects.
=
Computational modeling is a tool used in mathematical psychology and cognitive psychology to simulate behavior. This method has several advantages. Since modern computers process information quickly, simulations can be run in a short time, allowing for high statistical power. Modeling also allows psychologists to visualize hypotheses about the functional organization of mental events that could not be directly observed in a human. Computational neuroscience uses mathematical models to simulate the brain. Another method is symbolic modeling, which represents many mental objects using variables and rules. Other types of modeling include dynamic systems and stochastic modeling.
=
Animal experiments aid in investigating many aspects of human psychology, including perception, emotion, learning, memory, and thought, to name a few. In the 1890s, Russian physiologist Ivan Pavlov famously used dogs to demonstrate classical conditioning. Non-human primates, cats, dogs, pigeons, and rats and other rodents are often used in psychological experiments. Ideally, controlled experiments introduce only one independent variable at a time, in order to ascertain its unique effects upon dependent variables. These conditions are approximated best in laboratory settings. In contrast, human environments and genetic backgrounds vary so widely, and depend upon so many factors, that it is difficult to control important variables for human subjects. There are pitfalls, however, in generalizing findings from animal studies to humans through animal models.
Comparative psychology is the scientific study of the behavior and mental processes of non-human animals, especially as these relate to the phylogenetic history, adaptive significance, and development of behavior. Research in this area explores the behavior of many species, from insects to primates. It is closely related to other disciplines that study animal behavior such as ethology. Research in comparative psychology sometimes appears to shed light on human behavior, but some attempts to connect the two have been quite controversial, for example the Sociobiology of E.O. Wilson. Animal models are often used to study neural processes related to human behavior, e.g. in cognitive neuroscience.
=
Qualitative research is often designed to answer questions about the thoughts, feelings, and behaviors of individuals. Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made.
Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler.
Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out.
=
Program evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness. In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.
=
Metascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within and from outside the scientific community.
 Confirmation bias 
In 1959, statistician Theodore Sterling examined the results of psychological studies and discovered that 97% of them supported their initial hypotheses, implying possible publication bias. Similarly, Fanelli (2010) found that 91.5% of psychiatry/psychology studies confirmed the effects they were looking for, and concluded that the odds of this happening (a positive result) was around five times higher than in fields such as space science or geosciences. Fanelli argued that this is because researchers in "softer" sciences have fewer constraints to their conscious and unconscious biases.
 Replication 
A replication crisis in psychology has emerged. Many notable findings in the field have not been replicated. Some researchers were even accused of publishing fraudulent results. Systematic efforts, including efforts by the Reproducibility Project of the Center for Open Science, to assess the extent of the problem found that as many as two-thirds of highly publicized findings in psychology failed to be replicated. Reproducibility has generally been stronger in cognitive psychology (in studies and journals) than social psychology and subfields of differential psychology. Other subfields of psychology have also been implicated in the replication crisis, including clinical psychology, developmental psychology, and a field closely related to psychology, educational research.
Focus on the replication crisis has led to other renewed efforts in the discipline to re-test important findings. In response to concerns about publication bias and data dredging (conducting a large number of statistical tests on a great many variables but restricting reporting to the results that were statistically significant), 295 psychology and medical journals have adopted result-blind peer review where studies are accepted not on the basis of their findings and after the studies are completed, but before the studies are conducted and upon the basis of the methodological rigor of their experimental designs and the theoretical justifications for their proposed statistical analysis before data collection or analysis is conducted. In addition, large-scale collaborations among researchers working in multiple labs in different countries have taken place. The collaborators regularly make their data openly available for different researchers to assess. Allen and Mehler estimated that 61 per cent of result-blind studies have yielded null results, in contrast to an estimated 5 to 20 per cent in traditional research.
 Misuse of statistics 
Some critics view statistical hypothesis testing as misplaced. Psychologist and statistician Jacob Cohen wrote in 1994 that psychologists routinely confuse statistical significance with practical importance, enthusiastically reporting great certainty in unimportant facts. Some psychologists have responded with an increased use of effect size statistics, rather than sole reliance on p-values.
=
In 2008, Arnett pointed out that most articles in American Psychological Association journals were about U.S. populations when U.S. citizens are only 5% of the world's population. He complained that psychologists had no basis for assuming psychological processes to be universal and generalizing research findings to the rest of the global population. In 2010, Henrich, Heine, and Norenzayan reported a bias in conducting psychology studies with participants from "WEIRD" ("Western, Educated, Industrialized, Rich, and Democratic") societies. Henrich et al. found that "96% of psychological samples come from countries with only 12% of the world's population" (p. 63). The article gave examples of results that differ significantly between people from WEIRD and tribal cultures, including the Müller-Lyer illusion. Arnett (2008), Altmaier and Hall (2008) and Morgan-Consoli et al. (2018) view the Western bias in research and theory as a serious problem considering psychologists are increasingly applying psychological principles developed in WEIRD regions in their research, clinical work, and consultation with populations around the world. In 2018, Rad, Martingano, and Ginges showed that nearly a decade after Henrich et al.'s paper, over 80% of the samples used in studies published in the journal Psychological Science employed WEIRD samples. Moreover, their analysis showed that several studies did not fully disclose the origin of their samples; the authors offered a set of recommendations to editors and reviewers to reduce WEIRD bias.
=
Similar to the WEIRD bias, starting in 2020, researchers of non-human behavior have started to emphasize the need to document the possibility of the STRANGE (Social background, Trappability and self-selection, Rearing history, Acclimation and habituation, Natural changes in responsiveness, Genetic makeup, and Experience) bias in study conclusions.
=
Some observers perceive a gap between scientific theory and its application—in particular, the application of unsupported or unsound clinical practices. Critics say there has been an increase in the number of mental health training programs that do not instill scientific competence. Practices such as "facilitated communication for infantile autism"; memory-recovery techniques including body work; and other therapies, such as rebirthing and reparenting, may be dubious or even dangerous, despite their popularity. These practices, however, are outside the mainstream practices taught in clinical psychology doctoral programs.
Ethical standards in the discipline have changed over time. Some famous past studies are today considered unethical and in violation of established codes (e.g., the Canadian Code of Conduct for Research Involving Humans, and the Belmont Report). The American Psychological Association has advanced a set of ethical principles and a code of conduct for the profession.
The most important contemporary standards include informed and voluntary consent. After World War II, the Nuremberg Code was established because of Nazi abuses of experimental subjects. Later, most countries (and scientific journals) adopted the Declaration of Helsinki. In the U.S., the National Institutes of Health established the Institutional Review Board in 1966, and in 1974 adopted the National Research Act (HR 7724). All of these measures encouraged researchers to obtain informed consent from human participants in experimental studies. A number of influential but ethically dubious studies led to the establishment of this rule; such studies included the MIT-Harvard Fernald School radioisotope studies, the Thalidomide tragedy, the Willowbrook hepatitis study, Stanley Milgram's studies of obedience to authority, and the Stanford Prison Experiment.
=
The ethics code of the American Psychological Association originated in 1951 as "Ethical Standards of Psychologists." This code has guided the formation of licensing laws in most American states. It has changed multiple times over the decades since its adoption, and contains both aspirational principles and binding ethical standards.
The APA's Ethical Principles of Psychologists and Code of Conduct consists of five General Principles, which are meant to guide psychologists to higher ethical practice where a particular standard does not apply. Those principles are:
A. Beneficence and Nonmaleficence - meaning the psychologists must work to benefit those they work with and "do no harm." This includes awareness of indirect benefits and harms their work might have on others due to personal, social, political, or other factors.
B. Fidelity and Responsibility - an awareness of public trust in the profession and adherence to ethical standards and clarification of roles to preserve that trust. This includes managing conflicts of interest, as well as committing some portion of a psychologist's professional time to low-cost or pro bono work.
C. Integrity - upholding honesty and accuracy in all psychological practices, including avoiding misrepresentations and fraud. In situations where psychologists would use deception (i.e., certain research), psychologists must consider the necessity, benefits, and harms, and mitigate any harms where possible.
D. Justice - an understanding that psychology must be for everyone's benefit, and that psychologists take special care to avoid unjust practices as a result of biases or limitations of expertise.
E. Respect for People's Rights and Dignity - the preservation of people's rights when working with psychologists, including confidentially, privacy, and autonomy. Psychologists should consider a multitude of factors, including a need for special safeguards for protected populations (e.g., minors, incarcerated individuals) and awareness of differences based on numerous factors, including culture, race, age, gender, and socioeconomic status.
In 1989, the APA revised its policies on advertising and referral fees to negotiate the end of an investigation by the Federal Trade Commission. The 1992 incarnation was the first to distinguish between "aspirational" ethical standards and "enforceable" ones. The APA code was further revised in 2010 to prevent the use of the code to justify violating human rights, which was in response to the participation of APA members in interrogations under the administration of United States President George W. Bush. Members of the public have a five-year window to file ethics complaints about APA members with the APA ethics committee; members of the APA have a three-year window.
The Canadian Psychological Association used the APA code until 1986, when it developed its own code drawing from four similar principles: 1) Respect for the Dignity of Persons and Peoples, 2) Responsible Caring, 3) Integrity in Relationships, 4) Responsibility to Society. The European Federation of Psychologist's Associations, have adopted a model code using the principles of the Canadian Code, while also drawing from the APA code.
Universities have ethics committees dedicated to protecting the rights (e.g., voluntary nature of participation in the research, privacy) and well-being (e.g., minimizing distress) of research participants. University ethics committees evaluate proposed research to ensure that researchers protect the rights and well-being of participants; an investigator's research project cannot be conducted unless approved by such an ethics committee.
The field of psychology also identifies certain categories of people that require additional or special protection due to particular vulnerabilities, unequal power dynamics, or diminished capacity for informed consent. This list often includes, but is not limited to, children, incarcerated individuals, pregnant women, human fetuses and neonates, institutionalized persons, those with physical or mental disabilities, and the educationally or economically disadvantaged.
Some of the ethical issues considered most important are the requirement to practice only within the area of competence, to maintain confidentiality with the patients, and to avoid sexual relations with them. Another important principle is informed consent, the idea that a patient or research subject must understand and freely choose a procedure they are undergoing. Some of the most common complaints against clinical psychologists include sexual misconduct and breaches in confidentiality or privacy.
Psychology ethics apply to all types of human contact in a psychologist's professional capacity, including therapy, assessment, teaching, training, work with research subjects, testimony in courts and before government bodies, consulting, and statements to the public or media pertaining to matters of psychology.
=
Research on other animals is governed by university ethics committees. Research on nonhuman animals cannot proceed without permission of the ethics committee, of the researcher's home institution. Ethical guidelines state that using non-human animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Psychologists can use certain research techniques on animals that could not be used on humans.
Comparative psychologist Harry Harlow drew moral condemnation for isolation experiments on rhesus macaque monkeys at the University of Wisconsin–Madison in the 1970s. The aim of the research was to produce an animal model of clinical depression. Harlow also devised what he called a "rape rack", to which the female isolates were tied in normal monkey mating posture. In 1974, American literary critic Wayne C. Booth wrote that, "Harry Harlow and his colleagues go on torturing their nonhuman primates decade after decade, invariably proving what we all knew in advance—that social creatures can be destroyed by destroying their social ties." He writes that Harlow made no mention of the criticism of the morality of his work.
Animal research is influential in psychology, while still being debated among academics. The testing of animals for research has led to medical breakthroughs in human medicine. Many psychologists argue animal experimentation is essential for human advancement, but must be regulated by the government to ensure ethicality.
=
American Psychological Association
Association for Psychological Science

--- TOPIC: Cognitive_science ---
Cognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Mental faculties of concern to cognitive scientists include perception, memory, attention, reasoning, language, and emotion. To understand these faculties, cognitive scientists borrow from fields such as psychology, philosophy, artificial intelligence, neuroscience, linguistics, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision-making to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that "thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures."
The cognitive sciences began as an intellectual movement in the 1950s, called the cognitive revolution. Cognitive science has a prehistory traceable back to ancient Greek philosophical texts (see Plato's Meno and Aristotle's De Anima).
The modern culture of cognitive science can be traced back to the early cyberneticists in the 1930s and 1940s, such as Warren McCulloch and Walter Pitts, who sought to understand the organizing principles of the mind. McCulloch and Pitts developed the first variants of what are now known as artificial neural networks, models of computation inspired by the structure of biological neural networks.
Another precursor was the early development of the theory of computation and the digital computer in the 1940s and 1950s. Kurt Gödel, Alonzo Church, Claude Shannon, Alan Turing, and John von Neumann were instrumental in these developments. The modern computer, or Von Neumann machine, would play a central role in cognitive science, both as a metaphor for the mind, and as a tool for investigation.
The first instance of cognitive science experiments being done at an academic institution took place at MIT Sloan School of Management, established by J.C.R. Licklider working within the psychology department and conducting experiments using computer memory as models for human cognition. In 1959, Noam Chomsky published a scathing review of B. F. Skinner's book Verbal Behavior. At the time, Skinner's behaviorist paradigm dominated the field of psychology within the United States. Most psychologists focused on functional relations between stimulus and response, without positing internal representations. Chomsky argued that in order to explain language, we needed a theory like generative grammar, which not only attributed internal representations but characterized their underlying order.
The term cognitive science was coined by Christopher Longuet-Higgins in his 1973 commentary on the Lighthill report, which concerned the then-current state of artificial intelligence research. In the same decade, the journal Cognitive Science and the Cognitive Science Society were founded. The founding meeting of the Cognitive Science Society was held at the University of California, San Diego in 1979, which resulted in cognitive science becoming an internationally visible enterprise. In 1972, Hampshire College started the first undergraduate education program in Cognitive Science, led by Neil Stillings. In 1982, with assistance from Professor Stillings, Vassar College became the first institution in the world to grant an undergraduate degree in Cognitive Science. In 1986, the first Cognitive Science Department in the world was founded at the University of California, San Diego.
In the 1970s and early 1980s, as access to computers increased, artificial intelligence research expanded. Researchers such as Marvin Minsky would write computer programs in languages such as LISP to attempt to formally characterize the steps that human beings went through, for instance, in making decisions and solving problems, in the hope of better understanding human thought, and also in the hope of creating artificial minds. This approach is known as "symbolic AI".
Eventually the limits of the symbolic AI research program became apparent. For instance, it seemed to be unrealistic to comprehensively list human knowledge in a form usable by a symbolic computer program. The late 80s and 90s saw the rise of neural networks and connectionism as a research paradigm. Under this point of view, often attributed to James McClelland and David Rumelhart, the mind could be characterized as a set of complex associations, represented as a layered network. Critics argue that there are some phenomena which are better captured by symbolic models, and that connectionist models are often so complex as to have little explanatory power. Recently symbolic and connectionist models have been combined, making it possible to take advantage of both forms of explanation. While both connectionism and symbolic approaches have proven useful for testing various hypotheses and exploring approaches to understanding aspects of cognition and lower level brain functions, neither are biologically realistic and therefore, both suffer from a lack of neuroscientific plausibility. Connectionism has proven useful for exploring computationally how cognition emerges in development and occurs in the human brain, and has provided alternatives to strictly domain-specific / domain general approaches. For example, scientists such as Jeff Elman, Liz Bates, and Annette Karmiloff-Smith have posited that networks in the brain emerge from the dynamic interaction between them and environmental input.
Recent developments in quantum computation, including the ability to run quantum circuits on quantum computers such as IBM Quantum Platform, has accelerated work using elements from quantum mechanics in cognitive models.
=
A central tenet of cognitive science is that a complete understanding of the mind/brain cannot be attained by studying only a single level. An example would be the problem of remembering a phone number and recalling it later. One approach to understanding this process would be to study behavior through direct observation, or naturalistic observation. A person could be presented with a phone number and be asked to recall it after some delay of time; then the accuracy of the response could be measured. Another approach to measure cognitive ability would be to study the firings of individual neurons while a person is trying to remember the phone number. Neither of these experiments on its own would fully explain how the process of remembering a phone number works. Even if the technology to map out every neuron in the brain in real-time were available and it were known when each neuron fired it would still be impossible to know how a particular firing of neurons translates into the observed behavior. Thus an understanding of how these two levels relate to each other is imperative. Francisco Varela, in The Embodied Mind: Cognitive Science and Human Experience, argues that "the new sciences of the mind need to enlarge their horizon to encompass both lived human experience and the possibilities for transformation inherent in human experience". On the classic cognitivist view, this can be provided by a functional level account of the process. Studying a particular phenomenon from multiple levels creates a better understanding of the processes that occur in the brain to give rise to a particular behavior.
Marr gave a famous description of three levels of analysis:
The computational theory, specifying the goals of the computation;
Representation and algorithms, giving a representation of the inputs and outputs and the algorithms which transform one into the other; and
The hardware implementation, or how algorithm and representation may be physically realized.
=
Cognitive science is an interdisciplinary field with contributors from various fields, including psychology, neuroscience, linguistics, philosophy of mind, computer science, anthropology and biology. Cognitive scientists work collectively in hope of understanding the mind and its interactions with the surrounding world much like other sciences do. The field regards itself as compatible with the physical sciences and uses the scientific method as well as simulation or modeling, often comparing the output of models with aspects of human cognition. Similarly to the field of psychology, there is some doubt whether there is a unified cognitive science, which have led some researchers to prefer 'cognitive sciences' in plural.
Many, but not all, who consider themselves cognitive scientists hold a functionalist view of the mind—the view that mental states and processes should be explained by their function – what they do. According to the multiple realizability account of functionalism, even non-human systems such as robots and computers can be ascribed as having cognitive states.
=
The term "cognitive" in "cognitive science" is used for "any kind of mental operation or structure that can be studied in precise terms" (Lakoff and Johnson, 1999). This conceptualization is very broad, and should not be confused with how "cognitive" is used in some traditions of analytic philosophy, where "cognitive" has to do only with formal rules and truth-conditional semantics.
The earliest entries for the word "cognitive" in the OED take it to mean roughly "pertaining to the action or process of knowing". The first entry, from 1586, shows the word was at one time used in the context of discussions of Platonic theories of knowledge. Most in cognitive science, however, presumably do not believe their field is the study of anything as certain as the knowledge sought by Plato.
Cognitive science is a large field, and covers a wide array of topics on cognition. However, it should be recognized that cognitive science has not always been equally concerned with every topic that might bear relevance to the nature and operation of minds. Classical cognitivists have largely de-emphasized or avoided social and cultural factors, embodiment, emotion, consciousness, animal cognition, and comparative and evolutionary psychologies. However, with the decline of behaviorism, internal states such as affects and emotions, as well as awareness and covert attention became approachable again. For example, situated and embodied cognition theories take into account the current state of the environment as well as the role of the body in cognition. With the newfound emphasis on information processing, observable behavior was no longer the hallmark of psychological theory, but the modeling or recording of mental states.
Below are some of the main topics that cognitive science is concerned with; see List of cognitive science topics for a more exhaustive list.
=
Artificial intelligence (AI) involves the study of cognitive phenomena in machines. One of the practical goals of AI is to implement aspects of human intelligence in computers. Computers are also widely used as a tool with which to study cognitive phenomena. Computational modeling uses simulations to study how human intelligence may be structured. (See § Computational modeling.)
There is some debate in the field as to whether the mind is best viewed as a huge array of small but individually feeble elements (i.e. neurons), or as a collection of higher-level structures such as symbols, schemes, plans, and rules. The former view uses connectionism to study the mind, whereas the latter emphasizes symbolic artificial intelligence. One way to view the issue is whether it is possible to accurately simulate a human brain on a computer without accurately simulating the neurons that make up the human brain.
=
Attention is the selection of important information. The human mind is bombarded with millions of stimuli and it must have a way of deciding which of this information to process. Attention is sometimes seen as a spotlight, meaning one can only shine the light on a particular set of information. Experiments that support this metaphor include the dichotic listening task (Cherry, 1957) and studies of inattentional blindness (Mack and Rock, 1998). In the dichotic listening task, subjects are bombarded with two different messages, one in each ear, and told to focus on only one of the messages. At the end of the experiment, when asked about the content of the unattended message, subjects cannot report it.
The psychological construct of attention is sometimes confused with the concept of intentionality due to some degree of semantic ambiguity in their definitions. At the beginning of experimental research on attention, Wilhelm Wundt defined this term as "that psychical process, which is operative in the clear perception of the narrow region of the content of consciousness." His experiments showed the limits of attention in space and time, which were 3-6 letters during an exposition of 1/10 s. Because this notion develops within the framework of the original meaning during a hundred years of research, the definition of attention would reflect the sense when it accounts for the main features initially attributed to this term – it is a process of controlling thought that continues over time. While intentionality is the power of minds to be about something, attention is the concentration of awareness on some phenomenon during a period of time, which is necessary to elevate the clear perception of the narrow region of the content of consciousness and which is feasible to control this focus in mind.
The significance of knowledge about the scope of attention for studying cognition is that it defines the intellectual functions of cognition such as apprehension, judgment, reasoning, and working memory. The development of attention scope increases the set of faculties responsible for the mind relies on how it perceives, remembers, considers, and evaluates in making decisions. The ground of this statement is that the more details (associated with an event) the mind may grasp for their comparison, association, and categorization, the closer apprehension, judgment, and reasoning of the event are in accord with reality. According to Latvian professor Sandra Mihailova and professor Igor Val Danilov, the more elements of the phenomenon (or phenomena ) the mind can keep in the scope of attention simultaneously, the more significant number of reasonable combinations within that event it can achieve, enhancing the probability of better understanding features and particularity of the phenomenon (phenomena). For example, three items in the focal point of consciousness yield six possible combinations (3 factorial) and four items – 24 (4 factorial) combinations. The number of reasonable combinations becomes significant in the case of a focal point with six items with 720 possible combinations (6 factorial).
=
Embodied cognition approaches to cognitive science emphasize the role of body and environment in cognition. This includes both neural and extra-neural bodily processes, and factors that range from affective and emotional processes, to posture, motor control, proprioception, and kinaesthesis, to autonomic processes that involve heartbeat and respiration, to the role of the enteric gut microbiome. It also includes accounts of how the body engages with or is coupled to social and physical environments. 4E cognition includes a broad range of views about brain-body-environment interaction, from causal embeddedness to stronger claims about how the mind extends to include tools and instruments, as well as the role of social interactions, action-oriented processes, and affordances. 4E theories range from those closer to classic cognitivism (so-called "weak" embodied cognition) to stronger extended and enactive versions that are sometimes referred to as radical embodied cognitive science.
A hypothesis of pre-perceptual multimodal integration supports embodied cognition approaches and converges two competing naturalist and constructivist viewpoints about cognition and the development of emotions. According to this hypothesis supported by empirical data, cognition and emotion development are initiated by the association of affective cues with stimuli responsible for triggering the neuronal pathways of simple reflexes. This pre-perceptual multimodal integration can succeed owing to neuronal coherence in mother-child dyads beginning from pregnancy. These cognitive-reflex and emotion-reflex stimuli conjunctions further form simple innate neuronal assemblies, shaping the cognitive and emotional neuronal patterns in statistical learning that are continuously connected with the neuronal pathways of reflexes.
=
The ability to learn and understand language is an extremely complex process. Language is acquired within the first few years of life, and all humans under normal circumstances are able to acquire language proficiently. A major driving force in the theoretical linguistic field is discovering the nature that language must have in the abstract in order to be learned in such a fashion. Some of the driving research questions in studying how the brain itself processes language include: (1) To what extent is linguistic knowledge innate or learned?, (2) Why is it more difficult for adults to acquire a second-language than it is for infants to acquire their first-language?, and (3) How are humans able to understand novel sentences?
The study of language processing ranges from the investigation of the sound patterns of speech to the meaning of words and whole sentences. Linguistics often divides language processing into orthography, phonetics, phonology, morphology, syntax, semantics, and pragmatics. Many aspects of language can be studied from each of these components and from their interaction.
The study of language processing in cognitive science is closely tied to the field of linguistics. Linguistics was traditionally studied as a part of the humanities, including studies of history, art and literature. In the last fifty years or so, more and more researchers have studied knowledge and use of language as a cognitive phenomenon, the main problems being how knowledge of language can be acquired and used, and what precisely it consists of. Linguists have found that, while humans form sentences in ways apparently governed by very complex systems, they are remarkably unaware of the rules that govern their own speech. Thus linguists must resort to indirect methods to determine what those rules might be, if indeed rules as such exist. In any event, if speech is indeed governed by rules, they appear to be opaque to any conscious consideration.
=
Learning and development are the processes by which we acquire knowledge and information over time. Infants are born with little or no knowledge (depending on how knowledge is defined), yet they rapidly acquire the ability to use language, walk, and recognize people and objects. Research in learning and development aims to explain the mechanisms by which these processes might take place.
A major question in the study of cognitive development is the extent to which certain abilities are innate or learned. This is often framed in terms of the nature and nurture debate. The nativist view emphasizes that certain features are innate to an organism and are determined by its genetic endowment. The empiricist view, on the other hand, emphasizes that certain abilities are learned from the environment. Although clearly both genetic and environmental input is needed for a child to develop normally, considerable debate remains about how genetic information might guide cognitive development. In the area of language acquisition, for example, some (such as Steven Pinker) have argued that specific information containing universal grammatical rules must be contained in the genes, whereas others (such as Jeffrey Elman and colleagues in Rethinking Innateness) have argued that Pinker's claims are biologically unrealistic. They argue that genes determine the architecture of a learning system, but that specific "facts" about how grammar works can only be learned as a result of experience.
=
Memory allows us to store information for later retrieval. Memory is often thought of as consisting of both a long-term and short-term store. Long-term memory allows us to store information over prolonged periods (days, weeks, years). We do not yet know the practical limit of long-term memory capacity. Short-term memory allows us to store information over short time scales (seconds or minutes).
Memory is also often grouped into declarative and procedural forms. Declarative memory—grouped into subsets of semantic and episodic forms of memory—refers to our memory for facts and specific knowledge, specific meanings, and specific experiences (e.g. "Are apples food?", or "What did I eat for breakfast four days ago?"). Procedural memory allows us to remember actions and motor sequences (e.g. how to ride a bicycle) and is often dubbed implicit knowledge or memory .
Cognitive scientists study memory just as psychologists do, but tend to focus more on how memory bears on cognitive processes, and the interrelationship between cognition and memory. One example of this could be, what mental processes does a person go through to retrieve a long-lost memory? Or, what differentiates between the cognitive process of recognition (seeing hints of something before remembering it, or memory in context) and recall (retrieving a memory, as in "fill-in-the-blank")?
=
Perception is the ability to take in information via the senses, and process it in some way. Vision and hearing are two dominant senses that allow us to perceive the environment. Some questions in the study of visual perception, for example, include: (1) How are we able to recognize objects?, (2) Why do we perceive a continuous visual environment, even though we only see small bits of it at any one time? One tool for studying visual perception is by looking at how people process optical illusions. The image on the right of a Necker cube is an example of a bistable percept, that is, the cube can be interpreted as being oriented in two different directions.
The study of haptic (tactile), olfactory, and gustatory stimuli also fall into the domain of perception.
Action is taken to refer to the output of a system. In humans, this is accomplished through motor responses. Spatial planning and movement, speech production, and complex motor movements are all aspects of action.
=
Many different methodologies are used to study cognitive science. As the field is highly interdisciplinary, research often cuts across multiple areas of study, drawing on research methods from psychology, neuroscience, computer science and systems theory.
=
In order to have a description of what constitutes intelligent behavior, one must study behavior itself. This type of research is closely tied to that in cognitive psychology and psychophysics. By measuring behavioral responses to different stimuli, one can understand something about how those stimuli are processed. Lewandowski & Strohmetz (2009) reviewed a collection of innovative uses of behavioral measurement in psychology including behavioral traces, behavioral observations, and behavioral choice. Behavioral traces are pieces of evidence that indicate behavior occurred, but the actor is not present (e.g., litter in a parking lot or readings on an electric meter). Behavioral observations involve the direct witnessing of the actor engaging in the behavior (e.g., watching how close a person sits next to another person). Behavioral choices are when a person selects between two or more options (e.g., voting behavior, choice of a punishment for another participant).
Reaction time. The time between the presentation of a stimulus and an appropriate response can indicate differences between two cognitive processes, and can indicate some things about their nature. For example, if in a search task the reaction times vary proportionally with the number of elements, then it is evident that this cognitive process of searching involves serial instead of parallel processing.
Psychophysical responses. Psychophysical experiments are an old psychological technique, which has been adopted by cognitive psychology. They typically involve making judgments of some physical property, e.g. the loudness of a sound. Correlation of subjective scales between individuals can show cognitive or sensory biases as compared to actual physical measurements. Some examples include:
sameness judgments for colors, tones, textures, etc.
threshold differences for colors, tones, textures, etc.
Eye tracking. This methodology is used to study a variety of cognitive processes, most notably visual perception and language processing. The fixation point of the eyes is linked to an individual's focus of attention. Thus, by monitoring eye movements, we can study what information is being processed at a given time. Eye tracking allows us to study cognitive processes on extremely short time scales. Eye movements reflect online decision making during a task, and they provide us with some insight into the ways in which those decisions may be processed.
=
Brain imaging involves analyzing activity within the brain while performing various tasks. This allows us to link behavior and brain function to help understand how information is processed. Different types of imaging techniques vary in their temporal (time-based) and spatial (location-based) resolution. Brain imaging is often used in cognitive neuroscience.
Single-photon emission computed tomography and positron emission tomography. SPECT and PET use radioactive isotopes, which are injected into the subject's bloodstream and taken up by the brain. By observing which areas of the brain take up the radioactive isotope, we can see which areas of the brain are more active than other areas. PET has similar spatial resolution to fMRI, but it has extremely poor temporal resolution.
Electroencephalography. EEG measures the electrical fields generated by large populations of neurons in the cortex by placing a series of electrodes on the scalp of the subject. This technique has an extremely high temporal resolution, but a relatively poor spatial resolution.
Functional magnetic resonance imaging. fMRI measures the relative amount of oxygenated blood flowing to different parts of the brain. More oxygenated blood in a particular region is assumed to correlate with an increase in neural activity in that part of the brain. This allows us to localize particular functions within different brain regions. fMRI has moderate spatial and temporal resolution.
Optical imaging. This technique uses infrared transmitters and receivers to measure the amount of light reflectance by blood near different areas of the brain. Since oxygenated and deoxygenated blood reflects light by different amounts, we can study which areas are more active (i.e., those that have more oxygenated blood). Optical imaging has moderate temporal resolution, but poor spatial resolution. It also has the advantage that it is extremely safe and can be used to study infants' brains.
Magnetoencephalography. MEG measures magnetic fields resulting from cortical activity. It is similar to EEG, except that it has improved spatial resolution since the magnetic fields it measures are not as blurred or attenuated by the scalp, meninges and so forth as the electrical activity measured in EEG is. MEG uses SQUID sensors to detect tiny magnetic fields.
=
Computational models require a mathematically and logically formal representation of a problem. Computer models are used in the simulation and experimental verification of different specific and general properties of intelligence. Computational modeling can help us understand the functional organization of a particular cognitive phenomenon.
Approaches to cognitive modeling can be categorized as: (1) symbolic, on abstract mental functions of an intelligent mind by means of symbols; (2) subsymbolic, on the neural and associative properties of the human brain; and (3) across the symbolic–subsymbolic border, including hybrid.
Symbolic modeling evolved from the computer science paradigms using the technologies of knowledge-based systems, as well as a philosophical perspective (e.g. "Good Old-Fashioned Artificial Intelligence" (GOFAI)). They were developed by the first cognitive researchers and later used in information engineering for expert systems. Since the early 1990s it was generalized in systemics for the investigation of functional human-like intelligence models, such as personoids, and, in parallel, developed as the SOAR environment. Recently, especially in the context of cognitive decision-making, symbolic cognitive modeling has been extended to the socio-cognitive approach, including social and organizational cognition, interrelated with a sub-symbolic non-conscious layer.
Subsymbolic modeling includes connectionist/neural network models. Connectionism relies on the idea that the mind/brain is composed of simple nodes and its problem-solving capacity derives from the connections between them. Neural nets are textbook implementations of this approach. Some critics of this approach feel that while these models approach biological reality as a representation of how the system works, these models lack explanatory powers because, even in systems endowed with simple connection rules, the emerging high complexity makes them less interpretable at the connection-level than they apparently are at the macroscopic level.
Other approaches gaining in popularity include (1) dynamical systems theory, (2) mapping symbolic models onto connectionist models (Neural-symbolic integration or hybrid intelligent systems), and (3) and Bayesian models, which are often drawn from machine learning.
All the above approaches tend either to be generalized to the form of integrated computational models of a synthetic/abstract intelligence (i.e. cognitive architecture) in order to be applied to the explanation and improvement of individual and social/organizational decision-making and reasoning or to focus on single simulative programs (or microtheories/"middle-range" theories) modelling specific cognitive faculties (e.g. vision, language, categorization etc.).
=
Research methods borrowed directly from neuroscience and neuropsychology can also help us to understand aspects of intelligence. These methods allow us to understand how intelligent behavior is implemented in a physical system.
Single-unit recording
Direct brain stimulation
Animal models
Postmortem studies
Cognitive science has given rise to models of human cognitive bias and risk perception, and has been influential in the development of behavioral finance, part of economics. It has also given rise to a new theory of the philosophy of mathematics (related to denotational mathematics), and many theories of artificial intelligence, persuasion and coercion. It has made its presence known in the philosophy of language and epistemology as well as constituting a substantial wing of modern linguistics. Fields of cognitive science have been influential in understanding the brain's particular functional systems (and functional deficits) ranging from speech production to auditory processing and visual perception. It has made progress in understanding how damage to particular areas of the brain affect cognition, and it has helped to uncover the root causes and results of specific dysfunction, such as dyslexia, anopsia, and hemispatial neglect.
Some of the more recognized names in cognitive science are usually either the most controversial or the most cited. Within philosophy, some familiar names include Daniel Dennett, who writes from a computational systems perspective, John Searle, known for his controversial Chinese room argument, and Jerry Fodor, who advocates functionalism.
Others include David Chalmers, who advocates Dualism and is also known for articulating the hard problem of consciousness, and Douglas Hofstadter, famous for writing Gödel, Escher, Bach, which questions the nature of words and thought.
In the realm of linguistics, Noam Chomsky and George Lakoff have been influential (both have also become notable as political commentators). In artificial intelligence, Marvin Minsky, Herbert A. Simon, and Allen Newell are prominent.
Popular names in the discipline of psychology include George A. Miller, James McClelland, Philip Johnson-Laird, Lawrence Barsalou, Vittorio Guidano, Howard Gardner and Steven Pinker. Anthropologists Dan Sperber, Edwin Hutchins, Bradd Shore, James Wertsch and Scott Atran, have been involved in collaborative projects with cognitive and social psychologists, political scientists and evolutionary biologists in attempts to develop general theories of culture formation, religion, and political association.
Computational theories (with models and simulations) have also been developed, by David Rumelhart, James McClelland and Philip Johnson-Laird.
Epistemics is a term coined in 1969 by the University of Edinburgh with the foundation of its School of Epistemics. Epistemics is to be distinguished from epistemology in that epistemology is the philosophical theory of knowledge, whereas epistemics signifies the scientific study of knowledge.
Christopher Longuet-Higgins has defined it as "the construction of formal models of the processes (perceptual, intellectual, and linguistic) by which knowledge and understanding are achieved and communicated."
In his 1978 essay "Epistemics: The Regulative Theory of Cognition", Alvin I. Goldman claims to have coined the term "epistemics" to describe a reorientation of epistemology. Goldman maintains that his epistemics is continuous with traditional epistemology and the new term is only to avoid opposition. Epistemics, in Goldman's version, differs only slightly from traditional epistemology in its alliance with the psychology of cognition; epistemics stresses the detailed study of mental processes and information-processing mechanisms that lead to knowledge or beliefs.
In the mid-1980s, the School of Epistemics was renamed as The Centre for Cognitive Science (CCS). In 1998, CCS was incorporated into the University of Edinburgh's School of Informatics.
One of the core aims of cognitive science is to achieve an integrated theory of cognition. This requires integrative mechanisms explaining how the information processing that occurs simultaneously in spatially segregated (sub-)cortical areas in the brain is coordinated and bound together to give rise to coherent perceptual and symbolic representations. One approach is to solve this "Binding problem" (that is, the problem of dynamically representing conjunctions of informational elements, from the most basic perceptual representations ("feature binding") to the most complex cognitive representations, like symbol structures ("variable binding")), by means of integrative synchronization mechanisms. In other words, one of the coordinating mechanisms appears to be the temporal (phase) synchronization of neural activity based on dynamical self-organizing processes in neural networks, described by the Binding-by-synchrony (BBS) Hypothesis from neurophysiology. Connectionist cognitive neuroarchitectures have been developed that use integrative synchronization mechanisms to solve this binding problem in perceptual cognition and in language cognition. In perceptual cognition the problem is to explain how elementary object properties and object relations, like the object color or the object form, can be dynamically bound together or can be integrated to a representation of this perceptual object by means of a synchronization mechanism ("feature binding", "feature linking"). In language cognition the problem is to explain how semantic concepts and syntactic roles can be dynamically bound together or can be integrated to complex cognitive representations like systematic and compositional symbol structures and propositions by means of a synchronization mechanism ("variable binding") (see also the "Symbolism vs. connectionism debate" in connectionism).
However, despite significant advances in understanding the integrated theory of cognition (specifically the Binding problem), the debate on this issue of beginning cognition is still in progress. From the different perspectives noted above, this problem can be reduced to the issue of how organisms at the simple reflexes stage of development overcome the threshold of the environmental chaos of sensory stimuli: electromagnetic waves, chemical interactions, and pressure fluctuations. The so-called Primary Data Entry (PDE) thesis poses doubts about the ability of such an organism to overcome this cue threshold on its own. In terms of mathematical tools, the PDE thesis underlines the insuperable high threshold of the cacophony of environmental stimuli (the stimuli noise) for young organisms at the onset of life. It argues that the temporal (phase) synchronization of neural activity based on dynamical self-organizing processes in neural networks, any dynamical bound together or integration to a representation of the perceptual object by means of a synchronization mechanism can not help organisms in distinguishing relevant cue (informative stimulus) for overcome this noise threshold.
Outlines
Outline of human intelligence – topic tree presenting the traits, capacities, models, and research fields of human intelligence, and more.
Outline of thought – topic tree that identifies many types of thoughts, types of thinking, aspects of thought, related fields, and more.
 Media related to Cognitive science at Wikimedia Commons
 Quotations related to Cognitive science at Wikiquote
 Learning materials related to Cognitive science at Wikiversity
"Cognitive Science" on the Stanford Encyclopedia of Philosophy
Cognitive Science Society
Cognitive Science Movie Index: A broad list of movies showcasing themes in the Cognitive Sciences Archived 4 September 2015 at the Wayback Machine
List of leading thinkers in cognitive science

--- TOPIC: Consciousness ---
Consciousness is being aware of something internal to one's self or being conscious of states or objects in one's external environment. It has been the topic of extensive explanations, analyses, and debate among philosophers, scientists, and theologians for millennia. There is no consensus on what exactly needs to be studied, or even if consciousness can be considered a scientific concept. In some explanations, it is synonymous with mind, while in others it is considered an aspect of it.
In the past, consciousness meant one's "inner life": the world of introspection, private thought, imagination, and volition. Today, it often includes any kind of cognition, experience, feeling, or perception. It may be awareness, awareness of awareness, metacognition, or self-awareness, either continuously changing or not. There is also a medical definition that helps, for example, to discern "coma" from other states. The disparate range of research, notions, and speculations raises some curiosity about whether the right questions are being asked.
Examples of the range of descriptions, definitions and explanations are: ordered distinction between self and environment, simple wakefulness, one's sense of selfhood or soul explored by "looking within", being a metaphorical "stream" of contents, or being a mental state, mental event, or mental process of the brain.
The words "conscious" and "consciousness" in the English language date to the 17th century, and the first recorded use of "conscious" as a simple adjective was applied figuratively to inanimate objects ("the conscious Groves", 1643). It derived from the Latin conscius (con- "together" and scio "to know") which meant "knowing with" or "having joint or common knowledge with another", especially as in sharing a secret. Thomas Hobbes in Leviathan (1651) wrote: "Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another". There were also many occurrences in Latin writings of the phrase conscius sibi, which translates literally as "knowing with oneself", or in other words "sharing knowledge with oneself about something". This phrase has the figurative sense of "knowing that one knows", which is something like the modern English word "conscious", but it was rendered into English as "conscious to oneself" or "conscious unto oneself". For example, Archbishop Ussher wrote in 1613 of "being so conscious unto myself of my great weakness".
The Latin conscientia, literally "knowledge-with", first appears in Roman juridical texts by writers such as Cicero. It means a kind of shared knowledge with moral value, specifically what a witness knows of someone else's deeds. Although René Descartes (1596–1650), writing in Latin, is generally taken to be the first philosopher to use conscientia in a way less like the traditional meaning and more like the way modern English speakers would use "conscience", his meaning is nowhere defined. In Search after Truth (Regulæ ad directionem ingenii ut et inquisitio veritatis per lumen naturale, Amsterdam 1701) he wrote the word with a gloss: conscientiâ, vel interno testimonio (translatable as "conscience, or internal testimony"). It might mean the knowledge of the value of one's own thoughts. One way that this shift during the seventeenth century from "conscience" to "consciousness" took place was through the poetry of John Milton, as the scholar Timothy M. Harrison has shown. 
The origin of the modern concept of consciousness is often attributed to John Locke who defined the word in his Essay Concerning Human Understanding, published in 1690, as "the perception of what passes in a man's own mind". The essay strongly influenced 18th-century British philosophy, and Locke's definition appeared in Samuel Johnson's celebrated Dictionary (1755).
The French term conscience is defined roughly like English "consciousness" in the 1753 volume of Diderot and d'Alembert's Encyclopédie as "the opinion or internal feeling that we ourselves have from what we do".
Scholars are divided  as to whether Aristotle had a concept of consciousness. He does not use any single word or terminology that is clearly similar to the phenomenon or concept defined by John Locke. Victor Caston contends that Aristotle did have a concept more clearly similar to perception.
Modern dictionary definitions of the word consciousness evolved over several centuries and reflect a range of seemingly related meanings, with some differences that have been controversial, such as the distinction between inward awareness and perception of the physical world, or the distinction between conscious and unconscious, or the notion of a mental entity or mental activity that is not physical.
The common-usage definitions of consciousness in Webster's Third New International Dictionary (1966) are as follows:
awareness or perception of an inward psychological or spiritual fact; intuitively perceived knowledge of something in one's inner self
inward awareness of an external object, state, or fact
concerned awareness; INTEREST, CONCERN—often used with an attributive noun [e.g. class consciousness]
the state or activity that is characterized by sensation, emotion, volition, or thought; mind in the broadest possible sense; something in nature that is distinguished from the physical
the totality in psychology of sensations, perceptions, ideas, attitudes, and feelings of which an individual or a group is aware at any given time or within a particular time span—
waking life (as that to which one returns after sleep, trance, fever) wherein all one's mental powers have returned . . .
the part of mental life or psychic content in psychoanalysis that is immediately available to the ego—
The Cambridge English Dictionary defines consciousness as "the state of being awake, thinking, and knowing what is happening around you", as well as "the state of understanding and realizing something".
The Oxford Living Dictionary defines consciousness as "[t]he state of being aware of and responsive to one's surroundings", "[a] person's awareness or perception of something", and "[t]he fact of awareness by the mind of itself and the world".
Philosophers have attempted to clarify technical distinctions by using a jargon of their own. The corresponding entry in the Routledge Encyclopedia of Philosophy (1998) reads:
Consciousness
Philosophers have used the term consciousness for four main topics: knowledge in general, intentionality, introspection (and the knowledge it specifically generates) and phenomenal experience... Something within one's mind is 'introspectively conscious' just in case one introspects it (or is poised to do so). Introspection is often thought to deliver one's primary knowledge of one's mental life. An experience or other mental entity is 'phenomenally conscious' just in case there is 'something it is like' for one to have it. The clearest examples are: perceptual experience, such as tastings and seeings; bodily-sensational experiences, such as those of pains, tickles and itches; imaginative experiences, such as those of one's own actions or perceptions; and streams of thought, as in the experience of thinking 'in words' or 'in images'. Introspection and phenomenality seem independent, or dissociable, although this is controversial.
=
During the early 19th century, the emerging field of geology inspired a popular metaphor that the mind likewise had hidden layers "which recorded the past of the individual". By 1875, most psychologists believed that "consciousness was but a small part of mental life", and this idea underlies the goal of Freudian therapy, to expose the unconscious layer of the mind.
Other metaphors from various sciences inspired other analyses of the mind, for example: Johann Friedrich Herbart described ideas as being attracted and repulsed like magnets; John Stuart Mill developed the idea of "mental chemistry" and "mental compounds", and Edward B. Titchener sought the "structure" of the mind by analyzing its "elements". The abstract idea of states of consciousness mirrored the concept of states of matter.
In 1892, William James noted that the "ambiguous word 'content' has been recently invented instead of 'object'" and that the metaphor of mind as a container seemed to minimize the dualistic problem of how "states of consciousness can know" things, or objects; by 1899 psychologists were busily studying the "contents of conscious experience by introspection and experiment". Another popular metaphor was James's doctrine of the stream of consciousness, with continuity, fringes, and transitions.
James discussed the difficulties of describing and studying psychological phenomena, recognizing that commonly used terminology was a necessary and acceptable starting point towards more precise, scientifically justified language. Prime examples were phrases like inner experience and personal consciousness:
The first and foremost concrete fact which every one will affirm to belong to his inner experience is the fact that consciousness of some sort goes on. 'States of mind' succeed each other in him. [...] But everyone knows what the terms mean [only] in a rough way; [...] When I say every 'state' or 'thought' is part of a personal consciousness, 'personal consciousness' is one of the terms in question. Its meaning we know so long as no one asks us to define it, but to give an accurate account of it is the most difficult of philosophic tasks. [...] The only states of consciousness that we naturally deal with are found in personal consciousnesses, minds, selves, concrete particular I's and you's.
=
Prior to the 20th century, philosophers treated the phenomenon of consciousness as the "inner world [of] one's own mind", and introspection was the mind "attending to" itself, an activity seemingly distinct from that of perceiving the "outer world" and its physical phenomena. In 1892 William James noted the distinction along with doubts about the inward character of the mind:'Things' have been doubted, but thoughts and feelings have never been doubted. The outer world, but never the inner world, has been denied. Everyone assumes that we have direct introspective acquaintance with our thinking activity as such, with our consciousness as something inward and contrasted with the outer objects which it knows. Yet I must confess that for my part I cannot feel sure of this conclusion. [...] It seems as if consciousness as an inner activity were rather a postulate than a sensibly given fact...
By the 1960s, for many philosophers and psychologists who talked about consciousness, the word no longer meant the 'inner world' but an indefinite, large category called awareness, as in the following example:
It is difficult for modern Western man to grasp that the Greeks really had no concept of consciousness in that they did not class together phenomena as varied as problem solving, remembering, imagining, perceiving, feeling pain, dreaming, and acting on the grounds that all these are manifestations of being aware or being conscious.
Many philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness. In The Macmillan Dictionary of Psychology (1989 edition), Stuart Sutherland emphasized external awareness, and expressed a skeptical attitude more than a definition:
Consciousness—The having of perceptions, thoughts, and feelings; awareness. The term is impossible to define except in terms that are unintelligible without a grasp of what consciousness means. Many fall into the trap of equating consciousness with self-consciousness—to be conscious it is only necessary to be aware of the external world. Consciousness is a fascinating but elusive phenomenon: it is impossible to specify what it is, what it does, or why it has evolved. Nothing worth reading has been written on it.
Using 'awareness', however, as a definition or synonym of consciousness is not a simple matter:
If awareness of the environment . . . is the criterion of consciousness, then even the protozoans are conscious. If awareness of awareness is required, then it is doubtful whether the great apes and human infants are conscious.
In 1974, philosopher Thomas Nagel used 'consciousness', 'conscious experience', 'subjective experience' and the 'subjective character of experience' as synonyms for something that "occurs at many levels of animal life ... [although] it is difficult to say in general what provides evidence of it." Nagel's terminology also included what has been described as "the standard 'what it's like' locution" in reference to the impenetrable subjectivity of any organism's experience which Nagel referred to as "inner life" without implying any kind of introspection. On Nagel's approach, Peter Hacker commented: "Consciousness, thus conceived, is extended to the whole domain of 'experience'—of 'Life' subjectively understood." He regarded this as a "novel analysis of consciousness" and has been particularly critical of Nagel's terminology and its philosophical consequences. In 2002 he attacked Nagel's 'what it's like' phrase as "malconstructed" and meaningless English—it sounds as if it asks for an analogy, but does not—and he called Nagel's approach logically "misconceived" as a definition of consciousness. In 2012 Hacker went further and asserted that Nagel had "laid the groundwork for ... forty years of fresh confusion about consciousness" and that "the contemporary philosophical conception of consciousness that is embraced by the 'consciousness studies community' is incoherent".
=
Many philosophers have argued that consciousness is a unitary concept that is understood by the majority of people despite the difficulty philosophers have had defining it. The term 'subjective experience', following Nagel, is ambiguous, as philosophers seem to differ from non-philosophers in their intuitions about its meaning. Max Velmans proposed that the "everyday understanding of consciousness" uncontroversially "refers to experience itself rather than any particular thing that we observe or experience" and he added that consciousness "is [therefore] exemplified by all the things that we observe or experience", whether thoughts, feelings, or perceptions. Velmans noted however, as of 2009, that there was a deep level of "confusion and internal division" among experts about the phenomenon of consciousness, because researchers lacked "a sufficiently well-specified use of the term...to agree that they are investigating the same thing". He argued additionally that "pre-existing theoretical commitments" to competing explanations of consciousness might be a source of bias.
Within the "modern consciousness studies" community the technical phrase 'phenomenal consciousness' is a common synonym for all forms of awareness, or simply 'experience', without differentiating between inner and outer, or between higher and lower types. With advances in brain research, "the presence or absence of experienced phenomena" of any kind underlies the work of those neuroscientists who seek "to analyze the precise relation of conscious phenomenology to its associated information processing" in the brain. This neuroscientific goal is to find the "neural correlates of consciousness" (NCC). One criticism of this goal is that it begins with a theoretical commitment to the neurological origin of all "experienced phenomena" whether inner or outer. Also, the fact that the easiest 'content of consciousness' to be so analyzed is "the experienced three-dimensional world (the phenomenal world) beyond the body surface" invites another criticism, that most consciousness research since the 1990s, perhaps because of bias, has focused on processes of external perception.
From a history of psychology perspective, Julian Jaynes rejected popular but "superficial views of consciousness" especially those which equate it with "that vaguest of terms, experience". In 1976 he insisted that if not for introspection, which for decades had been ignored or taken for granted rather than explained, there could be no "conception of what consciousness is" and in 1990, he reaffirmed the traditional idea of the phenomenon called 'consciousness', writing that "its denotative definition is, as it was for René Descartes, John Locke, and David Hume, what is introspectable". Jaynes saw consciousness as an important but small part of human mentality, and he asserted: "there can be no progress in the science of consciousness until ... what is introspectable [is] sharply distinguished" from the unconscious processes of cognition such as perception, reactive awareness and attention, and automatic forms of learning, problem-solving, and decision-making.
The cognitive science point of view—with an inter-disciplinary perspective involving fields such as psychology, linguistics and anthropology—requires no agreed definition of "consciousness" but studies the interaction of many processes besides perception. For some researchers, consciousness is linked to some kind of "selfhood", for example to certain pragmatic issues such as the feeling of agency and the effects of regret and action on experience of one's own body or social identity. Similarly Daniel Kahneman, who focused on systematic errors in perception, memory and decision-making, has differentiated between two kinds of mental processes, or cognitive "systems": the "fast" activities that are primary, automatic and "cannot be turned off", and the "slow", deliberate, effortful activities of a secondary system "often associated with the subjective experience of agency, choice, and concentration". Kahneman's two systems have been described as "roughly corresponding to unconscious and conscious processes". The two systems can interact, for example in sharing the control of attention. While System 1 can be impulsive, "System 2 is in charge of self-control", and "When we think of ourselves, we identify with System 2, the conscious, reasoning self that has beliefs, makes choices, and decides what to think about and what to do".
Some have argued that we should eliminate the concept from our understanding of the mind, a position known as consciousness semanticism.
=
In medicine, a "level of consciousness" terminology is used to describe a patient's arousal and responsiveness, which can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the level of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted. The degree or level of consciousness is measured by standardized behavior observation scales such as the Glasgow Coma Scale.
While historically philosophers have defended various views on consciousness, surveys indicate that physicalism is now the dominant position among contemporary philosophers of mind. For an overview of the field, approaches often include both historical perspectives (e.g., Descartes, Locke, Kant) and organization by key issues in contemporary debates. An alternative is to focus primarily on current philosophical stances and empirical findings.
=
Philosophers differ from non-philosophers in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is too narrow, either because the concept of consciousness is embedded in our intuitions, or because we all are illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of entities, or identities, acting in the world. Thus, by speaking of "consciousness" we end up leading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings.
=
Ned Block argues that discussions on consciousness often fail to properly distinguish phenomenal consciousness from access consciousness. These terms had been used before Block, but he adopted the short forms P-consciousness and A-consciousness. According to Block:
P-consciousness is raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies and responses at the center. These experiences, considered independently of any impact on behavior, are called qualia.
A-consciousness is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on.
Block adds that P-consciousness does not allow of easy definition: he admits that he "cannot define P-consciousness in any remotely noncircular way.
Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the hard problem of consciousness.
Some philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book Consciousness and Experience that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness of; state/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)—and that even this list omits several more obscure forms.
There is also debate over whether or not A-consciousness and P-consciousness always coexist or if they can exist separately. Although P-consciousness without A-consciousness is more widely accepted, there have been some hypothetical examples of A without P. Block, for instance, suggests the case of a "zombie" that is computationally identical to a person but without any subjectivity. However, he remains somewhat skeptical concluding "I don't know whether there are any actual cases of A-consciousness without P-consciousness, but I hope I have illustrated their conceptual possibility".
=
Sam Harris observes: "At the level of your experience, you are not a body of cells, organelles, and atoms; you are consciousness and its ever-changing contents". Seen in this way, consciousness is a subjectively experienced, ever-present field in which things (the contents of consciousness) come and go.
Christopher Tricker argues that this field of consciousness is symbolized by the mythical bird that opens the Daoist classic the Zhuangzi. This bird's name is Of a Flock (peng 鵬), yet its back is countless thousands of miles across and its wings are like clouds arcing across the heavens. "Like Of a Flock, whose wings arc across the heavens, the wings of your consciousness span to the horizon. At the same time, the wings of every other being's consciousness span to the horizon. You are of a flock, one bird among kin."
=
Mental processes (such as consciousness) and physical processes (such as brain events) seem to be correlated, however the specific nature of the connection is unknown.
The first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as mind–body dualism. Descartes proposed that consciousness resides within an immaterial domain he called res cogitans (the realm of thought), in contrast to the domain of material things, which he called res extensa (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.
Although it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. However, no alternative solution has gained general acceptance. Proposed solutions can be divided broadly into two categories: dualist solutions that maintain Descartes's rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics), and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind is made out of matter), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these schools of thought.
Since the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book Man a Machine (L'homme machine). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.
A few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories have been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing. Empirical evidence is against the notion of quantum consciousness, an experiment about wave function collapse led by Catalina Curceanu in 2022 suggests that quantum consciousness, as suggested by Roger Penrose and Stuart Hameroff, is highly implausible.
Apart from the general question of the "hard problem" of consciousness (which is, roughly speaking, the question of how mental experience can arise from a physical basis), a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.
=
Many philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. The problem of other minds is a philosophical problem traditionally stated as the following epistemological question: Given that I can only observe the behavior of others, how can I know that others have minds? The problem of other minds is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois, and by Colin Allen (a professor at the University of Pittsburgh) regarding the literature and research studying artificial intelligence in androids.
The most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior; we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in a research paper titled "The Unimagined Preposterousness of Zombies", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can do, including the fact that they can tell us about their experiences.
=
The term "qualia" was introduced in philosophical literature by C. I. Lewis. The word is derived from Latin and means "of what sort". It is basically a quantity or property of something as perceived or experienced by an individual, like the scent of rose, the taste of wine, or the pain of a headache. They are difficult to articulate or describe. The philosopher and scientist Daniel Dennett describes them as "the way things seem to us", while philosopher and cognitive scientist David Chalmers expanded on qualia as the "hard problem of consciousness" in the 1990s. When qualia are experienced, activity is simulated in the brain, and these processes are called neural correlates of consciousness (NCCs). Many scientific studies have been done to attempt to link particular brain regions with emotions or experiences.
Species which experience qualia are said to have sentience, which is central to the animal rights movement, because it includes the ability to experience pain and suffering.
=
An unsolved problem in the philosophy of consciousness is how it relates to the nature of personal identity. This includes questions regarding whether someone is the "same person" from moment to moment. If that is the case, another question is what exactly the "identity carrier" is that makes a conscious being "the same" being from one moment to the next. The problem of determining personal identity also includes questions such as Benj Hellie's vertiginous question, which can be summarized as "Why am I me and not someone else?". The philosophical problems regarding the nature of personal identity have been extensively discussed by Thomas Nagel in his book The View from Nowhere.
A common view of personal identity is that an individual has a continuous identity that persists from moment to moment, with an individual having a continuous identity consisting of a line segment stretching across time from birth to death. In the case of an afterlife as described in Abrahamic religions, one's personal identity is believed to stretch infinitely into the future, forming a ray or line. This notion of identity is similar to the form of dualism advocated by René Descartes. However, some philosophers argue that this common notion of personal identity is unfounded. Daniel Kolak has argued extensively against it in his book I am You. Kolak refers to the aforementioned notion of personal identity being linear as "Closed individualism". Another view of personal identity according to Kolak is "Empty individualism", in which one's personal identity only exists for a single moment of time. However, Kolak advocates for a view of personal identity called Open individualism, in which all consciousness is in reality a single being and individual personal identity in reality does not exist at all. Another philosopher who has contested the notion of personal identity is Derek Parfit. In his book Reasons and Persons, he describes a thought experiment known as the teletransportation paradox. In Buddhist philosophy, the concept of anattā refers to the idea that the self is an illusion.
Other philosophers have argued that Hellie's vertiginous question has a number of philosophical implications relating to the metaphysical nature of consciousness. Christian List argues that the vertiginous question and the existence of first-personal facts is evidence against physicalism, and evidence against other third-personal metaphysical pictures, including standard versions of dualism. List also argues that the vertiginous question implies a "quadrilemma" for theories of consciousness. He claims that at most three of the following metaphysical claims can be true: 'first-person realism', 'non-solipsism', 'non-fragmentation', and 'one world'—and at least one of these four must be false. List has proposed a model he calls the "many-worlds theory of consciousness" in order to reconcile the subjective nature of consciousness without lapsing into solipsism. Vincent Conitzer argues that the nature of identity is connected to A series and B series theories of time, and that A-theory being true implies that the "I" is metaphysically distinguished from other perspectives. Other philosophical theories regarding the metaphysical nature of self are Caspar Hare's theories of perspectival realism, in which things within perceptual awareness have a defining intrinsic property that exists absolutely and not relative to anything, and egocentric presentism, in which the experiences of other individuals are not present in the way that one's current perspective is.
For many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. The Science and Religion Forum 1984 annual conference, 'From Artificial Intelligence to Human Consciousness' identified the nature of consciousness as a matter for investigation; Donald Michie was a keynote speaker. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called Consciousness Studies, giving rise to a stream of experimental work published in books, journals such as Consciousness and Cognition, Frontiers in Consciousness Research, Psyche, and the Journal of Consciousness Studies, along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness and the Society for Consciousness Studies.
Modern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.
=
Experimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness.
For example, subjects who stare continuously at a Necker cube usually report that they experience it "flipping" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness, and suitable experimental manipulations can lead to increasing priming effects despite decreasing prime identification (double dissociation).
Verbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors—but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.
Although verbal report is in practice the "gold standard" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity, and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies related to the neuroscience of free will have also shown that the influence consciousness has on decision-making is not always straightforward.
 Mirror test and contingency awareness 
Another approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, orcas, pigeons, European magpies and elephants have all been observed to pass this test. While some other animals like pigs have been shown to find food by looking into the mirror.
Contingency awareness is another such approach, which is basically the conscious understanding of one's actions and its effects on one's environment. It is recognized as a factor in self-recognition. The brain processes during contingency awareness and learning is believed to rely on an intact medial temporal lobe and age. A study done in 2020 involving transcranial direct current stimulation, Magnetic resonance imaging (MRI) and eyeblink classical conditioning supported the idea that the parietal cortex serves as a substrate for contingency awareness and that age-related disruption of this region is sufficient to impair awareness.
=
A major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains—that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, which will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.
Another idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llinás, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations. Thalamus-cortex interaction plays a pivotal role in the state of consciousness, and may play a role in the content of consciousness.[1]
A number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex (V1) show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a "top-down" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a "bottom-up" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry). Furthermore, top-down feedback from higher to lower visual brain areas may be weaker or absent in the peripheral visual field, as suggested by some experimental data and theoretical arguments; nevertheless humans can perceive visual inputs in the peripheral visual field arising from bottom-up V1 neural activities. Meanwhile, bottom-up V1 activities for the central visual fields can be vetoed, and thus made invisible to perception, by the top-down feedback, when these bottom-up signals are inconsistent with the brain's internal model of the visual world.
Modulation of neural responses may correlate with phenomenal experiences. In contrast to the raw electrical responses that do not correlate with consciousness, the modulation of these responses by other stimuli correlates surprisingly well with an important aspect of consciousness: namely with the phenomenal experience of stimulus intensity (brightness, contrast). In the research group of Danko Nikolić it has been shown that some of the changes in the subjectively perceived brightness correlated with the modulation of firing rates while others correlated with the modulation of neural synchrony. An fMRI investigation suggested that these findings were strictly limited to the primary visual areas. This indicates that, in the primary visual areas, changes in firing rates and synchrony can be considered as neural correlates of qualia—at least for some type of qualia.
In 2013, the perturbational complexity index (PCI) was proposed, a measure of the algorithmic complexity of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.
Assuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious—a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds—there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness–brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologs can be identified? The general conclusion from the study by Butler, et al. is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologs have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homolog/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.
Joaquin Fuster of UCLA has advocated the position of the importance of the prefrontal cortex in humans, along with the areas of Wernicke and Broca, as being of particular importance to the development of human language capacities neuro-anatomically necessary for the emergence of higher-order consciousness in humans.
A study in 2016 looked at lesions in specific areas of the brainstem that were associated with coma and vegetative states. A small region of the rostral dorsolateral pontine tegmentum in the brainstem was suggested to drive consciousness through functional connectivity with two cortical regions, the left ventral anterior insular cortex, and the pregenual anterior cingulate cortex. These three regions may work together as a triad to maintain consciousness.
Krista and Tatiana Hogan have a unique thalamic connection that may provide insight into the philosophical and neurological foundations of consciousness. It has been argued that there's no empirical test that can conclusively establish that for some sensations, the twins share one token experience rather than two exactly matching token experiences. Yet background considerations about the way the brain has specific locations for conscious contents, combined with the evident overlapping pathways in the twins' brains, arguably implies that the twins share some conscious experiences. If this is true, then the twins may offer a proof of concept for how experiences in general could be shared between brains.
=
Clear definitions of consciousness in academic literature are rare. David Chalmers declared the task the hard problem of consciousness. However academic definitions do exist, from Tononi's integrated information theory, Craig MacKenzie, and Cleeremans and Jimenez - the latter being a Definition of Learning with remarkable similarity to both Tononi and MacKenzie's definitions. Both Bernard Baars and Igor Aleksander worked out the aspects necessary for consciousness.
Tononi's definition is as follows: 
according to Integrated information theory (IIT), consciousness requires a grouping of elements within a system that have physical cause-effect power upon one another. This in turn implies that only reentrant architecture consisting of feedback loops, whether neural or computational, will realize consciousness.
McKenzie's definition begins: 
Consciousness is the capacity to generate desires and decisions about perceived or imagined realities by distinguishing self from non-self through the use of perception, memory and imagination.
...
According to Axel Cleeremans and Luis Jiménez, learning is defined as:
a set of phylogenetically advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments.
This definition is notable for its similarity to the global workspace theory (GWT) theatre analogy
=
A wide range of empirical theories of consciousness have been proposed. Adrian Doerig and colleagues list 13 notable theories, while Anil Seth and Tim Bayne list 22 notable theories.
 Global workspace theory 
Global workspace theory (GWT) is a cognitive architecture and theory of consciousness proposed by the cognitive psychologist Bernard Baars in 1988. Baars explains the theory with the metaphor of a theater, with conscious processes represented by an illuminated stage. This theater integrates inputs from a variety of unconscious and otherwise autonomous networks in the brain and then broadcasts them to unconscious networks (represented in the metaphor by a broad, unlit "audience"). The theory has since been expanded upon by other scientists including cognitive neuroscientist Stanislas Dehaene and Lionel Naccache.
See also the Dehaene–Changeux model.
 Integrated information theory 
Integrated information theory (IIT), pioneered by neuroscientist Giulio Tononi in 2004, postulates that consciousness resides in the information being processed and arises once the information reaches a certain level of complexity. IIT proposes a 1:1 mapping between conscious states and precise, formal mathematical descriptions of those mental states. Proponents of this model suggest that it may provide a physical grounding for consciousness in neurons, as they provide the mechanism by which information is integrated. This also relates to the "hard problem of consciousness" proposed by David Chalmers. In 2023, 124 scholars signed a letter saying that IIT gets disproportionate media attention relative to its supporting empirical evidence, and called it "pseudoscience", arguing that its core assumptions are not adequately testable. This led to academic debate, as some other researchers objected to the "pseudoscience" characterization.
 Orchestrated objective reduction 
Orchestrated objective reduction (Orch-OR), or the quantum theory of mind, was proposed by scientists Roger Penrose and Stuart Hameroff, and states that consciousness originates at the quantum level inside neurons. The mechanism is held to be a quantum process called objective reduction that is orchestrated by cellular structures called microtubules, which form the cytoskeleton around which the brain is built. The duo proposed that these quantum processes accounted for creativity, innovation, and problem-solving abilities. Penrose published his views in the book The Emperor's New Mind. In 2014, the discovery of quantum vibrations inside microtubules gave new life to the argument.
However, scientists and philosophers have criticized Penrose's interpretation of Gödel's theorem and his conclusion that quantum phenomena play a role in human cognition.
 Attention schema theory 
In 2011, Michael Graziano and Kastner proposed the "attention schema" theory of awareness. Graziano went on to publish an expanded discussion of this theory in his book "Consciousness and the Social Brain". In that theory, specific cortical areas, notably in the superior temporal sulcus and the temporo-parietal junction, are used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to these cortical regions can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of explaining the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional states. To be aware of X is explained by constructing a model of one's attentional focus on X.
 Entropic brain theory 
The entropic brain is a theory of conscious states informed by neuroimaging research with psychedelic drugs. The theory suggests that the brain in primary states such as rapid eye movement (REM) sleep, early psychosis and under the influence of psychedelic drugs, is in a disordered state; normal waking consciousness constrains some of this freedom and makes possible metacognitive functions such as internal self-administered reality testing and self-awareness. Criticism has included questioning whether the theory has been adequately tested.
 Projective consciousness model 
In 2017, work by David Rudrauf and colleagues, including Karl Friston, applied the active inference paradigm to consciousness, leading to the projective consciousness model (PCM), a model of how sensory data is integrated with priors in a process of projective transformation. The authors argue that, while their model identifies a key relationship between computation and phenomenology, it does not completely solve the hard problem of consciousness or completely close the explanatory gap.
 Claustrum being the conductor for consciousness 
In 2004, a proposal was made by molecular biologist Francis Crick (co-discoverer of the double helix), which stated that to bind together an individual's experience, a conductor of an orchestra is required. Together with neuroscientist Christof Koch, he proposed that this conductor would have to collate information rapidly from various regions of the brain. The duo reckoned that the claustrum was well suited for the task. However, Crick died while working on the idea.
The proposal is backed by a study done in 2014, where a team at the George Washington University induced unconsciousness in a 54-year-old woman suffering from intractable epilepsy by stimulating her claustrum. The woman underwent depth electrode implantation and electrical stimulation mapping. The electrode between the left claustrum and anterior-dorsal insula was the one which induced unconsciousness. Correlation for interactions affecting medial parietal and posterior frontal channels during stimulation increased significantly as well. Their findings suggested that the left claustrum or anterior insula is an important part of a network that subserves consciousness, and that disruption of consciousness is related to increased EEG signal synchrony within frontal-parietal networks. However, this remains an isolated, hence inconclusive study.
A study published in 2022 opposed the idea Claustrum is the seat of consciousness but instead concluded that it is more like a "router" transferring command and information across the brain. The study showed that when the Claustrum is disabled, complex tasks could not be performed.
=
The emergence of consciousness during biological evolution remains a topic of ongoing scientific inquiry. The survival value of consciousness is still a matter of exploration and understanding. While consciousness appears to play a crucial role in human cognition, decision-making, and self-awareness, its adaptive significance across different species remains a subject of debate.
Some people question whether consciousness has any survival value. Some argue that consciousness is a by-product of evolution. Thomas Henry Huxley for example defends in an essay titled "On the Hypothesis that Animals are Automata, and its History" an epiphenomenalist theory of consciousness, according to which consciousness is a causally inert effect of neural activity—"as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery". To this William James objects in his essay Are We Automata? by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops a similar evolutionary argument in the book The Self and Its Brain.
Opinions are divided on when and how consciousness first arose. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Other authors date the origins of consciousness to the first animals with nervous systems or early vertebrates in the Cambrian over 500 million years ago. Donald Griffin suggests in his book Animal Minds a gradual evolution of consciousness. Further exploration of the origins of consciousness, particularly in molluscs, has been done by Peter Godfrey Smith in his book Metazoa.
Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the integration consensus. Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see Neural correlates section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyzes, etc.), and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of Ezequiel Morsella.
As noted earlier, even among writers who consider consciousness to be well-defined, there is widespread dispute about which animals other than humans can be said to possess it. Edelman has described this distinction as that of humans possessing higher-order consciousness while sharing the trait of primary consciousness with non-human animals (see previous paragraph). Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article "Evolution of consciousness", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness ("[a] psychon ... linked to [a] dendron through quantum physics"). Bernard Baars proposed that once in place, this "recursive" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.
Other philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function F being performed by conscious organism O and non-conscious organism O*, it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement. Consciousness in this sense has been compared to the blind spot in the retina where it is not an adaption of the retina, but instead just a by-product of the way the retinal axons were wired. Several scholars including Pinker, Chomsky, Edelman, and Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness (see Neural correlates section above).
=
There are some brain states in which consciousness seems to be absent, including dreamless sleep or coma. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.
The two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.
Research conducted on the effects of partial epileptic seizures on consciousness found that patients who have partial epileptic seizures experience altered states of consciousness. In partial epileptic seizures, consciousness is impaired or lost while some aspects of consciousness, often automated behaviors, remain intact. Studies found that when measuring the qualitative features during partial epileptic seizures, patients exhibited an increase in arousal and became absorbed in the experience of the seizure, followed by difficulty in focusing and shifting attention.
A variety of psychoactive drugs, including alcohol, have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, empathogens–entactogens such as MDMA ("Ecstasy"), or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, dimethyltryptamine, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.
There has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.
The most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.
The medical approach to consciousness is scientifically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. The medical approach mainly focuses on the amount of consciousness a person has: in medicine, consciousness is assessed as a "level" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.
Consciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may have disorders of consciousness or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as the Karen Ann Quinlan case, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.
=
In medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond meaningfully to questions and commands. If so, the patient is asked for their name, current location, and current day and time. A patient who can answer all of these questions is said to be "alert and oriented times four" (sometimes denoted "A&Ox4" on a medical chart), and is usually considered fully conscious.
The more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3–15, with a score of 3 to 8 indicating coma, and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the best motor response (ranging from "no motor response" to "obeys commands"), the best eye response (ranging from "no eye opening" to "eyes opening spontaneously") and the best verbal response (ranging from "no verbal response" to "fully oriented"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.
In 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.
=
Medical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in possible irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.
Medical experts increasingly view anosognosia as a disorder of consciousness. Anosognosia is a Greek-derived term meaning "unawareness of disease". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the left with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the left side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer or an explanation that does not make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton–Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.
=
Of the eight types of consciousness in the Lycan classification, some are detectable in utero and others develop years after birth. Psychologist and educator William Foulkes studied children's dreams and concluded that prior to the shift in cognitive maturation that humans experience during ages five to seven, children lack the Lockean consciousness that Lycan had labeled "introspective consciousness" and that Foulkes labels "self-reflection". In a 2020 paper, Katherine Nelson and Robyn Fivush use "autobiographical consciousness" to label essentially the same faculty, and agree with Foulkes on the timing of this faculty's acquisition. Nelson and Fivush contend that "language is the tool by which humans create a new, uniquely human form of consciousness, namely, autobiographical consciousness". Julian Jaynes had staked out these positions decades earlier. Citing the developmental steps that lead the infant to autobiographical consciousness, Nelson and Fivush point to the acquisition of "theory of mind", calling theory of mind "necessary for autobiographical consciousness" and defining it as "understanding differences between one's own mind and others' minds in terms of beliefs, desires, emotions and thoughts". They write, "The hallmark of theory of mind, the understanding of false belief, occurs ... at five to six years of age".
=
The topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because non-human animals, lacking the ability to express human language, cannot tell humans about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.
Philosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled "What Is it Like to Be a Bat?". He said that an organism is conscious "if and only if there is something that it is like to be that organism—something it is like for the organism"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive—Donald Griffin's 2001 book Animal Minds reviews a substantial portion of the evidence.
On July 7, 2012, eminent scientists from different branches of neuroscience gathered at the University of Cambridge to celebrate the Francis Crick Memorial Conference, which deals with consciousness in humans and pre-linguistic consciousness in nonhuman animals. After the conference, they signed in the presence of Stephen Hawking, the 'Cambridge Declaration on Consciousness', which summarizes the most important findings of the survey:
"We decided to reach a consensus and make a statement directed to the public that is not scientific. It's obvious to everyone in this room that animals have consciousness, but it is not obvious to the rest of the world. It is not obvious to the rest of the Western world or the Far East. It is not obvious to the society."
"Convergent evidence indicates that non-human animals ..., including all mammals and birds, and other creatures, ... have the necessary neural substrates of consciousness and the capacity to exhibit intentional behaviors."
=
The idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:
It is desirable to guard against the possibility of exaggerated ideas that might arise as to the powers of the Analytical Engine. ... The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths. Its province is to assist us in making available what we are already acquainted with.
One of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled Computing Machinery and Intelligence. Turing disavowed any interest in terminology, saying that even "Can machines think?" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes passé and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology. Jürgen Schmidhuber argues that consciousness is the result of compression. As an agent sees representation of itself recurring in the environment, the compression of this representation can be called consciousness.
In a lively exchange over what has come to be referred to as "the Chinese room argument", John Searle sought to refute the claim of proponents of what he calls "strong artificial intelligence (AI)" that a computer program can be conscious, though he does agree with advocates of "weak AI" that computer programs can be formatted to "simulate" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room with one monolingual English speaker (Searle himself, in fact), a book that designates a combination of Chinese symbols to be output paired with Chinese symbol input, and boxes filled with Chinese symbols. In this case, the English speaker is acting as a computer and the rulebook as a program. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be effectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped.
In the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was "causal powers" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.
In 2014, Victor Argonov has suggested a non-Turing test for machine consciousness based on a machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that a machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.
Nick Bostrom has argued in 2023 that, being very sure that large language models (LLMs) are not conscious, would require unwarranted confidence; in which consciousness theory is correct and how it applies to machines. He views consciousness as a matter of degree, and argued that machines could in theory be much more conscious than humans. David Chalmers addressed the question of whether large language models could be conscious, arguing that current systems provide at most weak evidence for consciousness. Chalmers notes that while LLMs exhibit impressive linguistic competence, their lack of unified agency, persistent goals, and integrated world-models counts against attributing consciousness under many leading theories. At the same time, he maintains that consciousness in machines cannot be ruled out in principle, and that more advanced systems with richer forms of integration, perception, and self-modeling might warrant serious consideration. Related philosophical work by Kristina Sekrst emphasizes the risk of conflating increasingly fluent linguistic behavior with evidence of consciousness or moral status, arguing that fluent linguistic output can function as a form of hallucinated mentality that is, from the outside, indistinguishable from conscious experience without thereby constituting evidence of inner phenomenal states. In a 2025 paper, neuroscientist and philosopher Anil Seth argues that while it is natural to ask whether AI systems could be conscious, especially large language models, current approaches that treat computation alone as a sufficient basis for consciousness are unlikely to succeed, and instead suggests that consciousness depends on organism-like biological processes, making true artificial consciousness unlikely on current trajectories but potentially more plausible in systems that are brain-like or life-like.
William James is usually credited with popularizing the idea that human consciousness flows like a stream, in his Principles of Psychology of 1890.
According to James, the "stream of thought" is governed by five characteristics:
Every thought tends to be part of a personal consciousness.
Within each personal consciousness thought is always changing.
Within each personal consciousness thought is sensibly continuous.
It always appears to deal with objects independent of itself.
It is interested in some parts of these objects to the exclusion of others.
A similar concept appears in Buddhist philosophy, expressed by the Sanskrit term Citta-saṃtāna, which is usually translated as mindstream or "mental continuum". Buddhist teachings describe that consciousness manifests moment to moment as sense impressions and mental phenomena that are continuously changing. The teachings list six triggers that can result in the generation of different mental events. These triggers are input from the five senses (seeing, hearing, smelling, tasting or touch sensations), or a thought (relating to the past, present or the future) that happen to arise in the mind. The mental events generated as a result of these triggers are: feelings, perceptions and intentions/behavior. The moment-by-moment manifestation of the mind-stream is said to happen in every person all the time. It even happens in a scientist who analyzes various phenomena in the world, or analyzes the material body including the organ brain. The manifestation of the mindstream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws. The purpose of the Buddhist practice of mindfulness is to understand the inherent nature of the consciousness and its characteristics.
=
In the West, the primary impact of the idea has been on literature rather than science: "stream of consciousness as a narrative mode" means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.
Here, for example, is a passage from Joyce's Ulysses about the thoughts of Molly Bloom:
Yes because he never did a thing like that before as ask to get his breakfast in bed with a couple of eggs since the City Arms hotel when he used to be pretending to be laid up with a sick voice doing his highness to make himself interesting for that old faggot Mrs Riordan that he thought he had a great leg of and she never left us a farthing all for masses for herself and her soul greatest miser ever was actually afraid to lay out 4d for her methylated spirit telling me all her ailments she had too much old chat in her about politics and earthquakes and the end of the world let us have a bit of fun first God help the world if all the women were her sort down on bathingsuits and lownecks of course nobody wanted her to wear them I suppose she was pious because no man would look at her twice I hope Ill never be like her a wonder she didnt want us to cover our faces but she was a well-educated woman certainly and her gabby talk about Mr Riordan here and Mr Riordan there I suppose he was glad to get shut of her.
The Upanishads hold the oldest recorded map of consciousness, as explored by sages through meditation.
The Canadian psychiatrist Richard Maurice Bucke, author of the 1901 book Cosmic Consciousness: A Study in the Evolution of the Human Mind, distinguished between three types of consciousness: 'Simple Consciousness', awareness of the body, possessed by many animals; 'Self Consciousness', awareness of being aware, possessed only by humans; and 'Cosmic Consciousness', awareness of the life and order of the universe, possessed only by humans who have attained "intellectual enlightenment or illumination".
Another thorough account of the spiritual approach is Ken Wilber's 1977 book The Spectrum of Consciousness, a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.
Other examples include the various levels of spiritual consciousness presented by Prem Saran Satsangi and Stuart Hameroff.
=
Lewis, Ralph. An Overview of the Leading Theories of Consciousness.Organizing and comparing the major candidate theories in the field. Psychology Today, November 25, 2023.
 Media related to Consciousness at Wikimedia Commons
Library resources in your library and in other libraries about Consciousness
 Consciousness Studies at Wikibooks
 Quotations related to Consciousness at Wikiquote
 The dictionary definition of Consciousness at Wiktionary

--- TOPIC: Language ---
Language is a structured system of communication that consists of grammar and vocabulary. It is the primary means by which humans convey meaning, both in spoken and signed forms, and may also be conveyed through writing. Human language is characterized by its cultural and historical diversity, with significant variations observed between cultures and across time. Human languages possess the properties of productivity and displacement, which enable the creation of an infinite number of sentences, and the ability to refer to objects, events, and ideas that are not immediately present in the discourse. The use of human language relies on social convention and is acquired through learning.
Estimates of the number of human languages in the world vary between 5,000 and 7,000. Precise estimates depend on an arbitrary distinction (dichotomy) established between languages and dialects. Natural languages are spoken, signed, or both; however, any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, writing, whistling, signing, or braille. In other words, human language is modality-independent, but written or signed language is the way to inscribe or encode the natural human speech or gestures.
Depending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, "language" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.
The scientific study of language is called linguistics. Critical examinations of languages, such as philosophy of language, the relationships between language and thought, how words represent experience, etc., have been debated at least since Gorgias and Plato in ancient Greek civilization. Thinkers such as Jean-Jacques Rousseau (1712–1778) have argued that language originated from emotions, while others like Immanuel Kant (1724–1804) have argued that languages originated from rational and logical thought. Twentieth century philosophers such as Ludwig Wittgenstein (1889–1951) argued that philosophy is really the study of language itself. Major figures in contemporary linguistics include Ferdinand de Saussure and Noam Chomsky.
Language is thought to have gradually diverged from earlier primate communication systems when early hominins acquired the ability to form a theory of mind and shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca's and Wernicke's areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. Language and culture are codependent. Therefore, in addition to its strictly communicative uses, language has social uses such as signifying group identity, social stratification, as well as use for social grooming and entertainment.
Languages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family; in contrast, a language that has been demonstrated not to have any living or non-living relationship with another language is called a language isolate. There are also many unclassified languages whose relationships have not been established, and spurious languages may have not existed at all. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100.
The English word language derives ultimately from Proto-Indo-European *dn̥ǵʰwéh₂s "tongue, speech, language" through Latin lingua, "language; tongue", and Old French language. The word is sometimes used to refer to codes, ciphers, and other kinds of artificially constructed communication systems such as formally defined computer languages used for computer programming. Unlike conventional human languages, a formal language in this sense is a system of signs for encoding and decoding information. This article specifically concerns the properties of natural human language as it is studied in the discipline of linguistics.
As an object of linguistic study, "language" has two primary meanings: an abstract concept, and a specific linguistic system, e.g. "French". The Swiss linguist Ferdinand de Saussure, who defined the modern discipline of linguistics, first explicitly formulated the distinction using the French word langage for language as a concept, langue as a specific instance of a language system, and parole for the concrete use of speech in a particular language.
When speaking of language as a general concept, definitions can be used which stress different aspects of the phenomenon. These definitions also entail different approaches and understandings of language, and they also inform different and often incompatible schools of linguistic theory. Debates about the nature and origin of language go back to the ancient world. Greek philosophers such as Gorgias and Plato debated the relation between words, concepts and reality. Gorgias argued that language could represent neither the objective experience nor human experience, and that communication and truth were therefore impossible. Plato maintained that communication is possible because language represents ideas and concepts that exist independently of, and prior to, language.
During the Enlightenment and its debates about human origins, it became fashionable to speculate about the origin of language. Thinkers such as Rousseau and Johann Gottfried Herder argued that language had originated in the instinctive expression of emotions, and that it was originally closer to music and poetry than to the logical expression of rational thought. Rationalist philosophers such as Kant and René Descartes held the opposite view. Around the turn of the 20th century, thinkers began to wonder about the role of language in shaping our experiences of the world – asking whether language simply reflects the objective structure of the world, or whether it creates concepts that in turn impose structure on our experience of the objective world. This led to the question of whether philosophical problems are really firstly linguistic problems. The resurgence of the view that language plays a significant role in the creation and circulation of concepts, and that the study of philosophy is essentially the study of language, is associated with what has been called the linguistic turn and philosophers such as Wittgenstein in 20th-century philosophy. These debates about language in relation to meaning and reference, cognition and consciousness remain active today.
=
One definition sees language primarily as the mental faculty that allows humans to undertake linguistic behaviour: to learn languages and to produce and understand utterances. This definition stresses the universality of language to all humans, and it emphasizes the biological basis for the human capacity for language as a unique development of the human brain. Proponents of the view that the drive to language acquisition is innate in humans argue that this is supported by the fact that all cognitively normal children raised in an environment where language is accessible will acquire language without formal instruction. Languages may even develop spontaneously in environments where people live or grow up together without a common language; for example, creole languages and spontaneously developed sign languages such as Nicaraguan Sign Language. This view, which can be traced back to the philosophers Kant and Descartes, understands language to be largely innate, for example, in Chomsky's theory of universal grammar, or American philosopher Jerry Fodor's extreme innatist theory. These kinds of definitions are often applied in studies of language within a cognitive science framework and in neurolinguistics.
=
Another definition sees language as a formal system of signs governed by grammatical rules of combination to communicate meaning. This definition stresses that human languages can be described as closed structural systems consisting of rules that relate particular signs to particular meanings. This structuralist view of language was first introduced by Ferdinand de Saussure, and his structuralism remains foundational for many approaches to language.
Some proponents of Saussure's view of language have advocated a formal approach that studies language structure by identifying its basic elements and then by presenting a formal account of the rules according to which the elements combine in order to form words and sentences. The main proponent of such a theory is Noam Chomsky, the originator of the generative theory of grammar, who has defined language as the construction of sentences that can be generated using transformational grammars. Chomsky considers these rules to be an innate feature of the human mind and to constitute the rudiments of what language is. By way of contrast, such transformational grammars are also commonly used in formal logic, in formal linguistics, and in applied computational linguistics. In the philosophy of language, the view of linguistic meaning as residing in the logical relations between propositions and reality was developed by philosophers such as Alfred Tarski, Bertrand Russell, and other formal logicians.
=
Yet another definition sees language as a system of communication that enables humans to exchange verbal or symbolic utterances. This definition stresses the social functions of language and the fact that humans use it to express themselves and to manipulate objects in their environment. Functional theories of grammar explain grammatical structures by their communicative functions, and understand the grammatical structures of language to be the result of an adaptive process by which grammar was "tailored" to serve the communicative needs of its users.
This view of language is associated with the study of language in pragmatic, cognitive, and interactive frameworks, as well as in sociolinguistics and linguistic anthropology. Functionalist theories tend to study grammar as dynamic phenomena, as structures that are always in the process of changing as they are employed by their speakers. This view places importance on the study of linguistic typology, or the classification of languages according to structural features, as processes of grammaticalization tend to follow trajectories that are partly dependent on typology. In the philosophy of language, the view of pragmatics as being central to language and meaning is often associated with Wittgenstein's later works and with ordinary language philosophers such as J. L. Austin, Paul Grice, John Searle, and W.O. Quine.
=
A number of features, many of which were described by Charles Hockett and called design features set human language apart from communication used by non-human animals.
Communication systems used by other animals such as bees or apes are closed systems that consist of a finite, usually very limited, number of possible ideas that can be expressed. In contrast, human language is open-ended and productive, meaning that it allows humans to produce a vast range of utterances from a finite set of elements, and to create new words and sentences. This is possible because human language is based on a dual code, in which a finite number of elements which are meaningless in themselves (e.g. sounds, letters or gestures) can be combined to form an infinite number of larger units of meaning (words and sentences). However, one study has demonstrated that an Australian bird, the chestnut-crowned babbler, is capable of using the same acoustic elements in different arrangements to create two functionally distinct vocalizations. Additionally, pied babblers have demonstrated the ability to generate two functionally distinct vocalisations composed of the same sound type, which can only be distinguished by the number of repeated elements.
Several species of animals have proved to be able to acquire forms of communication through social learning: for instance a bonobo named Kanzi learned to express itself using a set of symbolic lexigrams. Similarly, many species of birds and whales learn their songs by imitating other members of their species. However, while some animals may acquire large numbers of words and symbols, none have been able to learn as many different signs as are generally known by an average 4 year old human, nor have any acquired anything resembling the complex grammar of human language.
Human languages differ from animal communication systems in that they employ grammatical and semantic categories, such as noun and verb, present and past, which may be used to express exceedingly complex meanings. It is distinguished by the property of recursivity: for example, a noun phrase can contain another noun phrase (as in "[[the chimpanzee]'s lips]") or a clause can contain another clause (as in "[I see [the dog is running]]"). Human language is the only known natural communication system whose adaptability may be referred to as modality independent. This means that it can be used not only for communication through one channel or medium, but through several. For example, spoken language uses the auditive modality, whereas sign languages and writing use the visual modality, and braille writing uses the tactile modality.
Human language is unusual in being able to refer to abstract concepts and to imagined or hypothetical events as well as events that took place in the past or may happen in the future. This ability to refer to events that are not at the same time or place as the speech event is called displacement, and while some animal communication systems can use displacement (such as the communication of bees that can communicate the location of sources of nectar that are out of sight), the degree to which it is used in human language is also considered unique.
Theories about the origin of language differ in regard to their basic assumptions about what language is. Some theories are based on the idea that language is so complex that one cannot imagine it simply appearing from nothing in its final form, but that it must have evolved from earlier pre-linguistic systems among our pre-human ancestors. These theories can be called continuity-based theories. The opposite viewpoint is that language is such a unique human trait that it cannot be compared to anything found among non-humans and that it must therefore have appeared suddenly in the transition from pre-hominids to early man. These theories can be defined as discontinuity-based. Similarly, theories based on the generative view of language pioneered by Noam Chomsky see language mostly as an innate faculty that is largely genetically encoded, whereas functionalist theories see it as a system that is largely cultural, learned through social interaction.
Continuity-based theories are held by a majority of scholars, but they vary in how they envision this development. Those who see language as being mostly innate, such as psychologist Steven Pinker, hold the precedents to be animal cognition, whereas those who see language as a socially learned tool of communication, such as psychologist Michael Tomasello, see it as having developed from animal communication in primates: either gestural or vocal communication to assist in cooperation. Other continuity-based models see language as having developed from music, a view already espoused by Rousseau, Herder, Humboldt, and Charles Darwin. A prominent proponent of this view is archaeologist Steven Mithen. Stephen Anderson states that the age of spoken languages is estimated at 60,000 to 100,000 years and that: Researchers on the evolutionary origin of language generally find it plausible to suggest that language was invented only once, and that all modern spoken languages are thus in some way related, even if that relation can no longer be recovered ... because of limitations on the methods available for reconstruction.
Because language emerged in the early prehistory of man, before the existence of any written records, its early development has left no historical traces, and it is believed that no comparable processes can be observed today. Theories that stress continuity often look at animals to see if, for example, primates display any traits that can be seen as analogous to what pre-human language must have been like. Early human fossils can be inspected for traces of physical adaptation to language use or pre-linguistic forms of symbolic behaviour. Among the signs in human fossils that may suggest linguistic abilities are: the size of the brain relative to body mass, the presence of a larynx capable of advanced sound production and the nature of tools and other manufactured artifacts.
It was mostly undisputed that pre-human australopithecines did not have communication systems significantly different from those found in great apes in general. However, a 2017 study on Ardipithecus ramidus challenges this belief. Scholarly opinions vary as to the developments since the appearance of the genus Homo some 2.5 million years ago. Some scholars assume the development of primitive language-like systems (proto-language) as early as Homo habilis (2.3 million years ago) while others place the development of primitive symbolic communication only with Homo erectus (1.8 million years ago) or Homo heidelbergensis (0.6 million years ago), and the development of language proper with anatomically modern Homo sapiens with the Upper Paleolithic revolution less than 100,000 years ago.
Chomsky is one prominent proponent of a discontinuity-based theory of human language origins. He suggests that for scholars interested in the nature of language, "talk about the evolution of the language capacity is beside the point." Chomsky proposes that perhaps "some random mutation took place [...] and it reorganized the brain, implanting a language organ in an otherwise primate brain." Though cautioning against taking this story literally, Chomsky insists that "it may be closer to reality than many other fairy tales that are told about evolutionary processes, including language."
In March 2024, researchers reported that the beginnings of human language began about 1.6 million years ago.
The study of language, linguistics, has been developing into a science since the first grammatical descriptions of particular languages in India more than 2000 years ago, after the development of the Brahmi script. Modern linguistics is a science that concerns itself with all aspects of language, examining it from all of the theoretical viewpoints described above.
=
The academic study of language is conducted within many different disciplinary areas and from different theoretical angles, all of which inform modern approaches to linguistics. For example, descriptive linguistics examines the grammar of single languages, theoretical linguistics develops theories on how best to conceptualize and define the nature of language based on data from the various extant human languages, sociolinguistics studies how languages are used for social purposes informing in turn the study of the social functions of language and grammatical description, neurolinguistics studies how language is processed in the human brain and allows the experimental testing of theories, computational linguistics builds on theoretical and descriptive linguistics to construct computational models of language often aimed at processing natural language or at testing linguistic hypotheses, and historical linguistics relies on grammatical and lexical descriptions of languages to trace their individual histories and reconstruct trees of language families by using the comparative method.
=
The formal study of language is often considered to have started in India with Pāṇini, the 5th century BC grammarian who formulated 3,959 rules of Sanskrit morphology. However, Sumerian scribes already studied the differences between Sumerian and Akkadian grammar around 1900 BC. Subsequent grammatical traditions developed in all of the ancient cultures that adopted writing.
In the 17th century AD, the French Port-Royal Grammarians developed the idea that the grammars of all languages were a reflection of the universal basics of thought, and therefore that grammar was universal. In the 18th century, the first use of the comparative method by British philologist and expert on ancient India William Jones sparked the rise of comparative linguistics. The scientific study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt. Early in the 20th century, Ferdinand de Saussure introduced the idea of language as a static system of interconnected units, defined through the oppositions between them.
By introducing a distinction between diachronic and synchronic analyses of language, he laid the foundation of the modern discipline of linguistics. Saussure also introduced several basic dimensions of linguistic analysis that are still fundamental in many contemporary linguistic theories, such as the distinctions between syntagm and paradigm, and the Langue-parole distinction, distinguishing language as an abstract system (langue), from language as a concrete manifestation of this system (parole).
=
In the 1960s, Noam Chomsky formulated the generative theory of language. According to this theory, the most basic form of language is a set of syntactic rules that is universal for all humans and which underlies the grammars of all human languages. This set of rules is called Universal Grammar; for Chomsky, describing it is the primary objective of the discipline of linguistics. Thus, he considered that the grammars of individual languages are only of importance to linguistics insofar as they allow us to deduce the universal underlying rules from which the observable linguistic variability is generated.
In opposition to the formal theories of the generative school, functional theories of language propose that since language is fundamentally a tool, its structures are best analyzed and understood by reference to their functions. Formal theories of grammar seek to define the different elements of language and describe the way they relate to each other as systems of formal rules or operations, while functional theories seek to define the functions performed by language and then relate them to the linguistic elements that carry them out. The framework of cognitive linguistics interprets language in terms of the concepts (which are sometimes universal, and sometimes specific to a particular language) which underlie its forms. Cognitive linguistics is primarily concerned with how the mind creates meaning through language.
Speaking is the default modality for language in all cultures with hearing members. The production of spoken language depends on sophisticated capacities for controlling the lips, tongue and other components of the vocal apparatus, the ability to acoustically decode speech sounds, and the neurological apparatus required for acquiring and producing language. The study of the genetic bases for human language is at an early stage: the only gene that has definitely been implicated in language production is FOXP2, which may cause a kind of congenital language disorder if affected by mutations.
=
The brain is the coordinating center of all linguistic activity; it controls both the production of linguistic cognition and of meaning and the mechanics of speech production. Nonetheless, our knowledge of the neurological bases for language is quite limited, though it has advanced considerably with the use of modern imaging techniques. The discipline of linguistics dedicated to studying the neurological aspects of language is called neurolinguistics.
Early work in neurolinguistics involved the study of language in people with brain lesions, to see how lesions in specific areas affect language and speech. In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with word repetition. The condition affects both spoken and written language. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production.
With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments.
=
Spoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose. By controlling the different parts of the speech apparatus, the airstream can be manipulated to produce different speech sounds.
The sound of speech can be analyzed into a combination of segmental and suprasegmental elements. The segmental elements are those that follow each other in sequences, which are usually represented by distinct letters in alphabetic scripts, such as the Roman script. In free flowing speech, there are no clear boundaries between one segment and the next, nor usually are there any audible pauses between them. Segments therefore are distinguished by their distinct sounds which are a result of their different articulations, and can be either vowels or consonants. Suprasegmental phenomena encompass such elements as stress, phonation type, voice timbre, and prosody or intonation, all of which may have effects across multiple segments.
Consonants and vowel segments combine to form syllables, which in turn combine to form utterances; these can be distinguished phonetically as the space between two inhalations. Acoustically, these different segments are characterized by different formant structures, that are visible in a spectrogram of the recorded sound wave. Formants are the amplitude peaks in the frequency spectrum of a specific sound.
Vowels are those sounds that have no audible friction caused by the narrowing or obstruction of some part of the upper vocal tract. They vary in quality according to the degree of lip aperture and the placement of the tongue within the oral cavity. Vowels are called close when the lips are relatively closed, as in the pronunciation of the vowel [i] (English "ee"), or open when the lips are relatively open, as in the vowel [a] (English "ah"). If the tongue is located towards the back of the mouth, the quality changes, creating vowels such as [u] (English "oo"). The quality also changes depending on whether the lips are rounded as opposed to unrounded, creating distinctions such as that between [i] (unrounded front vowel such as English "ee") and [y] (rounded front vowel such as German "ü").
Consonants are those sounds that have audible friction or closure at some point within the upper vocal tract. Consonant sounds vary by place of articulation, i.e. the place in the vocal tract where the airflow is obstructed, commonly at the lips, teeth, alveolar ridge, palate, velum, uvula, or glottis. Each place of articulation produces a different set of consonant sounds, which are further distinguished by manner of articulation, or the kind of friction, whether full closure, in which case the consonant is called occlusive or stop, or different degrees of aperture creating fricatives and approximants. Consonants can also be either voiced or unvoiced, depending on whether the vocal cords are set in vibration by airflow during the production of the sound. Voicing is what separates English [s] in bus (unvoiced sibilant) from [z] in buzz (voiced sibilant).
Some speech sounds, both vowels and consonants, involve release of air flow through the nasal cavity, and these are called nasals or nasalized sounds. Other sounds are defined by the way the tongue moves within the mouth such as the l-sounds (called laterals, because the air flows along both sides of the tongue), and the r-sounds (called rhotics).
By using these speech organs, humans can produce hundreds of distinct sounds: some appear very often in the world's languages, whereas others are much more common in certain language families, language areas, or even specific to a single language.
Human languages display considerable plasticity in their deployment of two fundamental modes: oral (speech and mouthing) and manual (sign and gesture). For example, it is common for oral language to be accompanied by gesture, and for sign language to be accompanied by mouthing. In addition, some language communities use both modes to convey lexical or grammatical meaning, each mode complementing the other. Such bimodal use of language is especially common in genres such as story-telling (with Plains Indian Sign Language and Australian Aboriginal sign languages used alongside oral language, for example), but also occurs in mundane conversation. For instance, many Australian languages have a rich set of case suffixes that provide details about the instrument used to perform an action. Others lack such grammatical precision in the oral mode, but supplement it with gesture to convey that information in the sign mode. In Iwaidja, for example, 'he went out for fish using a torch' is spoken as simply "he-hunted fish torch", but the word for 'torch' is accompanied by a gesture indicating that it was held. In another example, the ritual language Damin had a heavily reduced oral vocabulary of only a few hundred words, each of which was very general in meaning, but which were supplemented by gesture for greater precision (e.g., the single word for fish, l*i, was accompanied by a gesture to indicate the kind of fish).
Secondary modes of language, by which a fundamental mode is conveyed in a different medium, include writing (including braille), sign (in manually coded language), whistling and drumming. Tertiary modes – such as semaphore, Morse code and spelling alphabets – convey the secondary mode of writing in a different medium. For some extinct languages that are maintained for ritual or liturgical purposes, writing may be the primary mode, with speech secondary.
When described as a system of symbolic communication, language is traditionally seen as consisting of three parts: signs, meanings, and a code connecting signs with their meanings. The study of the process of semiosis, how signs and meanings are combined, used, and interpreted is called semiotics. Signs can be composed of sounds, gestures, letters, or symbols, depending on whether the language is spoken, signed, or written, and they can be combined into complex signs, such as words and phrases. When used in communication, a sign is encoded and transmitted by a sender through a channel to a receiver who decodes it.
Some of the properties that define human language as opposed to other communication systems are: the arbitrariness of the linguistic sign, meaning that there is no predictable connection between a linguistic sign and its meaning; the duality of the linguistic system, meaning that linguistic structures are built by combining elements into larger structures that can be seen as layered, e.g. how sounds build words and words build phrases; the discreteness of the elements of language, meaning that the elements out of which linguistic signs are constructed are discrete units, e.g. sounds and words, that can be distinguished from each other and rearranged in different patterns; and the productivity of the linguistic system, meaning that the finite number of linguistic elements can be combined into a theoretically infinite number of combinations.
The rules by which signs can be combined to form words and phrases are called syntax or grammar. The meaning that is connected to individual signs, morphemes, words, phrases, and texts is called semantics. The division of language into separate but connected systems of sign and meaning goes back to the first linguistic studies of de Saussure and is now used in almost all branches of linguistics.
=
Languages express meaning by relating a sign form to a meaning, or its content. Sign forms must be something that can be perceived, for example, in sounds, images, or gestures, and then related to a specific meaning by social convention. Because the basic relation of meaning for most linguistic signs is based on social convention, linguistic signs can be considered arbitrary, in the sense that the convention is established socially and historically, rather than by means of a natural relation between a specific sign form and its meaning.
Thus, languages must have a vocabulary of signs related to specific meaning. The English sign "dog" denotes, for example, a member of the species Canis familiaris. In a language, the array of arbitrary signs connected to specific meanings is called the lexicon, and a single sign connected to a meaning is called a lexeme. Not all meanings in a language are represented by single words. Often, semantic concepts are embedded in the morphology or syntax of the language in the form of grammatical categories.
All languages contain the semantic structure of predication: a structure that predicates a property, state, or action. Traditionally, semantics has been understood to be the study of how speakers and interpreters assign truth values to statements, so that meaning is understood to be the process by which a predicate can be said to be true or false about an entity, e.g. "[x [is y]]" or "[x [does y]]". Recently, this model of semantics has been complemented with more dynamic models of meaning that incorporate shared knowledge about the context in which a sign is interpreted into the production of meaning. Such models of meaning are explored in the field of pragmatics.
=
Depending on modality, language structure can be based on systems of sounds (speech), gestures (sign languages), or graphic or tactile symbols (writing). The ways in which languages use sounds or signs to construct meaning are studied in phonology.
Sounds as part of a linguistic system are called phonemes. Phonemes are abstract units of sound, defined as the smallest units in a language that can serve to distinguish between the meaning of a pair of minimally different words, a so-called minimal pair. In English, for example, the words bat [bæt] and pat [pʰæt] form a minimal pair, in which the distinction between /b/ and /p/ differentiates the two words, which have different meanings. However, each language contrasts sounds in different ways. For example, in a language that does not distinguish between voiced and unvoiced consonants, the sounds [p] and [b] (if they both occur) could be considered a single phoneme, and consequently, the two pronunciations would have the same meaning. Similarly, the English language does not distinguish phonemically between aspirated and non-aspirated pronunciations of consonants, as many other languages like Korean and Hindi do: the unaspirated /p/ in spin [spɪn] and the aspirated /p/ in pin [pʰɪn] are considered to be merely different ways of pronouncing the same phoneme (such variants of a single phoneme are called allophones), whereas in Mandarin Chinese, the same difference in pronunciation distinguishes between the words [pʰá] 'crouch' and [pá] 'eight' (the accent above the á means that the vowel is pronounced with a high tone).
All spoken languages have phonemes of at least two different categories, vowels and consonants, that can be combined to form syllables. As well as segments such as consonants and vowels, some languages also use sound in other ways to convey meaning. Many languages, for example, use stress, pitch, duration, and tone to distinguish meaning. Because these phenomena operate outside of the level of single segments, they are called suprasegmental. Some languages have only a few phonemes, for example, Rotokas and Pirahã language with 11 and 10 phonemes respectively, whereas languages like Taa may have as many as 141 phonemes. In sign languages, the equivalent to phonemes (formerly called cheremes) are defined by the basic elements of gestures, such as hand shape, orientation, location, and motion, which correspond to manners of articulation in spoken language.
Writing systems represent language using visual symbols, which may or may not correspond to the sounds of spoken language. The Latin alphabet (and those on which it is based or that have been derived from it) was originally based on the representation of single sounds, so that words were constructed from letters that generally denote a single consonant or vowel in the structure of the word. In syllabic scripts, such as the Inuktitut syllabary, each sign represents a whole syllable. In logographic scripts, each sign represents an entire word, and will generally bear no relation to the sound of that word in spoken language.
Because all languages have a very large number of words, no purely logographic scripts are known to exist. Written language represents the way spoken sounds and words follow one after another by arranging symbols according to a pattern that follows a certain direction. The direction used in a writing system is entirely arbitrary and established by convention. Some writing systems use the horizontal axis (left to right as the Latin script or right to left as the Arabic script), while others such as traditional Chinese writing use the vertical dimension (from top to bottom). A few writing systems use opposite directions for alternating lines, and others, such as the ancient Maya script, can be written in either direction and rely on graphic cues to show the reader the direction of reading.
In order to represent the sounds of the world's languages in writing, linguists have developed the International Phonetic Alphabet, designed to represent all of the discrete sounds that are known to contribute to meaning in human languages.
=
Grammar is the study of how meaningful elements called morphemes within a language can be combined into utterances. Morphemes can either be free or bound. If they are free to be moved around within an utterance, they are usually called words, and if they are bound to other words or morphemes, they are called affixes. The way in which meaningful elements can be combined within a language is governed by rules. The study of the rules for the internal structure of words are called morphology. The rules of the internal structure of phrases and sentences are called syntax.
 Grammatical categories 
Grammar can be described as a system of categories and a set of rules that determine how categories combine to form different aspects of meaning. Languages differ widely in whether they are encoded through the use of categories or lexical units. However, several categories are so common as to be nearly universal. Such universal categories include the encoding of the grammatical relations of participants and predicates by grammatically distinguishing between their relations to a predicate, the encoding of temporal and spatial relations on predicates, and a system of grammatical person governing reference to and distinction between speakers and addressees and those about whom they are speaking.
 Word classes 
Languages organize their parts of speech into classes according to their functions and positions relative to other parts. All languages, for instance, make a basic distinction between a group of words that prototypically denotes things and concepts and a group of words that prototypically denotes actions and events. The first group, which includes English words such as "dog" and "song", are usually called nouns. The second, which includes "think" and "sing", are called verbs. Another common category is the adjective: words that describe properties or qualities of nouns, such as "red" or "big". Word classes can be "open" if new words can continuously be added to the class, or relatively "closed" if there is a fixed number of words in a class. In English, the class of pronouns is closed, whereas the class of adjectives is open, since an infinite number of adjectives can be constructed from verbs (e.g. "saddened") or nouns (e.g. with the -like suffix, as in "noun-like"). In other languages such as Korean, the situation is the opposite, and new pronouns can be constructed, whereas the number of adjectives is fixed.
Word classes also carry out differing functions in grammar. Prototypically, verbs are used to construct predicates, while nouns are used as arguments of predicates. In a sentence such as "Sally runs", the predicate is "runs", because it is the word that predicates a specific state about its argument "Sally". Some verbs such as "curse" can take two arguments, e.g. "Sally cursed John". A predicate that can only take a single argument is called intransitive, while a predicate that can take two arguments is called transitive.
Many other word classes exist in different languages, such as conjunctions like "and" that serve to join two sentences, articles that introduce a noun, interjections such as "wow!", or ideophones like "splash" that mimic the sound of some event. Some languages have positionals that describe the spatial position of an event or entity. Many languages have classifiers that identify countable nouns as belonging to a particular type or having a particular shape. For instance, in Japanese, the general noun classifier for humans is nin (人), and it is used for counting humans, whatever they are called:
san-nin no gakusei (三人の学生) lit. "3 human-classifier of student" – three students
For trees, it would be:
san-bon no ki (三本の木) lit. "3 classifier-for-long-objects of tree" – three trees
 Morphology 
In linguistics, the study of the internal structure of complex words and the processes by which words are formed is called morphology. In most languages, it is possible to construct complex words that are built of several morphemes. For instance, the English word "unexpected" can be analyzed as being composed of the three morphemes "un-", "expect" and "-ed".
Morphemes can be classified according to whether they are independent morphemes, so-called roots, or whether they can only co-occur attached to other morphemes. These bound morphemes or affixes can be classified according to their position in relation to the root: prefixes precede the root, suffixes follow the root, and infixes are inserted in the middle of a root. Affixes serve to modify or elaborate the meaning of the root. Some languages change the meaning of words by changing the phonological structure of a word, for example, the English word "run", which in the past tense is "ran". This process is called ablaut. Furthermore, morphology distinguishes between the process of inflection, which modifies or elaborates on a word, and the process of derivation, which creates a new word from an existing one. In English, the verb "sing" has the inflectional forms "singing" and "sung", which are both verbs, and the derivational form "singer", which is a noun derived from the verb with the agentive suffix "-er".
Languages differ widely in how much they rely on morphological processes of word formation. In some languages, for example, Chinese, there are no morphological processes, and all grammatical information is encoded syntactically by forming strings of single words. This type of morpho-syntax is often called isolating, or analytic, because there is almost a full correspondence between a single word and a single aspect of meaning. Most languages have words consisting of several morphemes, but they vary in the degree to which morphemes are discrete units. In many languages, notably in most Indo-European languages, single morphemes may have several distinct meanings that cannot be analyzed into smaller segments. For example, in Latin, the word bonus, or "good", consists of the root bon-, meaning "good", and the suffix -us, which indicates masculine gender, singular number, and nominative case. These languages are called fusional languages, because several meanings may be fused into a single morpheme. The opposite of fusional languages are agglutinative languages which construct words by stringing morphemes together in chains, but with each morpheme as a discrete semantic unit. An example of such a language is Turkish, where for example, the word evlerinizden, or "from your houses", consists of the morphemes, ev-ler-iniz-den with the meanings house-plural-your-from. The languages that rely on morphology to the greatest extent are traditionally called polysynthetic languages. They may express the equivalent of an entire English sentence in a single word. For example, in Persian the single word نفهمیدمش, nafahmidamesh means I didn't understand it consisting of morphemes na-fahm-id-am-esh with the meanings, "negation.understand.past.I.it". As another example with more complexity, in the Yupik word tuntussuqatarniksatengqiggtuq, which means "He had not yet said again that he was going to hunt reindeer", the word consists of the morphemes tuntu-ssur-qatar-ni-ksaite-ngqiggte-uq with the meanings, "reindeer-hunt-future-say-negation-again-third.person.singular.indicative", and except for the morpheme tuntu ("reindeer") none of the other morphemes can appear in isolation.
Many languages use morphology to cross-reference words within a sentence. This is sometimes called agreement. For example, in many Indo-European languages, adjectives must cross-reference the noun they modify in terms of number, case, and gender, so that the Latin adjective bonus, or "good", is inflected to agree with a noun that is masculine gender, singular number, and nominative case. In many polysynthetic languages, verbs cross-reference their subjects and objects. In these types of languages, a single verb may include information that would require an entire sentence in English. For example, in the Basque phrase ikusi nauzu, or "you saw me", the past tense auxiliary verb n-au-zu (similar to English "do") agrees with both the subject (you) expressed by the n- prefix, and with the object (me) expressed by the – zu suffix. The sentence could be directly transliterated as "see you-did-me"
 Syntax 
Another way in which languages convey meaning is through the order of words within a sentence. The grammatical rules for how to produce new sentences from words that are already known is called syntax. The syntactical rules of a language determine why a sentence in English such as "I love you" is meaningful, but "*love you I" is not. Syntactical rules determine how word order and sentence structure is constrained, and how those constraints contribute to meaning. For example, in English, the two sentences "the slaves were cursing the master" and "the master was cursing the slaves" mean different things, because the role of the grammatical subject is encoded by the noun being in front of the verb, and the role of object is encoded by the noun appearing after the verb. Conversely, in Latin, both Dominus servos vituperabat and Servos vituperabat dominus mean "the master was reprimanding the slaves", because servos, or "slaves", is in the accusative case, showing that they are the grammatical object of the sentence, and dominus, or "master", is in the nominative case, showing that he is the subject.
Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase "John is talking to Lucy" is turned into a question, it becomes "Who is John talking to?", and not "John is talking to who?". The latter example may be used as a way of placing special emphasis on "who", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence "the cat sat on the mat". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun.
The reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, "the cat" is one phrase, and "on the mat" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: "[And] on the mat, the cat sat". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner.
=
Languages can be classified in relation to their grammatical types. Languages that belong to different families nonetheless often have features in common, and these shared features tend to correlate. For example, languages can be classified on the basis of their basic word order, the relative order of the verb, and its constituents in a normal indicative sentence. In English, the basic order is SVO (subject–verb–object): "The snake(S) bit(V) the man(O)", whereas for example, the corresponding sentence in the Australian language Gamilaraay would be d̪uyugu n̪ama d̪ayn yiːy (snake man bit), SOV. Word order type is relevant as a typological parameter, because basic word order type corresponds with other syntactic parameters, such as the relative order of nouns and adjectives, or of the use of prepositions or postpositions. Such correlations are called implicational universals. For example, most (but not all) languages that are of the SOV type have postpositions rather than prepositions, and have adjectives before nouns.
All languages structure sentences into Subject, Verb, and Object, but languages differ in the way they classify the relations between actors and actions. English uses the nominative-accusative word typology: in English transitive clauses, the subjects of both intransitive sentences ("I run") and transitive sentences ("I love you") are treated in the same way, shown here by the nominative pronoun I. Some languages, called ergative, Gamilaraay among them, distinguish instead between Agents and Patients. In ergative languages, the single participant in an intransitive sentence, such as "I run", is treated the same as the patient in a transitive sentence, giving the equivalent of "me run". Only in transitive sentences would the equivalent of the pronoun "I" be used. In this way the semantic roles can map onto the grammatical relations in different ways, grouping an intransitive subject either with Agents (accusative type) or Patients (ergative type) or even making each of the three roles differently, which is called the tripartite type.
The shared features of languages which belong to the same typological class type may have arisen completely independently. Their co-occurrence might be due to universal laws governing the structure of natural languages, "language universals", or they might be the result of languages evolving convergent solutions to the recurring communicative problems that humans use language to solve.
While humans have the ability to learn any language, they only do so if they grow up in an environment in which language exists and is used by others. Language is therefore dependent on communities of speakers in which children learn language from their elders and peers and themselves transmit language to their own children. Languages are used by those who speak them to communicate and to solve a plethora of social tasks. Many aspects of language use can be seen to be adapted specifically to these purposes. Owing to the way in which language is transmitted between generations and within communities, language perpetually changes, diversifying into new languages or converging due to language contact. The process is similar to the process of evolution, where the process of descent with modification leads to the formation of a phylogenetic tree.
However, languages differ from biological organisms in that they readily incorporate elements from other languages through the process of diffusion, as speakers of different languages come into contact. Humans also frequently speak more than one language, acquiring their first language or languages as children, or learning new languages as they grow up. Because of the increased language contact in the globalizing world, many small languages are becoming endangered as their speakers shift to other languages that afford the possibility to participate in larger and more influential speech communities.
=
When studying the way in which words and signs are used, it is often the case that words have different meanings, depending on the social context of use. An important example of this is the process called deixis, which describes the way in which certain words refer to entities through their relation between a specific point in time and space when the word is uttered. Such words are, for example, the word, "I" (which designates the person speaking), "now" (which designates the moment of speaking), and "here" (which designates the position of speaking). Signs also change their meanings over time, as the conventions governing their usage gradually change. The study of how the meaning of linguistic expressions changes depending on context is called pragmatics. Deixis is an important part of the way that we use language to point out entities in the world. Pragmatics is concerned with the ways in which language use is patterned and how these patterns contribute to meaning. For example, in all languages, linguistic expressions can be used not just to transmit information, but to perform actions. Certain actions are made only through language, but nonetheless have tangible effects, e.g. the act of "naming", which creates a new name for some entity, or the act of "pronouncing someone man and wife", which creates a social contract of marriage. These types of acts are called speech acts, although they can also be carried out through writing or hand signing.
The form of linguistic expression often does not correspond to the meaning that it actually has in a social context. For example, if at a dinner table a person asks, "Can you reach the salt?", that is, in fact, not a question about the length of the arms of the one being addressed, but a request to pass the salt across the table. This meaning is implied by the context in which it is spoken; these kinds of effects of meaning are called conversational implicatures. These social rules for which ways of using language are considered appropriate in certain situations and how utterances are to be understood in relation to their context vary between communities, and learning them is a large part of acquiring communicative competence in a language.
=
All healthy, normally developing human beings learn to use language. Children acquire the language or languages used around them: whichever languages they receive sufficient exposure to during childhood. The development is essentially the same for children acquiring sign or oral languages. This learning process is referred to as first-language acquisition, since unlike many other kinds of learning, it requires no direct teaching or specialized study. In The Descent of Man, naturalist Charles Darwin called this process "an instinctive tendency to acquire an art".
First language acquisition proceeds in a fairly regular sequence, though there is a wide degree of variation in the timing of particular stages among normally developing infants. Studies published in 2013 have indicated that unborn fetuses are capable of language acquisition to some degree. From birth, newborns respond more readily to human speech than to other sounds. Around one month of age, babies appear to be able to distinguish between different speech sounds. Around six months of age, a child will begin babbling, producing the speech sounds or handshapes of the languages used around them. Words appear around the age of 12 to 18 months; the average vocabulary of an eighteen-month-old child is around 50 words. A child's first utterances are holophrases (literally "whole-sentences"), utterances that use just one word to communicate some idea. Several months after a child begins producing words, the child will produce two-word utterances, and within a few more months will begin to produce telegraphic speech, or short sentences that are less grammatically complex than adult speech, but that do show regular syntactic structure. From roughly the age of three to five years, a child's ability to speak or sign is refined to the point that it resembles adult language.
Acquisition of second and additional languages can come at any age, through exposure in daily life or courses. Children learning a second language are more likely to achieve native-like fluency than adults, but in general, it is very rare for someone speaking a second language to pass completely for a native speaker. An important difference between first language acquisition and additional language acquisition is that the process of additional language acquisition is influenced by languages that the learner already knows.
=
Languages, understood as the particular set of speech norms of a particular community, are also a part of the larger culture of the community that speaks them. Languages differ not only in pronunciation, vocabulary, and grammar, but also through having different "cultures of speaking." Humans use language as a way of signalling identity with one cultural group as well as difference from others. Even among speakers of one language, several different ways of using the language exist, and each is used to signal affiliation with particular subgroups within a larger culture. Linguists and anthropologists, particularly sociolinguists, ethnolinguists, and linguistic anthropologists have specialized in studying how ways of speaking vary between speech communities.
Linguists use the term "varieties" to refer to the different ways of speaking a language. This term includes geographically or socioculturally defined dialects as well as the jargons or styles of subcultures. Linguistic anthropologists and sociologists of language define communicative style as the ways that language is used and understood within a particular culture.
Because norms for language use are shared by members of a specific group, communicative style also becomes a way of displaying and constructing group identity. Linguistic differences may become salient markers of divisions between social groups, for example, speaking a language with a particular accent may imply membership of an ethnic minority or social class, one's area of origin, or status as a second language speaker. These kinds of differences are not part of the linguistic system, but are an important part of how people use language as a social tool for constructing groups.
However, many languages also have grammatical conventions that signal the social position of the speaker in relation to others through the use of registers that are related to social hierarchies or divisions. In many languages, there are stylistic or even grammatical differences between the ways men and women speak, between age groups, or between social classes, just as some languages employ different words depending on who is listening. For example, in the Australian language Dyirbal, a married man must use a special set of words to refer to everyday items when speaking in the presence of his mother-in-law. Some cultures, for example, have elaborate systems of "social deixis", or systems of signalling social distance through linguistic means. In English, social deixis is shown mostly through distinguishing between addressing some people by first name and others by surname, and in titles such as "Mrs.", "boy", "Doctor", or "Your Honor", but in other languages, such systems may be highly complex and codified in the entire grammar and vocabulary of the language. For instance, in languages of east Asia such as Thai, Burmese, and Javanese, different words are used according to whether a speaker is addressing someone of higher or lower rank than oneself in a ranking system with animals and children ranking the lowest and gods and members of royalty as the highest.
=
Throughout history a number of different ways of representing language in graphic media have been invented. These are called writing systems.
The use of writing has made language even more useful to humans. It makes it possible to store large amounts of information outside of the human body and retrieve it again, and it allows communication across physical distances and timespans that would otherwise be impossible. Many languages conventionally employ different genres, styles, and registers in written and spoken language, and in some communities, writing traditionally takes place in an entirely different language than the one spoken. There is some evidence that the use of writing also has effects on the cognitive development of humans, perhaps because acquiring literacy generally requires explicit and formal education.
The invention of the first writing systems is roughly contemporary with the beginning of the Bronze Age in the late 4th millennium BC. The Sumerian archaic cuneiform script and the Egyptian hieroglyphs are generally considered to be the earliest writing systems, both emerging out of their ancestral proto-literate symbol systems from 3400 to 3200 BC with the earliest coherent texts from about 2600 BC. It is generally agreed that Sumerian writing was an independent invention; however, it is debated whether Egyptian writing was developed completely independently of Sumerian, or was a case of cultural diffusion. A similar debate exists for the Chinese script, which developed around 1200 BC. The pre-Columbian Mesoamerican writing systems (including among others Olmec and Maya scripts) are generally believed to have had independent origins.
=
All languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be "decay" or a sign of slipping norms of language usage, it is natural and inevitable.
Changes may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be conditioned in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be regular, which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be sporadic, affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant *bʰ became /b/ in the Germanic languages, the previous *b in turn became /p/, and the previous *p became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have p in words like pater and pisces, whereas Germanic languages, like English, have father and fish.
Another example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation. Another source of sound change is the erosion of words as pronunciation gradually becomes increasingly indistinct and shortens words, leaving out syllables or sounds. This kind of change caused Latin mea domina to eventually become the French madame and American English ma'am.
Change also happens in the grammar of languages as discourse patterns such as idioms or particular constructions become grammaticalized. This frequently happens when words or morphemes erode and the grammatical system is unconsciously rearranged to compensate for the lost element. For example, in some varieties of Caribbean Spanish the final /s/ has eroded away. Since Standard Spanish uses final /s/ in the morpheme marking the second person subject "you" in verbs, the Caribbean varieties now have to express the second person using the pronoun tú. This means that the sentence "what's your name" is ¿como te llamas? [ˈkomo te ˈjamas] in Standard Spanish, but [ˈkomo ˈtu te ˈjama] in Caribbean Spanish. The simple sound change has affected both morphology and syntax. Another common cause of grammatical change is the gradual petrification of idioms into new grammatical forms, for example, the way the English "going to" construction lost its aspect of movement and in some varieties of English has almost become a full-fledged future tense (e.g. I'm gonna).
Language change may be motivated by "language internal" factors, such as changes in pronunciation motivated by certain sounds being difficult to distinguish aurally or to produce, or through patterns of change that cause some rare types of constructions to drift towards more common types. Other causes of language change are social, such as when certain pronunciations become emblematic of membership in certain groups, such as social classes, or with ideologies, and therefore are adopted by those who wish to identify with those groups or ideas. In this way, issues of identity and politics can have profound effects on language structure.
=
One source of language change is contact and the resulting diffusion of linguistic traits between languages. Language contact occurs when speakers of two or more languages or varieties interact on a regular basis. Multilingualism is likely to have been the norm throughout human history and most people in the modern world are multilingual. Before the rise of the concept of the ethno-national state, monolingualism was characteristic mainly of populations inhabiting small islands. But with the ideology that made one people, one state, and one language the most desirable political arrangement, monolingualism started to spread throughout the world. There are only 250 countries in the world corresponding to some 6,000 languages, which means that most countries are multilingual and most languages therefore exist in close contact with other languages.
When speakers of different languages interact closely, it is typical for their languages to influence each other. Through sustained language contact over long periods, linguistic traits diffuse between languages, and languages belonging to different families may converge to become more similar. In areas where many languages are in close contact, this may lead to the formation of language areas in which unrelated languages share a number of linguistic features. A number of such language areas have been documented, among them, the Balkan language area, the Mesoamerican language area, and the Ethiopian language area. Also, larger areas such as South Asia, Europe, and Southeast Asia have sometimes been considered language areas because of the widespread diffusion of specific areal features.
Language contact may also lead to a variety of other linguistic phenomena, including language convergence, borrowing, and relexification (the replacement of much of the native vocabulary with that of another language). In situations of extreme and sustained language contact, it may lead to the formation of new mixed languages that cannot be considered to belong to a single language family. One type of mixed language called pidgins occurs when adult speakers of two different languages interact on a regular basis, but in a situation where neither group learns to speak the language of the other group fluently. In such a case, they will often construct a communication form that has traits of both languages, and that has a simplified grammatical and phonological structure. The language comes to contain mostly the grammatical and phonological categories that exist in both languages. Pidgin languages are defined by not having any native speakers, but only being spoken by people who have another language as their first language. But if the Pidgin language becomes the main language of a speech community, then eventually children will grow up learning the Pidgin language as their first language. As the generation of child learners grows up, the pidgin will often be seen to change its structure and acquire a greater degree of complexity. This type of language is generally called a creole language. An example of such mixed languages is Tok Pisin, the official language of Papua New Guinea, which originally arose as a Pidgin based on English and Austronesian languages; others are Kreyòl ayisyen, the French-based creole language spoken in Haiti, and Michif, a mixed language of Canada, based on the Native American language Cree and French.
SIL Ethnologue defines a "living language" as "one that has at least one speaker for whom it is their first language". The exact number of known living languages varies from 6,000 to 7,000, depending on the precision of one's definition of "language", and in particular, on how one defines the distinction between a "language" and a "dialect". As of 2016, Ethnologue cataloged 7,097 living human languages. The Ethnologue establishes linguistic groups based on studies of mutual intelligibility, and therefore often includes more categories than more conservative classifications. For example, the Danish language that most scholars consider a single language with several dialects is classified as two distinct languages (Danish and Jutish) by the Ethnologue.
According to the Ethnologue, 389 languages (nearly 6%) have more than a million speakers. These languages together account for 94% of the world's population, whereas 94% of the world's languages account for the remaining 6% of the global population.
=
There is no clear distinction between a language and a dialect, notwithstanding a famous aphorism attributed to linguist Max Weinreich that "a language is a dialect with an army and navy". For example, national boundaries frequently override linguistic difference in determining whether two linguistic varieties are languages or dialects. Hakka, Cantonese and Mandarin are, for example, often classified as "dialects" of Chinese, even though they are more different from each other than Swedish is from Norwegian. Before the Yugoslav Wars, Serbo-Croatian was generally considered a single language with two normative variants, but due to sociopolitical reasons, Croatian and Serbian are now often treated as separate languages and employ different writing systems. In other words, the distinction may hinge on political considerations as much as on cultural differences as on distinctive writing systems or the degree of mutual intelligibility. The latter is, in fact, a rather unreliable criterion to discriminate languages and dialects. Pluricentric languages, which are languages with more than one standard variety, are a case in point. Standard American English and Standard RP (English) English, for instance, may in some areas be more different than languages with names, e.g. Swedish and Norwegian. A complex social process of "language making" underlies these assignments of status and in some cases even linguistic experts may not agree (e.g. the One Standard German Axiom). The language making process is dynamic and subject to change over time.
=
The world's languages can be grouped into language families consisting of languages that can be shown to have common ancestry. Linguists recognize many hundreds of language families, although some of them can possibly be grouped into larger units as more evidence becomes available and in-depth studies are carried out. At present, there are also dozens of language isolates: languages that cannot be shown to be related to any other languages in the world. Among them are Basque, spoken in Europe, Zuni of New Mexico, Purépecha of Mexico, Ainu of Japan, Burushaski of Pakistan, and many others.
The language family of the world that has the most speakers is the Indo-European languages, spoken by 46% of the world's population. This family includes major world languages like English, Spanish, French, German, Russian, and Hindustani (Hindi/Urdu). The Indo-European family spread first through hypothesized Indo-European migrations that would have taken place some time in the period c. 8000–1500 BCE, and subsequently through much later European colonial expansion, which brought the Indo-European languages to a politically and often numerically dominant position in the Americas and much of Africa. The Sino-Tibetan languages are spoken by 20% of the world's population and include many of the languages of East Asia, including Hakka, Mandarin Chinese, Cantonese, and hundreds of smaller languages.
Africa is home to a large number of language families, the largest of which is the Niger-Congo language family, which includes such languages as Swahili, Shona, and Yoruba. Speakers of the Niger-Congo languages account for 6.9% of the world's population. A similar number of people speak the Afroasiatic languages, which include the populous Semitic languages such as Arabic, Hebrew language, and the languages of the Sahara region, such as the Berber languages and Hausa.
The Austronesian languages are spoken by 5.5% of the world's population and stretch from Madagascar to maritime Southeast Asia all the way to Oceania. It includes such languages as Malagasy, Māori, Samoan, and many of the indigenous languages of Indonesia and Taiwan. The Austronesian languages are considered to have originated in Taiwan around 3000 BC and spread through the Oceanic region through island-hopping, based on an advanced nautical technology. Other populous language families are the Dravidian languages of South Asia (among them Kannada, Tamil, and Telugu), the Turkic languages of Central Asia (such as Turkish), the Austroasiatic (among them Khmer), and Tai–Kadai languages of Southeast Asia (including Thai).
The areas of the world in which there is the greatest linguistic diversity, such as the Americas, Papua New Guinea, West Africa, and South-Asia, contain hundreds of small language families. These areas together account for the majority of the world's languages, though not the majority of speakers. In the Americas, some of the largest language families include the Quechua, Arawak, and Tupi-Guarani families of South America, the Uto-Aztecan, Oto-Manguean, and Mayan of Mesoamerica, and the Na-Dene, Iroquoian, and Algonquian language families of North America. In Australia, most indigenous languages belong to the Pama-Nyungan family, whereas New Guinea is home to a large number of small families and isolates, as well as a number of Austronesian languages. Due to its remoteness and geographical fragmentation, Papua New Guinea emerges in fact as the leading location worldwide for both species (8% of world total) and linguistic richness – with 830 living tongues (12% of world total).
=
Language endangerment occurs when a language is at risk of falling out of use as its speakers die out or shift to speaking another language. Language loss occurs when the language has no more native speakers, and becomes a dead language. If eventually no one speaks the language at all, it becomes an extinct language. While languages have always gone extinct throughout human history, they have been disappearing at an accelerated rate in the 20th and 21st centuries due to the processes of globalization and neo-colonialism, where the economically powerful languages dominate other languages.
The more commonly spoken languages dominate the less commonly spoken languages, so the less commonly spoken languages eventually disappear from populations. Of the between 6,000 and 7,000 languages spoken as of 2010, between 50 and 90% of those are expected to have become extinct by the year 2100. The top 20 languages, those spoken by more than 50 million speakers each, are spoken by 50% of the world's population, whereas many of the other languages are spoken by smaller communities, most of them with less than 10,000 speakers.
The United Nations Educational, Scientific and Cultural Organization (UNESCO) operates with five levels of language endangerment: "safe", "vulnerable" (not spoken by children outside the home), "definitely endangered" (not spoken by children), "severely endangered" (only spoken by the oldest generations), and "critically endangered" (spoken by a few members of the oldest generation, often semi-speakers). Despite claims that the world would be better off if most adopted a single common lingua franca, such as English or Esperanto, there is a consensus that the loss of languages harms the cultural diversity of the world. It is a common belief, going back to the biblical narrative of the tower of Babel in the Old Testament, that linguistic diversity causes political conflict, but many of the world's major episodes of violence have taken place in situations with low linguistic diversity, such as the Yugoslav and American Civil War, or the genocide of Rwanda.
Many projects aim to prevent or slow this loss by revitalizing endangered languages and promoting education and literacy in minority languages. Across the world, many countries have enacted specific legislation to protect and stabilize the language of indigenous speech communities. A minority of linguists have argued that language loss is a natural process that should not be counteracted and that documenting endangered languages for posterity is sufficient.
The University of Waikato is using the Welsh language as a model for their Māori language revitalisation programme, as they deem Welsh to be the world's leading example for the survival of languages. In 2019, Hawaiian TV company Oiwi visited a Welsh language centre in Nant Gwrtheyrn, North Wales, to help find ways of preserving their Ōlelo Hawaiʻi language.
=
Crystal, David (1997). The Cambridge Encyclopedia of Language. Cambridge: Cambridge University Press.
Cysouw, Michael; Good, Jeff (2013). "Languoid, doculect and glossonym: Formalizing the notion 'language'". Language Documentation and Conservation. 7: 331–359. hdl:10125/4606.
Allison Parshall, "Pain Language: The sound of 'ow' transcends borders", Scientific American, vol. 332, no. 2 (February 2025), pp. 16–18.   "Many languages have an interjection word for expressing pain. [Katarzyna Pisanski et al., writing in the Journal of the Acoustical Society of America, have] found that pain interjections tend to contain the vowel sound 'ah' (written as [a] in the International Phonetic Alphabet) and letter combinations that incorporate it, such as 'ow' and 'ai.' These patterns may point back to the origins of human language itself." (p. 16.) "Researchers are continually discovering cases of symbolism, or sound iconicity, in which a word's intrinsic nature has some connection to its meaning. These cases run counter to decades of linguistic theory, which had regarded language as fundamentally arbitrary... [Many words onomatopoeically imitate a sound. Also] there's the 'bouba-kiki' effect, whereby people from varying cultures are more likely to associate the nonsense word 'bouba' with a rounded shape and 'kiki' with a spiked one.... [S]omehow we all have a feeling about this,' says Aleksandra Ćwiek... [She and her colleagues have] show[n] that people associate the trilled 'R' sound with roughness and the 'L' sound with smoothness. Mark Dingemanse... in 2013 found [that] the conversational 'Huh?' and similar words in other languages may be universal." (p. 18.)
Stix, Gary, "Thinking without Words: Cognition doesn't require language, it turns out" (interview with Evelina Fedorenko, a cognitive neuroscientist at the Massachusetts Institute of Technology), Scientific American, vol. 332, no. 3 (March 2025), pp. 86–88. "[I]n the tradition of linguist Noam Chomsky... we use language for thinking: to think is why language evolved in our species. [However, evidence that thought and language are separate systems is found, for example, by] looking at deficits in different abilities – for instance, in people with brain damage... who have impairments in language – some form of aphasia [ – yet are clearly able to think]." (p. 87.) Conversely, "large language models such as GPT-2... do language very well [but t]hey're not so good at thinking, which... nicely align[s] with the idea that the language system by itself is not what makes you think." (p. 88.)
Swadesh, Morris (1934). "The phonemic principle". Language. 10 (2): 117–129. doi:10.2307/409603. JSTOR 409603.
World Atlas of Language Structures: a large database of structural (phonological, grammatical, lexical) properties of languages
Ethnologue: Languages of the World is a comprehensive catalog of all of the world's known living languages

--- TOPIC: Linguistics ---
Linguistics is the scientific study of language. The areas of linguistic analysis are syntax (rules governing the structure of sentences), semantics (meaning), morphology (structure of words), phonetics (speech sounds and equivalent gestures in sign languages), phonology (the abstract sound system of a particular language, and analogous systems of sign languages), and pragmatics (how the context of use contributes to meaning). Subdisciplines such as biolinguistics (the study of the biological variables and evolution of language) and psycholinguistics (the study of psychological factors in human language) bridge many of these divisions.
Linguistics encompasses many branches and subfields that span both theoretical and practical applications. Theoretical linguistics is concerned with understanding the universal and fundamental nature of language and developing a general theoretical framework for describing it. Applied linguistics seeks to utilize the scientific findings of the study of language for practical purposes, such as developing methods of improving language education and literacy. Mathematical linguistics is the application of mathematics to model phenomena and solve problems in general linguistics and theoretical linguistics. Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. 
Linguistic features may be studied through a variety of perspectives: synchronically (by describing the structure of a language at a specific point in time) or diachronically (through the historical development of a language over a period of time), in monolinguals or in multilinguals, among children or among adults, in terms of how it is being learnt or how it was acquired, as abstract objects or as cognitive structures, through written texts or through oral elicitation, and finally through mechanical data collection or practical fieldwork.
Linguistics emerged from the field of philology, of which some branches are more qualitative and holistic in approach. Today, philology and linguistics are variably described as related fields, subdisciplines, or separate fields of language study, but, by and large, linguistics can be seen as an umbrella term. Linguistics is also related to the philosophy of language, stylistics, rhetoric, semiotics, lexicography, and translation.
=
Historical linguistics is the study of how language changes over history, particularly with regard to a specific language or a group of languages. Western trends in historical linguistics date back to roughly the late 18th century, when the discipline grew out of philology, the study of ancient texts and oral traditions.
Historical linguistics emerged as one of the first few sub-disciplines in the field, and was most widely practised during the late 19th century. Despite a shift in focus in the 20th century towards formalism and generative grammar, which studies the universal properties of language, historical research today still remains a significant field of linguistic inquiry. Subfields of the discipline include language change and grammaticalization.
Historical linguistics studies language change either diachronically (through a comparison of different time periods in the past and present) or in a synchronic manner (by observing developments between different variations that exist within the current linguistic stage of a language).
At first, historical linguistics was the cornerstone of comparative linguistics, which involves a study of the relationship between different languages. At that time, scholars of historical linguistics were only concerned with creating different categories of language families, and reconstructing prehistoric proto-languages by using both the comparative method and the method of internal reconstruction. Internal reconstruction is the method by which an element that contains a certain meaning is re-used in different contexts or environments where there is a variation in either sound or analogy.
The reason for this had been to describe well-known Indo-European languages, many of which had detailed documentation and long written histories. Scholars of historical linguistics also studied Uralic languages, another European language family for which very little written material existed back then. After that, there also followed significant work on the corpora of other languages, such as the Austronesian languages and the Native American language families.
In historical work, the uniformitarian principle is generally the underlying working hypothesis, occasionally also clearly expressed. The principle was expressed early by William Dwight Whitney, who considered it imperative, a "must", of historical linguistics to "look to find the same principle operative also in the very outset of that [language] history."
The above approach of comparativism in linguistics is now, however, only a small part of the much broader discipline called historical linguistics. The comparative study of specific Indo-European languages is considered a highly specialized field today, while comparative research is carried out over the subsequent internal developments in a language: in particular, over the development of modern standard varieties of languages, and over the development of a language from its standardized form to its varieties.
For instance, some scholars also tried to establish super-families, linking, for example, Indo-European, Uralic, and other language families to a hypothetical Nostratic language group. While these attempts are still not widely accepted as credible methods, they provide necessary information to establish relatedness in language change. This is generally hard to find for events long ago, due to the occurrence of chance word resemblances and variations between language groups. A limit of around 10,000 years is often assumed for the functional purpose of conducting research. It is also hard to date various proto-languages. Even though several methods are available, these languages can be dated only approximately.
In modern historical linguistics, we examine how languages change over time, focusing on the relationships between dialects within a specific period. This includes studying morphological, syntactical, and phonetic shifts. Connections between dialects in the past and present are also explored.
=
Syntax is the study of how words and morphemes combine to form larger units such as phrases and sentences. Central concerns of syntax include word order, grammatical relations, constituency, agreement, the nature of crosslinguistic variation, and the relationship between form and meaning. There are numerous approaches to syntax that differ in their central assumptions and goals.
=
Morphology is the study of words, including the principles by which they are formed, and how they relate to one another within a language. Most approaches to morphology investigate the structure of words in terms of morphemes, which are the smallest units in a language with some independent meaning. Morphemes include roots that can exist as words by themselves, but also categories such as affixes that can only appear as part of a larger word. For example, in English the root catch and the suffix -ing are both morphemes; catch may appear as its own word, or it may be combined with -ing to form the new word catching. Morphology also analyzes how words behave as parts of speech, and how they may be inflected to express grammatical categories including number, tense, and aspect. Concepts such as productivity are concerned with how speakers create words in specific contexts, which evolves over the history of a language.
The discipline that deals specifically with the sound changes occurring within morphemes is morphophonology.
=
Semantics and pragmatics are branches of linguistics concerned with meaning. These subfields have traditionally been divided according to aspects of meaning: "semantics" refers to grammatical and lexical meanings, while "pragmatics" is concerned with meaning in context. Within linguistics, the subfield of formal semantics studies the denotations of sentences and how they are composed from the meanings of their constituent expressions. Formal semantics draws heavily on philosophy of language and uses formal tools from logic and computer science. On the other hand, cognitive semantics explains linguistic meaning via aspects of general cognition, drawing on ideas from cognitive science such as prototype theory.
Pragmatics focuses on phenomena such as speech acts, implicature, and talk in interaction. Unlike semantics, which examines meaning that is conventional or "coded" in a given language, pragmatics studies how the transmission of meaning depends not only on the structural and linguistic knowledge (grammar, lexicon, etc.) of the speaker and listener, but also on the context of the utterance, any pre-existing knowledge about those involved, the inferred intent of the speaker, and other factors.
=
Phonetics and phonology are branches of linguistics concerned with sounds (or the equivalent aspects of sign languages). Phonetics is largely concerned with the physical aspects of sounds such as their articulation, acoustics, production, and perception. Phonology is concerned with the linguistic abstractions and categorizations of sounds, and it tells us what sounds are in a language, how they do and can combine into words, and explains why certain phonetic features are important to identifying a word.
=
Linguistic structures are pairings of meaning and form. Any particular pairing of meaning and form is a Saussurean linguistic sign. For instance, the meaning "cat" is represented worldwide with a wide variety of different sound patterns (in oral languages), movements of the hands and face (in sign languages), and written symbols (in written languages). Linguistic patterns have proven their importance for the knowledge engineering field especially with the ever-increasing amount of available data.
Linguists focusing on structure attempt to understand the rules regarding language use that native speakers know (not always consciously). All linguistic structures can be broken down into component parts that are combined according to (sub)conscious rules, over multiple levels of analysis. For instance, consider the structure of the word "tenth" on two different levels of analysis. On the level of internal word structure (known as morphology), the word "tenth" is made up of one linguistic form indicating a number and another form indicating ordinality. The rule governing the combination of these forms ensures that the ordinality marker "th" follows the number "ten." On the level of sound structure (known as phonology), structural analysis shows that the "n" sound in "tenth" is made differently from the "n" sound in "ten" spoken alone. Although most speakers of English are consciously aware of the rules governing internal structure of the word pieces of "tenth", they are less often aware of the rule governing its sound structure. Linguists focused on structure find and analyze rules such as these, which govern how native speakers use language.
=
Grammar is a system of rules which governs the production and use of utterances in a given language. These rules apply to sound as well as meaning, and include componential subsets of rules, such as those pertaining to phonology (the organization of phonetic sound systems), morphology (the formation and composition of words), and syntax (the formation and composition of phrases and sentences). Modern frameworks that deal with the principles of grammar include structural and functional linguistics, and generative linguistics.
Sub-fields that focus on a grammatical study of language include the following:
Phonetics, the study of the physical properties of speech sound production and perception, and delves into their acoustic and articulatory properties
Phonology, the study of sounds as abstract elements in the speaker's mind that distinguish meaning (phonemes)
Morphology, the study of morphemes, or the internal structures of words and how they can be modified
Syntax, the study of how words combine to form grammatical phrases and sentences
Semantics, the study of lexical and grammatical aspects of meaning
Pragmatics, the study of how utterances are used in communicative acts, and the role played by situational context and non-linguistic knowledge in the transmission of meaning
Discourse analysis, the analysis of language use in texts (spoken, written, or signed)
Stylistics, the study of linguistic factors (rhetoric, diction, stress) that place a discourse in context
Semiotics, the study of signs and sign processes (semiosis), indication, designation, likeness, analogy, metaphor, symbolism, signification, and communication
=
Discourse is language as social practice (Baynham, 1995) and is a multilayered concept. As a social practice, discourse embodies different ideologies through written and spoken texts. Discourse analysis can examine or expose these ideologies. Discourse not only influences genre, which is selected based on specific contexts but also, at a micro level, shapes language as text (spoken or written) down to the phonological and lexico-grammatical levels. Grammar and discourse are linked as parts of a system. A particular discourse becomes a language variety when it is used in this way for a particular purpose, and is referred to as a register. There may be certain lexical additions (new words) that are brought into play because of the expertise of the community of people within a certain domain of specialization. Thus, registers and discourses distinguish themselves not only through specialized vocabulary but also, in some cases, through distinct stylistic choices. People in the medical fraternity, for example, may use some medical terminology in their communication that is specialized to the field of medicine. This is often referred to as being part of the "medical discourse", and so on.
=
The lexicon is a catalogue of words and terms that are stored in a speaker's mind. The lexicon consists of words and bound morphemes, which are parts of words that can not stand alone, like affixes. In some analyses, compound words and certain classes of idiomatic expressions and other collocations are also considered to be part of the lexicon. Dictionaries represent attempts at listing, in alphabetical order, the lexicon of a given language; usually, however, bound morphemes are not included. Lexicography, closely linked with the domain of semantics, is the science of mapping the words into an encyclopedia or a dictionary. The creation and addition of new words (into the lexicon) is called coining or neologization, and the new words are called neologisms.
It is often believed that a speaker's capacity for language lies in the quantity of words stored in the lexicon. However, this is often considered a myth by linguists. The capacity for the use of language is considered by many linguists to lie primarily in the domain of grammar, and to be linked with competence, rather than with the growth of vocabulary. Even a very small lexicon is theoretically capable of producing an infinite number of sentences.
Vocabulary size is relevant as a measure of comprehension. There is general consensus that reading comprehension of a written text in English requires 98% coverage, meaning that the person understands 98% of the words in the text. The question of how much vocabulary is needed is therefore related to which texts or conversations need to be understood. A common estimate is 6-7,000 word families to understand a wide range of conversations and 8-9,000 word families to be able to read a wide range of written texts.
=
Stylistics also involves the study of written, signed, or spoken discourse through varying speech communities, genres, and editorial or narrative formats in the mass media. It involves the study and interpretation of texts for aspects of their linguistic and tonal style. Stylistic analysis entails the analysis of description of particular dialects and registers used by speech communities. Stylistic features include rhetoric, diction, stress, satire, irony, dialogue, and other forms of phonetic variations. Stylistic analysis can also include the study of language in canonical works of literature, popular fiction, news, advertisements, and other forms of communication in popular culture as well. It is usually seen as a variation in communication that changes from speaker to speaker and community to community. In short, Stylistics is the interpretation of text.
In the 1960s, Jacques Derrida, for instance, further distinguished between speech and writing, by proposing that written language be studied as a linguistic medium of communication in itself. Palaeography is therefore the discipline that studies the evolution of written scripts (as signs and symbols) in language. The formal study of language also led to the growth of fields like psycholinguistics, which explores the representation and function of language in the mind; neurolinguistics, which studies language processing in the brain; biolinguistics, which studies the biology and evolution of language; and language acquisition, which investigates how children and adults acquire the knowledge of one or more languages.
Modern linguistics is primarily descriptive. Linguists describe and explain features of language without making subjective judgments on whether a particular feature or usage is "good" or "bad". This is analogous to practice in other sciences: a zoologist studies the animal kingdom without making subjective judgments on whether a particular species is "better" or "worse" than another.
Prescription, on the other hand, is an attempt to promote particular linguistic usages over others, often favoring a particular dialect or "acrolect". This may have the aim of establishing a linguistic standard, which can aid communication over large geographical areas. It may also, however, be an attempt by speakers of one language or dialect to exert influence over speakers of other languages or dialects (see Linguistic imperialism). An extreme version of prescriptivism can be found among censors, who attempt to eradicate words and structures that they consider to be destructive to society. Prescription, however, may be practised appropriately in language instruction, like in ELT, where certain fundamental grammatical rules and lexical items need to be introduced to a second-language speaker who is attempting to acquire the language.
=
Most contemporary linguists work under the assumption that spoken data and signed data are more fundamental than written data. This is because
Speech appears to be universal to all human beings capable of producing and perceiving it, while there have been many cultures and speech communities that lack written communication;
Features appear in speech which are not always recorded in writing, including phonological rules, sound changes, and speech errors;
All natural writing systems reflect a spoken language (or potentially a signed one), even with pictographic scripts like Dongba writing Naxi homophones with the same pictogram, and text in writing systems used for two languages changing to fit the spoken language being recorded;
Speech evolved before human beings invented writing;
Individuals learn to speak and process spoken language more easily and earlier than they do with writing.
Nonetheless, linguists agree that the study of written language can be worthwhile and valuable. For research that relies on corpus linguistics and computational linguistics, written language is often much more convenient for processing large amounts of linguistic data. Large corpora of spoken language are difficult to create and hard to find, and are typically transcribed and written. In addition, linguists have turned to text-based discourse occurring in various formats of computer-mediated communication as a viable site for linguistic inquiry.
The study of writing systems themselves, graphemics, is, in any case, considered a branch of linguistics.
=
Before the 20th century, linguists analysed language on a diachronic plane, which was historical in focus. This meant that they would compare linguistic features and try to analyse language from the point of view of how it had changed between then and later. However, with the rise of Saussurean linguistics in the 20th century, the focus shifted to a more synchronic approach, where the study was geared towards analysis and comparison between different language variations, which existed at the same given point of time.
At another level, the syntagmatic plane of linguistic analysis entails the comparison between the way words are sequenced, within the syntax of a sentence. For example, the article "the" is followed by a noun, because of the syntagmatic relation between the words. The paradigmatic plane, on the other hand, focuses on an analysis that is based on the paradigms or concepts that are embedded in a given text. In this case, words of the same type or class may be replaced in the text with each other to achieve the same conceptual understanding.
The earliest activities in the description of language have been attributed to the 6th-century BC Indian grammarian Pāṇini who composed a formal description of the Sanskrit language in his Aṣṭādhyāyī. Today, modern-day theories on grammar employ many of the principles that were laid down then.
=
Before the 20th century, the term philology, first attested in 1716, was commonly used to refer to the study of language, which was then predominantly historical in focus. Since Ferdinand de Saussure's insistence on the importance of synchronic analysis, however, this focus has shifted and the term philology is now generally used for the "study of a language's grammar, history, and literary tradition", especially in the United States (where philology has never been very popularly considered as the "science of language").
Although the term linguist in the sense of "a student of language" dates from 1641, the term linguistics is first attested in 1847. It is now the usual term in English for the scientific study of language, though linguistic science is sometimes used.
Linguistics is a multi-disciplinary field of research that combines tools from natural sciences, social sciences, formal sciences, and the humanities. Many linguists, such as David Crystal, conceptualize the field as being primarily scientific. The term linguist applies to someone who studies language or is a researcher within the field, or to someone who uses the tools of the discipline to describe and analyse specific languages.
=
An early formal study of language was undertaken in India by the 6th-century BC grammarian Pāṇini, who formulated 3,959 rules of Sanskrit morphology. Pāṇini's systematic classification of the sounds of Sanskrit into consonants and vowels, and word classes, such as nouns and verbs, was the first known instance of its kind. In the Middle East, Sibawayh, a Persian, made a detailed description of Arabic in AD 760 in his monumental work, Al-kitab fii an-naħw (الكتاب في النحو, The Book on Grammar), the first known author to distinguish between sounds and phonemes (sounds as units of a linguistic system). Western interest in the study of languages began somewhat later than in the East, but the grammarians of the classical languages did not use the same methods or reach the same conclusions as their contemporaries in the Indic world. Early interest in language in the West was a part of philosophy, not of grammatical description. The first insights into semantic theory were made by Plato in his Cratylus dialogue, where he argues that words denote concepts that are eternal and exist in the world of ideas. This work is the first to use the word etymology to describe the history of a word's meaning. Around 280 BC, one of Alexander the Great's successors founded a university (see Musaeum) in Alexandria, where a school of philologists studied the ancient texts in Greek, and taught Greek to speakers of other languages. While this school was the first to use the word "grammar" in its modern sense, Plato had used the word in its original meaning as "téchnē grammatikḗ" (Τέχνη Γραμματική), the "art of writing", which is also the title of one of the most important works of the Alexandrine school by Dionysius Thrax. Throughout the Middle Ages, the study of language was subsumed under the topic of philology, the study of ancient languages and texts, practised by such educators as Roger Ascham, Wolfgang Ratke, and John Amos Comenius.
=
In the 18th century, the first use of the comparative method by William Jones sparked the rise of comparative linguistics. Bloomfield attributes "the first great scientific linguistic work of the world" to Jacob Grimm, who wrote Deutsche Grammatik. It was soon followed by other authors writing similar comparative studies on other language groups of Europe. The study of language was broadened from Indo-European to language in general by Wilhelm von Humboldt, of whom Bloomfield asserts:
This study received its foundation at the hands of the Prussian statesman and scholar Wilhelm von Humboldt (1767–1835), especially in the first volume of his work on Kavi, the literary language of Java, entitled Über die Verschiedenheit des menschlichen Sprachbaues und ihren Einfluß auf die geistige Entwickelung des Menschengeschlechts (On the Variety of the Structure of Human Language and its Influence upon the Mental Development of the Human Race).
=
There was a shift of focus from historical and comparative linguistics to synchronic analysis in early 20th century. Structural analysis was improved by Leonard Bloomfield, Louis Hjelmslev; and Zellig Harris who also developed methods of discourse analysis. Functional analysis was developed by the Prague linguistic circle and André Martinet. As sound recording devices became commonplace in the 1960s, dialectal recordings were made and archived, and the audio-lingual method provided a technological solution to foreign language learning. The 1960s also saw a new rise of comparative linguistics: the study of language universals in linguistic typology. Towards the end of the century the field of linguistics became divided into further areas of interest with the advent of language technology and digitalized corpora.
=
Sociolinguistics is the study of how language is shaped by social factors. This sub-discipline focuses on the synchronic approach of linguistics, and looks at how a language in general, or a set of languages, display variation and varieties at a given point in time. The study of language variation and the different varieties of language through dialects, registers, and idiolects can be tackled through a study of style, as well as through analysis of discourse. Sociolinguists research both style and discourse in language, as well as the theoretical factors that are at play between language and society.
=
Developmental linguistics is the study of the development of linguistic ability in individuals, particularly the acquisition of language in childhood. Some of the questions that developmental linguistics looks into are how children acquire different languages, how adults can acquire a second language, and what the process of language acquisition is.
=
Neurolinguistics is the study of the structures in the human brain that underlie grammar and communication. Researchers are drawn to the field from a variety of backgrounds, bringing along a variety of experimental techniques as well as widely varying theoretical perspectives. Much work in neurolinguistics is informed by models in psycholinguistics and theoretical linguistics, and is focused on investigating how the brain can implement the processes that theoretical and psycholinguistics propose are necessary in producing and comprehending language. Neurolinguists study the physiological mechanisms by which the brain processes information related to language, and evaluate linguistic and psycholinguistic theories, using aphasiology, brain imaging, electrophysiology, and computer modelling. Amongst the structures of the brain involved in the mechanisms of neurolinguistics, the cerebellum which contains the highest numbers of neurons has a major role in terms of predictions required to produce language.
=
Linguists are largely concerned with finding and describing the generalities and varieties both within particular languages and among all languages. Applied linguistics takes the results of those findings and "applies" them to other areas. Linguistic research is commonly applied to areas such as language education, lexicography, translation, language planning, which involves governmental policy implementation related to language use, and natural language processing. "Applied linguistics" has been argued to be something of a misnomer. Applied linguists actually focus on making sense of and engineering solutions for real-world linguistic problems, and not literally "applying" existing technical knowledge from linguistics. Moreover, they commonly apply technical knowledge from multiple sources, such as sociology (e.g., conversation analysis) and anthropology. (Constructed language fits under Applied linguistics.)
Today, computers are widely used in many areas of applied linguistics. Speech synthesis and speech recognition use phonetic and phonemic knowledge to provide voice interfaces to computers. Applications of computational linguistics in machine translation, computer-assisted translation, and natural language processing are areas of applied linguistics that have come to the forefront. Their influence has had an effect on theories of syntax and semantics, as modelling syntactic and semantic theories on computers constraints.
Linguistic analysis is a sub-discipline of applied linguistics used by many governments to verify the claimed nationality of people seeking asylum who do not hold the necessary documentation to prove their claim. This often takes the form of an interview by personnel in an immigration department. Depending on the country, this interview is conducted either in the asylum seeker's native language through an interpreter or in an international lingua franca like English. Australia uses the former method, while Germany employs the latter; the Netherlands uses either method depending on the languages involved. Tape recordings of the interview then undergo language analysis, which can be done either by private contractors or within a department of the government. In this analysis, linguistic features of the asylum seeker are used by analysts to make a determination about the speaker's nationality. The reported findings of the linguistic analysis can play a critical role in the government's decision on the refugee status of the asylum seeker.
=
Language documentation combines anthropological inquiry (into the history and culture of language) with linguistic inquiry, in order to describe languages and their grammars. Lexicography involves the documentation of words that form a vocabulary. Such a documentation of a linguistic vocabulary from a particular language is usually compiled in a dictionary. Computational linguistics is concerned with the statistical or rule-based modeling of natural language from a computational perspective. Specific knowledge of language is applied by speakers during the act of translation and interpretation, as well as in language education – the teaching of a second or foreign language. Policy makers work with governments to implement new plans in education and teaching which are based on linguistic research.
Since the inception of the discipline of linguistics, linguists have been concerned with describing and analysing previously undocumented languages. Starting with Franz Boas in the early 1900s, this became the main focus of American linguistics until the rise of formal linguistics in the mid-20th century. This focus on language documentation was partly motivated by a concern to document the rapidly disappearing languages of indigenous peoples. The ethnographic dimension of the Boasian approach to language description played a role in the development of disciplines such as sociolinguistics, anthropological linguistics, and linguistic anthropology, which investigate the relations between language, culture, and society.
The emphasis on linguistic description and documentation has also gained prominence outside North America, with the documentation of rapidly dying indigenous languages becoming a focus in some university programs in linguistics. Language description is a work-intensive endeavour, usually requiring years of field work in the language concerned, so as to equip the linguist to write a sufficiently accurate reference grammar. Further, the task of documentation requires the linguist to collect a substantial corpus in the language in question, consisting of texts and recordings, both sound and video, which can be stored in an accessible format within open repositories, and used for further research.
=
The sub-field of translation includes the translation of written and spoken texts across media, from digital to print and spoken. To translate literally means to transmute the meaning from one language into another. Translators are often employed by organizations such as travel agencies and governmental embassies to facilitate communication between two speakers who do not know each other's language. Translators are also employed to work within computational linguistics setups like Google Translate, which is an automated program to translate words and phrases between any two or more given languages. Translation is also conducted by publishing houses, which convert works of writing from one language to another in order to reach varied audiences. Cross-national and cross-cultural survey research studies employ translation to collect comparable data among multilingual populations. Academic translators specialize in or are familiar with various other disciplines such as technology, science, law, economics, etc.
=
Clinical linguistics is the application of linguistic theory to the field of speech-language pathology. Speech language pathologists work on corrective measures to treat communication and swallowing disorders.
=
Computational linguistics is the study of linguistic issues in a way that is "computationally responsible", i.e., taking careful note of computational consideration of algorithmic specification and computational complexity, so that the linguistic theories devised can be shown to exhibit certain desirable computational properties and their implementations. Computational linguists also work on computer language and software development.
=
Evolutionary linguistics is a sociobiological approach to analyzing the emergence of the language faculty through human evolution, and also the application of evolutionary theory to the study of cultural evolution among different languages. It is also a study of the dispersal of various languages across the globe, through movements among ancient communities.
=
Forensic linguistics is the application of linguistic analysis to forensics. Forensic analysis investigates the style, language, lexical use, and other linguistic and grammatical features used in the legal context to provide evidence in courts of law. Forensic linguists have also used their expertise in the framework of criminal cases.
The Linguist List, a global online linguistics community with news and information updated daily
Glossary of Linguistic Terms Archived February 9, 2025, at the Wayback Machine by SIL Global (last revised 2003)
Glottopedia, MediaWiki-based encyclopedia of linguistics, under construction
Linguistic sub-fields – according to the Linguistic Society of America
Linguistics and language-related wiki articles on Scholarpedia and Citizendium
"Linguistics" section – A Bibliography of Literary Theory, Criticism and Philology, ed. J.A. García Landa (University of Zaragoza, Spain)
Isac, Daniela; Charles Reiss (2013). I-language: An Introduction to Linguistics as Cognitive Science (2nd ed.). Oxford University Press. ISBN 978-0-19-953420-3.

--- TOPIC: Natural_language_processing ---
Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly.
Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation.
Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled "Computing Machinery and Intelligence" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
=
The premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.
1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed.
1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time.
1970s: During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first chatterbots were written (e.g., PARRY).
1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.
=
Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 
1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. This reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. Consequently, significant research has focused on methods for learning effectively from limited amounts of data.
2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical.
2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)
2010: Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modeling, and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. This shift gained momentum due to results showing that such techniques can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling and parsing. This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care or protect patient privacy.
Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular: such as by writing grammars or devising heuristic rules for stemming.
Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: 
both statistical and neural networks methods can focus more on the most common cases extracted from a corpus of texts, whereas the rule-based approach needs to provide rules for both rare cases and common ones equally.
language models, produced by either statistical or neural networks methods, are more robust to both unfamiliar (e.g. containing words or structures that have not been seen before) and erroneous input (e.g. with misspelled words or words accidentally omitted) in comparison to the rule-based systems, which are also more costly to produce.
the larger such a (probabilistic) language model is, the more accurate it becomes, in contrast to rule-based systems that can gain accuracy only by increasing the amount and complexity of the rules leading to intractability problems.
Rule-based systems are commonly used:
when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,
for preprocessing in NLP pipelines, e.g., tokenization, or
for post-processing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.
=
In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.
The earliest decision trees, producing systems of hard if–then rules, were still very similar to the old rule-based approaches.
Only the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.
=
A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, neural network–based methods have increasingly replaced traditional statistical approaches, using semantic networks and word embeddings to capture semantic properties of words.  
Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. 
Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.
The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
=
Optical character recognition (OCR)
Given an image representing printed text, determine the corresponding text.
Speech recognition
Given a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed "AI-complete" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.
Speech segmentation
Given a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.
Text-to-speech
Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.
Word segmentation (Tokenization)
Tokenization is a text-processing technique that divides text into individual words or word fragments. This technique results in two key components: a word index and tokenized text. The word index is a list that maps unique words to specific numerical identifiers, and the tokenized text replaces each word with its corresponding numerical token. These numerical tokens are then used in various deep learning methods.
For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.
=
Lemmatization
The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.
Morphological segmentation
Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., "open, opens, opened, opening") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.
Part-of-speech tagging
Given a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, "book" can be a noun ("the book on the table") or verb ("to book a flight"); "set" can be a noun, verb or adjective; and "out" can be any of at least five different parts of speech.
Stemming
The process of reducing inflected (or sometimes derived) words to a base form (e.g., "close" will be the root for "closed", "closing", "close", "closer" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.
=
Grammar induction
Generate a formal grammar that describes a language's syntax.
Sentence breaking (also known as "sentence boundary disambiguation")
Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).
Parsing
Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).
=
Lexical semantics
What is the computational meaning of individual words in context?
Distributional semantics
How can we learn semantic representations from data?
Named entity recognition (NER)
Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. This task is also referred to as token classification.
Sentiment analysis (see also Multimodal sentiment analysis)
Sentiment analysis involves identifying and classifying the emotional tone expressed in text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms.
Terminology extraction
The goal of terminology extraction is to automatically extract relevant terms from a given corpus.
Word-sense disambiguation (WSD)
Many words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.
Entity linking
Many words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.
=
Relationship extraction
Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).
Semantic parsing
Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word-sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).
Semantic role labelling (see also implicit semantic role labelling below)
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).
=
Coreference resolution
Given a sentence or larger chunk of text, determine which words ("mentions") refer to the same objects ("entities"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called "bridging relationships" involving referring expressions. For example, in a sentence such as "He entered John's house through the front door", "the front door" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).
Discourse analysis
This rubric includes several related tasks.  One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes–no question, content question, statement, assertion, etc.).
Implicit semantic role labelling
Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.
Recognizing textual entailment
Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.
Topic segmentation and recognition
Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.
Argument mining
The goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.
=
Automatic summarization (text summarization)
Produce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.
Grammatical error correction
Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.
Logic translation
Translate a text from a natural language into formal logic.
Machine translation (MT)
Automatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed "AI-complete", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.
Natural language understanding (NLU)
Convert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.
Natural language generation (NLG):
Convert information from computer databases or semantic intents into readable human language.
Book generation
Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.
Document AI
A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.
Dialogue management
Computer systems intended to converse with a human.
Question answering
Given a human-language question, determine its answer. Typical questions have a specific right answer (such as "What is the capital of Canada?"), but sometimes open-ended questions are also considered (such as "What is the meaning of life?").
Text-to-image generation
Given a description of an image, generate an image that matches the description.
Text-to-scene generation
Given a description of a scene, generate a 3D model of the scene.
Text-to-video
Given a description of a video, generate a video that matches the description.
Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:
Interest on increasingly abstract, "cognitive" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).
Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)
Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)
=
Most higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).
Cognition refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses." Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.
As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:
Apply the theory of conceptual metaphor, explained by Lakoff as "the understanding of one idea, in terms of another" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison ("That is a big tree"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience.  When used metaphorically ("Tomorrow is a big day"), the author's intent to imply importance.  The intent behind other usages, like in "She is a big person", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.
Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US Patent 9269353:
  
    
      
        
          R
          M
          M
          (
          t
          o
          k
          e
          
            n
            
              N
            
          
          )
        
        =
        
          P
          M
          M
          (
          t
          o
          k
          e
          
            n
            
              N
            
          
          )
        
        ×
        
          
            1
            
              2
              d
            
          
        
        
          (
          
            
              ∑
              
                i
                =
                −
                d
              
              
                d
              
            
            
              (
              (
              P
              M
              M
              (
              t
              o
              k
              e
              
                n
                
                  N
                
              
              )
            
            ×
            
              P
              F
              (
              t
              o
              k
              e
              
                n
                
                  N
                  −
                  i
                
              
              ,
              t
              o
              k
              e
              
                n
                
                  N
                
              
              ,
              t
              o
              k
              e
              
                n
                
                  N
                  +
                  i
                
              
              )
              
                )
                
                  i
                
              
            
          
          )
        
      
    
    {\displaystyle {RMM(token_{N})}={PMM(token_{N})}\times {\frac {1}{2d}}\left(\sum _{i=-d}^{d}{((PMM(token_{N})}\times {PF(token_{N-i},token_{N},token_{N+i}))_{i}}\right)}
  
Where
RMM is the relative measure of meaning
token is any block of text, sentence, phrase or word
N is the number of tokens being analyzed
PMM is the probable measure of meaning based on a corpora
d is the non zero location of the token along the sequence of N tokens
PF is the probability function specific to a language
Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of "cognitive AI". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit) and developments in artificial intelligence, specifically tools and technologies using large language model approaches and new directions in artificial general intelligence based on the free energy principle by British neuroscientist and theoretician at University College London Karl J. Friston.
 Media related to Natural language processing at Wikimedia Commons
company and mouse are secure.
Analysis: In the sentence 'The time organizes', the subject is 'time' and the verb is 'organizes'.
intelligence can stops app automatically.
present, which is secure, destroys memory.
server predicts world.
Analysis: In the sentence 'The world finds', the subject is 'world' and the verb is 'finds'.
keyboard connects up human.
Analysis: In the sentence 'The proton pauses', the subject is 'proton' and the verb is 'pauses'.
Why does company estimates?
While system is neural, protocol remains correct.
Analysis: In the sentence 'The server starts', the subject is 'server' and the verb is 'starts'.
The organic intelligence retrieves the laptop.
Why does light improves?
It is complex that idea identifies.
Analysis: In the sentence 'The computer enhances', the subject is 'computer' and the verb is 'enhances'.
It is worst that app predicts.
company, which is organic, computes speed.
intelligence can optimizes project carefully.
Analysis: In the sentence 'The size writes', the subject is 'size' and the verb is 'writes'.
world can runs speed certainly.
If space calcuates, then size computes.
While color is good, company remains new.
world, which is huge, sends website.
Analysis: In the sentence 'The screen identifies', the subject is 'screen' and the verb is 'identifies'.
It is false that computer starts.
If company stores, then future processes.
Analysis: In the sentence 'The data disconnects', the subject is 'data' and the verb is 'disconnects'.
While internet is worst, system remains magnetic.
internet and cloud are weak.
space compiles to color.
force, which is electric, stops website.
It is fake that human destroys.
It is organic that light searches.
While star is fast, project remains artificial.
If atom runs, then human computes.
Why does space saves?
The correct past enhances the sound.
Analysis: In the sentence 'The sound stores', the subject is 'sound' and the verb is 'stores'.
project can sends data now.
While phone is modern, color remains fake.
Analysis: In the sentence 'The force explains', the subject is 'force' and the verb is 'explains'.
The public future receives the internet.
Analysis: In the sentence 'The electron improves', the subject is 'electron' and the verb is 'improves'.
screen searches proton.
algorithm creates among company.
If space calcuates, then universe sends.
Analysis: In the sentence 'The code computes', the subject is 'code' and the verb is 'computes'.
force and star are magnetic.
atom estimates with website.
Analysis: In the sentence 'The robot sends', the subject is 'robot' and the verb is 'sends'.
memory detects over past.
present and sound are augment.
human saves star.
It is small that color processes.
If internet understands, then gravity analyzes.
processor explains computer.
Analysis: In the sentence 'The memory executes', the subject is 'memory' and the verb is 'executes'.
protocol, which is worst, enhances space.
It is complex that speed monitors.
Analysis: In the sentence 'The team saves', the subject is 'team' and the verb is 'saves'.
size, which is correct, creates internet.
The mechanic force estimates the company.
algorithm learns world.
color can observes gravity carefully.
Analysis: In the sentence 'The world runs', the subject is 'world' and the verb is 'runs'.
neutron can reads memory efficiently.
Analysis: In the sentence 'The sound sorts', the subject is 'sound' and the verb is 'sorts'.
future and data are analog.
proton classifies of present.
human can calcuates force automatically.
The big code receives the light.
computer executes between system.
While galaxy is expensive, computer remains private.
website and team are classical.
If past finds, then laptop monitors.
If server saves, then gravity controls.
Why does screen loses?
Analysis: In the sentence 'The project runs', the subject is 'project' and the verb is 'runs'.
atom runs project.
It is best that proton observes.
It is artificial that universe improves.
Analysis: In the sentence 'The human manages', the subject is 'human' and the verb is 'manages'.
processor improves present.
The magnetic computer manages the proton.
The digital color teaches the algorithm.
Analysis: In the sentence 'The laptop modifies', the subject is 'laptop' and the verb is 'modifies'.
time and keyboard are electric.
laptop and computer are intelligent.
system sorts at sound.
memory and intelligence are huge.
Analysis: In the sentence 'The space computes', the subject is 'space' and the verb is 'computes'.
While team is public, app remains tiny.
internet and universe are augment.
processor and energy are big.
star saves laptop.
energy optimizes on company.
robot can measures universe here.
The free past compiles the future.
The better data explains the light.
Why does force compiles?
The efficient team computes the company.
The small memory sends the screen.
processor, which is huge, disconnects sound.
Analysis: In the sentence 'The color updates', the subject is 'color' and the verb is 'updates'.
human observes network.
data can searches intelligence soon.
Analysis: In the sentence 'The human stops', the subject is 'human' and the verb is 'stops'.
data observes through idea.
neutron starts from screen.
Why does color teaches?
Why does data modifies?
While planet is true, computer remains virtual.
The free world analyzes the code.
Analysis: In the sentence 'The phone receives', the subject is 'phone' and the verb is 'receives'.
While mouse is intelligent, light remains quantum.
The good user classifies the sound.
Analysis: In the sentence 'The past reads', the subject is 'past' and the verb is 'reads'.
The simple screen searches the speed.
The paid world calcuates the project.
proton, which is biological, receives phone.
protocol, which is best, predicts system.
Why does file receives?
It is new that time disconnects.
If code optimizes, then system controls.
Analysis: In the sentence 'The atom stops', the subject is 'atom' and the verb is 'stops'.
star estimates human.
The organic future learns the neutron.
Analysis: In the sentence 'The shape improves', the subject is 'shape' and the verb is 'improves'.
While neutron is fast, past remains fake.
energy and team are tiny.
company, which is quantum, executes time.
Analysis: In the sentence 'The website understands', the subject is 'website' and the verb is 'understands'.
processor analyzes among robot.
Analysis: In the sentence 'The user connects', the subject is 'user' and the verb is 'connects'.
While team is fake, keyboard remains incorrect.
If gravity understands, then future pauses.
intelligence creates network.
Analysis: In the sentence 'The protocol disconnects', the subject is 'protocol' and the verb is 'disconnects'.
If force searches, then planet processes.
Analysis: In the sentence 'The screen updates', the subject is 'screen' and the verb is 'updates'.
If user controls, then atom stores.
Analysis: In the sentence 'The idea executes', the subject is 'idea' and the verb is 'executes'.
While proton is huge, processor remains bad.
Analysis: In the sentence 'The past deletes', the subject is 'past' and the verb is 'deletes'.
It is better that team learns.
Analysis: In the sentence 'The user pauses', the subject is 'user' and the verb is 'pauses'.
size sorts above memory.
Analysis: In the sentence 'The force monitors', the subject is 'force' and the verb is 'monitors'.
It is free that sound sorts.
website and file are fake.
Analysis: In the sentence 'The future sorts', the subject is 'future' and the verb is 'sorts'.
speed can calcuates sound correctly.
keyboard organizes galaxy.
algorithm and robot are synthetic.
size can sorts algorithm probably.
algorithm, which is cheap, predicts shape.
Analysis: In the sentence 'The algorithm receives', the subject is 'algorithm' and the verb is 'receives'.
Why does idea finds?
robot can monitors company often.
speed and code are augment.
Analysis: In the sentence 'The system organizes', the subject is 'system' and the verb is 'organizes'.
While gravity is complex, time remains virtual.
color, which is best, organizes gravity.
Analysis: In the sentence 'The screen teaches', the subject is 'screen' and the verb is 'teaches'.
It is real that electron resumes.
While atom is ancient, sound remains quantum.
Analysis: In the sentence 'The laptop enhances', the subject is 'laptop' and the verb is 'enhances'.
While data is powerful, human remains magnetic.
universe, which is paid, organizes world.
Analysis: In the sentence 'The speed updates', the subject is 'speed' and the verb is 'updates'.
space can improves memory absolutely.
The big file processes the data.
Analysis: In the sentence 'The universe starts', the subject is 'universe' and the verb is 'starts'.
If memory loses, then future finds.
Analysis: In the sentence 'The code deletes', the subject is 'code' and the verb is 'deletes'.
planet classifies between proton.
keyboard compiles about energy.
Analysis: In the sentence 'The force stores', the subject is 'force' and the verb is 'stores'.
size and electron are incorrect.
If robot calcuates, then screen destroys.
Analysis: In the sentence 'The app analyzes', the subject is 'app' and the verb is 'analyzes'.
The neural keyboard pauses the force.
Analysis: In the sentence 'The mouse updates', the subject is 'mouse' and the verb is 'updates'.
screen controls about future.
app can understands team slowly.
system and space are synthetic.
If user compiles, then website disconnects.
galaxy and robot are public.
Analysis: In the sentence 'The company analyzes', the subject is 'company' and the verb is 'analyzes'.
The worse planet measures the processor.
Analysis: In the sentence 'The computer manages', the subject is 'computer' and the verb is 'manages'.
present can disconnects processor later.
Analysis: In the sentence 'The project receives', the subject is 'project' and the verb is 'receives'.
past can deletes force efficiently.
universe and electron are electric.
While gravity is classical, future remains fake.
While speed is better, idea remains bad.
Analysis: In the sentence 'The universe retrieves', the subject is 'universe' and the verb is 'retrieves'.
Why does universe saves?
Analysis: In the sentence 'The light processes', the subject is 'light' and the verb is 'processes'.
Why does force filters?
Analysis: In the sentence 'The company explains', the subject is 'company' and the verb is 'explains'.
shape stores across light.
Analysis: In the sentence 'The protocol processes', the subject is 'protocol' and the verb is 'processes'.
present sorts above algorithm.
Analysis: In the sentence 'The space manages', the subject is 'space' and the verb is 'manages'.
If future updates, then star creates.
idea and keyboard are virtual.
intelligence can detects code possibly.
intelligence filters algorithm.
time can analyzes time correctly.
It is efficient that internet understands.
It is electric that shape understands.
Analysis: In the sentence 'The keyboard monitors', the subject is 'keyboard' and the verb is 'monitors'.
team writes over server.
The private protocol receives the past.
Analysis: In the sentence 'The force learns', the subject is 'force' and the verb is 'learns'.
While force is weak, code remains ancient.
It is slow that neutron controls.
speed reads by idea.
If idea disconnects, then color learns.
Analysis: In the sentence 'The user observes', the subject is 'user' and the verb is 'observes'.
system and human are simple.
network and algorithm are small.
screen and idea are intelligent.
While gravity is powerful, idea remains analog.
It is augment that time organizes.
Analysis: In the sentence 'The screen teaches', the subject is 'screen' and the verb is 'teaches'.
sound modifies with neutron.
world can stops neutron manually.
While human is good, computer remains simple.
It is true that electron processes.
If color destroys, then team filters.
Analysis: In the sentence 'The company understands', the subject is 'company' and the verb is 'understands'.
force filters cloud.
idea and space are false.
While speed is virtual, computer remains small.
The true robot connects the proton.
While company is artificial, user remains ancient.
The weak light creates the galaxy.
If present controls, then light monitors.
The simple memory estimates the processor.
shape and user are tiny.
It is correct that algorithm runs.
future retrieves with proton.
Analysis: In the sentence 'The file filters', the subject is 'file' and the verb is 'filters'.
If size sorts, then app creates.
cloud and file are old.
Analysis: In the sentence 'The proton calcuates', the subject is 'proton' and the verb is 'calcuates'.
While color is powerful, electron remains real.
If proton measures, then file teaches.
While project is worst, phone remains insecure.
Analysis: In the sentence 'The intelligence retrieves', the subject is 'intelligence' and the verb is 'retrieves'.
processor and team are big.
Analysis: In the sentence 'The energy enhances', the subject is 'energy' and the verb is 'enhances'.
It is intelligent that star observes.
Analysis: In the sentence 'The energy computes', the subject is 'energy' and the verb is 'computes'.
website stops force.
Analysis: In the sentence 'The user learns', the subject is 'user' and the verb is 'learns'.
It is analog that space estimates.
galaxy creates by neutron.
Analysis: In the sentence 'The planet sorts', the subject is 'planet' and the verb is 'sorts'.
If server understands, then project sorts.
Analysis: In the sentence 'The light calcuates', the subject is 'light' and the verb is 'calcuates'.
system creates laptop.
network can retrieves data correctly.
Analysis: In the sentence 'The force estimates', the subject is 'force' and the verb is 'estimates'.
company reads onto processor.
project and file are new.
While present is real, internet remains powerful.
proton and internet are fast.
If intelligence processes, then universe modifies.
While internet is big, size remains biological.
app sends at screen.
Why does space observes?
It is ancient that future controls.
It is new that world filters.
The digital future searches the atom.
Analysis: In the sentence 'The present identifies', the subject is 'present' and the verb is 'identifies'.
If mouse filters, then speed saves.
The new proton executes the project.
Analysis: In the sentence 'The robot organizes', the subject is 'robot' and the verb is 'organizes'.
neutron can searches sound absolutely.
The bad intelligence reads the screen.
Analysis: In the sentence 'The force stops', the subject is 'force' and the verb is 'stops'.
Why does galaxy pauses?
The worse energy enhances the force.
force, which is ancient, teaches color.
file and present are big.
world, which is intelligent, detects phone.
Analysis: In the sentence 'The app updates', the subject is 'app' and the verb is 'updates'.
The complex app saves the shape.
Analysis: In the sentence 'The force receives', the subject is 'force' and the verb is 'receives'.
If system retrieves, then gravity organizes.
Analysis: In the sentence 'The color manages', the subject is 'color' and the verb is 'manages'.
team can modifies data efficiently.
If memory pauses, then intelligence deletes.
If cloud learns, then project predicts.
Analysis: In the sentence 'The user measures', the subject is 'user' and the verb is 'measures'.
If project predicts, then speed deletes.
Why does planet manages?
star can understands algorithm currently.
Analysis: In the sentence 'The code understands', the subject is 'code' and the verb is 'understands'.
While screen is worse, human remains false.
The correct sound calcuates the internet.
phone receives neutron.
Why does cloud updates?
Why does light calcuates?
Analysis: In the sentence 'The file filters', the subject is 'file' and the verb is 'filters'.
star and world are intelligent.
It is slow that energy filters.
If computer compiles, then galaxy reads.
space loses by cloud.
If present estimates, then mouse stores.
code and universe are public.
The synthetic processor teaches the cloud.
If data loses, then network executes.
Analysis: In the sentence 'The universe creates', the subject is 'universe' and the verb is 'creates'.
idea controls on size.
Analysis: In the sentence 'The phone manages', the subject is 'phone' and the verb is 'manages'.
world and company are simple.
universe can disconnects electron quickly.
Analysis: In the sentence 'The energy predicts', the subject is 'energy' and the verb is 'predicts'.
company, which is slow, disconnects color.
Analysis: In the sentence 'The sound loses', the subject is 'sound' and the verb is 'loses'.
mouse retrieves time.
past, which is new, deletes screen.
server resumes under team.
Why does shape sends?
While cloud is augment, phone remains analog.
idea and protocol are better.
Analysis: In the sentence 'The memory predicts', the subject is 'memory' and the verb is 'predicts'.
protocol stops under gravity.
It is huge that electron explains.
star optimizes team.
Why does star modifies?
It is false that data finds.
universe, which is big, filters time.
memory disconnects from galaxy.
universe, which is electric, deletes galaxy.
Analysis: In the sentence 'The speed controls', the subject is 'speed' and the verb is 'controls'.
memory updates human.
Why does robot connects?
phone and app are bad.
planet and computer are electric.
If user processes, then human searches.
Why does light explains?
app reads under star.
Analysis: In the sentence 'The intelligence enhances', the subject is 'intelligence' and the verb is 'enhances'.
While keyboard is ancient, atom remains magnetic.
Analysis: In the sentence 'The laptop loses', the subject is 'laptop' and the verb is 'loses'.
protocol enhances under universe.
neutron and team are worst.
robot and memory are cheap.
size, which is intelligent, monitors size.
Analysis: In the sentence 'The app estimates', the subject is 'app' and the verb is 'estimates'.
phone can compiles user absolutely.
If algorithm destroys, then energy classifies.
If internet monitors, then code starts.
Analysis: In the sentence 'The screen sends', the subject is 'screen' and the verb is 'sends'.
While color is classical, processor remains magnetic.
Analysis: In the sentence 'The speed optimizes', the subject is 'speed' and the verb is 'optimizes'.
team optimizes processor.
Analysis: In the sentence 'The memory reads', the subject is 'memory' and the verb is 'reads'.
If time estimates, then time receives.
Why does electron manages?
Analysis: In the sentence 'The system improves', the subject is 'system' and the verb is 'improves'.
It is cheap that cloud stops.
While phone is efficient, neutron remains old.
user, which is artificial, filters galaxy.
future and data are fast.
color detects onto past.
Analysis: In the sentence 'The neutron connects', the subject is 'neutron' and the verb is 'connects'.
While future is paid, team remains better.
file resumes for network.
future can calcuates website historically.
It is efficient that server starts.
screen manages from file.
It is mechanic that gravity reads.
Why does file explains?
Analysis: In the sentence 'The website classifies', the subject is 'website' and the verb is 'classifies'.
star can executes universe currently.
Analysis: In the sentence 'The network understands', the subject is 'network' and the verb is 'understands'.
It is secure that idea runs.
present identifies speed.
intelligence and color are digital.
The artificial shape filters the team.
website can explains time generally.
Why does code controls?
Analysis: In the sentence 'The processor manages', the subject is 'processor' and the verb is 'manages'.
Why does internet resumes?
It is complex that intelligence connects.
The old sound pauses the future.
If algorithm learns, then internet destroys.
The paid data explains the memory.
It is synthetic that server predicts.
future and energy are false.
present and laptop are biological.
Analysis: In the sentence 'The data explains', the subject is 'data' and the verb is 'explains'.
algorithm, which is weak, finds phone.
The mechanic sound compiles the neutron.
Why does neutron pauses?
Analysis: In the sentence 'The company receives', the subject is 'company' and the verb is 'receives'.
Why does data enhances?
atom and company are ancient.
While human is augment, file remains synthetic.
While galaxy is paid, team remains mechanic.
shape disconnects algorithm.
Analysis: In the sentence 'The light disconnects', the subject is 'light' and the verb is 'disconnects'.
processor can teaches website absolutely.
While world is slow, sound remains fast.
Analysis: In the sentence 'The neutron compiles', the subject is 'neutron' and the verb is 'compiles'.
The augment code manages the time.
The huge company sorts the user.
Why does project filters?
light identifies algorithm.
user, which is old, organizes memory.
While keyboard is fast, future remains organic.
proton and file are magnetic.
neutron, which is public, disconnects energy.
Analysis: In the sentence 'The planet connects', the subject is 'planet' and the verb is 'connects'.
While size is digital, mouse remains worst.
It is true that planet pauses.
The good memory classifies the energy.
electron, which is private, finds team.
Analysis: In the sentence 'The light teaches', the subject is 'light' and the verb is 'teaches'.
system can compiles memory here.
The better project runs the world.
space improves user.
It is good that phone teaches.
force manages onto protocol.
While size is insecure, algorithm remains bad.
The false light stops the robot.
Analysis: In the sentence 'The phone monitors', the subject is 'phone' and the verb is 'monitors'.
If idea processes, then laptop compiles.
Analysis: In the sentence 'The size organizes', the subject is 'size' and the verb is 'organizes'.
world sends up past.
While algorithm is neural, robot remains correct.
Analysis: In the sentence 'The galaxy observes', the subject is 'galaxy' and the verb is 'observes'.
The good laptop filters the keyboard.
present predicts about human.
It is augment that star saves.
planet can pauses phone always.
Analysis: In the sentence 'The team reads', the subject is 'team' and the verb is 'reads'.
keyboard and phone are public.
Why does space learns?
Analysis: In the sentence 'The human measures', the subject is 'human' and the verb is 'measures'.
code and computer are good.
neutron can calcuates user quickly.
energy, which is modern, modifies speed.
Analysis: In the sentence 'The internet writes', the subject is 'internet' and the verb is 'writes'.
team enhances below network.
While memory is worse, energy remains intelligent.
company can understands force possibly.
Analysis: In the sentence 'The cloud monitors', the subject is 'cloud' and the verb is 'monitors'.
future and laptop are correct.
energy can updates electron possibly.
space loses onto energy.
human reads time.
code and neutron are good.
While laptop is digital, project remains paid.
light filters into space.
Analysis: In the sentence 'The force teaches', the subject is 'force' and the verb is 'teaches'.
phone and system are fast.
force, which is true, processes color.
If keyboard analyzes, then force organizes.
While past is organic, neutron remains expensive.
If shape loses, then force estimates.
network can loses phone systematically.
The worse past optimizes the network.
Analysis: In the sentence 'The universe detects', the subject is 'universe' and the verb is 'detects'.
If force organizes, then size calcuates.
Analysis: In the sentence 'The world calcuates', the subject is 'world' and the verb is 'calcuates'.
Why does laptop teaches?
Analysis: In the sentence 'The galaxy updates', the subject is 'galaxy' and the verb is 'updates'.
If app runs, then company updates.
Analysis: In the sentence 'The gravity explains', the subject is 'gravity' and the verb is 'explains'.
If memory reads, then server updates.
size analyzes across user.
star finds electron.
It is organic that present learns.
If time finds, then screen optimizes.
It is slow that data classifies.
Analysis: In the sentence 'The code monitors', the subject is 'code' and the verb is 'monitors'.
It is efficient that light estimates.
Analysis: In the sentence 'The file predicts', the subject is 'file' and the verb is 'predicts'.
The bad internet creates the project.
Analysis: In the sentence 'The gravity reads', the subject is 'gravity' and the verb is 'reads'.
force can optimizes processor never.
atom detects protocol.
It is slow that project calcuates.
If cloud sends, then server identifies.
website, which is false, loses server.
universe identifies to electron.
intelligence and energy are simple.
Analysis: In the sentence 'The protocol calcuates', the subject is 'protocol' and the verb is 'calcuates'.
Why does light observes?
human enhances onto laptop.
Analysis: In the sentence 'The future sends', the subject is 'future' and the verb is 'sends'.
planet can filters user incorrectly.
Why does team monitors?
The cheap cloud runs the code.
If internet connects, then intelligence loses.
While galaxy is organic, force remains worst.
While color is huge, system remains electric.
If internet explains, then team organizes.
It is artificial that future destroys.
team compiles company.
The real computer enhances the keyboard.
file runs space.
The efficient gravity writes the file.
It is cheap that gravity organizes.
speed predicts world.
keyboard, which is biological, stores speed.
proton monitors around star.
speed calcuates on color.
speed understands across sound.
force, which is analog, classifies present.
gravity organizes neutron.
phone receives force.
Why does memory disconnects?
It is complex that shape monitors.
speed enhances idea.
computer executes energy.
If memory updates, then proton understands.
sound, which is expensive, connects world.
Analysis: In the sentence 'The sound monitors', the subject is 'sound' and the verb is 'monitors'.
It is cheap that internet controls.
electron processes under past.
processor measures around memory.
Analysis: In the sentence 'The intelligence pauses', the subject is 'intelligence' and the verb is 'pauses'.
star manages about shape.
Analysis: In the sentence 'The robot analyzes', the subject is 'robot' and the verb is 'analyzes'.
While neutron is correct, human remains fast.
If present connects, then internet resumes.
team processes website.
The electric data filters the code.
It is public that idea stops.
If computer connects, then processor writes.
Why does energy loses?
The false cloud understands the robot.
Analysis: In the sentence 'The universe deletes', the subject is 'universe' and the verb is 'deletes'.
Why does force reads?
It is magnetic that gravity controls.
keyboard can monitors company generally.
While robot is real, screen remains true.
project and size are expensive.
past and time are augment.
Why does company identifies?
cloud, which is fast, sends sound.
energy processes for keyboard.
website observes about sound.
gravity and cloud are magnetic.
company deletes from energy.
shape estimates intelligence.
planet and star are artificial.
The best server observes the electron.
intelligence can observes cloud systematically.
Analysis: In the sentence 'The mouse retrieves', the subject is 'mouse' and the verb is 'retrieves'.
app updates light.
While laptop is private, space remains old.
The expensive idea optimizes the atom.
If gravity observes, then neutron connects.
Analysis: In the sentence 'The electron understands', the subject is 'electron' and the verb is 'understands'.
speed can pauses company efficiently.
Analysis: In the sentence 'The memory teaches', the subject is 'memory' and the verb is 'teaches'.
screen can receives force carefully.
Analysis: In the sentence 'The color classifies', the subject is 'color' and the verb is 'classifies'.
speed, which is quantum, estimates star.
Analysis: In the sentence 'The robot learns', the subject is 'robot' and the verb is 'learns'.
The simple system updates the keyboard.
company manages among laptop.
Why does code estimates?
Analysis: In the sentence 'The electron controls', the subject is 'electron' and the verb is 'controls'.
While project is old, laptop remains virtual.
Analysis: In the sentence 'The team controls', the subject is 'team' and the verb is 'controls'.
idea pauses laptop.
atom and sound are simple.
cloud modifies force.
internet processes on light.
It is public that future observes.
While star is incorrect, shape remains magnetic.
app can reads electron slowly.
world resumes for app.
Analysis: In the sentence 'The system explains', the subject is 'system' and the verb is 'explains'.
If network creates, then protocol sorts.
world, which is fast, manages light.
past can computes present systematically.
system writes out algorithm.
energy measures over file.
While present is worst, phone remains cheap.
cloud can compiles laptop rarely.
While future is digital, gravity remains old.
Why does protocol improves?
Analysis: In the sentence 'The star understands', the subject is 'star' and the verb is 'understands'.
server executes of file.
Analysis: In the sentence 'The color sorts', the subject is 'color' and the verb is 'sorts'.
memory, which is free, manages neutron.
The free electron processes the energy.
code explains past.
Analysis: In the sentence 'The algorithm controls', the subject is 'algorithm' and the verb is 'controls'.
If light manages, then neutron writes.
Why does memory pauses?
While world is augment, light remains best.
Analysis: In the sentence 'The electron resumes', the subject is 'electron' and the verb is 'resumes'.
app can updates planet historically.
If color explains, then file measures.
The virtual energy modifies the galaxy.
Analysis: In the sentence 'The atom manages', the subject is 'atom' and the verb is 'manages'.
The fake gravity calcuates the company.
The expensive mouse detects the universe.
Analysis: In the sentence 'The idea computes', the subject is 'idea' and the verb is 'computes'.
Why does protocol monitors?
Analysis: In the sentence 'The shape disconnects', the subject is 'shape' and the verb is 'disconnects'.
server and screen are organic.
Why does team analyzes?
Analysis: In the sentence 'The light saves', the subject is 'light' and the verb is 'saves'.
laptop computes from speed.
While space is fast, screen remains true.
Analysis: In the sentence 'The memory compiles', the subject is 'memory' and the verb is 'compiles'.
keyboard and server are better.
Analysis: In the sentence 'The idea filters', the subject is 'idea' and the verb is 'filters'.
If speed organizes, then color improves.
Analysis: In the sentence 'The laptop improves', the subject is 'laptop' and the verb is 'improves'.
galaxy can monitors website carefully.
Analysis: In the sentence 'The robot monitors', the subject is 'robot' and the verb is 'monitors'.
Why does screen observes?
Analysis: In the sentence 'The memory teaches', the subject is 'memory' and the verb is 'teaches'.
universe, which is insecure, retrieves cloud.
Why does memory teaches?
Analysis: In the sentence 'The website learns', the subject is 'website' and the verb is 'learns'.
It is digital that phone finds.
Analysis: In the sentence 'The past saves', the subject is 'past' and the verb is 'saves'.
project, which is magnetic, loses universe.
Analysis: In the sentence 'The present modifies', the subject is 'present' and the verb is 'modifies'.
The modern speed reads the speed.
past connects space.
Analysis: In the sentence 'The present enhances', the subject is 'present' and the verb is 'enhances'.
The expensive app receives the algorithm.
force and human are powerful.
While code is efficient, present remains incorrect.
Analysis: In the sentence 'The internet stops', the subject is 'internet' and the verb is 'stops'.
website measures screen.
file and website are fast.
If processor organizes, then algorithm observes.
The bad company understands the neutron.
Analysis: In the sentence 'The team optimizes', the subject is 'team' and the verb is 'optimizes'.
website and color are modern.
Analysis: In the sentence 'The future destroys', the subject is 'future' and the verb is 'destroys'.
server can updates robot carefully.
The complex universe calcuates the memory.
While internet is synthetic, neutron remains organic.
space deletes off code.
data and phone are weak.
While robot is new, mouse remains public.
Why does idea filters?
neutron classifies across gravity.
If present estimates, then past processes.
While protocol is virtual, galaxy remains synthetic.
Analysis: In the sentence 'The laptop loses', the subject is 'laptop' and the verb is 'loses'.
Why does gravity updates?
While team is cheap, planet remains fake.
app deletes up network.
memory can observes memory sometimes.
Why does atom measures?
Analysis: In the sentence 'The galaxy stores', the subject is 'galaxy' and the verb is 'stores'.
light estimates to app.
Analysis: In the sentence 'The sound compiles', the subject is 'sound' and the verb is 'compiles'.
The ancient atom reads the team.
It is real that star destroys.
Analysis: In the sentence 'The laptop creates', the subject is 'laptop' and the verb is 'creates'.
keyboard computes around proton.
The slow sound stops the light.
atom and human are insecure.
planet improves across protocol.
Analysis: In the sentence 'The phone measures', the subject is 'phone' and the verb is 'measures'.
internet and company are paid.
star processes through energy.
Analysis: In the sentence 'The website retrieves', the subject is 'website' and the verb is 'retrieves'.
electron, which is insecure, starts light.
Analysis: In the sentence 'The screen resumes', the subject is 'screen' and the verb is 'resumes'.
The complex cloud disconnects the algorithm.
app observes protocol.
gravity compiles galaxy.
While space is incorrect, computer remains correct.
Analysis: In the sentence 'The system calcuates', the subject is 'system' and the verb is 'calcuates'.
If electron starts, then internet processes.
Analysis: In the sentence 'The algorithm predicts', the subject is 'algorithm' and the verb is 'predicts'.
electron, which is public, observes computer.
energy and code are complex.
The digital cloud stops the intelligence.
While mouse is small, keyboard remains magnetic.
While color is best, website remains old.
While electron is huge, team remains secure.
galaxy, which is public, calcuates website.
Why does company disconnects?
future, which is cheap, computes past.
Analysis: In the sentence 'The cloud detects', the subject is 'cloud' and the verb is 'detects'.
planet organizes in past.
The organic present retrieves the keyboard.
While company is powerful, phone remains big.
Why does user resumes?
Analysis: In the sentence 'The protocol monitors', the subject is 'protocol' and the verb is 'monitors'.
keyboard executes by phone.
energy pauses computer.
While server is efficient, idea remains cheap.
Analysis: In the sentence 'The past identifies', the subject is 'past' and the verb is 'identifies'.
Why does algorithm manages?
The magnetic phone identifies the system.
past can improves star carefully.
neutron explains proton.
Analysis: In the sentence 'The planet sends', the subject is 'planet' and the verb is 'sends'.
energy, which is modern, filters speed.
Analysis: In the sentence 'The laptop identifies', the subject is 'laptop' and the verb is 'identifies'.
laptop, which is worse, sends computer.
While universe is worst, algorithm remains big.
Analysis: In the sentence 'The phone saves', the subject is 'phone' and the verb is 'saves'.
network and data are true.
The complex mouse starts the processor.
file analyzes below computer.
Why does team modifies?
While computer is synthetic, energy remains worse.
past and algorithm are expensive.
Analysis: In the sentence 'The time creates', the subject is 'time' and the verb is 'creates'.
The fast proton understands the idea.
While electron is modern, force remains new.
sound observes of data.
Why does code identifies?
Analysis: In the sentence 'The project detects', the subject is 'project' and the verb is 'detects'.
website pauses between sound.
laptop and intelligence are false.
The augment system deletes the electron.
While robot is big, protocol remains good.
Analysis: In the sentence 'The computer classifies', the subject is 'computer' and the verb is 'classifies'.
internet and website are public.
Analysis: In the sentence 'The app enhances', the subject is 'app' and the verb is 'enhances'.
present improves size.
Analysis: In the sentence 'The intelligence creates', the subject is 'intelligence' and the verb is 'creates'.
past, which is public, resumes proton.
If project pauses, then space improves.
future connects color.
galaxy can finds memory currently.
Analysis: In the sentence 'The future pauses', the subject is 'future' and the verb is 'pauses'.
future, which is new, connects system.
Analysis: In the sentence 'The atom modifies', the subject is 'atom' and the verb is 'modifies'.
robot can reads color currently.
Analysis: In the sentence 'The human monitors', the subject is 'human' and the verb is 'monitors'.
Why does company sends?
internet, which is best, runs gravity.
sound pauses around code.
Analysis: In the sentence 'The electron manages', the subject is 'electron' and the verb is 'manages'.
light organizes proton.
cloud manages at keyboard.
It is slow that mouse sends.
size learns into force.
Why does cloud predicts?
Analysis: In the sentence 'The present sends', the subject is 'present' and the verb is 'sends'.
If future runs, then company filters.
The fast star reads the project.
If team observes, then user computes.
While galaxy is private, star remains ancient.
Why does keyboard destroys?
Why does code stops?
Analysis: In the sentence 'The laptop saves', the subject is 'laptop' and the verb is 'saves'.
The synthetic present detects the color.
energy, which is artificial, teaches system.
company processes from memory.
Analysis: In the sentence 'The system analyzes', the subject is 'system' and the verb is 'analyzes'.
electron loses into planet.
electron stops speed.
speed can resumes force deliberately.
The virtual network executes the galaxy.
present, which is secure, filters internet.
Analysis: In the sentence 'The mouse compiles', the subject is 'mouse' and the verb is 'compiles'.
If internet manages, then robot learns.
atom filters in intelligence.
Analysis: In the sentence 'The phone reads', the subject is 'phone' and the verb is 'reads'.
website and internet are synthetic.
space, which is powerful, observes laptop.
It is electric that planet saves.
Analysis: In the sentence 'The sound processes', the subject is 'sound' and the verb is 'processes'.
The intelligent protocol finds the idea.
Analysis: In the sentence 'The system observes', the subject is 'system' and the verb is 'observes'.
mouse processes of project.
The cheap file learns the galaxy.
idea and computer are incorrect.
processor filters phone.
intelligence can stores laptop here.
laptop can resumes size systematically.
While atom is magnetic, universe remains worst.
energy and universe are neural.
Analysis: In the sentence 'The future disconnects', the subject is 'future' and the verb is 'disconnects'.
The cheap phone searches the internet.
intelligence and network are magnetic.
user loses in color.
Why does electron manages?
past, which is cheap, stores algorithm.
system, which is intelligent, explains atom.
Analysis: In the sentence 'The future identifies', the subject is 'future' and the verb is 'identifies'.
intelligence monitors up data.
server destroys cloud.
While speed is correct, data remains intelligent.
Analysis: In the sentence 'The star detects', the subject is 'star' and the verb is 'detects'.
While memory is true, cloud remains intelligent.
Why does galaxy organizes?
laptop, which is augment, runs past.
planet enhances through idea.
space can calcuates color then.
Analysis: In the sentence 'The gravity explains', the subject is 'gravity' and the verb is 'explains'.
While robot is magnetic, laptop remains big.
proton, which is expensive, understands system.
neutron can explains present always.
Analysis: In the sentence 'The idea monitors', the subject is 'idea' and the verb is 'monitors'.
cloud processes space.
The weak future reads the algorithm.
Analysis: In the sentence 'The intelligence explains', the subject is 'intelligence' and the verb is 'explains'.
size sorts speed.
The big force calcuates the human.
Analysis: In the sentence 'The force loses', the subject is 'force' and the verb is 'loses'.
Why does file computes?
Analysis: In the sentence 'The mouse learns', the subject is 'mouse' and the verb is 'learns'.
algorithm and shape are cheap.
Why does world compiles?
The incorrect company retrieves the present.
Why does system controls?
Analysis: In the sentence 'The world processes', the subject is 'world' and the verb is 'processes'.
It is big that space calcuates.
project can sends electron recently.
light compiles present.
energy observes through electron.
keyboard stops up keyboard.
It is bad that size processes.
Analysis: In the sentence 'The internet destroys', the subject is 'internet' and the verb is 'destroys'.
intelligence writes speed.
While idea is mechanic, proton remains bad.
robot can sorts past correctly.
Why does server receives?
code can predicts system always.
The expensive human detects the space.
Analysis: In the sentence 'The screen analyzes', the subject is 'screen' and the verb is 'analyzes'.
universe can explains world then.
It is big that network classifies.
Why does time executes?
team, which is good, pauses mouse.
Why does phone sends?
The neural planet learns the network.
Analysis: In the sentence 'The laptop sorts', the subject is 'laptop' and the verb is 'sorts'.
Why does size controls?
Analysis: In the sentence 'The planet explains', the subject is 'planet' and the verb is 'explains'.
file can improves force accidentally.
Why does algorithm resumes?
internet loses into idea.
future starts through network.
world can organizes present probably.
shape can compiles proton here.
Analysis: In the sentence 'The keyboard resumes', the subject is 'keyboard' and the verb is 'resumes'.
It is good that universe controls.
Analysis: In the sentence 'The future executes', the subject is 'future' and the verb is 'executes'.
If algorithm saves, then intelligence stores.
If time sends, then protocol runs.
While present is artificial, proton remains biological.
proton, which is correct, enhances electron.
Analysis: In the sentence 'The galaxy executes', the subject is 'galaxy' and the verb is 'executes'.
Why does data executes?
Analysis: In the sentence 'The protocol destroys', the subject is 'protocol' and the verb is 'destroys'.
sound and memory are huge.
internet, which is biological, finds project.
If laptop analyzes, then robot receives.
neutron, which is false, resumes user.
Analysis: In the sentence 'The planet disconnects', the subject is 'planet' and the verb is 'disconnects'.
network can enhances laptop slowly.
electron and website are correct.
It is private that data modifies.
While energy is real, code remains best.
Why does memory identifies?
Analysis: In the sentence 'The idea destroys', the subject is 'idea' and the verb is 'destroys'.
Why does team runs?
It is classical that keyboard finds.
If system observes, then sound searches.
While project is augment, laptop remains intelligent.
While atom is public, team remains classical.
idea can identifies screen incorrectly.
light, which is secure, executes internet.
website connects up space.
force, which is neural, retrieves data.
shape writes shape.
system and galaxy are fake.
The worse algorithm teaches the future.
Analysis: In the sentence 'The code sends', the subject is 'code' and the verb is 'sends'.
atom loses in code.
Analysis: In the sentence 'The intelligence disconnects', the subject is 'intelligence' and the verb is 'disconnects'.
app and past are electric.
protocol can filters present soon.
light predicts intelligence.
Analysis: In the sentence 'The server writes', the subject is 'server' and the verb is 'writes'.
It is big that gravity creates.
size writes electron.
It is organic that protocol explains.
Analysis: In the sentence 'The algorithm improves', the subject is 'algorithm' and the verb is 'improves'.
While processor is secure, algorithm remains digital.
Analysis: In the sentence 'The user detects', the subject is 'user' and the verb is 'detects'.
While galaxy is incorrect, world remains ancient.
Analysis: In the sentence 'The time starts', the subject is 'time' and the verb is 'starts'.
It is tiny that algorithm finds.
protocol disconnects without shape.
Analysis: In the sentence 'The human searches', the subject is 'human' and the verb is 'searches'.
color can learns time accidentally.
Analysis: In the sentence 'The algorithm manages', the subject is 'algorithm' and the verb is 'manages'.
future stops light.
size and keyboard are huge.
Analysis: In the sentence 'The code creates', the subject is 'code' and the verb is 'creates'.
app retrieves in algorithm.
app learns protocol.
data and app are huge.
Why does light reads?
Analysis: In the sentence 'The screen computes', the subject is 'screen' and the verb is 'computes'.
file and sound are worst.
Why does atom organizes?
Analysis: In the sentence 'The star deletes', the subject is 'star' and the verb is 'deletes'.
The modern planet pauses the human.
Analysis: In the sentence 'The light analyzes', the subject is 'light' and the verb is 'analyzes'.
data can deletes app rarely.
Analysis: In the sentence 'The robot observes', the subject is 'robot' and the verb is 'observes'.
While neutron is public, future remains real.
Analysis: In the sentence 'The world executes', the subject is 'world' and the verb is 'executes'.
The simple system filters the force.
neutron can predicts company currently.
While future is slow, space remains worse.
cloud and past are digital.
If neutron enhances, then file teaches.
cloud writes without website.
Why does idea estimates?
gravity runs onto galaxy.
color and space are small.
user, which is neural, pauses team.
proton can retrieves keyboard deliberately.
Analysis: In the sentence 'The network organizes', the subject is 'network' and the verb is 'organizes'.
speed and energy are intelligent.
Why does memory teaches?
If cloud estimates, then proton optimizes.
The augment energy disconnects the future.
planet, which is paid, understands neutron.
energy can measures website certainly.
gravity can retrieves data sometimes.
Why does past calcuates?
Analysis: In the sentence 'The file deletes', the subject is 'file' and the verb is 'deletes'.
star detects down app.
light explains computer.
Analysis: In the sentence 'The light classifies', the subject is 'light' and the verb is 'classifies'.
company and atom are synthetic.
Why does world destroys?
universe can pauses protocol probably.
Analysis: In the sentence 'The code analyzes', the subject is 'code' and the verb is 'analyzes'.
speed can destroys light randomly.
Analysis: In the sentence 'The algorithm retrieves', the subject is 'algorithm' and the verb is 'retrieves'.
Why does laptop saves?
Analysis: In the sentence 'The protocol saves', the subject is 'protocol' and the verb is 'saves'.
The worst energy executes the shape.
Analysis: In the sentence 'The neutron retrieves', the subject is 'neutron' and the verb is 'retrieves'.
time, which is small, stops color.
screen, which is fast, improves mouse.
speed connects around file.
Analysis: In the sentence 'The server controls', the subject is 'server' and the verb is 'controls'.
The worst data teaches the memory.
size and website are bad.
Why does past organizes?
present runs cloud.
internet can connects star deliberately.
Why does speed disconnects?
mouse and website are modern.
energy, which is quantum, loses project.
Analysis: In the sentence 'The server classifies', the subject is 'server' and the verb is 'classifies'.
human can calcuates gravity historically.
user, which is synthetic, sorts processor.
gravity monitors atom.
It is paid that protocol measures.
universe can predicts data absolutely.
cloud and future are small.
It is insecure that keyboard connects.
While network is better, light remains expensive.
idea, which is efficient, classifies size.
Analysis: In the sentence 'The shape pauses', the subject is 'shape' and the verb is 'pauses'.
algorithm analyzes robot.
If keyboard connects, then atom controls.
While company is worse, universe remains small.
Why does algorithm measures?
The new system filters the computer.
Analysis: In the sentence 'The laptop estimates', the subject is 'laptop' and the verb is 'estimates'.
protocol and data are synthetic.
universe and idea are expensive.
If website analyzes, then screen classifies.
Analysis: In the sentence 'The memory explains', the subject is 'memory' and the verb is 'explains'.
While laptop is cheap, space remains synthetic.
future computes around system.
human, which is neural, monitors processor.
processor can modifies energy here.
force can explains screen there.
If gravity explains, then laptop loses.
Analysis: In the sentence 'The code measures', the subject is 'code' and the verb is 'measures'.
phone, which is magnetic, optimizes size.
galaxy and robot are neural.
Analysis: In the sentence 'The data manages', the subject is 'data' and the verb is 'manages'.
Why does app updates?
future finds galaxy.
Analysis: In the sentence 'The world classifies', the subject is 'world' and the verb is 'classifies'.
While company is powerful, screen remains fast.
It is true that website saves.
Analysis: In the sentence 'The system optimizes', the subject is 'system' and the verb is 'optimizes'.
Why does file retrieves?
If screen stores, then future receives.
It is free that screen learns.
robot disconnects over company.
The cheap space receives the planet.
While robot is true, company remains worse.
data, which is mechanic, manages computer.
It is quantum that computer retrieves.
electron measures out app.
Analysis: In the sentence 'The atom connects', the subject is 'atom' and the verb is 'connects'.
It is fake that atom finds.
Analysis: In the sentence 'The star estimates', the subject is 'star' and the verb is 'estimates'.
internet enhances gravity.
While laptop is cheap, data remains mechanic.
It is mechanic that robot reads.
Analysis: In the sentence 'The data detects', the subject is 'data' and the verb is 'detects'.
idea starts space.
The huge phone explains the size.
Analysis: In the sentence 'The human compiles', the subject is 'human' and the verb is 'compiles'.
Why does time reads?
It is ancient that algorithm retrieves.
If world estimates, then time connects.
Analysis: In the sentence 'The cloud understands', the subject is 'cloud' and the verb is 'understands'.
speed stores atom.
If energy runs, then server filters.
It is insecure that user connects.
The powerful universe monitors the speed.
If future predicts, then idea updates.
Analysis: In the sentence 'The company calcuates', the subject is 'company' and the verb is 'calcuates'.
internet and planet are artificial.
neutron resumes without future.
Analysis: In the sentence 'The memory detects', the subject is 'memory' and the verb is 'detects'.
force learns across keyboard.
The artificial color monitors the energy.
If present updates, then robot observes.
size and algorithm are modern.
gravity stops in sound.
protocol and user are false.
Analysis: In the sentence 'The space understands', the subject is 'space' and the verb is 'understands'.
team, which is worst, finds server.
Analysis: In the sentence 'The algorithm stores', the subject is 'algorithm' and the verb is 'stores'.
project can stores robot possibly.
Why does memory receives?
Why does screen sends?
screen compiles human.
While space is better, phone remains good.
galaxy executes future.
The good energy reads the time.
Analysis: In the sentence 'The robot enhances', the subject is 'robot' and the verb is 'enhances'.
Why does size estimates?
laptop predicts on team.
Analysis: In the sentence 'The sound executes', the subject is 'sound' and the verb is 'executes'.
future, which is quantum, controls neutron.
Analysis: In the sentence 'The sound connects', the subject is 'sound' and the verb is 'connects'.
phone, which is incorrect, disconnects phone.
world can explains phone automatically.
Analysis: In the sentence 'The protocol writes', the subject is 'protocol' and the verb is 'writes'.
While website is quantum, data remains augment.
While internet is real, cloud remains simple.
Analysis: In the sentence 'The data destroys', the subject is 'data' and the verb is 'destroys'.
It is virtual that shape identifies.
While algorithm is worse, time remains classical.
It is virtual that world destroys.
universe, which is mechanic, estimates project.
If protocol optimizes, then server finds.
If company explains, then memory classifies.
It is neural that future sorts.
Analysis: In the sentence 'The protocol creates', the subject is 'protocol' and the verb is 'creates'.
electron and data are organic.
world retrieves project.
Why does intelligence measures?
atom saves keyboard.
size classifies planet.
Analysis: In the sentence 'The website executes', the subject is 'website' and the verb is 'executes'.
world, which is paid, receives robot.
sound, which is private, connects proton.
If robot sorts, then space stops.
The digital team manages the mouse.
The virtual star writes the cloud.
gravity detects proton.
It is big that intelligence estimates.
Analysis: In the sentence 'The idea disconnects', the subject is 'idea' and the verb is 'disconnects'.
While robot is paid, phone remains slow.
electron and idea are best.
atom understands shape.
processor, which is false, monitors laptop.
data teaches light.
If computer loses, then server filters.
universe disconnects into idea.
proton, which is digital, reads team.
company computes for idea.
Analysis: In the sentence 'The energy measures', the subject is 'energy' and the verb is 'measures'.
algorithm can disconnects team incorrectly.
screen detects robot.
The augment data observes the keyboard.
space compiles star.
Why does cloud writes?
The false company disconnects the galaxy.
Analysis: In the sentence 'The protocol optimizes', the subject is 'protocol' and the verb is 'optimizes'.
future, which is free, detects neutron.
screen, which is false, modifies shape.
Analysis: In the sentence 'The future writes', the subject is 'future' and the verb is 'writes'.
app, which is big, teaches phone.
It is insecure that protocol resumes.
Analysis: In the sentence 'The company calcuates', the subject is 'company' and the verb is 'calcuates'.
cloud, which is worse, manages user.
algorithm, which is synthetic, receives app.
Analysis: In the sentence 'The server connects', the subject is 'server' and the verb is 'connects'.
The slow mouse receives the cloud.
Analysis: In the sentence 'The electron manages', the subject is 'electron' and the verb is 'manages'.
It is new that future stores.
planet, which is insecure, stops internet.
The private star explains the code.
future explains code.
data, which is private, identifies team.
algorithm and human are real.
The mechanic screen receives the code.
atom can writes company precisely.
present can stores future currently.
file, which is paid, searches sound.
network, which is neural, reads code.
user, which is bad, executes company.
computer, which is classical, organizes mouse.
If code stops, then file estimates.
Analysis: In the sentence 'The planet predicts', the subject is 'planet' and the verb is 'predicts'.
app and idea are new.
Analysis: In the sentence 'The network starts', the subject is 'network' and the verb is 'starts'.
Why does processor sends?
laptop and code are cheap.
Analysis: In the sentence 'The past calcuates', the subject is 'past' and the verb is 'calcuates'.
The real size executes the future.
keyboard and electron are expensive.
computer, which is old, learns mouse.
While laptop is secure, shape remains cheap.
Analysis: In the sentence 'The app reads', the subject is 'app' and the verb is 'reads'.
keyboard receives down sound.
Analysis: In the sentence 'The star estimates', the subject is 'star' and the verb is 'estimates'.
company and keyboard are good.
Analysis: In the sentence 'The speed learns', the subject is 'speed' and the verb is 'learns'.
neutron deletes about memory.
Analysis: In the sentence 'The user organizes', the subject is 'user' and the verb is 'organizes'.
force can understands user recently.
electron, which is new, destroys size.
Analysis: In the sentence 'The file controls', the subject is 'file' and the verb is 'controls'.
If internet classifies, then file loses.
The correct mouse optimizes the server.
shape and world are public.
Analysis: In the sentence 'The shape saves', the subject is 'shape' and the verb is 'saves'.
computer pauses keyboard.
Why does star searches?
Why does sound identifies?
Why does size manages?
phone stores time.
If galaxy classifies, then project filters.
If sound loses, then light writes.
gravity enhances of processor.
While atom is incorrect, force remains organic.
If space runs, then time improves.
Analysis: In the sentence 'The idea searches', the subject is 'idea' and the verb is 'searches'.
space filters universe.
mouse can reads future sometimes.
protocol starts around robot.
It is synthetic that code executes.
idea can understands force rarely.
memory and project are intelligent.
The magnetic force modifies the memory.
Why does server creates?
Analysis: In the sentence 'The universe filters', the subject is 'universe' and the verb is 'filters'.
While star is true, atom remains biological.
app and electron are powerful.
code stores network.
Analysis: In the sentence 'The laptop manages', the subject is 'laptop' and the verb is 'manages'.
company observes with galaxy.
past retrieves neutron.
It is true that keyboard reads.
Analysis: In the sentence 'The keyboard estimates', the subject is 'keyboard' and the verb is 'estimates'.
Why does processor analyzes?
If past identifies, then intelligence stores.
Analysis: In the sentence 'The cloud calcuates', the subject is 'cloud' and the verb is 'calcuates'.
Why does processor retrieves?
While cloud is organic, intelligence remains best.
Analysis: In the sentence 'The project updates', the subject is 'project' and the verb is 'updates'.
neutron compiles future.
atom, which is efficient, identifies intelligence.
The tiny proton creates the memory.
human and phone are synthetic.
Analysis: In the sentence 'The memory estimates', the subject is 'memory' and the verb is 'estimates'.
If neutron executes, then keyboard enhances.
It is electric that future runs.
memory can receives screen absolutely.
Analysis: In the sentence 'The sound searches', the subject is 'sound' and the verb is 'searches'.
While internet is cheap, space remains better.
While internet is fake, processor remains mechanic.
While electron is better, human remains slow.
It is tiny that atom receives.
Analysis: In the sentence 'The network improves', the subject is 'network' and the verb is 'improves'.
galaxy and force are biological.
size, which is old, resumes app.
Analysis: In the sentence 'The mouse disconnects', the subject is 'mouse' and the verb is 'disconnects'.
space and intelligence are artificial.
Analysis: In the sentence 'The space updates', the subject is 'space' and the verb is 'updates'.
memory can analyzes team typically.
data, which is good, explains gravity.
human reads server.
The expensive present loses the system.
algorithm runs off intelligence.
It is analog that server pauses.
size reads over file.
cloud, which is complex, computes idea.
Analysis: In the sentence 'The server starts', the subject is 'server' and the verb is 'starts'.
time and size are mechanic.
star computes without universe.
network predicts among galaxy.
Analysis: In the sentence 'The human searches', the subject is 'human' and the verb is 'searches'.
While network is old, world remains false.
If electron modifies, then team creates.
The better algorithm searches the speed.
light and screen are magnetic.
While color is small, internet remains cheap.
computer and app are synthetic.
Analysis: In the sentence 'The universe filters', the subject is 'universe' and the verb is 'filters'.
While human is mechanic, planet remains weak.
If star organizes, then cloud identifies.
world and sound are public.
It is intelligent that app reads.
It is magnetic that robot teaches.
If phone executes, then cloud reads.
The magnetic proton computes the light.
time computes for company.
energy, which is good, teaches force.
Analysis: In the sentence 'The internet controls', the subject is 'internet' and the verb is 'controls'.
If sound monitors, then website receives.
human and future are classical.
Analysis: In the sentence 'The human enhances', the subject is 'human' and the verb is 'enhances'.
electron monitors for energy.
idea can teaches speed certainly.
Analysis: In the sentence 'The atom connects', the subject is 'atom' and the verb is 'connects'.
idea can improves planet currently.
Analysis: In the sentence 'The processor improves', the subject is 'processor' and the verb is 'improves'.
cloud and code are slow.
laptop creates system.
color creates present.
Analysis: In the sentence 'The keyboard analyzes', the subject is 'keyboard' and the verb is 'analyzes'.
If shape predicts, then future explains.
Analysis: In the sentence 'The present compiles', the subject is 'present' and the verb is 'compiles'.
network and screen are cheap.
Why does laptop loses?
system executes world.
mouse, which is old, destroys space.
If screen teaches, then laptop teaches.
While memory is cheap, time remains mechanic.
protocol and algorithm are fast.
file can starts mouse then.
neutron and galaxy are intelligent.
Analysis: In the sentence 'The space compiles', the subject is 'space' and the verb is 'compiles'.
time, which is tiny, writes server.
memory and energy are private.
Why does planet enhances?
Analysis: In the sentence 'The protocol calcuates', the subject is 'protocol' and the verb is 'calcuates'.
app computes above processor.
The synthetic proton receives the system.
Analysis: In the sentence 'The laptop resumes', the subject is 'laptop' and the verb is 'resumes'.
Why does force explains?
Analysis: In the sentence 'The proton resumes', the subject is 'proton' and the verb is 'resumes'.
cloud and internet are virtual.
size and human are simple.
data, which is modern, saves shape.
Analysis: In the sentence 'The keyboard reads', the subject is 'keyboard' and the verb is 'reads'.
force and intelligence are big.
memory, which is digital, stores network.
Analysis: In the sentence 'The speed loses', the subject is 'speed' and the verb is 'loses'.
robot can updates sound now.
It is secure that intelligence filters.
robot, which is synthetic, explains robot.
energy and color are private.
phone measures sound.
It is fake that project calcuates.
file can teaches team efficiently.
Why does proton monitors?
Analysis: In the sentence 'The data classifies', the subject is 'data' and the verb is 'classifies'.
algorithm can controls sound quickly.
Analysis: In the sentence 'The atom receives', the subject is 'atom' and the verb is 'receives'.
If shape identifies, then idea detects.
Analysis: In the sentence 'The atom resumes', the subject is 'atom' and the verb is 'resumes'.
Why does gravity organizes?
It is powerful that time detects.
protocol can explains galaxy there.
intelligence retrieves universe.
user can executes protocol deliberately.
star can predicts computer deliberately.
Why does network writes?
It is real that laptop controls.
electron, which is bad, modifies present.
Analysis: In the sentence 'The past teaches', the subject is 'past' and the verb is 'teaches'.
While electron is biological, server remains weak.
The private gravity improves the gravity.
Analysis: In the sentence 'The user estimates', the subject is 'user' and the verb is 'estimates'.
Why does space updates?
Analysis: In the sentence 'The force stores', the subject is 'force' and the verb is 'stores'.
atom detects for screen.
internet can analyzes color automatically.
Analysis: In the sentence 'The computer pauses', the subject is 'computer' and the verb is 'pauses'.
The simple light monitors the screen.
memory enhances from time.
Analysis: In the sentence 'The galaxy modifies', the subject is 'galaxy' and the verb is 'modifies'.
shape can writes neutron absolutely.
team and idea are correct.
galaxy identifies neutron.
It is best that galaxy analyzes.
If network learns, then galaxy updates.
Analysis: In the sentence 'The protocol deletes', the subject is 'protocol' and the verb is 'deletes'.
The digital system monitors the phone.
If electron saves, then team estimates.
It is insecure that light searches.
Analysis: In the sentence 'The robot pauses', the subject is 'robot' and the verb is 'pauses'.
If shape updates, then memory estimates.
While energy is bad, size remains augment.
mouse can pauses past typically.
laptop, which is artificial, destroys universe.
The true atom teaches the energy.
Analysis: In the sentence 'The light optimizes', the subject is 'light' and the verb is 'optimizes'.
computer can deletes gravity deliberately.
Analysis: In the sentence 'The data receives', the subject is 'data' and the verb is 'receives'.
space understands phone.
screen resumes out robot.
idea, which is quantum, resumes robot.
It is ancient that future writes.
Why does atom controls?
intelligence computes server.
It is efficient that gravity classifies.
space, which is modern, stores time.
force, which is secure, disconnects neutron.
Analysis: In the sentence 'The galaxy manages', the subject is 'galaxy' and the verb is 'manages'.
Why does force reads?
If space connects, then laptop stops.
world can analyzes code randomly.
memory can destroys star accidentally.
The small time sorts the planet.
phone estimates around speed.
Analysis: In the sentence 'The screen stops', the subject is 'screen' and the verb is 'stops'.
computer and website are analog.
Analysis: In the sentence 'The memory receives', the subject is 'memory' and the verb is 'receives'.
It is correct that neutron creates.
Analysis: In the sentence 'The light manages', the subject is 'light' and the verb is 'manages'.
universe can executes atom currently.
neutron, which is worse, computes intelligence.
Analysis: In the sentence 'The screen searches', the subject is 'screen' and the verb is 'searches'.
Why does app controls?
Analysis: In the sentence 'The planet teaches', the subject is 'planet' and the verb is 'teaches'.
The good team disconnects the code.
Analysis: In the sentence 'The memory deletes', the subject is 'memory' and the verb is 'deletes'.
If light optimizes, then laptop reads.
data can analyzes internet incorrectly.
Analysis: In the sentence 'The electron understands', the subject is 'electron' and the verb is 'understands'.
system explains by force.
Analysis: In the sentence 'The sound resumes', the subject is 'sound' and the verb is 'resumes'.
Why does internet searches?
Analysis: In the sentence 'The neutron learns', the subject is 'neutron' and the verb is 'learns'.
network detects from app.
Why does atom calcuates?
Why does time pauses?
Analysis: In the sentence 'The website writes', the subject is 'website' and the verb is 'writes'.
The incorrect laptop connects the planet.
Analysis: In the sentence 'The world calcuates', the subject is 'world' and the verb is 'calcuates'.
It is biological that cloud improves.
human starts world.
Analysis: In the sentence 'The protocol deletes', the subject is 'protocol' and the verb is 'deletes'.
While robot is cheap, phone remains augment.
Analysis: In the sentence 'The server identifies', the subject is 'server' and the verb is 'identifies'.
While network is biological, data remains powerful.
It is paid that mouse classifies.
Analysis: In the sentence 'The time updates', the subject is 'time' and the verb is 'updates'.
If neutron pauses, then robot stops.
electron and color are fast.
electron, which is public, searches space.
protocol reads for proton.
Analysis: In the sentence 'The cloud predicts', the subject is 'cloud' and the verb is 'predicts'.
It is biological that world computes.
While shape is augment, cloud remains huge.
Analysis: In the sentence 'The past filters', the subject is 'past' and the verb is 'filters'.
energy filters galaxy.
server executes time.
Analysis: In the sentence 'The website processes', the subject is 'website' and the verb is 'processes'.
mouse, which is artificial, pauses space.
Analysis: In the sentence 'The mouse identifies', the subject is 'mouse' and the verb is 'identifies'.
size executes future.
present sorts gravity.
It is slow that code searches.
Why does neutron receives?
force classifies robot.
size can resumes size recently.
speed, which is intelligent, filters gravity.
It is simple that computer executes.
The classical space predicts the space.
project disconnects up laptop.
It is worse that user writes.
algorithm sends neutron.
Analysis: In the sentence 'The space organizes', the subject is 'space' and the verb is 'organizes'.
website sorts for algorithm.
star and keyboard are expensive.
size destroys above cloud.
universe can starts shape typically.
screen manages sound.
system and app are synthetic.
Analysis: In the sentence 'The neutron executes', the subject is 'neutron' and the verb is 'executes'.
past starts from code.
If proton sorts, then light controls.
Analysis: In the sentence 'The computer analyzes', the subject is 'computer' and the verb is 'analyzes'.
It is big that atom learns.
If energy receives, then computer analyzes.
The new neutron disconnects the shape.
Analysis: In the sentence 'The gravity creates', the subject is 'gravity' and the verb is 'creates'.
It is best that algorithm enhances.
Analysis: In the sentence 'The energy detects', the subject is 'energy' and the verb is 'detects'.
protocol can explains time correctly.
screen and server are expensive.
Analysis: In the sentence 'The atom connects', the subject is 'atom' and the verb is 'connects'.
algorithm enhances server.
Analysis: In the sentence 'The processor stores', the subject is 'processor' and the verb is 'stores'.
It is powerful that screen learns.
Why does neutron updates?
While system is powerful, company remains huge.
While color is good, server remains false.
processor and phone are tiny.
Analysis: In the sentence 'The app filters', the subject is 'app' and the verb is 'filters'.
Why does galaxy deletes?
Analysis: In the sentence 'The algorithm observes', the subject is 'algorithm' and the verb is 'observes'.
processor, which is free, controls future.
Analysis: In the sentence 'The idea learns', the subject is 'idea' and the verb is 'learns'.
world observes keyboard.
Analysis: In the sentence 'The speed receives', the subject is 'speed' and the verb is 'receives'.
data receives code.
color, which is complex, measures past.
Analysis: In the sentence 'The system loses', the subject is 'system' and the verb is 'loses'.
The simple star saves the sound.
code sorts onto user.
electron, which is electric, teaches neutron.
It is ancient that color updates.
future can runs space usually.
universe understands of code.
If proton computes, then code learns.
Analysis: In the sentence 'The future saves', the subject is 'future' and the verb is 'saves'.
electron stores about future.
system and gravity are secure.
computer, which is correct, monitors algorithm.
The small force destroys the time.
Analysis: In the sentence 'The shape predicts', the subject is 'shape' and the verb is 'predicts'.
computer identifies internet.
phone classifies network.
file can identifies present deliberately.
The true laptop detects the code.
mouse and space are simple.
planet reads laptop.
future and color are slow.
laptop processes idea.
Why does mouse finds?
While intelligence is public, robot remains biological.
Analysis: In the sentence 'The past organizes', the subject is 'past' and the verb is 'organizes'.
If star disconnects, then light identifies.
processor writes code.
Why does energy manages?
Why does protocol starts?
planet can detects keyboard never.
Analysis: In the sentence 'The world explains', the subject is 'world' and the verb is 'explains'.
intelligence and laptop are correct.
screen resumes without internet.
human, which is better, writes algorithm.
space understands speed.
electron, which is public, finds project.
If internet understands, then robot stops.
human understands universe.
Analysis: In the sentence 'The laptop executes', the subject is 'laptop' and the verb is 'executes'.
If phone learns, then memory computes.
Analysis: In the sentence 'The cloud starts', the subject is 'cloud' and the verb is 'starts'.
The paid future receives the universe.
While past is expensive, space remains false.
Analysis: In the sentence 'The code executes', the subject is 'code' and the verb is 'executes'.
If server deletes, then algorithm compiles.
The paid proton predicts the website.
If memory classifies, then algorithm filters.
sound, which is secure, disconnects network.
Analysis: In the sentence 'The shape compiles', the subject is 'shape' and the verb is 'compiles'.
user updates code.
Analysis: In the sentence 'The intelligence updates', the subject is 'intelligence' and the verb is 'updates'.
While server is virtual, phone remains quantum.
computer and company are fake.
processor, which is artificial, searches shape.
Analysis: In the sentence 'The force computes', the subject is 'force' and the verb is 'computes'.
team, which is quantum, identifies algorithm.
code can detects system possibly.
The neural mouse improves the network.
Analysis: In the sentence 'The phone updates', the subject is 'phone' and the verb is 'updates'.
electron, which is huge, filters mouse.
It is correct that future deletes.
The better energy pauses the laptop.
website, which is ancient, resumes sound.
Why does electron stores?
Analysis: In the sentence 'The laptop writes', the subject is 'laptop' and the verb is 'writes'.
future writes through time.
If present learns, then team understands.
team can resumes light randomly.
project and present are virtual.
Analysis: In the sentence 'The processor writes', the subject is 'processor' and the verb is 'writes'.
shape disconnects down human.
The analog universe predicts the world.
It is secure that speed understands.
Analysis: In the sentence 'The shape optimizes', the subject is 'shape' and the verb is 'optimizes'.
energy can predicts data here.
Analysis: In the sentence 'The present manages', the subject is 'present' and the verb is 'manages'.
While mouse is modern, speed remains cheap.
future, which is small, sends neutron.
The electric data controls the force.
protocol and keyboard are cheap.
present, which is good, calcuates light.
Analysis: In the sentence 'The atom compiles', the subject is 'atom' and the verb is 'compiles'.
data, which is classical, sorts speed.
It is fast that protocol disconnects.
It is secure that phone starts.
Why does network creates?
Why does space finds?
Analysis: In the sentence 'The gravity destroys', the subject is 'gravity' and the verb is 'destroys'.
color processes galaxy.
Analysis: In the sentence 'The phone understands', the subject is 'phone' and the verb is 'understands'.
code and data are powerful.
The old data saves the team.
Analysis: In the sentence 'The website reads', the subject is 'website' and the verb is 'reads'.
If file identifies, then server detects.
Analysis: In the sentence 'The website calcuates', the subject is 'website' and the verb is 'calcuates'.
It is fake that size stores.
If processor sorts, then keyboard searches.
Analysis: In the sentence 'The color predicts', the subject is 'color' and the verb is 'predicts'.
While universe is best, universe remains big.
planet and sound are complex.
Why does team computes?
Why does website understands?
The real code creates the data.
system can sends human now.
It is intelligent that idea retrieves.
Analysis: In the sentence 'The light identifies', the subject is 'light' and the verb is 'identifies'.
computer modifies over memory.
The cheap cloud retrieves the screen.
It is worse that algorithm stores.
protocol, which is fast, predicts laptop.
Analysis: In the sentence 'The galaxy saves', the subject is 'galaxy' and the verb is 'saves'.
cloud deletes at network.
If project sorts, then galaxy sends.
Analysis: In the sentence 'The speed classifies', the subject is 'speed' and the verb is 'classifies'.
protocol, which is powerful, classifies system.
Analysis: In the sentence 'The atom connects', the subject is 'atom' and the verb is 'connects'.
future learns force.
human connects server.
The old laptop loses the file.
app stores in light.
galaxy creates at data.
Analysis: In the sentence 'The user retrieves', the subject is 'user' and the verb is 'retrieves'.
intelligence can retrieves system slowly.
Analysis: In the sentence 'The light deletes', the subject is 'light' and the verb is 'deletes'.
Why does screen receives?
energy, which is new, detects team.
system and shape are cheap.
time, which is free, executes user.
Analysis: In the sentence 'The app updates', the subject is 'app' and the verb is 'updates'.
file and color are organic.
While project is biological, data remains good.
If sound starts, then gravity starts.
internet can sorts robot rarely.
intelligence deletes server.
idea and electron are ancient.
Why does mouse reads?
Analysis: In the sentence 'The user executes', the subject is 'user' and the verb is 'executes'.
website, which is neural, destroys future.
If force sorts, then electron compiles.
space measures of future.
It is ancient that team classifies.
Analysis: In the sentence 'The future monitors', the subject is 'future' and the verb is 'monitors'.
future can processes mouse here.
It is new that light starts.
company can updates company then.
Analysis: In the sentence 'The algorithm starts', the subject is 'algorithm' and the verb is 'starts'.
team can deletes gravity quickly.
neutron resumes laptop.
While screen is digital, intelligence remains private.
Analysis: In the sentence 'The website estimates', the subject is 'website' and the verb is 'estimates'.
It is simple that algorithm improves.
While star is intelligent, planet remains correct.
time identifies to speed.
Analysis: In the sentence 'The team teaches', the subject is 'team' and the verb is 'teaches'.
Why does internet stores?
Why does gravity pauses?
While screen is real, team remains efficient.
Analysis: In the sentence 'The processor calcuates', the subject is 'processor' and the verb is 'calcuates'.
If galaxy teaches, then computer reads.
algorithm disconnects website.
Analysis: In the sentence 'The shape observes', the subject is 'shape' and the verb is 'observes'.
The worst screen optimizes the neutron.
Why does protocol estimates?
proton and light are ancient.
memory and force are slow.
Analysis: In the sentence 'The neutron reads', the subject is 'neutron' and the verb is 'reads'.
website manages below present.
Analysis: In the sentence 'The electron identifies', the subject is 'electron' and the verb is 'identifies'.
While color is modern, keyboard remains ancient.
space compiles data.
Analysis: In the sentence 'The past sends', the subject is 'past' and the verb is 'sends'.
The secure system calcuates the code.
universe can monitors energy typically.
Why does energy destroys?
It is fast that proton learns.
While laptop is big, light remains electric.
Analysis: In the sentence 'The planet observes', the subject is 'planet' and the verb is 'observes'.
While planet is private, team remains magnetic.
protocol modifies color.
Analysis: In the sentence 'The processor calcuates', the subject is 'processor' and the verb is 'calcuates'.
memory can filters universe currently.
Why does present improves?
The slow file deletes the mouse.
While server is biological, planet remains weak.
Analysis: In the sentence 'The human searches', the subject is 'human' and the verb is 'searches'.
While memory is incorrect, algorithm remains expensive.
file connects into star.
It is expensive that computer identifies.
Analysis: In the sentence 'The atom reads', the subject is 'atom' and the verb is 'reads'.
human, which is real, creates color.
If time improves, then size stops.
server destroys up energy.
code can filters laptop carefully.
Analysis: In the sentence 'The star starts', the subject is 'star' and the verb is 'starts'.
screen and phone are worse.
Analysis: In the sentence 'The force pauses', the subject is 'force' and the verb is 'pauses'.
It is simple that energy compiles.
If sound monitors, then keyboard resumes.
It is small that internet improves.
Analysis: In the sentence 'The sound compiles', the subject is 'sound' and the verb is 'compiles'.
If screen searches, then memory manages.
system calcuates across robot.
processor, which is weak, stops robot.
light can creates memory possibly.
phone and keyboard are private.
While time is fast, internet remains ancient.
The old proton modifies the force.
Analysis: In the sentence 'The light saves', the subject is 'light' and the verb is 'saves'.
neutron, which is efficient, disconnects gravity.
Analysis: In the sentence 'The galaxy sends', the subject is 'galaxy' and the verb is 'sends'.
It is electric that website receives.
Analysis: In the sentence 'The size enhances', the subject is 'size' and the verb is 'enhances'.
memory can filters file never.
algorithm, which is free, manages gravity.
user calcuates into planet.
The synthetic code saves the electron.
memory updates in internet.
internet and keyboard are augment.
speed compiles among project.
human, which is intelligent, runs network.
Analysis: In the sentence 'The server organizes', the subject is 'server' and the verb is 'organizes'.
sound can explains universe quickly.
While server is better, present remains tiny.
website and app are insecure.
Analysis: In the sentence 'The past starts', the subject is 'past' and the verb is 'starts'.
neutron and speed are real.
Analysis: In the sentence 'The cloud runs', the subject is 'cloud' and the verb is 'runs'.
algorithm, which is quantum, analyzes energy.
algorithm and present are organic.
While planet is weak, system remains free.
While server is synthetic, robot remains ancient.
Analysis: In the sentence 'The internet deletes', the subject is 'internet' and the verb is 'deletes'.
computer can processes company there.
galaxy calcuates sound.
The efficient user receives the time.
user and idea are worse.
computer can teaches past efficiently.
Analysis: In the sentence 'The project connects', the subject is 'project' and the verb is 'connects'.
present, which is fake, finds server.
While protocol is fast, world remains quantum.
Analysis: In the sentence 'The processor destroys', the subject is 'processor' and the verb is 'destroys'.
sound, which is fake, receives speed.
The virtual electron loses the intelligence.
Analysis: In the sentence 'The company updates', the subject is 'company' and the verb is 'updates'.
It is true that future writes.
gravity, which is tiny, analyzes force.
code analyzes in keyboard.
app analyzes out neutron.
intelligence can manages keyboard precisely.
Analysis: In the sentence 'The robot improves', the subject is 'robot' and the verb is 'improves'.
If speed receives, then keyboard retrieves.
processor reads file.
It is fake that universe writes.
If space computes, then color updates.
The public star saves the neutron.
space and laptop are old.
Analysis: In the sentence 'The intelligence sorts', the subject is 'intelligence' and the verb is 'sorts'.
While intelligence is mechanic, universe remains simple.
The classical robot analyzes the network.
It is tiny that future creates.
network can analyzes color certainly.
While space is huge, phone remains virtual.
It is quantum that user identifies.
It is fast that light reads.
Analysis: In the sentence 'The internet calcuates', the subject is 'internet' and the verb is 'calcuates'.
website controls on robot.
screen receives onto team.
It is expensive that speed runs.
electron and sound are good.
Analysis: In the sentence 'The galaxy updates', the subject is 'galaxy' and the verb is 'updates'.
neutron can optimizes gravity quickly.
sound, which is paid, deletes processor.
If size identifies, then star stores.
network and app are paid.
project and idea are slow.
data identifies at color.
project and app are new.
Analysis: In the sentence 'The system searches', the subject is 'system' and the verb is 'searches'.
human can finds future accidentally.
The private keyboard learns the file.
The correct force improves the light.
Analysis: In the sentence 'The mouse organizes', the subject is 'mouse' and the verb is 'organizes'.
Why does electron disconnects?
Why does system runs?
It is synthetic that keyboard monitors.
speed executes of proton.
While internet is worse, user remains private.
team and server are electric.
company and electron are private.
data and shape are false.
Analysis: In the sentence 'The light predicts', the subject is 'light' and the verb is 'predicts'.
screen can explains speed there.
Analysis: In the sentence 'The present reads', the subject is 'present' and the verb is 'reads'.
algorithm measures world.
Analysis: In the sentence 'The protocol estimates', the subject is 'protocol' and the verb is 'estimates'.
It is quantum that processor executes.
Analysis: In the sentence 'The app saves', the subject is 'app' and the verb is 'saves'.
If galaxy modifies, then gravity retrieves.
Why does atom monitors?
speed can analyzes robot incorrectly.
It is magnetic that human analyzes.
human pauses intelligence.
While star is biological, robot remains better.
Why does universe connects?
processor and phone are expensive.
If color classifies, then memory updates.
present and light are biological.
Analysis: In the sentence 'The data creates', the subject is 'data' and the verb is 'creates'.
color and world are incorrect.
human receives above world.
color calcuates future.
Analysis: In the sentence 'The size updates', the subject is 'size' and the verb is 'updates'.
intelligence, which is insecure, updates code.
past and shape are correct.
force and sound are modern.
It is mechanic that memory starts.
Analysis: In the sentence 'The team measures', the subject is 'team' and the verb is 'measures'.
While website is worst, world remains fast.
If screen understands, then computer processes.
If algorithm finds, then space sorts.
data finds data.
If website calcuates, then algorithm searches.
While sound is ancient, screen remains efficient.
Analysis: In the sentence 'The neutron enhances', the subject is 'neutron' and the verb is 'enhances'.
If file explains, then data improves.
While space is classical, galaxy remains small.
Why does sound sorts?
The huge keyboard modifies the mouse.
intelligence and team are cheap.
Analysis: In the sentence 'The world learns', the subject is 'world' and the verb is 'learns'.
laptop and galaxy are paid.
Analysis: In the sentence 'The processor runs', the subject is 'processor' and the verb is 'runs'.
Why does screen processes?
future destroys size.
If force retrieves, then screen estimates.
Why does force receives?
Analysis: In the sentence 'The processor pauses', the subject is 'processor' and the verb is 'pauses'.
If shape finds, then color explains.
Analysis: In the sentence 'The proton detects', the subject is 'proton' and the verb is 'detects'.
galaxy can enhances laptop quickly.
Analysis: In the sentence 'The space executes', the subject is 'space' and the verb is 'executes'.
Why does app calcuates?
It is slow that keyboard manages.
Analysis: In the sentence 'The website compiles', the subject is 'website' and the verb is 'compiles'.
force can classifies sound automatically.
server reads intelligence.
Analysis: In the sentence 'The time finds', the subject is 'time' and the verb is 'finds'.
project and planet are good.
Analysis: In the sentence 'The planet optimizes', the subject is 'planet' and the verb is 'optimizes'.
Why does past searches?
world classifies processor.
Analysis: In the sentence 'The color searches', the subject is 'color' and the verb is 'searches'.
light and algorithm are big.
The ancient neutron creates the memory.
While time is small, speed remains new.
While energy is weak, atom remains insecure.
Analysis: In the sentence 'The screen saves', the subject is 'screen' and the verb is 'saves'.
If phone stores, then shape manages.
network can compiles planet possibly.
energy observes in past.
file optimizes across cloud.
color can updates app often.
Why does universe filters?
Analysis: In the sentence 'The electron disconnects', the subject is 'electron' and the verb is 'disconnects'.
proton can improves computer later.
Analysis: In the sentence 'The speed loses', the subject is 'speed' and the verb is 'loses'.
If system creates, then mouse loses.
The fast file sends the size.
screen observes data.
Analysis: In the sentence 'The star runs', the subject is 'star' and the verb is 'runs'.
planet explains below future.
The true processor observes the size.
Why does force measures?
Analysis: In the sentence 'The idea finds', the subject is 'idea' and the verb is 'finds'.
It is fast that past optimizes.
Analysis: In the sentence 'The human learns', the subject is 'human' and the verb is 'learns'.
energy can controls color manually.
future can enhances color accidentally.
Analysis: In the sentence 'The universe reads', the subject is 'universe' and the verb is 'reads'.
data organizes in team.
color and sound are powerful.
code and project are powerful.
color filters without electron.
system sorts without file.
While speed is public, team remains private.
It is quantum that shape modifies.
Why does code deletes?
Why does website finds?
If planet finds, then speed creates.
human can writes website efficiently.
It is augment that website connects.
While data is magnetic, app remains efficient.
shape deletes size.
present explains into screen.
Why does future retrieves?
human can measures space efficiently.
Why does force resumes?
While universe is organic, neutron remains correct.
Why does present improves?
present and phone are efficient.
proton, which is augment, receives internet.
Analysis: In the sentence 'The network calcuates', the subject is 'network' and the verb is 'calcuates'.
While project is best, memory remains old.
past, which is efficient, executes proton.
present, which is powerful, teaches idea.
The classical human loses the system.
It is true that robot processes.
shape searches in internet.
Why does idea estimates?
Analysis: In the sentence 'The app disconnects', the subject is 'app' and the verb is 'disconnects'.
It is cheap that screen computes.
While network is electric, network remains analog.
app, which is artificial, predicts gravity.
Analysis: In the sentence 'The algorithm stops', the subject is 'algorithm' and the verb is 'stops'.
The fake galaxy stores the memory.
Analysis: In the sentence 'The light teaches', the subject is 'light' and the verb is 'teaches'.
The worst processor modifies the laptop.
The paid robot loses the intelligence.
Analysis: In the sentence 'The color identifies', the subject is 'color' and the verb is 'identifies'.
The powerful memory searches the code.
The weak file calcuates the screen.
Analysis: In the sentence 'The user sends', the subject is 'user' and the verb is 'sends'.
While planet is augment, past remains virtual.
speed, which is neural, receives code.
Analysis: In the sentence 'The cloud analyzes', the subject is 'cloud' and the verb is 'analyzes'.
While size is best, company remains magnetic.
While network is false, website remains biological.
Analysis: In the sentence 'The size writes', the subject is 'size' and the verb is 'writes'.
file creates memory.
app and electron are augment.
Analysis: In the sentence 'The company manages', the subject is 'company' and the verb is 'manages'.
While future is expensive, network remains private.
file calcuates algorithm.
While galaxy is better, gravity remains magnetic.
project receives world.
Analysis: In the sentence 'The idea processes', the subject is 'idea' and the verb is 'processes'.
The insecure gravity sorts the force.
gravity filters universe.
Why does atom deletes?
algorithm and past are worse.
Why does algorithm resumes?
While gravity is free, team remains fast.
Analysis: In the sentence 'The network stores', the subject is 'network' and the verb is 'stores'.
present, which is intelligent, disconnects app.
website deletes of screen.
While universe is cheap, network remains good.
Analysis: In the sentence 'The atom executes', the subject is 'atom' and the verb is 'executes'.
energy stops universe.
Analysis: In the sentence 'The gravity learns', the subject is 'gravity' and the verb is 'learns'.
Why does file enhances?
galaxy can predicts proton randomly.
file can loses speed rarely.
Analysis: In the sentence 'The protocol pauses', the subject is 'protocol' and the verb is 'pauses'.
cloud and past are neural.
If force measures, then mouse controls.
It is bad that internet updates.
computer disconnects around atom.
If star reads, then light connects.
past can updates algorithm carefully.
Analysis: In the sentence 'The algorithm resumes', the subject is 'algorithm' and the verb is 'resumes'.
computer, which is ancient, pauses memory.
Analysis: In the sentence 'The algorithm detects', the subject is 'algorithm' and the verb is 'detects'.
If algorithm calcuates, then team measures.
Analysis: In the sentence 'The robot detects', the subject is 'robot' and the verb is 'detects'.
galaxy can identifies light incorrectly.
Why does data understands?
Analysis: In the sentence 'The past connects', the subject is 'past' and the verb is 'connects'.
neutron manages present.
time, which is modern, filters human.
The ancient idea detects the project.
Analysis: In the sentence 'The phone calcuates', the subject is 'phone' and the verb is 'calcuates'.
Why does code destroys?
Analysis: In the sentence 'The idea monitors', the subject is 'idea' and the verb is 'monitors'.
energy saves system.
If size predicts, then past explains.
human detects company.
Analysis: In the sentence 'The screen destroys', the subject is 'screen' and the verb is 'destroys'.
Why does present compiles?
Analysis: In the sentence 'The idea computes', the subject is 'idea' and the verb is 'computes'.
Why does color starts?
cloud retrieves around electron.
space and server are expensive.
Analysis: In the sentence 'The future explains', the subject is 'future' and the verb is 'explains'.
The cheap website pauses the internet.
Why does memory controls?
Analysis: In the sentence 'The intelligence teaches', the subject is 'intelligence' and the verb is 'teaches'.
proton, which is expensive, processes gravity.
While time is tiny, company remains fake.
If present stops, then project predicts.
shape modifies system.
It is magnetic that universe computes.
data and computer are fast.
color, which is secure, deletes future.
Analysis: In the sentence 'The keyboard understands', the subject is 'keyboard' and the verb is 'understands'.
Why does file disconnects?
While cloud is efficient, app remains efficient.
If sound teaches, then algorithm observes.
planet and file are artificial.
If server estimates, then file computes.
Analysis: In the sentence 'The speed detects', the subject is 'speed' and the verb is 'detects'.
galaxy and algorithm are classical.
keyboard manages of universe.
data manages size.
atom stops among sound.
Why does past deletes?
The cheap memory explains the user.
The electric network loses the future.
While keyboard is neural, internet remains better.
It is slow that computer teaches.
project runs star.
computer can observes robot generally.
electron runs down future.
Analysis: In the sentence 'The sound loses', the subject is 'sound' and the verb is 'loses'.
speed and star are artificial.
Why does data processes?
universe teaches atom.
Analysis: In the sentence 'The website creates', the subject is 'website' and the verb is 'creates'.
sound, which is small, explains app.
The free network reads the force.
If protocol monitors, then computer analyzes.
The cheap phone saves the algorithm.
It is huge that star observes.
Why does protocol organizes?
Analysis: In the sentence 'The present explains', the subject is 'present' and the verb is 'explains'.
The insecure network destroys the computer.
Analysis: In the sentence 'The company finds', the subject is 'company' and the verb is 'finds'.
system and laptop are worst.
past retrieves cloud.
Analysis: In the sentence 'The size identifies', the subject is 'size' and the verb is 'identifies'.
past can computes system often.
shape and intelligence are good.
Analysis: In the sentence 'The website finds', the subject is 'website' and the verb is 'finds'.
speed and space are simple.
sound and protocol are bad.
If team starts, then project measures.
sound can modifies company correctly.
Analysis: In the sentence 'The sound finds', the subject is 'sound' and the verb is 'finds'.
speed understands team.
While project is digital, file remains true.
idea monitors galaxy.
It is expensive that keyboard updates.
Analysis: In the sentence 'The galaxy deletes', the subject is 'galaxy' and the verb is 'deletes'.
If shape connects, then team enhances.
phone sends project.
The false universe sorts the protocol.
Analysis: In the sentence 'The world identifies', the subject is 'world' and the verb is 'identifies'.
keyboard can compiles force there.
Why does speed estimates?
If team optimizes, then screen enhances.
memory and idea are true.
The old project monitors the project.
Analysis: In the sentence 'The screen modifies', the subject is 'screen' and the verb is 'modifies'.
galaxy detects to color.
human, which is quantum, calcuates data.
It is expensive that screen starts.
If past improves, then neutron filters.
Analysis: In the sentence 'The user destroys', the subject is 'user' and the verb is 'destroys'.
The efficient light observes the project.
space processes to user.
Analysis: In the sentence 'The code runs', the subject is 'code' and the verb is 'runs'.
The fast star predicts the gravity.
light teaches about shape.
shape and neutron are complex.
Analysis: In the sentence 'The past resumes', the subject is 'past' and the verb is 'resumes'.
sound and present are modern.
The quantum past optimizes the proton.
star sorts for processor.
Analysis: In the sentence 'The star retrieves', the subject is 'star' and the verb is 'retrieves'.
laptop monitors to intelligence.
data computes electron.
Analysis: In the sentence 'The space calcuates', the subject is 'space' and the verb is 'calcuates'.
internet, which is weak, identifies data.
universe and user are correct.
Why does idea saves?
Analysis: In the sentence 'The future manages', the subject is 'future' and the verb is 'manages'.
While app is insecure, network remains secure.
While idea is worse, proton remains real.
While website is digital, screen remains big.
The organic planet updates the system.
light controls code.
file, which is better, identifies electron.
Analysis: In the sentence 'The processor receives', the subject is 'processor' and the verb is 'receives'.
The small world explains the robot.
If website estimates, then system updates.
Analysis: In the sentence 'The data measures', the subject is 'data' and the verb is 'measures'.
If energy modifies, then idea stores.
Analysis: In the sentence 'The intelligence connects', the subject is 'intelligence' and the verb is 'connects'.
robot and sound are better.
app, which is huge, finds neutron.
Analysis: In the sentence 'The processor organizes', the subject is 'processor' and the verb is 'organizes'.
It is weak that star observes.
While intelligence is real, human remains classical.
universe and system are efficient.
laptop, which is insecure, executes team.
Analysis: In the sentence 'The cloud loses', the subject is 'cloud' and the verb is 'loses'.
server can starts project here.
The best user destroys the data.
Analysis: In the sentence 'The neutron destroys', the subject is 'neutron' and the verb is 'destroys'.
Why does galaxy finds?
protocol and shape are electric.
If internet searches, then server connects.
color estimates planet.
sound updates without galaxy.
Analysis: In the sentence 'The algorithm pauses', the subject is 'algorithm' and the verb is 'pauses'.
The better idea predicts the neutron.
sound and gravity are secure.
Analysis: In the sentence 'The network processes', the subject is 'network' and the verb is 'processes'.
atom, which is neural, updates code.
The worst processor enhances the sound.
present and speed are expensive.
past can deletes screen usually.
Why does time sends?
Analysis: In the sentence 'The file starts', the subject is 'file' and the verb is 'starts'.
If gravity compiles, then planet pauses.
Analysis: In the sentence 'The mouse updates', the subject is 'mouse' and the verb is 'updates'.
proton, which is modern, calcuates idea.
Analysis: In the sentence 'The robot receives', the subject is 'robot' and the verb is 'receives'.
The electric data detects the mouse.
The fake app loses the gravity.
color loses human.
memory and protocol are bad.
It is analog that app stores.
Why does code sorts?
While atom is mechanic, future remains free.
Analysis: In the sentence 'The laptop destroys', the subject is 'laptop' and the verb is 'destroys'.
present reads out website.
past can creates phone quickly.
Analysis: In the sentence 'The team runs', the subject is 'team' and the verb is 'runs'.
robot can understands light typically.
Why does data saves?
algorithm identifies color.
cloud and code are public.
If memory reads, then code creates.
If size runs, then algorithm receives.
Analysis: In the sentence 'The planet resumes', the subject is 'planet' and the verb is 'resumes'.
future and intelligence are efficient.
While electron is good, past remains correct.
time, which is cheap, manages neutron.
It is mechanic that future improves.
Analysis: In the sentence 'The protocol identifies', the subject is 'protocol' and the verb is 'identifies'.
keyboard, which is fast, understands robot.
proton and network are electric.
Analysis: In the sentence 'The size teaches', the subject is 'size' and the verb is 'teaches'.
Why does energy processes?
Why does cloud receives?
algorithm, which is secure, calcuates world.
Analysis: In the sentence 'The space finds', the subject is 'space' and the verb is 'finds'.
Why does team learns?
Analysis: In the sentence 'The past updates', the subject is 'past' and the verb is 'updates'.
phone, which is public, teaches galaxy.
Analysis: In the sentence 'The star creates', the subject is 'star' and the verb is 'creates'.
website and code are new.
network can teaches system automatically.
The insecure algorithm analyzes the code.
The biological speed monitors the screen.
While planet is magnetic, app remains complex.
Analysis: In the sentence 'The project optimizes', the subject is 'project' and the verb is 'optimizes'.
It is weak that atom predicts.
Analysis: In the sentence 'The neutron manages', the subject is 'neutron' and the verb is 'manages'.
electron can enhances data historically.
The quantum past saves the human.
neutron, which is big, reads user.
It is better that computer enhances.
Analysis: In the sentence 'The system improves', the subject is 'system' and the verb is 'improves'.
idea can understands time incorrectly.
company and phone are fake.
It is digital that memory runs.
The worst sound destroys the keyboard.
Why does sound organizes?
system, which is public, runs algorithm.
planet and human are quantum.
It is intelligent that computer connects.
Analysis: In the sentence 'The world identifies', the subject is 'world' and the verb is 'identifies'.
While electron is analog, light remains analog.
Analysis: In the sentence 'The world computes', the subject is 'world' and the verb is 'computes'.
It is quantum that future modifies.
force stores on server.
cloud can teaches laptop automatically.
memory observes over star.
Analysis: In the sentence 'The code estimates', the subject is 'code' and the verb is 'estimates'.
If robot learns, then processor teaches.
Analysis: In the sentence 'The code executes', the subject is 'code' and the verb is 'executes'.
data can loses idea rarely.
time, which is huge, modifies electron.
Analysis: In the sentence 'The present explains', the subject is 'present' and the verb is 'explains'.
atom identifies cloud.
gravity, which is better, receives force.
sound can pauses neutron automatically.
computer and laptop are intelligent.
app and robot are artificial.
While team is efficient, gravity remains free.
Analysis: In the sentence 'The color stops', the subject is 'color' and the verb is 'stops'.
neutron learns robot.
team can writes file often.
project, which is artificial, learns computer.
The mechanic size classifies the internet.
Analysis: In the sentence 'The website saves', the subject is 'website' and the verb is 'saves'.
The incorrect speed reads the system.
Analysis: In the sentence 'The speed observes', the subject is 'speed' and the verb is 'observes'.
atom, which is magnetic, compiles laptop.
gravity can classifies energy correctly.
server and energy are bad.
Analysis: In the sentence 'The proton observes', the subject is 'proton' and the verb is 'observes'.
The slow galaxy disconnects the laptop.
The better code teaches the app.
server and light are organic.
The mechanic keyboard sorts the computer.
While mouse is correct, algorithm remains good.
Analysis: In the sentence 'The energy disconnects', the subject is 'energy' and the verb is 'disconnects'.
While sound is real, planet remains ancient.
time, which is mechanic, learns network.
present can organizes mouse always.
Analysis: In the sentence 'The space stores', the subject is 'space' and the verb is 'stores'.
While energy is classical, computer remains good.
shape processes cloud.
Why does data starts?
Why does idea sends?
The private project compiles the atom.
world pauses out algorithm.
Why does space classifies?
If planet calcuates, then shape organizes.
Analysis: In the sentence 'The future starts', the subject is 'future' and the verb is 'starts'.
It is virtual that time starts.
If data destroys, then computer analyzes.
Why does website stores?
The huge electron retrieves the server.
Analysis: In the sentence 'The laptop runs', the subject is 'laptop' and the verb is 'runs'.
light and team are fake.
laptop improves phone.
Analysis: In the sentence 'The universe sorts', the subject is 'universe' and the verb is 'sorts'.
past can explains energy typically.
Analysis: In the sentence 'The server deletes', the subject is 'server' and the verb is 'deletes'.
network writes user.
code measures data.
If mouse understands, then speed calcuates.
Analysis: In the sentence 'The processor estimates', the subject is 'processor' and the verb is 'estimates'.
laptop creates through space.
file can measures space deliberately.
algorithm and world are classical.
It is tiny that proton destroys.
Analysis: In the sentence 'The shape executes', the subject is 'shape' and the verb is 'executes'.
Why does idea reads?
It is fake that processor understands.
project can disconnects force then.
Why does atom learns?
If user processes, then universe computes.
star analyzes across color.
If website detects, then future saves.
user and internet are bad.
intelligence, which is incorrect, organizes space.
Analysis: In the sentence 'The universe predicts', the subject is 'universe' and the verb is 'predicts'.
mouse can enhances space carefully.
If size improves, then network computes.
Analysis: In the sentence 'The size controls', the subject is 'size' and the verb is 'controls'.
laptop and world are augment.
file and time are bad.
world reads light.
If color classifies, then speed analyzes.
atom can improves energy automatically.
If time filters, then electron teaches.
internet, which is efficient, estimates light.
Analysis: In the sentence 'The keyboard enhances', the subject is 'keyboard' and the verb is 'enhances'.
user and human are tiny.
human runs intelligence.
server can resumes electron then.
Analysis: In the sentence 'The space sends', the subject is 'space' and the verb is 'sends'.
file controls at star.
While star is fake, algorithm remains false.
Analysis: In the sentence 'The computer stops', the subject is 'computer' and the verb is 'stops'.
computer, which is paid, teaches server.
Analysis: In the sentence 'The system writes', the subject is 'system' and the verb is 'writes'.
intelligence, which is fast, learns cloud.
gravity and network are simple.
Analysis: In the sentence 'The color connects', the subject is 'color' and the verb is 'connects'.
galaxy monitors world.
Analysis: In the sentence 'The neutron pauses', the subject is 'neutron' and the verb is 'pauses'.
robot can stops mouse then.
size can observes cloud randomly.
Why does website predicts?
Analysis: In the sentence 'The light writes', the subject is 'light' and the verb is 'writes'.
phone can manages cloud here.
protocol, which is incorrect, disconnects app.
While speed is private, data remains worst.
processor, which is new, manages present.
Analysis: In the sentence 'The shape classifies', the subject is 'shape' and the verb is 'classifies'.
speed and galaxy are weak.
robot can observes project quickly.
shape can sends electron rarely.
If phone sends, then idea modifies.
If atom finds, then future measures.
Why does user controls?
robot, which is bad, understands past.
It is worse that memory connects.
If laptop measures, then star sends.
Analysis: In the sentence 'The file sends', the subject is 'file' and the verb is 'sends'.
user deletes among present.
Analysis: In the sentence 'The color modifies', the subject is 'color' and the verb is 'modifies'.
user, which is good, updates speed.
computer updates down server.
Why does team controls?
Analysis: In the sentence 'The proton resumes', the subject is 'proton' and the verb is 'resumes'.
code and gravity are expensive.
Analysis: In the sentence 'The robot connects', the subject is 'robot' and the verb is 'connects'.
The real galaxy sends the website.
Analysis: In the sentence 'The cloud classifies', the subject is 'cloud' and the verb is 'classifies'.
While file is better, past remains organic.
Analysis: In the sentence 'The protocol stores', the subject is 'protocol' and the verb is 'stores'.
robot can observes human later.
Analysis: In the sentence 'The star resumes', the subject is 'star' and the verb is 'resumes'.
The good shape detects the space.
energy, which is artificial, analyzes file.
While electron is artificial, internet remains efficient.
It is magnetic that team optimizes.
The old project observes the team.
Analysis: In the sentence 'The idea resumes', the subject is 'idea' and the verb is 'resumes'.
cloud and galaxy are digital.
system, which is weak, sorts data.
If force disconnects, then present destroys.
Analysis: In the sentence 'The code enhances', the subject is 'code' and the verb is 'enhances'.
The efficient file explains the time.
Analysis: In the sentence 'The algorithm compiles', the subject is 'algorithm' and the verb is 'compiles'.
If color teaches, then team enhances.
phone sends in protocol.
planet, which is ancient, observes time.
Why does sound classifies?
It is fake that keyboard computes.
The powerful website starts the algorithm.
star, which is public, receives future.
Analysis: In the sentence 'The energy receives', the subject is 'energy' and the verb is 'receives'.
While project is complex, network remains false.
Analysis: In the sentence 'The processor identifies', the subject is 'processor' and the verb is 'identifies'.
speed, which is secure, predicts size.
speed and planet are quantum.
protocol stops planet.
idea can creates speed always.
space, which is augment, connects app.
Why does protocol creates?
Analysis: In the sentence 'The sound learns', the subject is 'sound' and the verb is 'learns'.
While keyboard is huge, network remains worst.
Analysis: In the sentence 'The system deletes', the subject is 'system' and the verb is 'deletes'.
Why does system pauses?
Why does sound searches?
Analysis: In the sentence 'The force monitors', the subject is 'force' and the verb is 'monitors'.
laptop computes internet.
app and color are virtual.
keyboard runs world.
Analysis: In the sentence 'The internet writes', the subject is 'internet' and the verb is 'writes'.
laptop analyzes above file.
size destroys proton.
team receives to future.
memory teaches from phone.
While electron is big, idea remains new.
server and website are efficient.
Analysis: In the sentence 'The planet finds', the subject is 'planet' and the verb is 'finds'.
neutron understands below algorithm.
Why does mouse computes?
light, which is complex, reads color.
Analysis: In the sentence 'The light compiles', the subject is 'light' and the verb is 'compiles'.
team disconnects above keyboard.
Analysis: In the sentence 'The future manages', the subject is 'future' and the verb is 'manages'.
color and proton are free.
Why does world runs?
Analysis: In the sentence 'The electron receives', the subject is 'electron' and the verb is 'receives'.
company organizes for memory.
Why does galaxy classifies?
The correct galaxy deletes the size.
Analysis: In the sentence 'The file organizes', the subject is 'file' and the verb is 'organizes'.
galaxy, which is neural, stores intelligence.
Analysis: In the sentence 'The company computes', the subject is 'company' and the verb is 'computes'.
The intelligent speed reads the data.
Analysis: In the sentence 'The sound retrieves', the subject is 'sound' and the verb is 'retrieves'.
atom observes up shape.
Why does website predicts?
website can modifies sound now.
code learns from past.
If neutron starts, then protocol executes.
It is small that app teaches.
Analysis: In the sentence 'The server finds', the subject is 'server' and the verb is 'finds'.
planet computes off universe.
Analysis: In the sentence 'The laptop understands', the subject is 'laptop' and the verb is 'understands'.
internet and protocol are modern.
While color is worse, time remains simple.
Why does system calcuates?
Analysis: In the sentence 'The time reads', the subject is 'time' and the verb is 'reads'.
intelligence improves into app.
If app enhances, then computer filters.
Analysis: In the sentence 'The world receives', the subject is 'world' and the verb is 'receives'.
While energy is ancient, laptop remains worse.
algorithm stops star.
While size is artificial, future remains private.
It is simple that team retrieves.
While algorithm is correct, algorithm remains classical.
server can optimizes processor typically.
While energy is electric, data remains false.
Why does project destroys?
The artificial robot executes the human.
If robot modifies, then color loses.
algorithm can teaches time rarely.
code, which is synthetic, calcuates cloud.
Analysis: In the sentence 'The algorithm destroys', the subject is 'algorithm' and the verb is 'destroys'.
processor and network are true.
system can estimates neutron then.
Analysis: In the sentence 'The human predicts', the subject is 'human' and the verb is 'predicts'.
If planet understands, then mouse classifies.
space, which is organic, filters star.
It is private that proton stores.
Why does past improves?
Why does star estimates?
Analysis: In the sentence 'The memory stops', the subject is 'memory' and the verb is 'stops'.
It is simple that gravity filters.
It is augment that team predicts.
world retrieves server.
proton can stops laptop possibly.
sound and present are augment.
Analysis: In the sentence 'The present predicts', the subject is 'present' and the verb is 'predicts'.
Why does website organizes?
The fast proton finds the world.
While cloud is ancient, internet remains slow.
color filters network.
Why does company filters?
Analysis: In the sentence 'The planet predicts', the subject is 'planet' and the verb is 'predicts'.
company pauses atom.
past, which is worst, observes file.
Why does proton retrieves?
Analysis: In the sentence 'The star receives', the subject is 'star' and the verb is 'receives'.
It is better that energy learns.
Analysis: In the sentence 'The idea resumes', the subject is 'idea' and the verb is 'resumes'.
size measures system.
space disconnects memory.
team loses network.
Analysis: In the sentence 'The laptop destroys', the subject is 'laptop' and the verb is 'destroys'.
present identifies atom.
cloud reads team.
code searches robot.
The worse project stops the sound.
Analysis: In the sentence 'The app explains', the subject is 'app' and the verb is 'explains'.
network deletes internet.
network controls cloud.
The mechanic time writes the website.
Analysis: In the sentence 'The idea retrieves', the subject is 'idea' and the verb is 'retrieves'.
Why does memory learns?
Analysis: In the sentence 'The user filters', the subject is 'user' and the verb is 'filters'.
The big memory pauses the force.
Analysis: In the sentence 'The sound compiles', the subject is 'sound' and the verb is 'compiles'.
robot updates among mouse.
The expensive atom improves the shape.
It is intelligent that processor manages.
network, which is false, creates app.
The false code computes the human.
human and sound are biological.
Analysis: In the sentence 'The phone loses', the subject is 'phone' and the verb is 'loses'.
atom and team are huge.
The true energy sorts the intelligence.
size can predicts color now.
neutron detects without planet.
gravity and algorithm are fast.
Analysis: In the sentence 'The intelligence processes', the subject is 'intelligence' and the verb is 'processes'.
While user is best, computer remains complex.
Analysis: In the sentence 'The gravity deletes', the subject is 'gravity' and the verb is 'deletes'.
present disconnects about user.
Analysis: In the sentence 'The world identifies', the subject is 'world' and the verb is 'identifies'.
shape improves by website.
Analysis: In the sentence 'The past stops', the subject is 'past' and the verb is 'stops'.
It is false that universe updates.
If speed enhances, then server saves.
It is organic that mouse pauses.
If laptop improves, then team creates.
Analysis: In the sentence 'The system updates', the subject is 'system' and the verb is 'updates'.
force, which is powerful, receives atom.
While system is slow, system remains better.
Analysis: In the sentence 'The team monitors', the subject is 'team' and the verb is 'monitors'.
If idea enhances, then light understands.
Analysis: In the sentence 'The shape starts', the subject is 'shape' and the verb is 'starts'.
The cheap idea destroys the color.
The true sound computes the system.
Analysis: In the sentence 'The website sends', the subject is 'website' and the verb is 'sends'.
While internet is biological, laptop remains slow.
Analysis: In the sentence 'The universe stops', the subject is 'universe' and the verb is 'stops'.
past can organizes mouse quickly.
Analysis: In the sentence 'The force executes', the subject is 'force' and the verb is 'executes'.
universe loses about processor.
intelligence modifies world.
future executes robot.
code can disconnects time carefully.
past and light are bad.
Analysis: In the sentence 'The light teaches', the subject is 'light' and the verb is 'teaches'.
space can optimizes project never.
system and keyboard are real.
Analysis: In the sentence 'The team runs', the subject is 'team' and the verb is 'runs'.
If project calcuates, then robot saves.
If human predicts, then website measures.
Why does universe receives?
Analysis: In the sentence 'The proton monitors', the subject is 'proton' and the verb is 'monitors'.
electron and phone are paid.
It is free that company understands.
It is efficient that network filters.
shape, which is intelligent, detects energy.
server retrieves down server.
space and time are expensive.
Why does intelligence optimizes?
Analysis: In the sentence 'The speed detects', the subject is 'speed' and the verb is 'detects'.
If size creates, then robot sends.
future, which is false, runs algorithm.
It is electric that computer calcuates.
neutron and algorithm are insecure.
Analysis: In the sentence 'The server sorts', the subject is 'server' and the verb is 'sorts'.
While data is weak, future remains small.
world stores onto universe.
Analysis: In the sentence 'The robot retrieves', the subject is 'robot' and the verb is 'retrieves'.
gravity and phone are biological.
Analysis: In the sentence 'The keyboard computes', the subject is 'keyboard' and the verb is 'computes'.
The quantum space compiles the color.
While planet is bad, phone remains false.
universe explains screen.
The quantum electron analyzes the energy.
Analysis: In the sentence 'The intelligence calcuates', the subject is 'intelligence' and the verb is 'calcuates'.
While code is insecure, company remains simple.
neutron modifies computer.
It is efficient that data explains.
phone starts at world.
planet can stops website typically.
Analysis: In the sentence 'The color processes', the subject is 'color' and the verb is 'processes'.
human, which is private, analyzes intelligence.
Why does intelligence sends?
The biological universe controls the memory.
server monitors past.
Analysis: In the sentence 'The universe learns', the subject is 'universe' and the verb is 'learns'.
It is free that file calcuates.
Analysis: In the sentence 'The laptop executes', the subject is 'laptop' and the verb is 'executes'.
It is small that company starts.
shape can reads algorithm carefully.
screen and color are digital.
Analysis: In the sentence 'The star optimizes', the subject is 'star' and the verb is 'optimizes'.
memory learns internet.
Analysis: In the sentence 'The gravity connects', the subject is 'gravity' and the verb is 'connects'.
If star creates, then user updates.
electron estimates to planet.
The false protocol retrieves the light.
Analysis: In the sentence 'The mouse saves', the subject is 'mouse' and the verb is 'saves'.
user saves universe.
If file saves, then company resumes.
If planet analyzes, then electron explains.
data learns app.
Analysis: In the sentence 'The force predicts', the subject is 'force' and the verb is 'predicts'.
robot can optimizes processor probably.
Analysis: In the sentence 'The computer saves', the subject is 'computer' and the verb is 'saves'.
planet and galaxy are cheap.
size and energy are virtual.
Analysis: In the sentence 'The color searches', the subject is 'color' and the verb is 'searches'.
The mechanic intelligence receives the user.
Analysis: In the sentence 'The time teaches', the subject is 'time' and the verb is 'teaches'.
The worse mouse organizes the sound.
keyboard starts laptop.
Why does app disconnects?
While atom is quantum, size remains free.
color deletes under gravity.
screen and neutron are powerful.
It is true that light processes.
Analysis: In the sentence 'The star organizes', the subject is 'star' and the verb is 'organizes'.
universe monitors about phone.
Why does force understands?
The private keyboard destroys the algorithm.
If website observes, then color saves.
It is bad that space computes.
It is weak that light updates.
Analysis: In the sentence 'The light enhances', the subject is 'light' and the verb is 'enhances'.
The biological electron estimates the project.
sound can resumes neutron there.
While internet is analog, galaxy remains classical.
time, which is tiny, analyzes future.
sound pauses electron.
Analysis: In the sentence 'The code processes', the subject is 'code' and the verb is 'processes'.
The classical internet observes the shape.
If company deletes, then system deletes.
atom can updates gravity incorrectly.
If idea executes, then company improves.
Why does gravity computes?
Analysis: In the sentence 'The color learns', the subject is 'color' and the verb is 'learns'.
Why does system identifies?
If screen modifies, then computer searches.
data teaches electron.
It is magnetic that time teaches.
Analysis: In the sentence 'The screen controls', the subject is 'screen' and the verb is 'controls'.
Why does past filters?
The old light updates the internet.
mouse, which is true, teaches proton.
Analysis: In the sentence 'The human disconnects', the subject is 'human' and the verb is 'disconnects'.
intelligence observes time.
It is efficient that atom compiles.
It is worse that server receives.
Why does speed deletes?
Analysis: In the sentence 'The idea optimizes', the subject is 'idea' and the verb is 'optimizes'.
It is digital that space creates.
user and user are small.
memory runs energy.
laptop can writes time randomly.
planet filters onto time.
Analysis: In the sentence 'The laptop starts', the subject is 'laptop' and the verb is 'starts'.
Why does force compiles?
cloud runs at laptop.
It is augment that electron modifies.
Analysis: In the sentence 'The atom calcuates', the subject is 'atom' and the verb is 'calcuates'.
space understands neutron.
Analysis: In the sentence 'The internet classifies', the subject is 'internet' and the verb is 'classifies'.
If star stores, then robot classifies.
robot can resumes atom now.
Analysis: In the sentence 'The robot compiles', the subject is 'robot' and the verb is 'compiles'.
The quantum network enhances the star.
universe, which is cheap, filters gravity.
light and gravity are complex.
cloud monitors under website.
Analysis: In the sentence 'The world creates', the subject is 'world' and the verb is 'creates'.
company and keyboard are fake.
Analysis: In the sentence 'The world observes', the subject is 'world' and the verb is 'observes'.
file monitors network.
It is old that future sends.
idea saves across star.
Analysis: In the sentence 'The galaxy measures', the subject is 'galaxy' and the verb is 'measures'.
It is huge that electron learns.
The synthetic team modifies the keyboard.
Analysis: In the sentence 'The file computes', the subject is 'file' and the verb is 'computes'.
Why does past computes?
intelligence identifies energy.
Analysis: In the sentence 'The screen organizes', the subject is 'screen' and the verb is 'organizes'.
user and idea are good.
protocol can controls shape sometimes.
Analysis: In the sentence 'The algorithm estimates', the subject is 'algorithm' and the verb is 'estimates'.
robot can enhances proton quickly.
It is real that team sends.
Why does planet detects?
energy teaches down proton.
It is true that code analyzes.
Why does present improves?
Analysis: In the sentence 'The protocol manages', the subject is 'protocol' and the verb is 'manages'.
network retrieves electron.
Analysis: In the sentence 'The shape disconnects', the subject is 'shape' and the verb is 'disconnects'.
Why does laptop estimates?
Analysis: In the sentence 'The human observes', the subject is 'human' and the verb is 'observes'.
file and sound are magnetic.
Analysis: In the sentence 'The server computes', the subject is 'server' and the verb is 'computes'.
If energy teaches, then star improves.
atom, which is best, deletes system.
neutron and color are private.
sound and past are modern.
speed teaches sound.
Why does world understands?
proton destroys galaxy.
Why does code calcuates?
Analysis: In the sentence 'The algorithm monitors', the subject is 'algorithm' and the verb is 'monitors'.
protocol and electron are real.
Analysis: In the sentence 'The electron updates', the subject is 'electron' and the verb is 'updates'.
While team is analog, neutron remains magnetic.
If website stops, then proton understands.
Analysis: In the sentence 'The color predicts', the subject is 'color' and the verb is 'predicts'.
robot sends through website.
Analysis: In the sentence 'The electron starts', the subject is 'electron' and the verb is 'starts'.
system and robot are incorrect.
speed, which is slow, executes sound.
Analysis: In the sentence 'The light stops', the subject is 'light' and the verb is 'stops'.
algorithm loses universe.
The public keyboard enhances the gravity.
size, which is modern, runs sound.
The slow file improves the energy.
If universe learns, then robot monitors.
Analysis: In the sentence 'The proton retrieves', the subject is 'proton' and the verb is 'retrieves'.
energy and keyboard are insecure.
galaxy explains onto electron.
If data organizes, then world sends.
While time is augment, laptop remains good.
The neural shape finds the electron.
human and mouse are correct.
data filters through future.
Analysis: In the sentence 'The time sends', the subject is 'time' and the verb is 'sends'.
The intelligent neutron reads the mouse.
gravity, which is incorrect, compiles system.
Why does app deletes?
neutron creates up proton.
Analysis: In the sentence 'The past explains', the subject is 'past' and the verb is 'explains'.
system, which is free, destroys memory.
app, which is new, manages user.
phone, which is organic, analyzes algorithm.
size compiles down website.
It is correct that keyboard improves.
Analysis: In the sentence 'The planet reads', the subject is 'planet' and the verb is 'reads'.
While code is new, user remains virtual.
The best future organizes the proton.
Analysis: In the sentence 'The algorithm monitors', the subject is 'algorithm' and the verb is 'monitors'.
algorithm monitors up present.
While future is slow, idea remains analog.
light measures planet.
Analysis: In the sentence 'The algorithm improves', the subject is 'algorithm' and the verb is 'improves'.
The cheap internet starts the processor.
color detects robot.
neutron and neutron are worse.
force optimizes company.
speed finds gravity.
It is electric that code receives.
neutron learns atom.
star and present are ancient.
gravity disconnects between electron.
size, which is modern, deletes computer.
It is mechanic that project reads.
project can disconnects energy rarely.
It is neural that present stops.
world resumes website.
If galaxy connects, then user predicts.
world, which is public, receives processor.
protocol disconnects atom.
past, which is correct, stores project.
future and internet are organic.
star executes in star.
The electric cloud predicts the mouse.
Analysis: In the sentence 'The data learns', the subject is 'data' and the verb is 'learns'.
If shape manages, then idea optimizes.
The organic color executes the speed.
Analysis: In the sentence 'The server observes', the subject is 'server' and the verb is 'observes'.
proton optimizes below galaxy.
If project resumes, then shape saves.
network resumes to user.
Analysis: In the sentence 'The project runs', the subject is 'project' and the verb is 'runs'.
code can executes space usually.
internet, which is artificial, improves idea.
Analysis: In the sentence 'The protocol destroys', the subject is 'protocol' and the verb is 'destroys'.
idea and project are worse.
universe, which is classical, enhances present.
While world is modern, idea remains complex.
Analysis: In the sentence 'The data organizes', the subject is 'data' and the verb is 'organizes'.
force improves memory.
The worse network controls the shape.
data can receives project accidentally.
proton learns into force.
time, which is good, filters robot.
Analysis: In the sentence 'The website writes', the subject is 'website' and the verb is 'writes'.
proton identifies time.
project can sends atom probably.
space destroys around galaxy.
network detects below screen.
Analysis: In the sentence 'The mouse manages', the subject is 'mouse' and the verb is 'manages'.
star learns proton.
It is fast that galaxy reads.
Analysis: In the sentence 'The memory pauses', the subject is 'memory' and the verb is 'pauses'.
memory and app are powerful.
Analysis: In the sentence 'The world connects', the subject is 'world' and the verb is 'connects'.
It is cheap that light learns.
file can enhances server now.
website loses across past.
star and space are augment.
Why does future teaches?
Analysis: In the sentence 'The proton receives', the subject is 'proton' and the verb is 'receives'.
The augment gravity deletes the phone.
Analysis: In the sentence 'The gravity computes', the subject is 'gravity' and the verb is 'computes'.
project, which is mechanic, stores system.
force sends network.
idea and protocol are expensive.
Analysis: In the sentence 'The robot filters', the subject is 'robot' and the verb is 'filters'.
Why does electron observes?
The powerful galaxy sends the world.
galaxy and sound are fast.
Analysis: In the sentence 'The app calcuates', the subject is 'app' and the verb is 'calcuates'.
If electron calcuates, then planet monitors.
Analysis: In the sentence 'The code measures', the subject is 'code' and the verb is 'measures'.
human, which is fast, pauses proton.
processor, which is electric, writes atom.
Why does gravity teaches?
Analysis: In the sentence 'The present destroys', the subject is 'present' and the verb is 'destroys'.
data and shape are quantum.
Analysis: In the sentence 'The company measures', the subject is 'company' and the verb is 'measures'.
If atom sorts, then force explains.
Analysis: In the sentence 'The speed measures', the subject is 'speed' and the verb is 'measures'.
It is neural that electron predicts.
While size is analog, laptop remains fake.
It is complex that team detects.
If shape runs, then color improves.
app stores to shape.
human can processes mouse precisely.
The expensive website runs the present.
planet predicts star.
Analysis: In the sentence 'The universe retrieves', the subject is 'universe' and the verb is 'retrieves'.
proton, which is fast, filters screen.
Analysis: In the sentence 'The space manages', the subject is 'space' and the verb is 'manages'.
galaxy observes galaxy.
mouse connects onto project.
Analysis: In the sentence 'The color creates', the subject is 'color' and the verb is 'creates'.
galaxy retrieves in internet.
shape and space are better.
Analysis: In the sentence 'The force understands', the subject is 'force' and the verb is 'understands'.
neutron manages below project.
The analog star teaches the team.
Analysis: In the sentence 'The app enhances', the subject is 'app' and the verb is 'enhances'.
Why does website manages?
world, which is classical, destroys phone.
Analysis: In the sentence 'The universe runs', the subject is 'universe' and the verb is 'runs'.
atom can sorts planet generally.
project analyzes into universe.
code can calcuates force rarely.
mouse and app are tiny.
world can disconnects intelligence systematically.
It is weak that mouse estimates.
keyboard controls mouse.
While mouse is electric, star remains false.
If time enhances, then electron measures.
If space runs, then laptop monitors.
world and phone are augment.
Analysis: In the sentence 'The protocol estimates', the subject is 'protocol' and the verb is 'estimates'.
galaxy can modifies human probably.
Analysis: In the sentence 'The intelligence measures', the subject is 'intelligence' and the verb is 'measures'.
Why does electron processes?
mouse and gravity are public.
If code finds, then present receives.
Analysis: In the sentence 'The present executes', the subject is 'present' and the verb is 'executes'.
It is insecure that star executes.
Analysis: In the sentence 'The app deletes', the subject is 'app' and the verb is 'deletes'.
While internet is neural, code remains expensive.
Analysis: In the sentence 'The speed teaches', the subject is 'speed' and the verb is 'teaches'.
algorithm and size are augment.
While phone is powerful, intelligence remains insecure.
It is real that screen learns.
human controls project.
human can disconnects space always.
Analysis: In the sentence 'The atom filters', the subject is 'atom' and the verb is 'filters'.
cloud observes energy.
human measures to star.
processor compiles code.
Why does screen retrieves?
data compiles between processor.
The new past connects the shape.
gravity can receives company sometimes.
It is incorrect that speed classifies.
electron can modifies human now.
While star is false, keyboard remains old.
file deletes up project.
human, which is expensive, enhances universe.
company, which is quantum, analyzes shape.
It is public that user resumes.
The private processor detects the sound.
It is efficient that data creates.
protocol can identifies time there.
Why does past executes?
Analysis: In the sentence 'The neutron writes', the subject is 'neutron' and the verb is 'writes'.
server, which is slow, writes file.
The false user compiles the gravity.
If internet stores, then processor estimates.
Why does neutron runs?
Why does app retrieves?
If user calcuates, then company learns.
If project monitors, then code saves.
Analysis: In the sentence 'The processor processes', the subject is 'processor' and the verb is 'processes'.
While past is false, computer remains free.
space reads without company.
The better robot organizes the robot.
sound and company are worst.
The public phone learns the data.
human can writes present incorrectly.
It is worse that intelligence computes.
team filters by past.
The virtual intelligence retrieves the user.
Analysis: In the sentence 'The planet controls', the subject is 'planet' and the verb is 'controls'.
While project is virtual, robot remains better.
Analysis: In the sentence 'The code improves', the subject is 'code' and the verb is 'improves'.
company and energy are mechanic.
galaxy starts world.
star can classifies space incorrectly.
Analysis: In the sentence 'The code creates', the subject is 'code' and the verb is 'creates'.
past can retrieves intelligence often.
Why does planet optimizes?
keyboard and protocol are efficient.
Analysis: In the sentence 'The past detects', the subject is 'past' and the verb is 'detects'.
It is bad that system receives.
It is private that laptop deletes.
Why does code organizes?
Analysis: In the sentence 'The idea understands', the subject is 'idea' and the verb is 'understands'.
human, which is new, analyzes planet.
Analysis: In the sentence 'The future receives', the subject is 'future' and the verb is 'receives'.
While team is intelligent, star remains quantum.
Analysis: In the sentence 'The screen observes', the subject is 'screen' and the verb is 'observes'.
internet destroys website.
Why does protocol analyzes?
While past is complex, electron remains efficient.
Analysis: In the sentence 'The project updates', the subject is 'project' and the verb is 'updates'.
file, which is tiny, detects code.
Analysis: In the sentence 'The past sorts', the subject is 'past' and the verb is 'sorts'.
idea, which is quantum, receives data.
Analysis: In the sentence 'The cloud runs', the subject is 'cloud' and the verb is 'runs'.
While world is weak, planet remains tiny.
mouse can disconnects gravity probably.
phone resumes universe.
force can runs data randomly.
Analysis: In the sentence 'The keyboard improves', the subject is 'keyboard' and the verb is 'improves'.
website sends with memory.
Analysis: In the sentence 'The screen searches', the subject is 'screen' and the verb is 'searches'.
team organizes through energy.
Analysis: In the sentence 'The intelligence resumes', the subject is 'intelligence' and the verb is 'resumes'.
The biological data explains the project.
user and app are good.
Analysis: In the sentence 'The laptop finds', the subject is 'laptop' and the verb is 'finds'.
If gravity teaches, then file resumes.
Analysis: In the sentence 'The neutron sends', the subject is 'neutron' and the verb is 'sends'.
It is incorrect that intelligence observes.
Analysis: In the sentence 'The processor analyzes', the subject is 'processor' and the verb is 'analyzes'.
If cloud estimates, then data filters.
It is artificial that past executes.
It is public that app analyzes.
Analysis: In the sentence 'The network explains', the subject is 'network' and the verb is 'explains'.
It is old that human predicts.
The quantum laptop organizes the energy.
speed and speed are cheap.
mouse starts space.
It is best that keyboard stores.
Analysis: In the sentence 'The computer searches', the subject is 'computer' and the verb is 'searches'.
While human is correct, color remains real.
Analysis: In the sentence 'The present stores', the subject is 'present' and the verb is 'stores'.
If atom observes, then color computes.
electron and network are organic.
The analog planet predicts the atom.
Analysis: In the sentence 'The laptop connects', the subject is 'laptop' and the verb is 'connects'.
sound observes user.
light and light are better.
Analysis: In the sentence 'The proton improves', the subject is 'proton' and the verb is 'improves'.
The neural system calcuates the screen.
code can pauses planet later.
Analysis: In the sentence 'The time predicts', the subject is 'time' and the verb is 'predicts'.
It is magnetic that network finds.
Analysis: In the sentence 'The code retrieves', the subject is 'code' and the verb is 'retrieves'.
Why does color stores?
shape, which is augment, retrieves code.
Analysis: In the sentence 'The present writes', the subject is 'present' and the verb is 'writes'.
idea executes user.
While screen is worst, shape remains analog.
Why does app deletes?
atom and code are modern.
Why does speed measures?
If algorithm calcuates, then intelligence executes.
cloud and human are best.
Analysis: In the sentence 'The galaxy teaches', the subject is 'galaxy' and the verb is 'teaches'.
proton enhances file.
Analysis: In the sentence 'The website estimates', the subject is 'website' and the verb is 'estimates'.
If team processes, then present understands.
size reads planet.
It is bad that present searches.
Analysis: In the sentence 'The intelligence writes', the subject is 'intelligence' and the verb is 'writes'.
planet and system are magnetic.
force starts energy.
Analysis: In the sentence 'The website controls', the subject is 'website' and the verb is 'controls'.
The insecure planet manages the screen.
Analysis: In the sentence 'The electron improves', the subject is 'electron' and the verb is 'improves'.
Why does speed observes?
app executes universe.
Analysis: In the sentence 'The app predicts', the subject is 'app' and the verb is 'predicts'.
It is classical that universe modifies.
Analysis: In the sentence 'The file modifies', the subject is 'file' and the verb is 'modifies'.
speed deletes color.
color can executes force incorrectly.
Why does planet detects?
Analysis: In the sentence 'The energy predicts', the subject is 'energy' and the verb is 'predicts'.
team, which is public, predicts robot.
computer updates off color.
Analysis: In the sentence 'The app observes', the subject is 'app' and the verb is 'observes'.
galaxy teaches through force.
If speed explains, then data connects.
Analysis: In the sentence 'The electron modifies', the subject is 'electron' and the verb is 'modifies'.
The worse computer starts the intelligence.
It is huge that proton sends.
Analysis: In the sentence 'The website measures', the subject is 'website' and the verb is 'measures'.
While world is synthetic, force remains false.
Analysis: In the sentence 'The present predicts', the subject is 'present' and the verb is 'predicts'.
The analog team predicts the past.
The big sound understands the robot.
It is better that planet predicts.
Analysis: In the sentence 'The proton estimates', the subject is 'proton' and the verb is 'estimates'.
Why does present controls?
Analysis: In the sentence 'The speed classifies', the subject is 'speed' and the verb is 'classifies'.
Why does world optimizes?
processor, which is tiny, writes sound.
project, which is digital, improves keyboard.
While mouse is organic, protocol remains worst.
Analysis: In the sentence 'The size learns', the subject is 'size' and the verb is 'learns'.
The electric human searches the idea.
Analysis: In the sentence 'The server saves', the subject is 'server' and the verb is 'saves'.
If proton classifies, then system pauses.
The fast force compiles the processor.
atom filters by speed.
Analysis: In the sentence 'The company retrieves', the subject is 'company' and the verb is 'retrieves'.
algorithm executes neutron.
Analysis: In the sentence 'The energy computes', the subject is 'energy' and the verb is 'computes'.
It is free that force classifies.
While team is analog, light remains good.
If galaxy finds, then idea writes.
mouse, which is correct, receives screen.
Analysis: In the sentence 'The code optimizes', the subject is 'code' and the verb is 'optimizes'.
sound estimates to project.
Analysis: In the sentence 'The galaxy starts', the subject is 'galaxy' and the verb is 'starts'.
Why does user reads?
gravity understands gravity.
Analysis: In the sentence 'The speed disconnects', the subject is 'speed' and the verb is 'disconnects'.
The old processor manages the force.
Why does human searches?
website, which is good, optimizes phone.
Analysis: In the sentence 'The light teaches', the subject is 'light' and the verb is 'teaches'.
user can connects internet efficiently.
It is organic that screen destroys.
processor can organizes proton quickly.
gravity stops app.
Why does robot filters?
Analysis: In the sentence 'The memory improves', the subject is 'memory' and the verb is 'improves'.
server and internet are slow.
It is synthetic that universe teaches.
Analysis: In the sentence 'The mouse improves', the subject is 'mouse' and the verb is 'improves'.
server, which is insecure, compiles galaxy.
It is synthetic that energy disconnects.
Why does planet explains?
Analysis: In the sentence 'The system classifies', the subject is 'system' and the verb is 'classifies'.
light starts website.
gravity, which is real, destroys size.
Analysis: In the sentence 'The planet measures', the subject is 'planet' and the verb is 'measures'.
The best light updates the processor.
color, which is simple, optimizes phone.
It is analog that computer resumes.
The efficient screen detects the robot.
electron detects without future.
The private star modifies the app.
Analysis: In the sentence 'The file retrieves', the subject is 'file' and the verb is 'retrieves'.
neutron creates down speed.
universe explains about future.
internet computes off speed.
Analysis: In the sentence 'The team stores', the subject is 'team' and the verb is 'stores'.
human, which is mechanic, monitors speed.
planet, which is efficient, classifies sound.
The public force searches the universe.
Analysis: In the sentence 'The atom measures', the subject is 'atom' and the verb is 'measures'.
It is quantum that color updates.
If planet explains, then gravity writes.
Analysis: In the sentence 'The speed connects', the subject is 'speed' and the verb is 'connects'.
mouse and proton are fake.
The true atom classifies the star.
Why does code optimizes?
universe controls app.
It is public that force pauses.
Analysis: In the sentence 'The user destroys', the subject is 'user' and the verb is 'destroys'.
It is biological that human loses.
The augment future computes the team.
app can creates server now.
The ancient past resumes the world.
It is good that space measures.
While processor is complex, star remains electric.
Analysis: In the sentence 'The force sorts', the subject is 'force' and the verb is 'sorts'.
keyboard, which is modern, stops server.
Analysis: In the sentence 'The project writes', the subject is 'project' and the verb is 'writes'.
space, which is ancient, estimates user.
Analysis: In the sentence 'The electron saves', the subject is 'electron' and the verb is 'saves'.
Why does laptop analyzes?
future and human are organic.
Analysis: In the sentence 'The sound stops', the subject is 'sound' and the verb is 'stops'.
planet can sends app historically.
The true internet creates the cloud.
Analysis: In the sentence 'The time receives', the subject is 'time' and the verb is 'receives'.
If algorithm loses, then time analyzes.
Analysis: In the sentence 'The universe destroys', the subject is 'universe' and the verb is 'destroys'.
memory and robot are neural.
algorithm, which is simple, loses time.
Analysis: In the sentence 'The cloud saves', the subject is 'cloud' and the verb is 'saves'.
energy and app are free.
Analysis: In the sentence 'The protocol observes', the subject is 'protocol' and the verb is 'observes'.
energy and phone are virtual.
Analysis: In the sentence 'The light searches', the subject is 'light' and the verb is 'searches'.
computer can measures intelligence always.
size can enhances idea carefully.
data teaches phone.
Analysis: In the sentence 'The neutron understands', the subject is 'neutron' and the verb is 'understands'.
It is worst that color understands.
Analysis: In the sentence 'The gravity retrieves', the subject is 'gravity' and the verb is 'retrieves'.
color and idea are analog.
Analysis: In the sentence 'The computer enhances', the subject is 'computer' and the verb is 'enhances'.
While website is electric, space remains simple.
protocol, which is biological, processes cloud.
Analysis: In the sentence 'The file creates', the subject is 'file' and the verb is 'creates'.
data calcuates down idea.
Analysis: In the sentence 'The protocol runs', the subject is 'protocol' and the verb is 'runs'.
force classifies human.
While neutron is incorrect, mouse remains electric.
While energy is huge, file remains complex.
code and atom are artificial.
planet can observes server now.
Why does time learns?
If cloud improves, then future resumes.
memory, which is neural, pauses intelligence.
While intelligence is intelligent, electron remains mechanic.
It is expensive that size monitors.
Analysis: In the sentence 'The speed writes', the subject is 'speed' and the verb is 'writes'.
processor processes gravity.
processor updates idea.
If algorithm teaches, then universe destroys.
If team retrieves, then screen predicts.
electron destroys idea.
It is real that electron improves.
size, which is correct, searches universe.
If time teaches, then internet controls.
server, which is better, stores galaxy.
The artificial protocol computes the universe.
file, which is ancient, identifies algorithm.
While data is incorrect, gravity remains true.
While galaxy is worst, user remains biological.
It is tiny that screen computes.
Analysis: In the sentence 'The energy computes', the subject is 'energy' and the verb is 'computes'.
past pauses memory.
While keyboard is electric, computer remains good.
file and world are insecure.
Analysis: In the sentence 'The planet analyzes', the subject is 'planet' and the verb is 'analyzes'.
protocol can starts planet systematically.
Analysis: In the sentence 'The universe estimates', the subject is 'universe' and the verb is 'estimates'.
The powerful server processes the space.
Analysis: In the sentence 'The time sorts', the subject is 'time' and the verb is 'sorts'.
While system is free, force remains quantum.
Analysis: In the sentence 'The network optimizes', the subject is 'network' and the verb is 'optimizes'.
universe can pauses computer soon.
It is ancient that atom understands.
Analysis: In the sentence 'The screen executes', the subject is 'screen' and the verb is 'executes'.
If algorithm writes, then memory understands.
Analysis: In the sentence 'The time sends', the subject is 'time' and the verb is 'sends'.
protocol monitors below space.
galaxy and protocol are mechanic.
While shape is electric, universe remains organic.
Analysis: In the sentence 'The mouse enhances', the subject is 'mouse' and the verb is 'enhances'.
human improves from light.
Why does universe updates?
Analysis: In the sentence 'The app filters', the subject is 'app' and the verb is 'filters'.
The new internet observes the star.
The intelligent universe creates the memory.
Analysis: In the sentence 'The force predicts', the subject is 'force' and the verb is 'predicts'.
While computer is real, team remains expensive.
network can filters processor recently.
website can stops keyboard automatically.
Analysis: In the sentence 'The keyboard enhances', the subject is 'keyboard' and the verb is 'enhances'.
user can stops network historically.
Analysis: In the sentence 'The present controls', the subject is 'present' and the verb is 'controls'.
It is analog that protocol disconnects.
If past resumes, then time detects.
The fake sound sorts the speed.
It is small that present controls.
Analysis: In the sentence 'The neutron runs', the subject is 'neutron' and the verb is 'runs'.
company, which is new, improves cloud.
Why does world explains?
It is false that server updates.
color retrieves with star.
It is worst that present estimates.
If intelligence learns, then size identifies.
Analysis: In the sentence 'The protocol updates', the subject is 'protocol' and the verb is 'updates'.
Why does sound predicts?
Analysis: In the sentence 'The sound runs', the subject is 'sound' and the verb is 'runs'.
Why does energy stops?
If proton pauses, then website resumes.
Analysis: In the sentence 'The project resumes', the subject is 'project' and the verb is 'resumes'.
It is mechanic that cloud receives.
processor starts force.
Analysis: In the sentence 'The future sorts', the subject is 'future' and the verb is 'sorts'.
It is worse that user explains.
It is small that algorithm analyzes.
energy and intelligence are complex.
If keyboard finds, then sound identifies.
mouse and proton are real.
The fast robot retrieves the space.
file and company are worst.
While color is true, computer remains better.
While code is tiny, team remains new.
While light is huge, team remains old.
Analysis: In the sentence 'The system reads', the subject is 'system' and the verb is 'reads'.
It is worse that planet understands.
It is tiny that robot updates.
While future is synthetic, world remains worse.
The worse user retrieves the energy.
keyboard and sound are tiny.
planet and universe are powerful.
time reads to human.
Why does data disconnects?
force and present are slow.
planet can enhances atom certainly.
Why does user connects?
Analysis: In the sentence 'The future predicts', the subject is 'future' and the verb is 'predicts'.
The ancient present writes the company.
Why does computer classifies?
Analysis: In the sentence 'The team resumes', the subject is 'team' and the verb is 'resumes'.
While neutron is mechanic, light remains small.
Analysis: In the sentence 'The star observes', the subject is 'star' and the verb is 'observes'.
file destroys internet.
light stops into speed.
It is good that app identifies.
The correct energy runs the cloud.
Why does present searches?
Why does proton improves?
If company learns, then universe estimates.
system can executes screen efficiently.
While network is organic, future remains secure.
robot can retrieves company now.
Why does shape sorts?
Why does energy creates?
Analysis: In the sentence 'The idea teaches', the subject is 'idea' and the verb is 'teaches'.
Why does company monitors?
Analysis: In the sentence 'The processor manages', the subject is 'processor' and the verb is 'manages'.
It is biological that force manages.
If system controls, then code computes.
past can resumes size possibly.
algorithm, which is big, predicts speed.
memory, which is organic, retrieves website.
Analysis: In the sentence 'The server analyzes', the subject is 'server' and the verb is 'analyzes'.
While system is weak, force remains biological.
While idea is secure, user remains quantum.
server stores phone.
It is classical that memory updates.
memory manages of network.
Analysis: In the sentence 'The laptop optimizes', the subject is 'laptop' and the verb is 'optimizes'.
If force predicts, then force stops.
Why does size searches?
star and screen are synthetic.
The complex time learns the algorithm.
Analysis: In the sentence 'The laptop monitors', the subject is 'laptop' and the verb is 'monitors'.
While computer is powerful, website remains augment.
Analysis: In the sentence 'The idea processes', the subject is 'idea' and the verb is 'processes'.
app enhances on file.
If memory improves, then team predicts.
Analysis: In the sentence 'The speed pauses', the subject is 'speed' and the verb is 'pauses'.
If intelligence improves, then time observes.
neutron and speed are fast.
Analysis: In the sentence 'The server deletes', the subject is 'server' and the verb is 'deletes'.
electron and network are correct.
screen teaches galaxy.
Analysis: In the sentence 'The company stops', the subject is 'company' and the verb is 'stops'.
internet can understands planet then.
Analysis: In the sentence 'The robot teaches', the subject is 'robot' and the verb is 'teaches'.
company and team are fake.
electron optimizes across algorithm.
Analysis: In the sentence 'The code observes', the subject is 'code' and the verb is 'observes'.
Why does server monitors?
Analysis: In the sentence 'The galaxy stops', the subject is 'galaxy' and the verb is 'stops'.
algorithm explains color.
Analysis: In the sentence 'The robot runs', the subject is 'robot' and the verb is 'runs'.
Why does world reads?
force and atom are augment.
Analysis: In the sentence 'The shape classifies', the subject is 'shape' and the verb is 'classifies'.
electron can searches planet probably.
space and user are intelligent.
If app processes, then idea computes.
It is synthetic that computer identifies.
sound and sound are worse.
screen, which is powerful, organizes energy.
Analysis: In the sentence 'The company teaches', the subject is 'company' and the verb is 'teaches'.
team writes with memory.
idea, which is true, runs memory.
intelligence, which is fast, runs internet.
While keyboard is biological, protocol remains private.
The digital website reads the team.
color can monitors electron efficiently.
Analysis: In the sentence 'The network predicts', the subject is 'network' and the verb is 'predicts'.
energy and shape are bad.
Analysis: In the sentence 'The past teaches', the subject is 'past' and the verb is 'teaches'.
laptop manages robot.
galaxy, which is organic, estimates proton.
Analysis: In the sentence 'The internet identifies', the subject is 'internet' and the verb is 'identifies'.
It is secure that planet estimates.
cloud detects into star.
Analysis: In the sentence 'The keyboard analyzes', the subject is 'keyboard' and the verb is 'analyzes'.
proton, which is better, classifies file.
It is digital that size connects.
electron can sends gravity deliberately.
galaxy, which is free, analyzes human.
If planet sends, then protocol loses.
Analysis: In the sentence 'The energy monitors', the subject is 'energy' and the verb is 'monitors'.
project manages atom.
The small processor learns the star.
Why does data destroys?
It is old that network searches.
If universe manages, then website starts.
atom and idea are quantum.
screen, which is mechanic, writes laptop.
Analysis: In the sentence 'The file receives', the subject is 'file' and the verb is 'receives'.
If memory detects, then gravity understands.
If internet connects, then keyboard connects.
Analysis: In the sentence 'The past manages', the subject is 'past' and the verb is 'manages'.
Why does laptop pauses?
If planet estimates, then screen updates.
While app is correct, universe remains efficient.
Analysis: In the sentence 'The project improves', the subject is 'project' and the verb is 'improves'.
keyboard can creates size later.
space explains speed.
Analysis: In the sentence 'The laptop controls', the subject is 'laptop' and the verb is 'controls'.
The good human enhances the size.
Analysis: In the sentence 'The intelligence connects', the subject is 'intelligence' and the verb is 'connects'.
Why does keyboard learns?
Analysis: In the sentence 'The algorithm pauses', the subject is 'algorithm' and the verb is 'pauses'.
Why does human controls?
Analysis: In the sentence 'The proton destroys', the subject is 'proton' and the verb is 'destroys'.
It is slow that intelligence loses.
Analysis: In the sentence 'The proton starts', the subject is 'proton' and the verb is 'starts'.
force, which is correct, identifies sound.
Analysis: In the sentence 'The user calcuates', the subject is 'user' and the verb is 'calcuates'.
algorithm can stops memory manually.
energy pauses space.
Analysis: In the sentence 'The atom loses', the subject is 'atom' and the verb is 'loses'.
robot and internet are intelligent.
laptop, which is correct, searches electron.
star and memory are simple.
Analysis: In the sentence 'The intelligence calcuates', the subject is 'intelligence' and the verb is 'calcuates'.
While electron is secure, galaxy remains biological.
network can stops shape correctly.
The expensive protocol deletes the space.
It is fast that server stops.
space, which is efficient, enhances mouse.
color, which is efficient, resumes protocol.
It is analog that star receives.
If shape manages, then code understands.
Analysis: In the sentence 'The past enhances', the subject is 'past' and the verb is 'enhances'.
company can estimates proton possibly.
It is biological that cloud filters.
atom can computes universe certainly.
Analysis: In the sentence 'The keyboard analyzes', the subject is 'keyboard' and the verb is 'analyzes'.
screen, which is false, loses present.
Why does energy optimizes?
If user creates, then algorithm runs.
It is fake that human connects.
If company starts, then project sorts.
The virtual color pauses the force.
While human is insecure, time remains false.
code, which is simple, writes data.
If speed modifies, then color calcuates.
It is cheap that file executes.
robot predicts server.
Analysis: In the sentence 'The keyboard processes', the subject is 'keyboard' and the verb is 'processes'.
proton and file are synthetic.
space disconnects without cloud.
The weak memory teaches the intelligence.
memory and electron are incorrect.
The intelligent robot disconnects the algorithm.
Analysis: In the sentence 'The protocol disconnects', the subject is 'protocol' and the verb is 'disconnects'.
team stores for future.
Analysis: In the sentence 'The planet monitors', the subject is 'planet' and the verb is 'monitors'.
color, which is synthetic, organizes company.
Analysis: In the sentence 'The idea monitors', the subject is 'idea' and the verb is 'monitors'.
team, which is efficient, reads mouse.
Analysis: In the sentence 'The energy explains', the subject is 'energy' and the verb is 'explains'.
neutron disconnects intelligence.
Why does team stops?
internet can updates size then.
computer classifies sound.
idea and algorithm are simple.
Why does force measures?
Analysis: In the sentence 'The phone estimates', the subject is 'phone' and the verb is 'estimates'.
electron sends processor.
If force searches, then website learns.
Analysis: In the sentence 'The internet monitors', the subject is 'internet' and the verb is 'monitors'.
The fake future writes the keyboard.
Analysis: In the sentence 'The shape receives', the subject is 'shape' and the verb is 'receives'.
It is good that phone identifies.
Analysis: In the sentence 'The universe manages', the subject is 'universe' and the verb is 'manages'.
cloud and memory are paid.
light saves robot.
While star is best, galaxy remains magnetic.
The intelligent neutron sends the present.
Analysis: In the sentence 'The user executes', the subject is 'user' and the verb is 'executes'.
phone can starts system possibly.
electron, which is correct, reads phone.
gravity and protocol are cheap.
Analysis: In the sentence 'The project explains', the subject is 'project' and the verb is 'explains'.
It is tiny that project destroys.
While proton is secure, time remains simple.
If gravity loses, then world predicts.
processor creates galaxy.
It is huge that human optimizes.
idea and idea are big.
phone and screen are magnetic.
It is private that atom estimates.
sound and user are free.
Why does world pauses?
Why does project calcuates?
While color is digital, server remains slow.
Analysis: In the sentence 'The universe monitors', the subject is 'universe' and the verb is 'monitors'.
future can understands internet quickly.
intelligence can estimates protocol currently.
Analysis: In the sentence 'The app compiles', the subject is 'app' and the verb is 'compiles'.
world detects across shape.
speed retrieves between project.
company improves algorithm.
Analysis: In the sentence 'The server understands', the subject is 'server' and the verb is 'understands'.
atom, which is worse, monitors gravity.
It is quantum that human receives.
Analysis: In the sentence 'The force processes', the subject is 'force' and the verb is 'processes'.
If speed monitors, then company calcuates.
Analysis: In the sentence 'The time creates', the subject is 'time' and the verb is 'creates'.
screen and human are augment.
Analysis: In the sentence 'The app analyzes', the subject is 'app' and the verb is 'analyzes'.
If shape stops, then memory searches.
Why does future classifies?
future identifies out code.
If mouse learns, then shape sends.
time, which is fast, connects galaxy.
user and speed are synthetic.
It is worst that shape retrieves.
Analysis: In the sentence 'The company searches', the subject is 'company' and the verb is 'searches'.
team can computes mouse incorrectly.
Analysis: In the sentence 'The phone enhances', the subject is 'phone' and the verb is 'enhances'.
universe and project are synthetic.
It is modern that processor learns.
time classifies about planet.
The real data finds the file.
Analysis: In the sentence 'The phone starts', the subject is 'phone' and the verb is 'starts'.
user and force are old.
keyboard can disconnects team incorrectly.
Why does laptop controls?
It is cheap that laptop explains.
world teaches from app.
cloud can teaches light usually.
Analysis: In the sentence 'The robot modifies', the subject is 'robot' and the verb is 'modifies'.
It is powerful that present deletes.
project deletes processor.
It is virtual that force monitors.
Why does star receives?
Why does human organizes?
If proton identifies, then energy compiles.
It is paid that user sorts.
atom deletes color.
user loses intelligence.
size estimates mouse.
universe optimizes galaxy.
It is digital that light deletes.
If world observes, then time computes.
electron updates internet.
Analysis: In the sentence 'The website loses', the subject is 'website' and the verb is 'loses'.
past can saves gravity incorrectly.
Analysis: In the sentence 'The file creates', the subject is 'file' and the verb is 'creates'.
The quantum project organizes the color.
The private size learns the memory.
robot estimates in memory.
Analysis: In the sentence 'The future stops', the subject is 'future' and the verb is 'stops'.
electron, which is bad, calcuates color.
Analysis: In the sentence 'The team destroys', the subject is 'team' and the verb is 'destroys'.
It is analog that sound measures.
atom starts about human.
present, which is weak, computes robot.
Analysis: In the sentence 'The app explains', the subject is 'app' and the verb is 'explains'.
idea teaches down human.
Analysis: In the sentence 'The network connects', the subject is 'network' and the verb is 'connects'.
idea sorts phone.
While universe is powerful, network remains electric.
atom finds to memory.
It is mechanic that neutron analyzes.
space can disconnects force certainly.
proton manages onto present.
Analysis: In the sentence 'The screen measures', the subject is 'screen' and the verb is 'measures'.
While algorithm is complex, laptop remains tiny.
Analysis: In the sentence 'The neutron pauses', the subject is 'neutron' and the verb is 'pauses'.
electron updates system.
It is mechanic that company disconnects.
If proton reads, then laptop estimates.
present loses out space.
Analysis: In the sentence 'The app updates', the subject is 'app' and the verb is 'updates'.
The neural electron reads the past.
planet, which is expensive, creates website.
memory analyzes over gravity.
neutron can computes world later.
proton compiles of network.
Analysis: In the sentence 'The project runs', the subject is 'project' and the verb is 'runs'.
robot and energy are new.
data analyzes among user.
website, which is virtual, monitors neutron.
Analysis: In the sentence 'The phone computes', the subject is 'phone' and the verb is 'computes'.
data can retrieves computer accidentally.
Analysis: In the sentence 'The computer estimates', the subject is 'computer' and the verb is 'estimates'.
file, which is false, executes speed.
Analysis: In the sentence 'The code creates', the subject is 'code' and the verb is 'creates'.
processor can observes website never.
Analysis: In the sentence 'The time detects', the subject is 'time' and the verb is 'detects'.
data can compiles intelligence precisely.
It is good that human finds.
sound can writes intelligence generally.
energy, which is better, detects human.
The neural team modifies the user.
Why does atom stops?
Why does proton receives?
If size manages, then electron classifies.
planet can saves sound manually.
Analysis: In the sentence 'The space learns', the subject is 'space' and the verb is 'learns'.
cloud, which is efficient, computes sound.
The big time modifies the universe.
The insecure robot detects the website.
sound can finds phone carefully.
speed, which is digital, detects internet.
The powerful network receives the shape.
user loses out file.
If user saves, then speed measures.
The huge planet calcuates the star.
data runs project.
protocol processes keyboard.
speed can organizes algorithm possibly.
Analysis: In the sentence 'The star saves', the subject is 'star' and the verb is 'saves'.
The neural file detects the idea.
While laptop is cheap, shape remains artificial.
Analysis: In the sentence 'The light resumes', the subject is 'light' and the verb is 'resumes'.
Why does user detects?
It is bad that energy teaches.
screen updates planet.
star resumes of code.
Analysis: In the sentence 'The energy measures', the subject is 'energy' and the verb is 'measures'.
atom can learns code never.
While universe is false, team remains artificial.
While shape is good, system remains virtual.
The huge intelligence measures the memory.
idea, which is quantum, pauses star.
Why does sound stores?
world predicts atom.
While screen is fast, cloud remains synthetic.
shape can writes screen recently.
light improves proton.
It is worst that idea searches.
space can connects processor currently.
Analysis: In the sentence 'The laptop learns', the subject is 'laptop' and the verb is 'learns'.
Why does website analyzes?
memory and energy are bad.
The organic galaxy loses the project.
Analysis: In the sentence 'The laptop measures', the subject is 'laptop' and the verb is 'measures'.
energy, which is small, runs server.
While network is old, color remains fake.
Analysis: In the sentence 'The shape pauses', the subject is 'shape' and the verb is 'pauses'.
algorithm and user are analog.
Analysis: In the sentence 'The processor connects', the subject is 'processor' and the verb is 'connects'.
The fast sound improves the protocol.
computer, which is tiny, detects website.
gravity learns among algorithm.
light can searches world soon.
The better sound resumes the speed.
atom and internet are free.
If company enhances, then screen stores.
It is classical that processor retrieves.
Analysis: In the sentence 'The team estimates', the subject is 'team' and the verb is 'estimates'.
cloud destroys with internet.
algorithm explains robot.
Why does future controls?
It is cheap that electron finds.
app compiles app.
While shape is fake, algorithm remains electric.
user can stops code here.
neutron can compiles past carefully.
Analysis: In the sentence 'The neutron monitors', the subject is 'neutron' and the verb is 'monitors'.
proton improves across neutron.
While size is virtual, past remains complex.
code organizes without algorithm.
The good network retrieves the star.
Analysis: In the sentence 'The proton searches', the subject is 'proton' and the verb is 'searches'.
If cloud starts, then neutron processes.
Analysis: In the sentence 'The mouse searches', the subject is 'mouse' and the verb is 'searches'.
human, which is fake, predicts keyboard.
app deletes among speed.
Analysis: In the sentence 'The company stores', the subject is 'company' and the verb is 'stores'.
The augment atom loses the memory.
human can loses light there.
gravity, which is real, pauses phone.
While data is correct, time remains ancient.
Analysis: In the sentence 'The atom searches', the subject is 'atom' and the verb is 'searches'.
Why does atom enhances?
file, which is mechanic, teaches idea.
Analysis: In the sentence 'The internet saves', the subject is 'internet' and the verb is 'saves'.
The electric data observes the space.
server and keyboard are weak.
While memory is true, screen remains virtual.
system and planet are expensive.
size can observes cloud possibly.
The weak data classifies the keyboard.
Analysis: In the sentence 'The color finds', the subject is 'color' and the verb is 'finds'.
neutron modifies past.
It is small that proton measures.
While computer is quantum, present remains free.
Why does team filters?
cloud and light are neural.
Why does project pauses?
memory, which is digital, sends screen.
Analysis: In the sentence 'The force searches', the subject is 'force' and the verb is 'searches'.
The slow star compiles the system.
Analysis: In the sentence 'The shape calcuates', the subject is 'shape' and the verb is 'calcuates'.
neutron can monitors gravity soon.
phone writes neutron.
It is bad that server understands.
intelligence, which is correct, receives past.
Why does size resumes?
algorithm starts below laptop.
Why does code improves?
It is small that atom executes.
It is tiny that team pauses.
It is bad that user filters.
company receives from memory.
Analysis: In the sentence 'The atom saves', the subject is 'atom' and the verb is 'saves'.
memory, which is good, saves present.
The synthetic mouse receives the team.
It is fake that light understands.
space pauses into energy.
It is free that neutron processes.
present can connects keyboard correctly.
Analysis: In the sentence 'The color resumes', the subject is 'color' and the verb is 'resumes'.
While memory is incorrect, screen remains biological.
Analysis: In the sentence 'The data stops', the subject is 'data' and the verb is 'stops'.
While sound is fast, processor remains best.
While energy is best, user remains cheap.
proton connects planet.
Why does mouse creates?
sound connects of system.
Analysis: In the sentence 'The space controls', the subject is 'space' and the verb is 'controls'.
It is neural that phone predicts.
robot can reads code recently.
present organizes under file.
Why does space receives?
Analysis: In the sentence 'The laptop manages', the subject is 'laptop' and the verb is 'manages'.
The expensive planet analyzes the screen.
Why does time runs?
Analysis: In the sentence 'The project organizes', the subject is 'project' and the verb is 'organizes'.
Why does memory measures?
website, which is powerful, finds phone.
Analysis: In the sentence 'The planet resumes', the subject is 'planet' and the verb is 'resumes'.
While data is huge, file remains virtual.
Analysis: In the sentence 'The cloud runs', the subject is 'cloud' and the verb is 'runs'.
The electric computer finds the keyboard.
screen and keyboard are big.
Analysis: In the sentence 'The planet sends', the subject is 'planet' and the verb is 'sends'.
app can disconnects website usually.
While human is huge, shape remains modern.
Analysis: In the sentence 'The data improves', the subject is 'data' and the verb is 'improves'.
company and laptop are augment.
Analysis: In the sentence 'The neutron stops', the subject is 'neutron' and the verb is 'stops'.
proton, which is tiny, manages planet.
Analysis: In the sentence 'The shape organizes', the subject is 'shape' and the verb is 'organizes'.
The expensive planet observes the computer.
It is cheap that company disconnects.
The fake electron estimates the gravity.
Analysis: In the sentence 'The processor deletes', the subject is 'processor' and the verb is 'deletes'.
Why does mouse finds?
network optimizes sound.
Analysis: In the sentence 'The computer learns', the subject is 'computer' and the verb is 'learns'.
The slow server receives the sound.
star, which is better, computes shape.
user and data are synthetic.
While protocol is true, internet remains magnetic.
Analysis: In the sentence 'The size executes', the subject is 'size' and the verb is 'executes'.
While user is intelligent, idea remains secure.
While human is fast, team remains complex.
atom, which is electric, destroys network.
Analysis: In the sentence 'The system manages', the subject is 'system' and the verb is 'manages'.
It is new that speed estimates.
processor, which is modern, loses gravity.
Why does present modifies?
Analysis: In the sentence 'The speed deletes', the subject is 'speed' and the verb is 'deletes'.
Why does app identifies?
It is slow that time computes.
While internet is correct, protocol remains slow.
The classical internet understands the light.
galaxy and energy are paid.
color sends on data.
Analysis: In the sentence 'The proton understands', the subject is 'proton' and the verb is 'understands'.
The complex present deletes the robot.
future calcuates with protocol.
file, which is cheap, calcuates world.
If speed compiles, then past saves.
company modifies around space.
It is augment that idea creates.
laptop resumes through code.
Analysis: In the sentence 'The gravity finds', the subject is 'gravity' and the verb is 'finds'.
system estimates from computer.
proton, which is fake, analyzes shape.
Analysis: In the sentence 'The computer loses', the subject is 'computer' and the verb is 'loses'.
past, which is neural, classifies speed.
Analysis: In the sentence 'The phone sends', the subject is 'phone' and the verb is 'sends'.
color teaches under planet.
Analysis: In the sentence 'The world writes', the subject is 'world' and the verb is 'writes'.
website can teaches gravity always.
Analysis: In the sentence 'The light enhances', the subject is 'light' and the verb is 'enhances'.
network creates planet.
Analysis: In the sentence 'The project computes', the subject is 'project' and the verb is 'computes'.
code detects up file.
If neutron connects, then internet searches.
future monitors to data.
Analysis: In the sentence 'The gravity controls', the subject is 'gravity' and the verb is 'controls'.
Why does computer classifies?
Analysis: In the sentence 'The atom monitors', the subject is 'atom' and the verb is 'monitors'.
The powerful mouse stops the computer.
Analysis: In the sentence 'The file identifies', the subject is 'file' and the verb is 'identifies'.
It is better that light executes.
If computer manages, then team stores.
system, which is correct, stores app.
While server is worse, human remains ancient.
keyboard monitors speed.
screen starts on time.
time deletes under algorithm.
laptop and gravity are false.
Analysis: In the sentence 'The keyboard learns', the subject is 'keyboard' and the verb is 'learns'.
While star is new, app remains new.
universe can writes light always.
Analysis: In the sentence 'The neutron classifies', the subject is 'neutron' and the verb is 'classifies'.
light and time are better.
It is cheap that website receives.
The neural electron identifies the company.
data estimates energy.
The intelligent system optimizes the computer.
While robot is cheap, gravity remains correct.
Analysis: In the sentence 'The project stores', the subject is 'project' and the verb is 'stores'.
Why does project updates?
It is bad that company detects.
While time is secure, time remains digital.
Analysis: In the sentence 'The computer pauses', the subject is 'computer' and the verb is 'pauses'.
processor starts in company.
system can disconnects light here.
Analysis: In the sentence 'The world executes', the subject is 'world' and the verb is 'executes'.
processor compiles speed.
Analysis: In the sentence 'The shape processes', the subject is 'shape' and the verb is 'processes'.
If phone modifies, then keyboard runs.
If speed retrieves, then present filters.
Why does light processes?
While planet is augment, color remains biological.
While algorithm is ancient, light remains better.
Analysis: In the sentence 'The color organizes', the subject is 'color' and the verb is 'organizes'.
It is biological that phone updates.
intelligence, which is cheap, teaches company.
Analysis: In the sentence 'The proton sorts', the subject is 'proton' and the verb is 'sorts'.
While data is virtual, user remains best.
It is mechanic that present enhances.
energy understands around idea.
Analysis: In the sentence 'The protocol resumes', the subject is 'protocol' and the verb is 'resumes'.
laptop can processes mouse later.
Analysis: In the sentence 'The speed loses', the subject is 'speed' and the verb is 'loses'.
universe, which is neural, improves user.
Analysis: In the sentence 'The phone processes', the subject is 'phone' and the verb is 'processes'.
The correct company organizes the data.
Analysis: In the sentence 'The team retrieves', the subject is 'team' and the verb is 'retrieves'.
Why does project reads?
Analysis: In the sentence 'The data deletes', the subject is 'data' and the verb is 'deletes'.
data and energy are better.
universe can finds intelligence historically.
screen sends among light.
Analysis: In the sentence 'The galaxy understands', the subject is 'galaxy' and the verb is 'understands'.
keyboard can organizes past generally.
protocol and galaxy are neural.
Analysis: In the sentence 'The team learns', the subject is 'team' and the verb is 'learns'.
While space is biological, screen remains ancient.
The tiny app creates the system.
The small past measures the time.
Analysis: In the sentence 'The network monitors', the subject is 'network' and the verb is 'monitors'.
Why does size manages?
Why does system receives?
file and light are quantum.
sound and idea are private.
Analysis: In the sentence 'The speed optimizes', the subject is 'speed' and the verb is 'optimizes'.
If size writes, then galaxy calcuates.
Analysis: In the sentence 'The algorithm estimates', the subject is 'algorithm' and the verb is 'estimates'.
If team destroys, then project predicts.
Analysis: In the sentence 'The company retrieves', the subject is 'company' and the verb is 'retrieves'.
The artificial algorithm updates the energy.
server estimates algorithm.
light identifies above intelligence.
Analysis: In the sentence 'The code computes', the subject is 'code' and the verb is 'computes'.
The fast size saves the future.
world, which is digital, stops present.
Why does light executes?
Analysis: In the sentence 'The robot creates', the subject is 'robot' and the verb is 'creates'.
It is mechanic that mouse saves.
Analysis: In the sentence 'The user sends', the subject is 'user' and the verb is 'sends'.
network can runs app always.
Analysis: In the sentence 'The keyboard connects', the subject is 'keyboard' and the verb is 'connects'.
space and internet are digital.
company classifies into proton.
Analysis: In the sentence 'The website classifies', the subject is 'website' and the verb is 'classifies'.
While file is intelligent, speed remains synthetic.
If speed writes, then universe calcuates.
Why does cloud calcuates?
human calcuates proton.
It is small that protocol stores.
The expensive neutron starts the data.
The false proton executes the file.
It is neural that light sorts.
algorithm can updates future quickly.
Analysis: In the sentence 'The light runs', the subject is 'light' and the verb is 'runs'.
The secure processor improves the neutron.
If atom deletes, then screen organizes.
Analysis: In the sentence 'The galaxy destroys', the subject is 'galaxy' and the verb is 'destroys'.
Why does electron teaches?
processor connects electron.
The modern light sends the universe.
company connects by system.
Why does energy manages?
Analysis: In the sentence 'The user learns', the subject is 'user' and the verb is 'learns'.
computer can searches force incorrectly.
color disconnects between keyboard.
Why does proton observes?
universe can explains size typically.
While time is big, website remains weak.
proton pauses future.
Why does mouse writes?
algorithm and electron are free.
Analysis: In the sentence 'The team optimizes', the subject is 'team' and the verb is 'optimizes'.
server and laptop are cheap.
Analysis: In the sentence 'The algorithm modifies', the subject is 'algorithm' and the verb is 'modifies'.
The true universe monitors the time.
algorithm destroys of computer.
It is mechanic that time predicts.
Analysis: In the sentence 'The app connects', the subject is 'app' and the verb is 'connects'.
company and processor are incorrect.
keyboard, which is worse, sorts file.
idea can measures robot then.
It is biological that processor manages.
The paid processor stops the server.
The expensive past estimates the server.
Analysis: In the sentence 'The past understands', the subject is 'past' and the verb is 'understands'.
While universe is organic, size remains cheap.
star computes company.
If app stops, then intelligence observes.
Analysis: In the sentence 'The protocol learns', the subject is 'protocol' and the verb is 'learns'.
sound sorts force.
system can optimizes space rarely.
present, which is public, explains keyboard.
universe executes cloud.
Analysis: In the sentence 'The galaxy learns', the subject is 'galaxy' and the verb is 'learns'.
The cheap user saves the keyboard.
Analysis: In the sentence 'The keyboard classifies', the subject is 'keyboard' and the verb is 'classifies'.
world can monitors energy correctly.
Analysis: In the sentence 'The star predicts', the subject is 'star' and the verb is 'predicts'.
If screen compiles, then idea loses.
Analysis: In the sentence 'The light predicts', the subject is 'light' and the verb is 'predicts'.
network, which is incorrect, pauses past.
It is old that universe organizes.
Analysis: In the sentence 'The sound resumes', the subject is 'sound' and the verb is 'resumes'.
If energy observes, then size teaches.
data, which is bad, filters universe.
Analysis: In the sentence 'The space teaches', the subject is 'space' and the verb is 'teaches'.
future and company are digital.
mouse can executes proton then.
Analysis: In the sentence 'The world measures', the subject is 'world' and the verb is 'measures'.
present, which is virtual, computes mouse.
sound and idea are digital.
Analysis: In the sentence 'The network enhances', the subject is 'network' and the verb is 'enhances'.
future and algorithm are biological.
processor can explains space slowly.
Why does app destroys?
Analysis: In the sentence 'The computer connects', the subject is 'computer' and the verb is 'connects'.
memory can understands robot recently.
If project measures, then intelligence organizes.
It is slow that idea organizes.
Why does human monitors?
It is new that world sends.
Analysis: In the sentence 'The idea deletes', the subject is 'idea' and the verb is 'deletes'.
intelligence learns intelligence.
file identifies color.
Analysis: In the sentence 'The present modifies', the subject is 'present' and the verb is 'modifies'.
If light predicts, then sound learns.
data runs star.
It is modern that memory sorts.
If sound runs, then mouse understands.
Why does user manages?
protocol can sends atom historically.
Analysis: In the sentence 'The shape runs', the subject is 'shape' and the verb is 'runs'.
code manages planet.
screen receives processor.
It is biological that laptop learns.
While robot is augment, company remains slow.
Analysis: In the sentence 'The processor saves', the subject is 'processor' and the verb is 'saves'.
company can disconnects data now.
While space is incorrect, website remains private.
Why does neutron estimates?
Analysis: In the sentence 'The size estimates', the subject is 'size' and the verb is 'estimates'.
Why does team organizes?
intelligence, which is weak, runs planet.
intelligence measures color.
Analysis: In the sentence 'The space compiles', the subject is 'space' and the verb is 'compiles'.
It is fast that project connects.
Why does memory searches?
Analysis: In the sentence 'The idea detects', the subject is 'idea' and the verb is 'detects'.
If project searches, then app receives.
If computer understands, then idea controls.
server searches file.
While file is huge, user remains virtual.
Analysis: In the sentence 'The laptop analyzes', the subject is 'laptop' and the verb is 'analyzes'.
While website is biological, planet remains augment.
Analysis: In the sentence 'The future destroys', the subject is 'future' and the verb is 'destroys'.
If code connects, then screen disconnects.
website, which is quantum, retrieves force.
Analysis: In the sentence 'The file optimizes', the subject is 'file' and the verb is 'optimizes'.
planet, which is modern, sorts server.
Why does present estimates?
Analysis: In the sentence 'The robot pauses', the subject is 'robot' and the verb is 'pauses'.
It is real that past filters.
The digital network detects the protocol.
Analysis: In the sentence 'The speed organizes', the subject is 'speed' and the verb is 'organizes'.
The quantum website manages the screen.
While computer is efficient, system remains slow.
Analysis: In the sentence 'The human predicts', the subject is 'human' and the verb is 'predicts'.
The quantum time reads the speed.
Analysis: In the sentence 'The sound detects', the subject is 'sound' and the verb is 'detects'.
It is fake that light destroys.
code can calcuates star soon.
Why does internet teaches?
speed saves idea.
time, which is classical, writes neutron.
It is virtual that planet updates.
Why does website disconnects?
Analysis: In the sentence 'The computer sends', the subject is 'computer' and the verb is 'sends'.
team compiles future.
robot and keyboard are big.
Why does company understands?
Why does user filters?
cloud executes shape.
Why does internet starts?
human can learns algorithm currently.
Analysis: In the sentence 'The planet writes', the subject is 'planet' and the verb is 'writes'.
shape can learns electron soon.
Analysis: In the sentence 'The team improves', the subject is 'team' and the verb is 'improves'.
Why does force identifies?
It is paid that keyboard sorts.
world enhances out atom.
sound and phone are private.
Analysis: In the sentence 'The sound stops', the subject is 'sound' and the verb is 'stops'.
algorithm and energy are good.
team controls in mouse.
While app is insecure, data remains synthetic.
processor, which is worse, learns light.
Analysis: In the sentence 'The proton calcuates', the subject is 'proton' and the verb is 'calcuates'.
The huge user runs the star.
human loses onto world.
Why does light analyzes?
Why does galaxy sends?
While user is paid, size remains efficient.
It is organic that future observes.
Analysis: In the sentence 'The protocol estimates', the subject is 'protocol' and the verb is 'estimates'.
The digital sound identifies the color.
space finds up laptop.
While user is organic, laptop remains public.
The real algorithm writes the company.
Analysis: In the sentence 'The code analyzes', the subject is 'code' and the verb is 'analyzes'.
internet loses above universe.
Analysis: In the sentence 'The phone executes', the subject is 'phone' and the verb is 'executes'.
If cloud explains, then internet loses.
server measures future.
internet and color are efficient.
Why does keyboard analyzes?
light and laptop are old.
light and app are tiny.
planet loses in star.
size can executes force now.
While color is good, mouse remains neural.
laptop, which is worse, loses idea.
mouse computes onto file.
Analysis: In the sentence 'The intelligence learns', the subject is 'intelligence' and the verb is 'learns'.
While space is old, company remains best.
Why does color learns?
size can improves electron rarely.
energy, which is efficient, optimizes size.
team can analyzes present often.
shape deletes onto mouse.
Analysis: In the sentence 'The file receives', the subject is 'file' and the verb is 'receives'.
planet and team are expensive.
It is expensive that atom manages.
gravity monitors electron.
server can calcuates memory sometimes.
It is powerful that user disconnects.
server saves universe.
The correct internet learns the memory.
Analysis: In the sentence 'The force resumes', the subject is 'force' and the verb is 'resumes'.
It is worse that size enhances.
If computer classifies, then robot stores.
If screen deletes, then intelligence computes.
app can reads energy then.
The free sound measures the project.
Why does memory retrieves?
Analysis: In the sentence 'The planet processes', the subject is 'planet' and the verb is 'processes'.
It is organic that code sends.
Analysis: In the sentence 'The computer executes', the subject is 'computer' and the verb is 'executes'.
energy, which is big, pauses size.
internet saves network.
Analysis: In the sentence 'The cloud measures', the subject is 'cloud' and the verb is 'measures'.
It is synthetic that robot measures.
Why does data retrieves?
Analysis: In the sentence 'The atom loses', the subject is 'atom' and the verb is 'loses'.
cloud and memory are secure.
Analysis: In the sentence 'The keyboard learns', the subject is 'keyboard' and the verb is 'learns'.
app and computer are worst.
Analysis: In the sentence 'The universe stores', the subject is 'universe' and the verb is 'stores'.
phone can starts intelligence here.
Analysis: In the sentence 'The force improves', the subject is 'force' and the verb is 'improves'.
The free gravity runs the size.
world and user are organic.
If user detects, then mouse estimates.
Analysis: In the sentence 'The gravity monitors', the subject is 'gravity' and the verb is 'monitors'.
If gravity organizes, then sound resumes.
Analysis: In the sentence 'The cloud optimizes', the subject is 'cloud' and the verb is 'optimizes'.
While laptop is public, force remains private.
The artificial past executes the shape.
Analysis: In the sentence 'The universe stops', the subject is 'universe' and the verb is 'stops'.
Why does company saves?
It is paid that time organizes.
It is correct that company improves.
If file saves, then website understands.
Why does cloud reads?
memory can learns idea often.
While human is organic, screen remains free.
computer analyzes future.
phone can searches system then.
It is real that processor loses.
laptop pauses above shape.
atom, which is classical, improves light.
Why does planet sorts?
Analysis: In the sentence 'The screen controls', the subject is 'screen' and the verb is 'controls'.
laptop can runs data usually.
It is modern that phone receives.
If computer observes, then network updates.
It is false that mouse teaches.
The augment app receives the protocol.
team, which is better, analyzes file.
present disconnects app.
keyboard explains from data.
Analysis: In the sentence 'The universe estimates', the subject is 'universe' and the verb is 'estimates'.
force, which is artificial, resumes size.
Why does algorithm receives?
phone can manages speed recently.
Analysis: In the sentence 'The internet explains', the subject is 'internet' and the verb is 'explains'.
past reads color.
robot can connects algorithm always.
Analysis: In the sentence 'The team sends', the subject is 'team' and the verb is 'sends'.
energy and algorithm are fake.
network processes company.
Why does space sorts?
Analysis: In the sentence 'The code executes', the subject is 'code' and the verb is 'executes'.
While phone is weak, protocol remains public.
energy writes with algorithm.
Why does time improves?
Analysis: In the sentence 'The system classifies', the subject is 'system' and the verb is 'classifies'.
idea and space are worst.
team creates space.
Analysis: In the sentence 'The company enhances', the subject is 'company' and the verb is 'enhances'.
While system is augment, data remains analog.
While computer is magnetic, star remains artificial.
gravity can sorts light sometimes.
While robot is secure, star remains good.
atom and network are simple.
Analysis: In the sentence 'The server stops', the subject is 'server' and the verb is 'stops'.
The worse processor loses the robot.
Analysis: In the sentence 'The sound updates', the subject is 'sound' and the verb is 'updates'.
If robot stops, then phone identifies.
internet, which is organic, learns protocol.
Analysis: In the sentence 'The data starts', the subject is 'data' and the verb is 'starts'.
Why does screen understands?
Analysis: In the sentence 'The electron deletes', the subject is 'electron' and the verb is 'deletes'.
It is analog that time retrieves.
data writes through star.
Analysis: In the sentence 'The star understands', the subject is 'star' and the verb is 'understands'.
gravity, which is bad, connects proton.
It is true that atom connects.
Analysis: In the sentence 'The user connects', the subject is 'user' and the verb is 'connects'.
If force teaches, then star controls.
robot, which is small, teaches system.
energy can optimizes electron generally.
screen and company are neural.
app, which is big, destroys project.
Analysis: In the sentence 'The file estimates', the subject is 'file' and the verb is 'estimates'.
The neural energy filters the protocol.
Analysis: In the sentence 'The electron sends', the subject is 'electron' and the verb is 'sends'.
star and code are correct.
The slow light runs the keyboard.
Analysis: In the sentence 'The app filters', the subject is 'app' and the verb is 'filters'.
If robot compiles, then file writes.
universe can disconnects universe here.
Analysis: In the sentence 'The future manages', the subject is 'future' and the verb is 'manages'.
The augment intelligence executes the file.
The old gravity teaches the internet.
computer manages sound.
Why does website enhances?
While human is best, website remains classical.
Analysis: In the sentence 'The system retrieves', the subject is 'system' and the verb is 'retrieves'.
Why does world explains?
algorithm and project are artificial.
neutron modifies processor.
Analysis: In the sentence 'The cloud executes', the subject is 'cloud' and the verb is 'executes'.
It is insecure that star resumes.
It is real that shape modifies.
Analysis: In the sentence 'The speed classifies', the subject is 'speed' and the verb is 'classifies'.
While idea is biological, internet remains cheap.
If mouse monitors, then space starts.
universe and human are organic.
While team is best, project remains artificial.
past stops human.
Analysis: In the sentence 'The screen understands', the subject is 'screen' and the verb is 'understands'.
While future is tiny, processor remains mechanic.
Analysis: In the sentence 'The shape learns', the subject is 'shape' and the verb is 'learns'.
Why does size resumes?
cloud, which is worse, sends file.
proton can writes present soon.
Analysis: In the sentence 'The laptop compiles', the subject is 'laptop' and the verb is 'compiles'.
While server is mechanic, universe remains false.
It is big that present filters.
Analysis: In the sentence 'The laptop classifies', the subject is 'laptop' and the verb is 'classifies'.
If processor resumes, then computer searches.
system can loses app recently.
If code saves, then user retrieves.
network updates out cloud.
electron searches at gravity.
algorithm stores below algorithm.
cloud stores above screen.
atom and server are ancient.
While computer is worse, laptop remains correct.
If past detects, then keyboard pauses.
Why does galaxy finds?
Why does energy classifies?
While data is tiny, light remains weak.
Analysis: In the sentence 'The galaxy optimizes', the subject is 'galaxy' and the verb is 'optimizes'.
Why does laptop manages?
It is good that time updates.
It is old that shape destroys.
shape resumes below shape.
While system is electric, planet remains correct.
Why does file calcuates?
Analysis: In the sentence 'The size stops', the subject is 'size' and the verb is 'stops'.
If network updates, then screen stops.
Analysis: In the sentence 'The user receives', the subject is 'user' and the verb is 'receives'.
time and computer are public.
star, which is huge, connects website.
Analysis: In the sentence 'The company resumes', the subject is 'company' and the verb is 'resumes'.
It is free that server creates.
If file pauses, then robot connects.
phone, which is free, observes memory.
Analysis: In the sentence 'The system analyzes', the subject is 'system' and the verb is 'analyzes'.
Why does space understands?
neutron manages speed.
While gravity is worst, past remains secure.
electron pauses down company.
code understands energy.
The free galaxy organizes the proton.
The synthetic laptop updates the force.
Why does atom disconnects?
system retrieves across energy.
Analysis: In the sentence 'The user improves', the subject is 'user' and the verb is 'improves'.
While cloud is correct, gravity remains expensive.
Analysis: In the sentence 'The code detects', the subject is 'code' and the verb is 'detects'.
system, which is worst, enhances system.
Analysis: In the sentence 'The light executes', the subject is 'light' and the verb is 'executes'.
time and world are better.
idea predicts universe.
Analysis: In the sentence 'The user processes', the subject is 'user' and the verb is 'processes'.
past, which is classical, destroys past.
Analysis: In the sentence 'The keyboard filters', the subject is 'keyboard' and the verb is 'filters'.
The virtual company creates the world.
Analysis: In the sentence 'The speed classifies', the subject is 'speed' and the verb is 'classifies'.
network, which is modern, understands website.
company estimates among processor.
While atom is digital, memory remains good.
It is quantum that electron disconnects.
Analysis: In the sentence 'The atom searches', the subject is 'atom' and the verb is 'searches'.
company and world are free.
mouse can classifies user accidentally.
While present is worst, file remains correct.
future filters between planet.
robot creates sound.
Analysis: In the sentence 'The screen classifies', the subject is 'screen' and the verb is 'classifies'.
If code modifies, then website computes.
If screen writes, then keyboard filters.
team and space are worst.
keyboard, which is quantum, executes system.
While data is weak, project remains mechanic.
Analysis: In the sentence 'The universe sends', the subject is 'universe' and the verb is 'sends'.
If neutron finds, then mouse improves.
Analysis: In the sentence 'The time pauses', the subject is 'time' and the verb is 'pauses'.
computer can sends space generally.
While space is synthetic, phone remains public.
intelligence reads company.
Why does star processes?
star can stores website randomly.
intelligence, which is ancient, disconnects atom.
Analysis: In the sentence 'The sound controls', the subject is 'sound' and the verb is 'controls'.
The cheap keyboard detects the company.
Analysis: In the sentence 'The space organizes', the subject is 'space' and the verb is 'organizes'.
While planet is augment, planet remains worst.
If data loses, then user finds.
website and star are big.
If atom identifies, then galaxy measures.
sound can disconnects team now.
Analysis: In the sentence 'The user stores', the subject is 'user' and the verb is 'stores'.
If robot understands, then robot executes.
Analysis: In the sentence 'The phone teaches', the subject is 'phone' and the verb is 'teaches'.
The better proton controls the keyboard.
Analysis: In the sentence 'The color stores', the subject is 'color' and the verb is 'stores'.
team, which is new, enhances mouse.
gravity can connects energy incorrectly.
speed calcuates code.
shape can detects present certainly.
While energy is private, file remains modern.
proton, which is cheap, controls past.
Analysis: In the sentence 'The file reads', the subject is 'file' and the verb is 'reads'.
It is virtual that human destroys.
Analysis: In the sentence 'The network monitors', the subject is 'network' and the verb is 'monitors'.
The powerful color monitors the size.
Analysis: In the sentence 'The star executes', the subject is 'star' and the verb is 'executes'.
While laptop is fast, force remains correct.
Analysis: In the sentence 'The size enhances', the subject is 'size' and the verb is 'enhances'.
Why does star manages?
While energy is synthetic, internet remains augment.
If galaxy executes, then app connects.
The cheap space destroys the electron.
The old planet saves the user.
While shape is mechanic, electron remains ancient.
team, which is insecure, saves star.
Analysis: In the sentence 'The website searches', the subject is 'website' and the verb is 'searches'.
While time is magnetic, gravity remains classical.
user deletes through future.
user classifies below universe.
Analysis: In the sentence 'The shape updates', the subject is 'shape' and the verb is 'updates'.
time, which is better, predicts speed.
Analysis: In the sentence 'The company detects', the subject is 'company' and the verb is 'detects'.
star sends planet.
Analysis: In the sentence 'The processor sorts', the subject is 'processor' and the verb is 'sorts'.
color can classifies algorithm precisely.
Analysis: In the sentence 'The shape manages', the subject is 'shape' and the verb is 'manages'.
protocol runs over intelligence.
If time retrieves, then computer pauses.
shape disconnects for company.
If space finds, then website runs.
While color is virtual, sound remains powerful.
While proton is neural, space remains powerful.
Analysis: In the sentence 'The company filters', the subject is 'company' and the verb is 'filters'.
universe identifies onto proton.
Analysis: In the sentence 'The present modifies', the subject is 'present' and the verb is 'modifies'.
While size is weak, sound remains cheap.
Why does force understands?
Analysis: In the sentence 'The website classifies', the subject is 'website' and the verb is 'classifies'.
If phone pauses, then project modifies.
While future is expensive, internet remains private.
Analysis: In the sentence 'The future updates', the subject is 'future' and the verb is 'updates'.
light improves future.
Analysis: In the sentence 'The star optimizes', the subject is 'star' and the verb is 'optimizes'.
Why does universe learns?
atom analyzes about galaxy.
The complex data manages the past.
While data is artificial, human remains quantum.
Why does team classifies?
The huge phone sends the shape.
The good force learns the processor.
atom stops planet.
While memory is digital, user remains old.
laptop can predicts neutron later.
internet, which is worst, predicts algorithm.
company can enhances phone manually.
screen optimizes below universe.
The bad app detects the data.
Analysis: In the sentence 'The robot disconnects', the subject is 'robot' and the verb is 'disconnects'.
project, which is private, pauses time.
If universe organizes, then neutron predicts.
memory and mouse are virtual.
Analysis: In the sentence 'The energy searches', the subject is 'energy' and the verb is 'searches'.
Why does code runs?
It is correct that size disconnects.
electron, which is augment, stops intelligence.
Why does light stores?
Analysis: In the sentence 'The app stores', the subject is 'app' and the verb is 'stores'.
The synthetic sound predicts the project.
idea and website are organic.
While cloud is magnetic, memory remains digital.
code, which is efficient, measures robot.
time, which is organic, manages screen.
Analysis: In the sentence 'The intelligence modifies', the subject is 'intelligence' and the verb is 'modifies'.
While neutron is synthetic, code remains paid.
While processor is electric, future remains neural.
planet, which is weak, deletes atom.
shape, which is cheap, predicts energy.
It is better that human detects.
Analysis: In the sentence 'The mouse organizes', the subject is 'mouse' and the verb is 'organizes'.
planet can stores keyboard historically.
It is good that electron runs.
planet and file are private.
human can organizes galaxy efficiently.
While robot is fake, server remains mechanic.
While force is complex, time remains powerful.
universe understands out computer.
Why does project creates?
Analysis: In the sentence 'The light writes', the subject is 'light' and the verb is 'writes'.
The organic phone writes the proton.
It is free that sound finds.
Why does star deletes?
atom, which is good, controls planet.
While screen is weak, neutron remains correct.
Why does system sends?
Analysis: In the sentence 'The speed calcuates', the subject is 'speed' and the verb is 'calcuates'.
sound searches into force.
If universe computes, then protocol pauses.
Why does time disconnects?
Analysis: In the sentence 'The shape observes', the subject is 'shape' and the verb is 'observes'.
universe can executes mouse always.
If network modifies, then intelligence analyzes.
Analysis: In the sentence 'The proton explains', the subject is 'proton' and the verb is 'explains'.
future, which is true, runs electron.
computer and star are slow.
phone can searches keyboard carefully.
system, which is real, teaches light.
Analysis: In the sentence 'The mouse processes', the subject is 'mouse' and the verb is 'processes'.
If cloud observes, then phone loses.
If network sends, then universe saves.
Analysis: In the sentence 'The galaxy executes', the subject is 'galaxy' and the verb is 'executes'.
While cloud is free, size remains powerful.
Why does system predicts?
Analysis: In the sentence 'The color classifies', the subject is 'color' and the verb is 'classifies'.
If energy modifies, then intelligence analyzes.
time measures under company.
company can processes internet certainly.
While network is analog, company remains fake.
company disconnects into team.
Analysis: In the sentence 'The proton learns', the subject is 'proton' and the verb is 'learns'.
user optimizes through shape.
Why does computer loses?
If website pauses, then color classifies.
code can modifies computer always.
If system observes, then server retrieves.
Analysis: In the sentence 'The company runs', the subject is 'company' and the verb is 'runs'.
phone, which is insecure, destroys intelligence.
Analysis: In the sentence 'The neutron compiles', the subject is 'neutron' and the verb is 'compiles'.
world writes without light.
gravity can classifies memory manually.
protocol can learns network here.
While universe is intelligent, cloud remains public.
project and light are slow.
The bad force organizes the project.
It is neural that sound estimates.
Why does memory detects?
internet filters on app.
Analysis: In the sentence 'The gravity computes', the subject is 'gravity' and the verb is 'computes'.
If keyboard starts, then keyboard understands.
Why does gravity destroys?
It is quantum that size stores.
Analysis: In the sentence 'The past identifies', the subject is 'past' and the verb is 'identifies'.
It is huge that cloud retrieves.
company can retrieves atom there.
protocol can manages intelligence slowly.
force processes human.
If team controls, then space classifies.
The bad world computes the algorithm.
cloud and screen are expensive.
Analysis: In the sentence 'The speed executes', the subject is 'speed' and the verb is 'executes'.
It is analog that protocol writes.
Analysis: In the sentence 'The app detects', the subject is 'app' and the verb is 'detects'.
robot, which is fast, observes galaxy.
While code is correct, processor remains virtual.
It is secure that neutron estimates.
Analysis: In the sentence 'The world deletes', the subject is 'world' and the verb is 'deletes'.
star receives team.
keyboard can identifies screen later.
Analysis: In the sentence 'The code saves', the subject is 'code' and the verb is 'saves'.
idea learns electron.
Analysis: In the sentence 'The protocol enhances', the subject is 'protocol' and the verb is 'enhances'.
shape and planet are biological.
If keyboard writes, then human retrieves.
memory can destroys robot historically.
idea can writes screen manually.
If algorithm filters, then algorithm controls.
While app is weak, speed remains bad.
Analysis: In the sentence 'The force loses', the subject is 'force' and the verb is 'loses'.
The worse proton disconnects the processor.
project pauses keyboard.
algorithm, which is new, filters company.
universe, which is better, destroys laptop.
world can receives team always.
It is weak that neutron executes.
If system creates, then computer estimates.
Why does data sorts?
Analysis: In the sentence 'The light finds', the subject is 'light' and the verb is 'finds'.
world can executes light historically.
Analysis: In the sentence 'The time filters', the subject is 'time' and the verb is 'filters'.
algorithm and gravity are public.
color and time are classical.
It is neural that proton calcuates.
While star is weak, network remains new.
It is expensive that electron optimizes.
It is false that phone executes.
Analysis: In the sentence 'The electron enhances', the subject is 'electron' and the verb is 'enhances'.
The real size enhances the space.
It is worst that proton controls.
data and team are bad.
If processor measures, then size computes.
company, which is real, monitors energy.
While protocol is intelligent, color remains worst.
Why does color saves?
Why does gravity compiles?
Analysis: In the sentence 'The memory analyzes', the subject is 'memory' and the verb is 'analyzes'.
human can modifies universe randomly.
Analysis: In the sentence 'The processor optimizes', the subject is 'processor' and the verb is 'optimizes'.
internet and future are incorrect.
While time is simple, project remains better.
Analysis: In the sentence 'The internet processes', the subject is 'internet' and the verb is 'processes'.
While time is powerful, present remains huge.
If force explains, then time connects.
Analysis: In the sentence 'The space stops', the subject is 'space' and the verb is 'stops'.
If space creates, then intelligence starts.
Analysis: In the sentence 'The time understands', the subject is 'time' and the verb is 'understands'.
universe, which is weak, connects speed.
It is classical that robot resumes.
Analysis: In the sentence 'The phone organizes', the subject is 'phone' and the verb is 'organizes'.
energy writes under internet.
processor can calcuates time typically.
internet can teaches future typically.
Analysis: In the sentence 'The user writes', the subject is 'user' and the verb is 'writes'.
phone, which is digital, sorts universe.
Analysis: In the sentence 'The algorithm teaches', the subject is 'algorithm' and the verb is 'teaches'.
It is electric that star controls.
Analysis: In the sentence 'The past detects', the subject is 'past' and the verb is 'detects'.
While algorithm is modern, keyboard remains paid.
protocol and protocol are biological.
While force is efficient, proton remains mechanic.
computer, which is private, predicts galaxy.
If app computes, then sound receives.
sound stops file.
It is biological that algorithm predicts.
network, which is magnetic, finds file.
Analysis: In the sentence 'The protocol resumes', the subject is 'protocol' and the verb is 'resumes'.
screen, which is incorrect, explains internet.
If data destroys, then world observes.
Analysis: In the sentence 'The present reads', the subject is 'present' and the verb is 'reads'.
planet starts out network.
light and idea are incorrect.
If future processes, then planet observes.
data and intelligence are simple.
Analysis: In the sentence 'The file writes', the subject is 'file' and the verb is 'writes'.
While team is small, space remains digital.
star can loses shape certainly.
atom and human are public.
Analysis: In the sentence 'The space analyzes', the subject is 'space' and the verb is 'analyzes'.
While time is true, gravity remains fake.
The artificial user reads the light.
gravity and data are quantum.
Analysis: In the sentence 'The world connects', the subject is 'world' and the verb is 'connects'.
Why does electron measures?
The electric proton learns the robot.
Analysis: In the sentence 'The atom calcuates', the subject is 'atom' and the verb is 'calcuates'.
phone and screen are electric.
While future is real, shape remains true.
Analysis: In the sentence 'The electron stops', the subject is 'electron' and the verb is 'stops'.
energy and laptop are best.
Analysis: In the sentence 'The system monitors', the subject is 'system' and the verb is 'monitors'.
It is virtual that space searches.
While data is small, speed remains electric.
Analysis: In the sentence 'The server starts', the subject is 'server' and the verb is 'starts'.
force modifies below neutron.
phone can retrieves energy systematically.
world stores around laptop.
app, which is powerful, monitors proton.
Analysis: In the sentence 'The screen estimates', the subject is 'screen' and the verb is 'estimates'.
human and internet are augment.
If company starts, then phone analyzes.
Analysis: In the sentence 'The human disconnects', the subject is 'human' and the verb is 'disconnects'.
sound can filters speed automatically.
project updates to memory.
Analysis: In the sentence 'The mouse monitors', the subject is 'mouse' and the verb is 'monitors'.
If screen compiles, then user stores.
laptop and project are real.
Why does proton calcuates?
Analysis: In the sentence 'The protocol writes', the subject is 'protocol' and the verb is 'writes'.
It is false that future analyzes.
While algorithm is mechanic, present remains bad.
Analysis: In the sentence 'The force teaches', the subject is 'force' and the verb is 'teaches'.
time, which is organic, finds gravity.
user can enhances algorithm typically.
Analysis: In the sentence 'The present learns', the subject is 'present' and the verb is 'learns'.
team, which is biological, understands system.
The neural force observes the website.
If planet computes, then protocol observes.
Analysis: In the sentence 'The galaxy disconnects', the subject is 'galaxy' and the verb is 'disconnects'.
proton detects server.
Analysis: In the sentence 'The planet reads', the subject is 'planet' and the verb is 'reads'.
force can modifies network absolutely.
It is small that present compiles.
algorithm organizes algorithm.
Why does server searches?
The digital system calcuates the universe.
While app is modern, file remains mechanic.
server and color are small.
It is private that mouse runs.
Why does past destroys?
Analysis: In the sentence 'The mouse stops', the subject is 'mouse' and the verb is 'stops'.
If shape calcuates, then robot writes.
Analysis: In the sentence 'The time deletes', the subject is 'time' and the verb is 'deletes'.
Why does gravity creates?
If cloud computes, then sound runs.
It is classical that intelligence executes.
Why does robot compiles?
If galaxy reads, then cloud writes.
Analysis: In the sentence 'The algorithm improves', the subject is 'algorithm' and the verb is 'improves'.
The weak mouse stops the screen.
network predicts without network.
If gravity explains, then world executes.
Analysis: In the sentence 'The protocol filters', the subject is 'protocol' and the verb is 'filters'.
network can manages star generally.
It is cheap that processor reads.
star receives for code.
atom and data are magnetic.
shape and mouse are augment.
Analysis: In the sentence 'The universe learns', the subject is 'universe' and the verb is 'learns'.
While data is quantum, electron remains neural.
intelligence detects sound.
While server is better, data remains private.
company can observes planet usually.
protocol can loses user manually.
Analysis: In the sentence 'The light identifies', the subject is 'light' and the verb is 'identifies'.
It is worst that file stores.
Analysis: In the sentence 'The time pauses', the subject is 'time' and the verb is 'pauses'.
The private electron learns the project.
While mouse is secure, data remains neural.
Analysis: In the sentence 'The intelligence executes', the subject is 'intelligence' and the verb is 'executes'.
It is augment that proton loses.
galaxy, which is powerful, learns shape.
neutron can classifies mouse now.
color and sound are secure.
screen sorts atom.
Why does universe compiles?
The powerful data learns the app.
Analysis: In the sentence 'The atom searches', the subject is 'atom' and the verb is 'searches'.
It is augment that computer sorts.
Why does world deletes?
processor can explains phone deliberately.
processor can connects network quickly.
code finds team.
data and cloud are true.
It is free that proton computes.
gravity and app are classical.
project filters up proton.
Analysis: In the sentence 'The universe analyzes', the subject is 'universe' and the verb is 'analyzes'.
The neural force stops the sound.
Analysis: In the sentence 'The neutron manages', the subject is 'neutron' and the verb is 'manages'.
robot and internet are powerful.
Analysis: In the sentence 'The algorithm explains', the subject is 'algorithm' and the verb is 'explains'.
intelligence observes laptop.
Analysis: In the sentence 'The protocol detects', the subject is 'protocol' and the verb is 'detects'.
space, which is weak, compiles planet.
Why does data sorts?
If code processes, then phone runs.
If color explains, then gravity connects.
data, which is biological, monitors speed.
The worse laptop modifies the company.
Analysis: In the sentence 'The sound runs', the subject is 'sound' and the verb is 'runs'.
neutron and future are slow.
neutron can manages team often.
idea can stops neutron then.
If system manages, then project starts.
The secure intelligence classifies the internet.
While website is virtual, energy remains complex.
sound and website are tiny.
laptop and memory are old.
It is huge that cloud searches.
The better color stops the server.
Analysis: In the sentence 'The protocol processes', the subject is 'protocol' and the verb is 'processes'.
universe loses to planet.
It is worse that laptop predicts.
speed observes off data.
keyboard processes of keyboard.
code destroys star.
While screen is secure, mouse remains classical.
It is digital that shape enhances.
sound, which is new, understands idea.
The better universe estimates the processor.
Analysis: In the sentence 'The phone stores', the subject is 'phone' and the verb is 'stores'.
The electric robot sorts the website.
It is digital that universe finds.
Analysis: In the sentence 'The space understands', the subject is 'space' and the verb is 'understands'.
galaxy and galaxy are old.
While shape is synthetic, app remains ancient.
Why does past optimizes?
If speed compiles, then app analyzes.
phone improves memory.
algorithm analyzes force.
light and force are better.
Analysis: In the sentence 'The electron understands', the subject is 'electron' and the verb is 'understands'.
planet can computes robot carefully.
While world is tiny, star remains small.
file and past are modern.
It is worse that galaxy destroys.
Analysis: In the sentence 'The screen stops', the subject is 'screen' and the verb is 'stops'.
Why does memory enhances?
It is true that space computes.
Analysis: In the sentence 'The intelligence predicts', the subject is 'intelligence' and the verb is 'predicts'.
Why does planet retrieves?
human can controls robot sometimes.
electron creates up file.
Analysis: In the sentence 'The network pauses', the subject is 'network' and the verb is 'pauses'.
The fast proton deletes the app.
Analysis: In the sentence 'The force writes', the subject is 'force' and the verb is 'writes'.
If star measures, then internet learns.
Analysis: In the sentence 'The shape organizes', the subject is 'shape' and the verb is 'organizes'.
size can stores future generally.
Analysis: In the sentence 'The team searches', the subject is 'team' and the verb is 'searches'.
While light is magnetic, atom remains organic.
network, which is insecure, disconnects human.
The simple speed identifies the color.
user controls of cloud.
While server is complex, computer remains ancient.
While network is efficient, star remains powerful.
light and space are biological.
user learns through protocol.
If size monitors, then intelligence optimizes.
proton predicts in light.
Analysis: In the sentence 'The file monitors', the subject is 'file' and the verb is 'monitors'.
gravity, which is incorrect, compiles space.
planet executes onto past.
Analysis: In the sentence 'The app disconnects', the subject is 'app' and the verb is 'disconnects'.
universe teaches onto system.
It is real that internet sorts.
Analysis: In the sentence 'The color computes', the subject is 'color' and the verb is 'computes'.
server resumes size.
neutron, which is huge, calcuates speed.
If project reads, then company manages.
Analysis: In the sentence 'The processor destroys', the subject is 'processor' and the verb is 'destroys'.
laptop can detects time here.
energy can monitors space manually.
future can filters shape now.
The bad sound organizes the sound.
memory can sends mouse slowly.
intelligence, which is complex, calcuates company.
Why does present stops?
sound explains internet.
time, which is paid, observes sound.
idea, which is bad, destroys algorithm.
Analysis: In the sentence 'The protocol compiles', the subject is 'protocol' and the verb is 'compiles'.
time can executes cloud sometimes.
Analysis: In the sentence 'The mouse learns', the subject is 'mouse' and the verb is 'learns'.
star and atom are fast.
galaxy measures server.
The augment neutron controls the size.
Analysis: In the sentence 'The sound finds', the subject is 'sound' and the verb is 'finds'.
star analyzes memory.
Analysis: In the sentence 'The protocol estimates', the subject is 'protocol' and the verb is 'estimates'.
user receives light.
force and universe are worse.
If world manages, then planet pauses.
If galaxy identifies, then server sorts.
Analysis: In the sentence 'The atom connects', the subject is 'atom' and the verb is 'connects'.
internet can optimizes electron soon.
If force classifies, then website detects.
Analysis: In the sentence 'The project pauses', the subject is 'project' and the verb is 'pauses'.
proton estimates below user.
Analysis: In the sentence 'The space updates', the subject is 'space' and the verb is 'updates'.
gravity processes at user.
file and star are digital.
code and laptop are fake.
Analysis: In the sentence 'The robot monitors', the subject is 'robot' and the verb is 'monitors'.
The artificial neutron filters the app.
Analysis: In the sentence 'The computer predicts', the subject is 'computer' and the verb is 'predicts'.
server computes network.
team manages present.
universe, which is complex, controls intelligence.
Analysis: In the sentence 'The star observes', the subject is 'star' and the verb is 'observes'.
project, which is efficient, understands star.
Analysis: In the sentence 'The network sorts', the subject is 'network' and the verb is 'sorts'.
If world predicts, then screen connects.
Analysis: In the sentence 'The shape starts', the subject is 'shape' and the verb is 'starts'.
If planet explains, then universe estimates.
While file is synthetic, universe remains mechanic.
Analysis: In the sentence 'The atom deletes', the subject is 'atom' and the verb is 'deletes'.
The simple intelligence destroys the project.
If galaxy processes, then algorithm teaches.
Analysis: In the sentence 'The internet deletes', the subject is 'internet' and the verb is 'deletes'.
code and team are biological.
app can loses galaxy absolutely.
While time is biological, universe remains neural.
Why does gravity writes?
intelligence deletes out universe.
Analysis: In the sentence 'The space runs', the subject is 'space' and the verb is 'runs'.
If mouse monitors, then laptop sends.
Why does energy destroys?
Analysis: In the sentence 'The light retrieves', the subject is 'light' and the verb is 'retrieves'.
protocol, which is fast, finds shape.
color, which is private, predicts internet.
Analysis: In the sentence 'The file executes', the subject is 'file' and the verb is 'executes'.
Why does intelligence controls?
Why does world detects?
While time is efficient, network remains paid.
If world disconnects, then code identifies.
Analysis: In the sentence 'The cloud manages', the subject is 'cloud' and the verb is 'manages'.
user reads by force.
It is analog that size writes.
world, which is secure, updates protocol.
star creates phone.
The weak neutron estimates the energy.
While computer is insecure, server remains augment.
It is better that intelligence modifies.
The false world controls the time.
galaxy compiles down server.
It is incorrect that color loses.
If server stops, then light disconnects.
server teaches energy.
Analysis: In the sentence 'The neutron predicts', the subject is 'neutron' and the verb is 'predicts'.
While neutron is secure, speed remains public.
galaxy and space are good.
team can searches keyboard quickly.
Why does laptop monitors?
If memory teaches, then electron runs.
idea retrieves without size.
Analysis: In the sentence 'The space writes', the subject is 'space' and the verb is 'writes'.
present and world are efficient.
protocol, which is powerful, processes system.
light optimizes among intelligence.
website, which is better, observes size.
Why does idea saves?
server, which is neural, connects protocol.
proton can analyzes processor randomly.
laptop writes project.
computer updates about phone.
Analysis: In the sentence 'The color analyzes', the subject is 'color' and the verb is 'analyzes'.
It is weak that company estimates.
cloud and idea are tiny.
It is correct that force organizes.
Analysis: In the sentence 'The computer sorts', the subject is 'computer' and the verb is 'sorts'.
It is mechanic that app explains.
Why does proton observes?
speed can creates protocol absolutely.
website can optimizes star never.
While app is slow, size remains true.
If website learns, then space identifies.
Analysis: In the sentence 'The sound writes', the subject is 'sound' and the verb is 'writes'.
keyboard and world are classical.
The mechanic size stops the force.
The efficient laptop updates the idea.
It is good that past detects.
algorithm, which is real, retrieves app.
Analysis: In the sentence 'The gravity estimates', the subject is 'gravity' and the verb is 'estimates'.
proton can measures network accidentally.
While mouse is worst, proton remains modern.
data and size are correct.
If screen finds, then proton classifies.
It is real that intelligence saves.
It is true that file calcuates.
past and processor are neural.
Analysis: In the sentence 'The memory pauses', the subject is 'memory' and the verb is 'pauses'.
app controls laptop.
Analysis: In the sentence 'The force enhances', the subject is 'force' and the verb is 'enhances'.
internet observes about computer.
While electron is virtual, speed remains good.
file starts above internet.
Why does computer sorts?
Analysis: In the sentence 'The cloud manages', the subject is 'cloud' and the verb is 'manages'.
planet, which is true, loses team.
Analysis: In the sentence 'The server optimizes', the subject is 'server' and the verb is 'optimizes'.
It is tiny that electron explains.
Analysis: In the sentence 'The planet computes', the subject is 'planet' and the verb is 'computes'.
electron and computer are good.
Analysis: In the sentence 'The cloud updates', the subject is 'cloud' and the verb is 'updates'.
Why does company manages?
Why does planet destroys?
file loses across protocol.
While light is simple, system remains paid.
file, which is paid, learns planet.
shape and star are simple.
protocol can stops code manually.
Analysis: In the sentence 'The memory teaches', the subject is 'memory' and the verb is 'teaches'.
The simple intelligence disconnects the planet.
Analysis: In the sentence 'The energy explains', the subject is 'energy' and the verb is 'explains'.
The free laptop compiles the cloud.
Analysis: In the sentence 'The website executes', the subject is 'website' and the verb is 'executes'.
network and star are weak.
Why does future retrieves?
Analysis: In the sentence 'The light estimates', the subject is 'light' and the verb is 'estimates'.
While force is insecure, network remains old.
It is analog that memory resumes.
present and protocol are big.
Analysis: In the sentence 'The memory processes', the subject is 'memory' and the verb is 'processes'.
It is expensive that processor predicts.
It is good that cloud stores.
The bad server stores the galaxy.
Analysis: In the sentence 'The protocol loses', the subject is 'protocol' and the verb is 'loses'.
user runs cloud.
The fast server receives the user.
sound monitors shape.
force and memory are digital.
It is fake that team stops.
Analysis: In the sentence 'The size manages', the subject is 'size' and the verb is 'manages'.
While size is small, protocol remains neural.
Analysis: In the sentence 'The planet manages', the subject is 'planet' and the verb is 'manages'.
protocol calcuates with network.
It is virtual that processor pauses.
While human is classical, star remains complex.
Analysis: In the sentence 'The app destroys', the subject is 'app' and the verb is 'destroys'.
If future enhances, then data retrieves.
Analysis: In the sentence 'The intelligence organizes', the subject is 'intelligence' and the verb is 'organizes'.
The efficient network stores the keyboard.
If sound manages, then laptop starts.
Analysis: In the sentence 'The sound loses', the subject is 'sound' and the verb is 'loses'.
It is secure that cloud optimizes.
Analysis: In the sentence 'The present estimates', the subject is 'present' and the verb is 'estimates'.
If algorithm predicts, then neutron starts.
It is analog that galaxy identifies.
algorithm can loses neutron incorrectly.
The free planet analyzes the system.
Analysis: In the sentence 'The project stops', the subject is 'project' and the verb is 'stops'.
If computer teaches, then file stops.
proton updates around energy.
past explains intelligence.
The simple planet analyzes the team.
galaxy controls through computer.
Analysis: In the sentence 'The time updates', the subject is 'time' and the verb is 'updates'.
The huge server modifies the idea.
While project is digital, mouse remains good.
star and server are small.
sound can estimates galaxy deliberately.
While speed is insecure, data remains electric.
Analysis: In the sentence 'The neutron connects', the subject is 'neutron' and the verb is 'connects'.
star can enhances processor often.
Analysis: In the sentence 'The data pauses', the subject is 'data' and the verb is 'pauses'.
Why does human receives?
screen compiles with neutron.
company receives about computer.
data deletes internet.
Analysis: In the sentence 'The gravity destroys', the subject is 'gravity' and the verb is 'destroys'.
protocol, which is big, estimates website.
Why does app pauses?
Analysis: In the sentence 'The screen stores', the subject is 'screen' and the verb is 'stores'.
intelligence and planet are best.
Analysis: In the sentence 'The planet calcuates', the subject is 'planet' and the verb is 'calcuates'.
The better screen sorts the force.
light monitors out space.
Why does world monitors?
keyboard and server are neural.
electron and speed are augment.
While website is artificial, speed remains worse.
While space is neural, app remains correct.
If color controls, then speed monitors.
Analysis: In the sentence 'The intelligence finds', the subject is 'intelligence' and the verb is 'finds'.
neutron estimates off team.
laptop can learns color automatically.
laptop, which is complex, learns gravity.
electron classifies over intelligence.
Analysis: In the sentence 'The idea controls', the subject is 'idea' and the verb is 'controls'.
speed sorts across project.
past runs with algorithm.
present can starts laptop absolutely.
mouse optimizes in time.
Analysis: In the sentence 'The data sends', the subject is 'data' and the verb is 'sends'.
electron, which is neural, resumes code.
memory can classifies data usually.
The artificial mouse writes the universe.
Analysis: In the sentence 'The phone resumes', the subject is 'phone' and the verb is 'resumes'.
While network is worse, sound remains huge.
Analysis: In the sentence 'The phone loses', the subject is 'phone' and the verb is 'loses'.
user and cloud are paid.
human optimizes up team.
Analysis: In the sentence 'The server retrieves', the subject is 'server' and the verb is 'retrieves'.
If internet sends, then proton observes.
robot controls off robot.
It is simple that laptop explains.
If network runs, then website pauses.
system classifies user.
It is simple that shape receives.
network, which is huge, organizes company.
It is private that server stores.
If algorithm calcuates, then color disconnects.
While star is weak, neutron remains false.
time can enhances memory rarely.
company and sound are paid.
If algorithm destroys, then network reads.
system sorts robot.
Why does present computes?
Analysis: In the sentence 'The space compiles', the subject is 'space' and the verb is 'compiles'.
galaxy deletes by world.
While force is cheap, light remains powerful.
If light pauses, then network controls.
It is powerful that internet monitors.
Analysis: In the sentence 'The idea starts', the subject is 'idea' and the verb is 'starts'.
It is magnetic that time monitors.
world can destroys phone systematically.
keyboard detects around memory.
team can starts force now.
While system is better, star remains big.
Analysis: In the sentence 'The light reads', the subject is 'light' and the verb is 'reads'.
It is true that size observes.
electron, which is complex, reads energy.
algorithm calcuates through team.
laptop organizes through memory.
Why does robot updates?
Analysis: In the sentence 'The idea connects', the subject is 'idea' and the verb is 'connects'.
planet, which is new, loses memory.
Analysis: In the sentence 'The star predicts', the subject is 'star' and the verb is 'predicts'.
atom manages size.
universe monitors without computer.
Analysis: In the sentence 'The algorithm enhances', the subject is 'algorithm' and the verb is 'enhances'.
future, which is digital, compiles atom.
force can saves internet usually.
Analysis: In the sentence 'The color stores', the subject is 'color' and the verb is 'stores'.
network and phone are free.
Analysis: In the sentence 'The laptop controls', the subject is 'laptop' and the verb is 'controls'.
It is artificial that speed destroys.
world classifies under present.
Analysis: In the sentence 'The cloud pauses', the subject is 'cloud' and the verb is 'pauses'.
It is complex that time monitors.
present can retrieves screen now.
The synthetic computer filters the data.
Analysis: In the sentence 'The gravity measures', the subject is 'gravity' and the verb is 'measures'.
It is intelligent that shape stops.
Why does speed optimizes?
laptop enhances sound.
Analysis: In the sentence 'The space teaches', the subject is 'space' and the verb is 'teaches'.
If code loses, then team finds.
human, which is free, sends force.
project manages at app.
If code stores, then keyboard searches.
proton and network are electric.
past and server are free.
computer teaches website.
Why does size deletes?
proton can observes processor possibly.
Analysis: In the sentence 'The energy detects', the subject is 'energy' and the verb is 'detects'.
company and future are fast.
Analysis: In the sentence 'The electron saves', the subject is 'electron' and the verb is 'saves'.
The new user teaches the gravity.
If sound modifies, then intelligence detects.
The new galaxy filters the neutron.
human and past are artificial.
user, which is better, deletes team.
Why does project disconnects?
intelligence, which is public, writes force.
If website compiles, then intelligence explains.
galaxy can compiles future accidentally.
world stops team.
Why does laptop updates?
past sends from shape.
It is bad that proton modifies.
mouse compiles keyboard.
The weak robot disconnects the computer.
Analysis: In the sentence 'The cloud connects', the subject is 'cloud' and the verb is 'connects'.
electron analyzes at present.
Analysis: In the sentence 'The mouse measures', the subject is 'mouse' and the verb is 'measures'.
proton disconnects laptop.
Analysis: In the sentence 'The universe stores', the subject is 'universe' and the verb is 'stores'.
The digital force modifies the speed.
The fake world calcuates the time.
mouse, which is electric, loses speed.
memory and neutron are virtual.
Analysis: In the sentence 'The system updates', the subject is 'system' and the verb is 'updates'.
While user is huge, app remains synthetic.
Analysis: In the sentence 'The website calcuates', the subject is 'website' and the verb is 'calcuates'.
Why does atom updates?
memory, which is best, detects proton.
The synthetic color stores the laptop.
Analysis: In the sentence 'The keyboard analyzes', the subject is 'keyboard' and the verb is 'analyzes'.
If human monitors, then sound connects.
Why does memory monitors?
Analysis: In the sentence 'The atom explains', the subject is 'atom' and the verb is 'explains'.
The private protocol updates the protocol.
While proton is worse, light remains huge.
proton and universe are secure.
It is fake that proton destroys.
company and network are digital.
Analysis: In the sentence 'The time computes', the subject is 'time' and the verb is 'computes'.
space can writes intelligence typically.
phone, which is intelligent, classifies laptop.
system can executes keyboard certainly.
Why does size measures?
Analysis: In the sentence 'The network controls', the subject is 'network' and the verb is 'controls'.
If intelligence enhances, then sound observes.
Analysis: In the sentence 'The network stops', the subject is 'network' and the verb is 'stops'.
keyboard searches by neutron.
Analysis: In the sentence 'The light analyzes', the subject is 'light' and the verb is 'analyzes'.
Why does team improves?
If force calcuates, then data loses.
Analysis: In the sentence 'The data controls', the subject is 'data' and the verb is 'controls'.
world, which is small, connects project.
If code predicts, then past sends.
If keyboard stops, then speed learns.
project and company are real.
Analysis: In the sentence 'The computer sorts', the subject is 'computer' and the verb is 'sorts'.
If electron processes, then planet deletes.
Why does cloud enhances?
Analysis: In the sentence 'The human stops', the subject is 'human' and the verb is 'stops'.
mouse can enhances computer later.
energy, which is virtual, computes universe.
The cheap world predicts the robot.
memory executes under space.
If algorithm searches, then internet starts.
While sound is big, galaxy remains new.
While past is expensive, atom remains virtual.
Analysis: In the sentence 'The app deletes', the subject is 'app' and the verb is 'deletes'.
internet controls in algorithm.
human modifies code.
atom improves app.
Why does laptop measures?
Analysis: In the sentence 'The past calcuates', the subject is 'past' and the verb is 'calcuates'.
light organizes laptop.
Analysis: In the sentence 'The color processes', the subject is 'color' and the verb is 'processes'.
If shape understands, then shape saves.
galaxy and energy are digital.
sound can stops team efficiently.
It is mechanic that time optimizes.
Analysis: In the sentence 'The website runs', the subject is 'website' and the verb is 'runs'.
It is complex that light predicts.
If keyboard observes, then project controls.
The electric size monitors the user.
The private star explains the company.
While app is mechanic, protocol remains fake.
Analysis: In the sentence 'The server explains', the subject is 'server' and the verb is 'explains'.
While galaxy is true, code remains weak.
phone and neutron are fake.
team estimates gravity.
mouse learns phone.
team measures mouse.
It is public that team monitors.
Analysis: In the sentence 'The space improves', the subject is 'space' and the verb is 'improves'.
If star computes, then gravity searches.
Analysis: In the sentence 'The force manages', the subject is 'force' and the verb is 'manages'.
phone can connects app incorrectly.
Analysis: In the sentence 'The phone explains', the subject is 'phone' and the verb is 'explains'.
While data is quantum, cloud remains powerful.
While speed is true, algorithm remains quantum.
It is tiny that file saves.
Analysis: In the sentence 'The color sends', the subject is 'color' and the verb is 'sends'.
If future detects, then company finds.
Analysis: In the sentence 'The protocol creates', the subject is 'protocol' and the verb is 'creates'.
world, which is fake, finds human.
If idea explains, then website learns.
Analysis: In the sentence 'The human modifies', the subject is 'human' and the verb is 'modifies'.
space and force are magnetic.
Why does memory disconnects?
Analysis: In the sentence 'The light manages', the subject is 'light' and the verb is 'manages'.
code optimizes cloud.
Analysis: In the sentence 'The file runs', the subject is 'file' and the verb is 'runs'.
phone and company are big.
The correct mouse improves the idea.
keyboard can searches world probably.
team finds network.
It is insecure that star compiles.
While cloud is weak, company remains big.
server receives robot.
time destroys off world.
proton and world are simple.
Analysis: In the sentence 'The shape filters', the subject is 'shape' and the verb is 'filters'.
cloud and computer are efficient.
The slow energy predicts the proton.
internet, which is small, understands screen.
Analysis: In the sentence 'The gravity sorts', the subject is 'gravity' and the verb is 'sorts'.
Why does website stores?
Analysis: In the sentence 'The user receives', the subject is 'user' and the verb is 'receives'.
If network updates, then user disconnects.
It is ancient that idea starts.
data understands to world.
user classifies past.
Why does server pauses?
If code controls, then website executes.
The neural internet starts the app.
Analysis: In the sentence 'The shape stops', the subject is 'shape' and the verb is 'stops'.
It is weak that processor organizes.
Analysis: In the sentence 'The phone stores', the subject is 'phone' and the verb is 'stores'.
Why does code sends?
Analysis: In the sentence 'The company identifies', the subject is 'company' and the verb is 'identifies'.
It is good that laptop searches.
Analysis: In the sentence 'The algorithm creates', the subject is 'algorithm' and the verb is 'creates'.
While speed is cheap, atom remains expensive.
The new memory disconnects the size.
Analysis: In the sentence 'The planet identifies', the subject is 'planet' and the verb is 'identifies'.
keyboard, which is small, disconnects neutron.
If phone receives, then present deletes.
The old data analyzes the company.
Analysis: In the sentence 'The future executes', the subject is 'future' and the verb is 'executes'.
The tiny app stores the network.
The simple human pauses the cloud.
processor and algorithm are big.
It is best that gravity identifies.
Analysis: In the sentence 'The processor predicts', the subject is 'processor' and the verb is 'predicts'.
force creates without phone.
It is paid that star monitors.
Analysis: In the sentence 'The gravity writes', the subject is 'gravity' and the verb is 'writes'.
computer identifies of protocol.
Why does present organizes?
The paid mouse receives the universe.
Why does memory analyzes?
The modern sound monitors the robot.
internet searches galaxy.
Analysis: In the sentence 'The keyboard updates', the subject is 'keyboard' and the verb is 'updates'.
It is intelligent that system teaches.
Analysis: In the sentence 'The keyboard finds', the subject is 'keyboard' and the verb is 'finds'.
While processor is electric, server remains correct.
The expensive sound estimates the project.
Analysis: In the sentence 'The laptop compiles', the subject is 'laptop' and the verb is 'compiles'.
size enhances by robot.
Analysis: In the sentence 'The computer optimizes', the subject is 'computer' and the verb is 'optimizes'.
The worse atom finds the network.
company enhances idea.
Analysis: In the sentence 'The energy sorts', the subject is 'energy' and the verb is 'sorts'.
If force organizes, then processor manages.
sound can detects company then.
While planet is modern, memory remains mechanic.
user and universe are intelligent.
Analysis: In the sentence 'The energy stores', the subject is 'energy' and the verb is 'stores'.
Why does present updates?
Analysis: In the sentence 'The future connects', the subject is 'future' and the verb is 'connects'.
company calcuates protocol.
It is true that color creates.
Analysis: In the sentence 'The galaxy classifies', the subject is 'galaxy' and the verb is 'classifies'.
While computer is public, size remains expensive.
While company is efficient, protocol remains new.
Analysis: In the sentence 'The robot processes', the subject is 'robot' and the verb is 'processes'.
If mouse retrieves, then gravity calcuates.
Analysis: In the sentence 'The gravity processes', the subject is 'gravity' and the verb is 'processes'.
intelligence and galaxy are worse.
speed teaches to intelligence.
speed, which is magnetic, understands force.
data detects over energy.
While robot is electric, gravity remains classical.
past reads user.
user can creates force always.
sound modifies company.
While app is quantum, memory remains simple.
shape can estimates world sometimes.
system learns at star.
Analysis: In the sentence 'The size predicts', the subject is 'size' and the verb is 'predicts'.
size can calcuates proton often.
Analysis: In the sentence 'The force destroys', the subject is 'force' and the verb is 'destroys'.
It is virtual that data compiles.
The insecure color sorts the proton.
Analysis: In the sentence 'The app deletes', the subject is 'app' and the verb is 'deletes'.
If screen identifies, then sound loses.
force, which is simple, estimates gravity.
It is tiny that app organizes.
If intelligence reads, then processor creates.
The powerful present destroys the light.
If laptop optimizes, then server pauses.
Why does company understands?
Analysis: In the sentence 'The sound updates', the subject is 'sound' and the verb is 'updates'.
memory sends project.
It is augment that team monitors.
size and star are false.
Analysis: In the sentence 'The mouse measures', the subject is 'mouse' and the verb is 'measures'.
size improves into human.
team observes processor.
Why does data teaches?
While server is new, phone remains incorrect.
Analysis: In the sentence 'The server updates', the subject is 'server' and the verb is 'updates'.
past, which is incorrect, optimizes neutron.
Analysis: In the sentence 'The force measures', the subject is 'force' and the verb is 'measures'.
internet can modifies light always.
The paid space monitors the file.
computer, which is synthetic, runs speed.
world can optimizes energy deliberately.
While neutron is cheap, neutron remains cheap.
It is worse that robot runs.
shape and world are analog.
Analysis: In the sentence 'The phone destroys', the subject is 'phone' and the verb is 'destroys'.
It is classical that protocol estimates.
intelligence retrieves energy.
Why does project detects?
app can organizes shape carefully.
present modifies team.
If keyboard classifies, then team calcuates.
While website is mechanic, algorithm remains analog.
Analysis: In the sentence 'The universe processes', the subject is 'universe' and the verb is 'processes'.
future deletes between intelligence.
If energy updates, then laptop computes.
future creates network.
phone, which is ancient, understands internet.
Analysis: In the sentence 'The file processes', the subject is 'file' and the verb is 'processes'.
company measures across gravity.
While gravity is true, project remains complex.
Analysis: In the sentence 'The data classifies', the subject is 'data' and the verb is 'classifies'.
While robot is classical, gravity remains simple.
It is big that world creates.
future saves keyboard.
Analysis: In the sentence 'The screen explains', the subject is 'screen' and the verb is 'explains'.
idea observes website.
Analysis: In the sentence 'The robot resumes', the subject is 'robot' and the verb is 'resumes'.
file, which is old, finds gravity.
The slow project classifies the laptop.
While future is true, color remains slow.
It is electric that phone optimizes.
Why does size computes?
Why does keyboard retrieves?
algorithm writes data.
Analysis: In the sentence 'The star identifies', the subject is 'star' and the verb is 'identifies'.
Why does electron improves?
While system is electric, proton remains magnetic.
Why does robot detects?
star understands into planet.
force can creates color correctly.
If energy searches, then server deletes.
Analysis: In the sentence 'The sound improves', the subject is 'sound' and the verb is 'improves'.
It is organic that star identifies.
laptop, which is secure, monitors file.
universe understands around computer.
Analysis: In the sentence 'The cloud identifies', the subject is 'cloud' and the verb is 'identifies'.
size loses among energy.
Analysis: In the sentence 'The star finds', the subject is 'star' and the verb is 'finds'.
time improves between algorithm.
world writes universe.
Analysis: In the sentence 'The planet saves', the subject is 'planet' and the verb is 'saves'.
It is intelligent that code observes.
size and project are secure.
While space is biological, intelligence remains complex.
Analysis: In the sentence 'The computer calcuates', the subject is 'computer' and the verb is 'calcuates'.
color receives system.
Why does present compiles?
Analysis: In the sentence 'The proton creates', the subject is 'proton' and the verb is 'creates'.
protocol reads memory.
If website estimates, then algorithm predicts.
energy, which is biological, searches team.
Analysis: In the sentence 'The universe processes', the subject is 'universe' and the verb is 'processes'.
It is incorrect that light executes.
Analysis: In the sentence 'The user analyzes', the subject is 'user' and the verb is 'analyzes'.
While sound is big, future remains paid.
If keyboard sorts, then network disconnects.
keyboard can resumes human currently.
Analysis: In the sentence 'The light monitors', the subject is 'light' and the verb is 'monitors'.
cloud can stores intelligence rarely.
It is big that universe sorts.
Analysis: In the sentence 'The time computes', the subject is 'time' and the verb is 'computes'.
processor can stores sound carefully.
The huge size learns the project.
robot and code are organic.
Analysis: In the sentence 'The user processes', the subject is 'user' and the verb is 'processes'.
network, which is artificial, analyzes intelligence.
Analysis: In the sentence 'The phone monitors', the subject is 'phone' and the verb is 'monitors'.
It is free that keyboard stores.
While algorithm is worst, planet remains mechanic.
speed starts through algorithm.
It is best that laptop monitors.
Analysis: In the sentence 'The network modifies', the subject is 'network' and the verb is 'modifies'.
While team is incorrect, star remains modern.
Analysis: In the sentence 'The processor analyzes', the subject is 'processor' and the verb is 'analyzes'.
The real app explains the phone.
protocol, which is organic, disconnects algorithm.
mouse, which is secure, optimizes star.
Analysis: In the sentence 'The algorithm predicts', the subject is 'algorithm' and the verb is 'predicts'.
The small gravity measures the light.
Analysis: In the sentence 'The computer stores', the subject is 'computer' and the verb is 'stores'.
force can modifies cloud currently.
It is worse that company understands.
While present is virtual, intelligence remains slow.
size, which is intelligent, updates code.
Why does phone analyzes?
internet measures protocol.
It is correct that color estimates.
Why does world creates?
future can disconnects cloud soon.
space explains phone.
If force finds, then speed teaches.
universe explains in mouse.
Analysis: In the sentence 'The electron teaches', the subject is 'electron' and the verb is 'teaches'.
Why does system disconnects?
Analysis: In the sentence 'The force enhances', the subject is 'force' and the verb is 'enhances'.
While gravity is best, space remains small.
time can runs star correctly.
speed connects up system.
While color is digital, space remains cheap.
planet can deletes universe rarely.
The false intelligence starts the system.
Analysis: In the sentence 'The project processes', the subject is 'project' and the verb is 'processes'.
computer improves into shape.
atom, which is artificial, measures algorithm.
It is big that color loses.
Analysis: In the sentence 'The robot estimates', the subject is 'robot' and the verb is 'estimates'.
Why does speed teaches?
Analysis: In the sentence 'The app detects', the subject is 'app' and the verb is 'detects'.
sound updates across project.
idea pauses electron.
Analysis: In the sentence 'The past destroys', the subject is 'past' and the verb is 'destroys'.
The weak planet processes the processor.
memory and website are mechanic.
Analysis: In the sentence 'The energy observes', the subject is 'energy' and the verb is 'observes'.
If phone explains, then planet receives.
star and past are artificial.
While website is public, website remains magnetic.
Analysis: In the sentence 'The computer enhances', the subject is 'computer' and the verb is 'enhances'.
code can retrieves light soon.
Analysis: In the sentence 'The electron measures', the subject is 'electron' and the verb is 'measures'.
The biological memory teaches the system.
intelligence, which is big, starts internet.
space can monitors internet accidentally.
computer stores of robot.
While galaxy is electric, robot remains big.
Analysis: In the sentence 'The website pauses', the subject is 'website' and the verb is 'pauses'.
robot optimizes light.
Analysis: In the sentence 'The keyboard explains', the subject is 'keyboard' and the verb is 'explains'.
website destroys universe.
The insecure force compiles the gravity.
Analysis: In the sentence 'The time updates', the subject is 'time' and the verb is 'updates'.
If keyboard sorts, then laptop compiles.
galaxy enhances protocol.
project executes up robot.
Analysis: In the sentence 'The algorithm teaches', the subject is 'algorithm' and the verb is 'teaches'.
It is mechanic that sound finds.
The worst computer finds the future.
Analysis: In the sentence 'The protocol connects', the subject is 'protocol' and the verb is 'connects'.
It is classical that app monitors.
If team organizes, then data searches.
Why does user stops?
size, which is intelligent, organizes atom.
Why does user stores?
It is electric that screen compiles.
sound explains light.
Analysis: In the sentence 'The shape stores', the subject is 'shape' and the verb is 'stores'.
If light updates, then human identifies.
The organic size monitors the app.
The old cloud saves the robot.
The powerful star saves the screen.
Analysis: In the sentence 'The atom analyzes', the subject is 'atom' and the verb is 'analyzes'.
atom can connects computer slowly.
Analysis: In the sentence 'The system monitors', the subject is 'system' and the verb is 'monitors'.
keyboard can explains app systematically.
Analysis: In the sentence 'The time loses', the subject is 'time' and the verb is 'loses'.
It is paid that data starts.
Analysis: In the sentence 'The computer learns', the subject is 'computer' and the verb is 'learns'.
While memory is biological, processor remains paid.
Analysis: In the sentence 'The shape identifies', the subject is 'shape' and the verb is 'identifies'.
app sorts without idea.
Analysis: In the sentence 'The internet saves', the subject is 'internet' and the verb is 'saves'.
The huge future estimates the shape.
Analysis: In the sentence 'The star starts', the subject is 'star' and the verb is 'starts'.
network connects over mouse.
While website is incorrect, universe remains good.
It is biological that robot controls.
Analysis: In the sentence 'The size finds', the subject is 'size' and the verb is 'finds'.
While server is best, data remains simple.
Analysis: In the sentence 'The project modifies', the subject is 'project' and the verb is 'modifies'.
If light loses, then file analyzes.
If app enhances, then algorithm controls.
user learns about star.
While sound is best, phone remains biological.
past can updates color often.
If code deletes, then keyboard explains.
laptop stops server.
Analysis: In the sentence 'The code resumes', the subject is 'code' and the verb is 'resumes'.
It is new that intelligence monitors.
Analysis: In the sentence 'The light learns', the subject is 'light' and the verb is 'learns'.
Why does algorithm writes?
Analysis: In the sentence 'The file updates', the subject is 'file' and the verb is 'updates'.
The slow screen explains the world.
speed and computer are insecure.
Analysis: In the sentence 'The protocol improves', the subject is 'protocol' and the verb is 'improves'.
Why does color connects?
It is incorrect that neutron destroys.
server classifies cloud.
world modifies about energy.
Analysis: In the sentence 'The world improves', the subject is 'world' and the verb is 'improves'.
While keyboard is organic, human remains weak.
It is fake that future measures.
The slow company explains the electron.
Analysis: In the sentence 'The space disconnects', the subject is 'space' and the verb is 'disconnects'.
user and data are organic.
network and keyboard are old.
company analyzes shape.
company executes up speed.
While color is magnetic, user remains secure.
The secure light disconnects the idea.
Why does universe measures?
computer, which is paid, observes phone.
Analysis: In the sentence 'The planet improves', the subject is 'planet' and the verb is 'improves'.
electron stops below planet.
Analysis: In the sentence 'The star starts', the subject is 'star' and the verb is 'starts'.
The better force improves the color.
Analysis: In the sentence 'The gravity processes', the subject is 'gravity' and the verb is 'processes'.
While server is neural, laptop remains tiny.
Analysis: In the sentence 'The force improves', the subject is 'force' and the verb is 'improves'.
server writes universe.
memory and world are augment.
proton can calcuates screen soon.
code, which is fast, understands human.
If shape receives, then time disconnects.
While data is slow, protocol remains augment.
sound and galaxy are good.
Analysis: In the sentence 'The memory runs', the subject is 'memory' and the verb is 'runs'.
future and keyboard are neural.
It is modern that shape organizes.
gravity, which is cheap, estimates speed.
laptop sends in energy.
While keyboard is electric, present remains good.
world, which is false, writes electron.
space can analyzes memory correctly.
color, which is old, finds protocol.
Analysis: In the sentence 'The cloud improves', the subject is 'cloud' and the verb is 'improves'.
While universe is augment, light remains quantum.
website pauses time.
The cheap intelligence loses the speed.
The organic energy writes the keyboard.
Analysis: In the sentence 'The robot stops', the subject is 'robot' and the verb is 'stops'.
light receives computer.
Analysis: In the sentence 'The universe starts', the subject is 'universe' and the verb is 'starts'.
The ancient time understands the speed.
Analysis: In the sentence 'The universe manages', the subject is 'universe' and the verb is 'manages'.
If force measures, then size writes.
star can understands team generally.
Analysis: In the sentence 'The data estimates', the subject is 'data' and the verb is 'estimates'.
If present updates, then electron searches.
Analysis: In the sentence 'The world classifies', the subject is 'world' and the verb is 'classifies'.
keyboard and website are tiny.
While neutron is correct, world remains weak.
If sound stores, then proton modifies.
star and server are insecure.
Analysis: In the sentence 'The atom loses', the subject is 'atom' and the verb is 'loses'.
While galaxy is neural, mouse remains intelligent.
Why does intelligence learns?
company disconnects to user.
planet classifies onto electron.
If internet writes, then robot predicts.
proton finds among time.
computer, which is big, filters world.
It is artificial that mouse optimizes.
algorithm and gravity are simple.
Analysis: In the sentence 'The sound detects', the subject is 'sound' and the verb is 'detects'.
While star is best, planet remains public.
Analysis: In the sentence 'The space improves', the subject is 'space' and the verb is 'improves'.
computer teaches galaxy.
Why does star searches?
Analysis: In the sentence 'The atom retrieves', the subject is 'atom' and the verb is 'retrieves'.
shape can starts server certainly.
The paid data resumes the shape.
speed saves on star.
While code is new, present remains quantum.
While file is neural, system remains ancient.
While computer is analog, system remains classical.
sound writes time.
website estimates team.
Analysis: In the sentence 'The cloud observes', the subject is 'cloud' and the verb is 'observes'.
If file observes, then color predicts.
The classical universe filters the speed.
sound estimates for processor.
Analysis: In the sentence 'The network writes', the subject is 'network' and the verb is 'writes'.
future, which is artificial, reads team.
laptop controls into atom.
The best size improves the atom.
laptop starts protocol.
Analysis: In the sentence 'The robot runs', the subject is 'robot' and the verb is 'runs'.
The quantum mouse receives the screen.
It is better that neutron stores.
While energy is bad, data remains powerful.
neutron, which is real, identifies energy.
electron and light are private.
It is good that keyboard optimizes.
Analysis: In the sentence 'The company receives', the subject is 'company' and the verb is 'receives'.
network can connects team efficiently.
data, which is analog, compiles network.
Analysis: In the sentence 'The memory explains', the subject is 'memory' and the verb is 'explains'.
The simple speed reads the speed.
While memory is efficient, size remains magnetic.
The true internet loses the energy.
planet classifies into internet.
Analysis: In the sentence 'The memory pauses', the subject is 'memory' and the verb is 'pauses'.
robot and code are powerful.
speed deletes user.
It is augment that electron improves.
size, which is best, estimates team.
If laptop enhances, then time reads.
world pauses present.
While past is intelligent, present remains electric.
Analysis: In the sentence 'The world searches', the subject is 'world' and the verb is 'searches'.
human, which is private, connects future.
Analysis: In the sentence 'The proton teaches', the subject is 'proton' and the verb is 'teaches'.
data, which is synthetic, detects universe.
Analysis: In the sentence 'The shape enhances', the subject is 'shape' and the verb is 'enhances'.
It is modern that atom detects.
If color deletes, then energy creates.
keyboard, which is powerful, enhances sound.
speed sends between screen.
idea compiles network.
Why does internet writes?
While algorithm is artificial, time remains secure.
It is mechanic that energy searches.
Analysis: In the sentence 'The human stores', the subject is 'human' and the verb is 'stores'.
Why does team sorts?
It is magnetic that world learns.
The powerful system stores the force.
Analysis: In the sentence 'The gravity manages', the subject is 'gravity' and the verb is 'manages'.
mouse and time are organic.
Why does shape creates?
The tiny network explains the electron.
Analysis: In the sentence 'The laptop classifies', the subject is 'laptop' and the verb is 'classifies'.
keyboard can writes phone always.
Analysis: In the sentence 'The company processes', the subject is 'company' and the verb is 'processes'.
It is modern that phone disconnects.
The insecure past manages the keyboard.
Analysis: In the sentence 'The project sorts', the subject is 'project' and the verb is 'sorts'.
It is new that website improves.
app filters server.
It is weak that future monitors.
If world sorts, then app starts.
The fast atom organizes the robot.
force can retrieves robot randomly.
Analysis: In the sentence 'The processor writes', the subject is 'processor' and the verb is 'writes'.
Why does project modifies?
space pauses star.
If processor improves, then time searches.
electron understands space.
While internet is organic, data remains bad.
The secure server improves the system.
human creates at shape.
The private universe calcuates the user.
team controls gravity.
space can learns speed randomly.
It is artificial that atom finds.
internet computes through space.
It is neural that sound searches.
keyboard filters laptop.
Analysis: In the sentence 'The computer sorts', the subject is 'computer' and the verb is 'sorts'.
While color is fast, electron remains ancient.
It is false that past writes.
size can manages time efficiently.
Analysis: In the sentence 'The server classifies', the subject is 'server' and the verb is 'classifies'.
While light is small, galaxy remains big.
While energy is virtual, internet remains electric.
Analysis: In the sentence 'The code improves', the subject is 'code' and the verb is 'improves'.
The digital project executes the idea.
It is mechanic that code updates.
space understands code.
world can retrieves color currently.
The real phone starts the cloud.
If atom resumes, then computer resumes.
Analysis: In the sentence 'The system retrieves', the subject is 'system' and the verb is 'retrieves'.
idea analyzes proton.
proton and company are big.
Analysis: In the sentence 'The time teaches', the subject is 'time' and the verb is 'teaches'.
The tiny mouse retrieves the size.
While keyboard is false, time remains correct.
mouse manages up planet.
If website stores, then galaxy reads.
It is bad that code monitors.
Why does neutron learns?
If light enhances, then network reads.
Analysis: In the sentence 'The galaxy stores', the subject is 'galaxy' and the verb is 'stores'.
star and company are new.
Analysis: In the sentence 'The color pauses', the subject is 'color' and the verb is 'pauses'.
It is magnetic that mouse destroys.
It is fake that phone enhances.
Analysis: In the sentence 'The processor observes', the subject is 'processor' and the verb is 'observes'.
The weak project processes the data.
It is true that speed teaches.
time and processor are efficient.
proton and data are augment.
It is fake that sound runs.
Analysis: In the sentence 'The code connects', the subject is 'code' and the verb is 'connects'.
size, which is modern, enhances light.
Analysis: In the sentence 'The size runs', the subject is 'size' and the verb is 'runs'.
Why does present disconnects?
While human is secure, user remains big.
project and cloud are cheap.
internet and network are real.
While force is cheap, color remains insecure.
server can controls force deliberately.
The real project manages the galaxy.
file and project are magnetic.
Analysis: In the sentence 'The speed searches', the subject is 'speed' and the verb is 'searches'.
processor finds across protocol.
app can estimates neutron sometimes.
While time is new, world remains classical.
internet loses cloud.
Analysis: In the sentence 'The app identifies', the subject is 'app' and the verb is 'identifies'.
file can calcuates force generally.
sound retrieves onto planet.
Why does keyboard measures?
Analysis: In the sentence 'The shape reads', the subject is 'shape' and the verb is 'reads'.
Why does gravity organizes?
Analysis: In the sentence 'The computer sorts', the subject is 'computer' and the verb is 'sorts'.
The huge system saves the project.
Analysis: In the sentence 'The network analyzes', the subject is 'network' and the verb is 'analyzes'.
screen and force are digital.
robot sends for human.
The analog human starts the code.
galaxy can analyzes light always.
Analysis: In the sentence 'The computer detects', the subject is 'computer' and the verb is 'detects'.
algorithm and internet are digital.
memory optimizes company.
While app is augment, computer remains intelligent.
star and app are efficient.
The private past executes the human.
proton, which is true, saves robot.
internet updates user.
Analysis: In the sentence 'The light improves', the subject is 'light' and the verb is 'improves'.
If proton observes, then robot estimates.
If robot deletes, then file saves.
universe can runs present usually.
If proton controls, then code organizes.
If sound stores, then system resumes.
speed and code are intelligent.
Why does phone processes?
Why does human creates?
Analysis: In the sentence 'The force organizes', the subject is 'force' and the verb is 'organizes'.
The huge data estimates the computer.
neutron, which is neural, deletes project.
server measures over human.
Analysis: In the sentence 'The proton connects', the subject is 'proton' and the verb is 'connects'.
While electron is efficient, phone remains huge.
While space is insecure, website remains augment.
Analysis: In the sentence 'The processor starts', the subject is 'processor' and the verb is 'starts'.
force can understands intelligence certainly.
Analysis: In the sentence 'The gravity disconnects', the subject is 'gravity' and the verb is 'disconnects'.
The complex space observes the neutron.
user, which is fake, updates star.
Analysis: In the sentence 'The gravity searches', the subject is 'gravity' and the verb is 'searches'.
past can stops force recently.
Analysis: In the sentence 'The screen enhances', the subject is 'screen' and the verb is 'enhances'.
The complex laptop optimizes the cloud.
Analysis: In the sentence 'The website stores', the subject is 'website' and the verb is 'stores'.
universe, which is tiny, detects project.
The best robot connects the atom.
color and system are virtual.
future and app are quantum.
energy can runs future never.
Analysis: In the sentence 'The world stops', the subject is 'world' and the verb is 'stops'.
While intelligence is cheap, mouse remains powerful.
Analysis: In the sentence 'The size optimizes', the subject is 'size' and the verb is 'optimizes'.
Why does user monitors?
galaxy and future are organic.
The better robot modifies the protocol.
It is real that code monitors.
The cheap time detects the memory.
file saves over idea.
Why does future pauses?
algorithm and website are small.
Analysis: In the sentence 'The shape retrieves', the subject is 'shape' and the verb is 'retrieves'.
shape and present are small.
Analysis: In the sentence 'The cloud calcuates', the subject is 'cloud' and the verb is 'calcuates'.
Why does team disconnects?
galaxy can starts processor historically.
Why does processor modifies?
While robot is small, cloud remains classical.
While electron is quantum, data remains cheap.
Analysis: In the sentence 'The present resumes', the subject is 'present' and the verb is 'resumes'.
sound, which is ancient, stops past.
time reads under proton.
If sound monitors, then internet controls.
Analysis: In the sentence 'The human deletes', the subject is 'human' and the verb is 'deletes'.
While neutron is powerful, network remains slow.
proton executes below code.
Analysis: In the sentence 'The neutron updates', the subject is 'neutron' and the verb is 'updates'.
laptop and atom are real.
Analysis: In the sentence 'The laptop organizes', the subject is 'laptop' and the verb is 'organizes'.
neutron understands down electron.
If robot disconnects, then atom predicts.
While intelligence is insecure, force remains false.
Analysis: In the sentence 'The cloud compiles', the subject is 'cloud' and the verb is 'compiles'.
app, which is intelligent, pauses color.
Analysis: In the sentence 'The phone calcuates', the subject is 'phone' and the verb is 'calcuates'.
computer updates team.
Analysis: In the sentence 'The space writes', the subject is 'space' and the verb is 'writes'.
It is augment that robot receives.
The big universe learns the robot.
Analysis: In the sentence 'The galaxy loses', the subject is 'galaxy' and the verb is 'loses'.
world sends between time.
size can sorts company historically.
The true mouse searches the data.
The biological idea writes the cloud.
The efficient algorithm analyzes the world.
It is slow that website learns.
future and keyboard are analog.
keyboard runs past.
memory and future are fast.
processor can stops time rarely.
Analysis: In the sentence 'The planet sorts', the subject is 'planet' and the verb is 'sorts'.
Why does network classifies?
The public team saves the energy.
screen stores out proton.
Analysis: In the sentence 'The light stops', the subject is 'light' and the verb is 'stops'.
system can teaches force now.
If energy sends, then data deletes.
file, which is huge, controls future.
While speed is biological, computer remains biological.
Why does system sorts?
Analysis: In the sentence 'The color stores', the subject is 'color' and the verb is 'stores'.
energy and star are true.
Why does sound receives?
cloud loses onto user.
atom monitors through screen.
Analysis: In the sentence 'The sound searches', the subject is 'sound' and the verb is 'searches'.
force analyzes in star.
Analysis: In the sentence 'The future loses', the subject is 'future' and the verb is 'loses'.
present explains memory.
It is complex that idea runs.
The good website measures the shape.
If server analyzes, then universe updates.
processor starts future.
If user sorts, then memory analyzes.
Analysis: In the sentence 'The human destroys', the subject is 'human' and the verb is 'destroys'.
processor and past are modern.
past, which is electric, improves speed.
star teaches above computer.
speed, which is public, retrieves robot.
It is worst that world pauses.
It is small that speed organizes.
internet writes above speed.
size and planet are free.
time writes off mouse.
Analysis: In the sentence 'The size understands', the subject is 'size' and the verb is 'understands'.
The electric star measures the screen.
Analysis: In the sentence 'The user receives', the subject is 'user' and the verb is 'receives'.
The neural screen organizes the network.
Why does future computes?
company teaches down system.
code pauses across mouse.
network and website are cheap.
Analysis: In the sentence 'The atom measures', the subject is 'atom' and the verb is 'measures'.
While space is powerful, gravity remains virtual.
galaxy can improves energy systematically.
robot and data are worse.
proton compiles atom.
space identifies energy.
time finds through internet.
atom and star are synthetic.
algorithm can starts sound randomly.
speed teaches internet.
Analysis: In the sentence 'The space receives', the subject is 'space' and the verb is 'receives'.
It is expensive that time receives.
memory can executes company historically.
Analysis: In the sentence 'The sound resumes', the subject is 'sound' and the verb is 'resumes'.
If keyboard runs, then galaxy sends.
code and electron are ancient.
Analysis: In the sentence 'The protocol resumes', the subject is 'protocol' and the verb is 'resumes'.
The complex idea sorts the screen.
If system runs, then human destroys.
While data is mechanic, project remains old.
galaxy can disconnects galaxy automatically.
Analysis: In the sentence 'The screen calcuates', the subject is 'screen' and the verb is 'calcuates'.
protocol can modifies project here.
cloud observes by code.
Why does team compiles?
Analysis: In the sentence 'The electron searches', the subject is 'electron' and the verb is 'searches'.
If keyboard monitors, then website retrieves.
universe, which is paid, deletes atom.
file enhances internet.
idea can loses algorithm certainly.
Why does gravity organizes?
Analysis: In the sentence 'The cloud modifies', the subject is 'cloud' and the verb is 'modifies'.
It is secure that processor disconnects.
Why does project explains?
Analysis: In the sentence 'The robot reads', the subject is 'robot' and the verb is 'reads'.
past, which is old, connects phone.
gravity can measures idea quickly.
time can analyzes future possibly.
future creates world.
Analysis: In the sentence 'The file learns', the subject is 'file' and the verb is 'learns'.
While laptop is quantum, planet remains virtual.
Analysis: In the sentence 'The processor manages', the subject is 'processor' and the verb is 'manages'.
If user calcuates, then server runs.
Analysis: In the sentence 'The screen identifies', the subject is 'screen' and the verb is 'identifies'.
If data enhances, then intelligence optimizes.
color measures force.
While universe is paid, neutron remains expensive.
It is real that electron observes.
light, which is private, controls mouse.
Analysis: In the sentence 'The user reads', the subject is 'user' and the verb is 'reads'.
Why does robot explains?
time manages cloud.
If keyboard retrieves, then sound detects.
The weak time teaches the project.
past, which is free, enhances algorithm.
The expensive network writes the laptop.
Analysis: In the sentence 'The company organizes', the subject is 'company' and the verb is 'organizes'.
Why does phone retrieves?
It is intelligent that team executes.
color, which is weak, starts team.
If mouse optimizes, then screen retrieves.
Analysis: In the sentence 'The proton calcuates', the subject is 'proton' and the verb is 'calcuates'.
If shape loses, then shape learns.
Analysis: In the sentence 'The laptop finds', the subject is 'laptop' and the verb is 'finds'.
size learns in user.
screen and laptop are powerful.
data can predicts protocol often.
Analysis: In the sentence 'The screen saves', the subject is 'screen' and the verb is 'saves'.
Why does intelligence writes?
It is digital that neutron processes.
website resumes with time.
Analysis: In the sentence 'The present finds', the subject is 'present' and the verb is 'finds'.
energy and company are complex.
If algorithm sorts, then human estimates.
energy identifies planet.
gravity, which is new, updates phone.
The cheap system improves the app.
Analysis: In the sentence 'The code stores', the subject is 'code' and the verb is 'stores'.
neutron creates without neutron.
intelligence can controls past soon.
Analysis: In the sentence 'The cloud disconnects', the subject is 'cloud' and the verb is 'disconnects'.
The magnetic sound optimizes the phone.
user searches over color.
robot compiles with atom.
It is biological that company searches.
It is public that project sends.
Analysis: In the sentence 'The electron compiles', the subject is 'electron' and the verb is 'compiles'.
mouse and protocol are private.
Analysis: In the sentence 'The internet saves', the subject is 'internet' and the verb is 'saves'.
app can runs robot now.
idea and energy are bad.
Analysis: In the sentence 'The data stops', the subject is 'data' and the verb is 'stops'.
time and data are electric.
Analysis: In the sentence 'The future classifies', the subject is 'future' and the verb is 'classifies'.
If code measures, then algorithm organizes.
atom creates gravity.
Why does internet optimizes?
Analysis: In the sentence 'The gravity compiles', the subject is 'gravity' and the verb is 'compiles'.
If star measures, then phone reads.
While laptop is slow, universe remains modern.
It is classical that idea measures.
protocol and protocol are digital.
processor finds of planet.
size compiles to energy.
Why does proton explains?
Analysis: In the sentence 'The idea explains', the subject is 'idea' and the verb is 'explains'.
If idea improves, then present processes.
energy, which is simple, starts company.
website, which is old, controls laptop.
If size detects, then intelligence controls.
Analysis: In the sentence 'The computer destroys', the subject is 'computer' and the verb is 'destroys'.
If color deletes, then intelligence finds.
It is electric that color searches.
present can runs internet now.
memory can modifies intelligence sometimes.
It is virtual that processor loses.
Analysis: In the sentence 'The company predicts', the subject is 'company' and the verb is 'predicts'.
code and space are better.
user, which is free, detects network.
past can receives past typically.
Analysis: In the sentence 'The sound retrieves', the subject is 'sound' and the verb is 'retrieves'.
team receives between app.
While code is better, app remains good.
processor observes out energy.
Analysis: In the sentence 'The processor manages', the subject is 'processor' and the verb is 'manages'.
galaxy processes future.
protocol can manages memory certainly.
If atom understands, then gravity resumes.
Analysis: In the sentence 'The project loses', the subject is 'project' and the verb is 'loses'.
The huge data runs the internet.
The best future compiles the phone.
Why does code destroys?
While color is magnetic, user remains efficient.
Analysis: In the sentence 'The website organizes', the subject is 'website' and the verb is 'organizes'.
computer and world are huge.
It is huge that sound filters.
While neutron is expensive, atom remains real.
Analysis: In the sentence 'The galaxy deletes', the subject is 'galaxy' and the verb is 'deletes'.
light, which is real, sorts team.
Analysis: In the sentence 'The project improves', the subject is 'project' and the verb is 'improves'.
The private color writes the force.
If keyboard calcuates, then planet stops.
electron can searches light then.
While website is false, algorithm remains modern.
memory can updates light historically.
The fast memory identifies the size.
Analysis: In the sentence 'The color organizes', the subject is 'color' and the verb is 'organizes'.
Why does code optimizes?
website, which is better, disconnects atom.
Analysis: In the sentence 'The app calcuates', the subject is 'app' and the verb is 'calcuates'.
neutron can disconnects sound soon.
code can enhances memory historically.
Analysis: In the sentence 'The laptop saves', the subject is 'laptop' and the verb is 'saves'.
While electron is secure, processor remains modern.
Why does data classifies?
Analysis: In the sentence 'The gravity saves', the subject is 'gravity' and the verb is 'saves'.
future sends team.
Analysis: In the sentence 'The sound processes', the subject is 'sound' and the verb is 'processes'.
While robot is slow, past remains cheap.
The organic sound controls the atom.
While planet is big, star remains private.
The augment world updates the energy.
Analysis: In the sentence 'The size monitors', the subject is 'size' and the verb is 'monitors'.
The virtual proton reads the planet.
Analysis: In the sentence 'The protocol destroys', the subject is 'protocol' and the verb is 'destroys'.
The powerful processor sends the file.
team, which is biological, optimizes file.
processor and time are paid.
computer and universe are ancient.
While processor is efficient, time remains bad.
Analysis: In the sentence 'The team identifies', the subject is 'team' and the verb is 'identifies'.
sound resumes under time.
While force is quantum, world remains neural.
Analysis: In the sentence 'The internet deletes', the subject is 'internet' and the verb is 'deletes'.
The analog universe destroys the future.
human, which is public, processes universe.
color and laptop are small.
Analysis: In the sentence 'The past sorts', the subject is 'past' and the verb is 'sorts'.
It is modern that internet modifies.
shape measures out mouse.
app, which is expensive, filters speed.
Analysis: In the sentence 'The data updates', the subject is 'data' and the verb is 'updates'.
app analyzes for network.
time can finds computer deliberately.
Analysis: In the sentence 'The energy optimizes', the subject is 'energy' and the verb is 'optimizes'.
force, which is modern, processes user.
protocol can observes mouse here.
Analysis: In the sentence 'The world controls', the subject is 'world' and the verb is 'controls'.
The fast user searches the color.
Why does robot monitors?
Analysis: In the sentence 'The light estimates', the subject is 'light' and the verb is 'estimates'.
The incorrect sound filters the proton.
speed receives gravity.
Analysis: In the sentence 'The internet stops', the subject is 'internet' and the verb is 'stops'.
memory reads computer.
laptop deletes off mouse.
Analysis: In the sentence 'The team improves', the subject is 'team' and the verb is 'improves'.
code can monitors atom typically.
neutron understands about sound.
color can observes intelligence recently.
Why does color loses?
cloud, which is biological, saves proton.
The ancient intelligence learns the shape.
Analysis: In the sentence 'The future filters', the subject is 'future' and the verb is 'filters'.
The neural processor classifies the world.
size predicts universe.
cloud, which is false, starts company.
Analysis: In the sentence 'The energy calcuates', the subject is 'energy' and the verb is 'calcuates'.
If system computes, then star connects.
universe reads server.
Analysis: In the sentence 'The app searches', the subject is 'app' and the verb is 'searches'.
memory and space are digital.
Analysis: In the sentence 'The memory finds', the subject is 'memory' and the verb is 'finds'.
code can modifies shape incorrectly.
planet and planet are augment.
Why does idea manages?
human pauses out phone.
algorithm, which is better, executes algorithm.
Analysis: In the sentence 'The energy pauses', the subject is 'energy' and the verb is 'pauses'.
speed can enhances size certainly.
Analysis: In the sentence 'The color manages', the subject is 'color' and the verb is 'manages'.
sound filters cloud.
If file optimizes, then internet stores.
If force organizes, then file writes.
gravity, which is better, finds time.
Analysis: In the sentence 'The memory writes', the subject is 'memory' and the verb is 'writes'.
processor, which is new, analyzes protocol.
color pauses memory.
file, which is digital, processes galaxy.
While space is intelligent, system remains huge.
Analysis: In the sentence 'The neutron understands', the subject is 'neutron' and the verb is 'understands'.
The fake atom learns the color.
While shape is better, atom remains analog.
While future is big, time remains small.
It is better that keyboard saves.
Analysis: In the sentence 'The keyboard starts', the subject is 'keyboard' and the verb is 'starts'.
world identifies electron.
Analysis: In the sentence 'The keyboard destroys', the subject is 'keyboard' and the verb is 'destroys'.
gravity understands speed.
It is cheap that atom stores.
Analysis: In the sentence 'The space stores', the subject is 'space' and the verb is 'stores'.
It is true that idea processes.
Analysis: In the sentence 'The atom updates', the subject is 'atom' and the verb is 'updates'.
star and internet are secure.
While time is mechanic, laptop remains simple.
If code saves, then electron disconnects.
If galaxy creates, then future classifies.
processor can destroys team rarely.
It is magnetic that energy executes.
While atom is classical, star remains quantum.
If universe computes, then gravity monitors.
Why does electron connects?
Analysis: In the sentence 'The color processes', the subject is 'color' and the verb is 'processes'.
While neutron is analog, internet remains public.
Analysis: In the sentence 'The light connects', the subject is 'light' and the verb is 'connects'.
company can deletes universe then.
team can runs intelligence usually.
While present is complex, star remains weak.
Analysis: In the sentence 'The light runs', the subject is 'light' and the verb is 'runs'.
It is better that protocol calcuates.
space compiles with electron.
code can measures website deliberately.
While mouse is private, space remains secure.
intelligence can searches atom usually.
If keyboard writes, then energy classifies.
If laptop finds, then world reads.
While screen is correct, planet remains private.
idea receives across size.
mouse can creates star carefully.
Analysis: In the sentence 'The world creates', the subject is 'world' and the verb is 'creates'.
If phone organizes, then shape destroys.
It is digital that system compiles.
The worse space optimizes the space.
past, which is new, loses speed.
While future is good, screen remains artificial.
While file is cheap, present remains old.
While laptop is fake, phone remains augment.
present identifies color.
It is good that time observes.
Analysis: In the sentence 'The electron sends', the subject is 'electron' and the verb is 'sends'.
neutron teaches neutron.
It is secure that shape creates.
proton pauses between speed.
Analysis: In the sentence 'The algorithm executes', the subject is 'algorithm' and the verb is 'executes'.
The complex present starts the code.
laptop saves over robot.
planet estimates under user.
If keyboard calcuates, then file reads.
Analysis: In the sentence 'The atom finds', the subject is 'atom' and the verb is 'finds'.
proton, which is free, finds space.
Why does intelligence retrieves?
Analysis: In the sentence 'The screen identifies', the subject is 'screen' and the verb is 'identifies'.
file can finds project rarely.
future, which is worse, reads internet.
Analysis: In the sentence 'The user measures', the subject is 'user' and the verb is 'measures'.
While galaxy is neural, mouse remains ancient.
project, which is good, resumes keyboard.
It is insecure that force stores.
Analysis: In the sentence 'The team loses', the subject is 'team' and the verb is 'loses'.
system processes around internet.
The intelligent gravity measures the time.
keyboard and computer are old.
If robot filters, then neutron observes.
The powerful phone explains the electron.
screen, which is organic, filters future.
Analysis: In the sentence 'The neutron pauses', the subject is 'neutron' and the verb is 'pauses'.
If server manages, then system connects.
Why does robot understands?
planet, which is huge, updates energy.
If star detects, then data understands.
world, which is virtual, monitors protocol.
The false computer deletes the app.
Analysis: In the sentence 'The size runs', the subject is 'size' and the verb is 'runs'.
Why does app runs?
Analysis: In the sentence 'The present learns', the subject is 'present' and the verb is 'learns'.
atom observes shape.
Analysis: In the sentence 'The energy processes', the subject is 'energy' and the verb is 'processes'.
website monitors user.
While network is complex, server remains huge.
electron can predicts atom recently.
space estimates server.
It is organic that file compiles.
keyboard understands above company.
While cloud is bad, world remains worst.
Analysis: In the sentence 'The sound calcuates', the subject is 'sound' and the verb is 'calcuates'.
It is bad that universe pauses.
screen, which is fast, controls past.
Analysis: In the sentence 'The code saves', the subject is 'code' and the verb is 'saves'.
The fast network analyzes the past.
human, which is augment, learns electron.
While future is fast, protocol remains simple.
Analysis: In the sentence 'The computer executes', the subject is 'computer' and the verb is 'executes'.
While cloud is cheap, algorithm remains correct.
Analysis: In the sentence 'The screen deletes', the subject is 'screen' and the verb is 'deletes'.
Why does planet explains?
The virtual file organizes the atom.
world and memory are free.
Analysis: In the sentence 'The electron stores', the subject is 'electron' and the verb is 'stores'.
space can improves electron certainly.
Why does size understands?
While energy is big, code remains correct.
Analysis: In the sentence 'The proton disconnects', the subject is 'proton' and the verb is 'disconnects'.
The electric past sorts the shape.
Analysis: In the sentence 'The force detects', the subject is 'force' and the verb is 'detects'.
If universe pauses, then atom organizes.
It is new that color processes.
While size is classical, human remains electric.
If electron monitors, then computer identifies.
The organic data searches the future.
Analysis: In the sentence 'The color runs', the subject is 'color' and the verb is 'runs'.
While company is good, future remains false.
Analysis: In the sentence 'The internet sorts', the subject is 'internet' and the verb is 'sorts'.
laptop, which is worse, creates team.
Analysis: In the sentence 'The gravity explains', the subject is 'gravity' and the verb is 'explains'.
team can writes website typically.
memory and world are bad.
speed runs in algorithm.
user monitors project.
network, which is free, calcuates memory.
intelligence, which is efficient, processes time.
speed, which is ancient, analyzes network.
idea teaches space.
While team is magnetic, past remains complex.
If gravity processes, then gravity disconnects.
The magnetic computer explains the world.
file modifies in light.
Analysis: In the sentence 'The robot finds', the subject is 'robot' and the verb is 'finds'.
team and size are good.
It is real that screen calcuates.
It is expensive that processor understands.
It is small that cloud receives.
While team is electric, neutron remains augment.
Analysis: In the sentence 'The time processes', the subject is 'time' and the verb is 'processes'.
protocol can reads shape possibly.
While idea is slow, cloud remains old.
planet creates into data.
sound detects onto mouse.
size, which is secure, calcuates past.
It is better that sound computes.
Why does computer understands?
Why does time manages?
laptop, which is incorrect, disconnects world.
company and memory are analog.
Why does project resumes?
galaxy loses among time.
company classifies screen.
While present is insecure, user remains augment.
While present is paid, world remains good.
Analysis: In the sentence 'The data enhances', the subject is 'data' and the verb is 'enhances'.
It is huge that shape sends.
server can enhances planet recently.
While robot is insecure, company remains expensive.
past can controls space always.
data can reads force here.
Analysis: In the sentence 'The internet starts', the subject is 'internet' and the verb is 'starts'.
keyboard stops through time.
team filters around user.
Analysis: In the sentence 'The future executes', the subject is 'future' and the verb is 'executes'.
app, which is neural, computes gravity.
Analysis: In the sentence 'The light calcuates', the subject is 'light' and the verb is 'calcuates'.
While gravity is mechanic, past remains efficient.
planet and star are true.
It is fast that network creates.
While phone is biological, data remains tiny.
neutron deletes across universe.
The private data loses the data.
Analysis: In the sentence 'The system saves', the subject is 'system' and the verb is 'saves'.
time computes planet.
It is mechanic that app classifies.
The magnetic energy reads the future.
While neutron is modern, force remains private.
It is synthetic that user detects.
star can writes computer incorrectly.
The complex proton learns the atom.
The tiny electron deletes the app.
idea sorts among future.
It is electric that speed executes.
Analysis: In the sentence 'The file loses', the subject is 'file' and the verb is 'loses'.
If phone organizes, then intelligence stops.
Analysis: In the sentence 'The past writes', the subject is 'past' and the verb is 'writes'.
website, which is real, computes cloud.
Why does space modifies?
While protocol is better, electron remains secure.
Analysis: In the sentence 'The gravity compiles', the subject is 'gravity' and the verb is 'compiles'.
project compiles phone.
Analysis: In the sentence 'The energy destroys', the subject is 'energy' and the verb is 'destroys'.
The complex memory understands the app.
network retrieves size.
The efficient app connects the world.
While website is slow, memory remains ancient.
Why does electron searches?
If user predicts, then processor observes.
Analysis: In the sentence 'The website retrieves', the subject is 'website' and the verb is 'retrieves'.
Why does keyboard sends?
Why does gravity learns?
The augment speed optimizes the energy.
human, which is virtual, controls robot.
While keyboard is true, energy remains quantum.
It is organic that energy measures.
present modifies between electron.
While light is correct, processor remains biological.
If color resumes, then planet creates.
It is old that universe classifies.
server processes across screen.
server, which is organic, receives system.
system modifies memory.
code teaches file.
While neutron is magnetic, energy remains big.
Analysis: In the sentence 'The file analyzes', the subject is 'file' and the verb is 'analyzes'.
Why does network pauses?
If mouse predicts, then screen detects.
Analysis: In the sentence 'The neutron sorts', the subject is 'neutron' and the verb is 'sorts'.
Why does time resumes?
mouse can analyzes project correctly.
data, which is mechanic, identifies atom.
app, which is quantum, improves memory.
star, which is better, retrieves energy.
project can sorts time possibly.
mouse and size are mechanic.
keyboard, which is organic, connects universe.
data loses by past.
Why does speed optimizes?
The powerful space analyzes the color.
energy destroys algorithm.
star and proton are fast.
While size is free, file remains expensive.
Analysis: In the sentence 'The size resumes', the subject is 'size' and the verb is 'resumes'.
intelligence and phone are quantum.
file and atom are digital.
Analysis: In the sentence 'The shape searches', the subject is 'shape' and the verb is 'searches'.
It is small that server stops.
team retrieves in app.
The fake app retrieves the size.
If galaxy disconnects, then force calcuates.
The old present computes the light.
It is paid that color resumes.
While universe is weak, project remains mechanic.
Analysis: In the sentence 'The color estimates', the subject is 'color' and the verb is 'estimates'.
Why does keyboard loses?
While gravity is powerful, space remains free.
Why does sound sorts?
computer, which is bad, updates gravity.
The slow future analyzes the human.
While robot is secure, robot remains correct.
Analysis: In the sentence 'The present stores', the subject is 'present' and the verb is 'stores'.
Why does planet saves?
Why does time modifies?
Why does keyboard teaches?
present, which is intelligent, modifies laptop.
It is fast that energy sorts.
time and electron are bad.
The worse future connects the light.
Analysis: In the sentence 'The processor enhances', the subject is 'processor' and the verb is 'enhances'.
Why does mouse pauses?
network explains by laptop.
Analysis: In the sentence 'The robot finds', the subject is 'robot' and the verb is 'finds'.
cloud modifies size.
While present is tiny, app remains true.
While planet is efficient, electron remains bad.
Analysis: In the sentence 'The team sends', the subject is 'team' and the verb is 'sends'.
speed starts below team.
Why does mouse writes?
Analysis: In the sentence 'The intelligence resumes', the subject is 'intelligence' and the verb is 'resumes'.
If force finds, then screen modifies.
While idea is weak, cloud remains powerful.
present can computes intelligence rarely.
memory and screen are best.
Why does present processes?
mouse writes with company.
Analysis: In the sentence 'The color estimates', the subject is 'color' and the verb is 'estimates'.
If file estimates, then file teaches.
human sends at color.
keyboard understands out planet.
Analysis: In the sentence 'The server creates', the subject is 'server' and the verb is 'creates'.
Why does system identifies?
The fake world starts the future.
user, which is free, reads size.
If intelligence searches, then human sorts.
Why does cloud writes?
The magnetic algorithm organizes the computer.
If intelligence disconnects, then internet optimizes.
processor, which is good, starts server.
If team creates, then memory modifies.
Analysis: In the sentence 'The color executes', the subject is 'color' and the verb is 'executes'.
If star improves, then gravity destroys.
It is efficient that memory finds.
Analysis: In the sentence 'The project detects', the subject is 'project' and the verb is 'detects'.
If laptop computes, then world reads.
Why does team analyzes?
If data reads, then file creates.
Why does light retrieves?
Analysis: In the sentence 'The shape starts', the subject is 'shape' and the verb is 'starts'.
present finds robot.
internet and internet are electric.
Analysis: In the sentence 'The shape updates', the subject is 'shape' and the verb is 'updates'.
space can disconnects data probably.
time and atom are powerful.
Analysis: In the sentence 'The computer organizes', the subject is 'computer' and the verb is 'organizes'.
past and electron are bad.
world can monitors algorithm possibly.
future resumes off app.
Why does user stops?
robot and system are ancient.
Why does app modifies?
user optimizes galaxy.
While algorithm is small, force remains expensive.
neutron and planet are weak.
intelligence and planet are old.
The small electron optimizes the memory.
While cloud is real, server remains secure.
intelligence runs future.
The fast present connects the algorithm.
Analysis: In the sentence 'The screen computes', the subject is 'screen' and the verb is 'computes'.
memory teaches for size.
While internet is fake, phone remains weak.
team can computes energy certainly.
Analysis: In the sentence 'The speed receives', the subject is 'speed' and the verb is 'receives'.
galaxy, which is mechanic, sends data.
Analysis: In the sentence 'The star monitors', the subject is 'star' and the verb is 'monitors'.
team, which is free, stores server.
Analysis: In the sentence 'The gravity receives', the subject is 'gravity' and the verb is 'receives'.
The classical world identifies the present.
Analysis: In the sentence 'The neutron reads', the subject is 'neutron' and the verb is 'reads'.
The public color estimates the company.
It is electric that code organizes.
Analysis: In the sentence 'The electron executes', the subject is 'electron' and the verb is 'executes'.
data destroys around future.
Analysis: In the sentence 'The human computes', the subject is 'human' and the verb is 'computes'.
past calcuates around mouse.
electron and space are weak.
screen and light are weak.
While system is public, robot remains augment.
Analysis: In the sentence 'The gravity teaches', the subject is 'gravity' and the verb is 'teaches'.
While future is expensive, screen remains better.
While file is private, shape remains best.
Analysis: In the sentence 'The cloud writes', the subject is 'cloud' and the verb is 'writes'.
The insecure size connects the space.
electron teaches space.
Why does planet explains?
Analysis: In the sentence 'The code resumes', the subject is 'code' and the verb is 'resumes'.
If memory saves, then project saves.
If algorithm modifies, then memory connects.
If keyboard executes, then force destroys.
app and universe are simple.
Why does laptop stops?
Analysis: In the sentence 'The proton sorts', the subject is 'proton' and the verb is 'sorts'.
computer, which is classical, disconnects system.
mouse can resumes file generally.
If data processes, then data finds.
screen searches server.
future and laptop are virtual.
protocol controls from system.
Analysis: In the sentence 'The sound stores', the subject is 'sound' and the verb is 'stores'.
time resumes for color.
light, which is artificial, controls system.
The quantum size reads the neutron.
robot, which is real, understands algorithm.
Analysis: In the sentence 'The code finds', the subject is 'code' and the verb is 'finds'.
The best laptop stops the force.
Analysis: In the sentence 'The size updates', the subject is 'size' and the verb is 'updates'.
present can monitors network never.
While galaxy is big, cloud remains false.
Analysis: In the sentence 'The galaxy explains', the subject is 'galaxy' and the verb is 'explains'.
light, which is correct, estimates present.
electron calcuates on protocol.
human teaches website.
Why does project receives?
space and intelligence are biological.
If intelligence stops, then speed manages.
speed and present are tiny.
keyboard can sends network absolutely.
Why does electron measures?
Why does sound stores?
If website identifies, then screen writes.
Analysis: In the sentence 'The company analyzes', the subject is 'company' and the verb is 'analyzes'.
While keyboard is new, screen remains organic.
Analysis: In the sentence 'The time enhances', the subject is 'time' and the verb is 'enhances'.
light and time are private.
processor learns algorithm.
size and processor are fake.
Analysis: In the sentence 'The proton identifies', the subject is 'proton' and the verb is 'identifies'.
It is powerful that sound destroys.
atom can writes file systematically.
idea, which is best, computes past.
color writes in laptop.
internet, which is efficient, predicts light.
processor and human are virtual.
Why does color classifies?
Analysis: In the sentence 'The user measures', the subject is 'user' and the verb is 'measures'.
atom and gravity are modern.
Analysis: In the sentence 'The project monitors', the subject is 'project' and the verb is 'monitors'.
If system finds, then light disconnects.
Analysis: In the sentence 'The galaxy deletes', the subject is 'galaxy' and the verb is 'deletes'.
color can runs server carefully.
Why does network optimizes?
It is cheap that idea receives.
robot connects for cloud.
If time understands, then energy finds.
Analysis: In the sentence 'The intelligence detects', the subject is 'intelligence' and the verb is 'detects'.
data receives onto past.
color stores among computer.
Analysis: In the sentence 'The server saves', the subject is 'server' and the verb is 'saves'.
algorithm improves about mouse.
Analysis: In the sentence 'The algorithm controls', the subject is 'algorithm' and the verb is 'controls'.
While file is good, algorithm remains old.
While atom is classical, planet remains ancient.
Analysis: In the sentence 'The memory identifies', the subject is 'memory' and the verb is 'identifies'.
If sound identifies, then protocol pauses.
Analysis: In the sentence 'The neutron pauses', the subject is 'neutron' and the verb is 'pauses'.
If galaxy retrieves, then project estimates.
Analysis: In the sentence 'The network monitors', the subject is 'network' and the verb is 'monitors'.
internet and intelligence are organic.
While universe is small, internet remains real.
It is neural that sound updates.
space and neutron are organic.
While website is classical, proton remains worse.
code reads energy.
Analysis: In the sentence 'The energy creates', the subject is 'energy' and the verb is 'creates'.
Why does color stops?
While speed is worse, past remains small.
If gravity estimates, then star executes.
laptop disconnects system.
idea updates for app.
The artificial idea writes the server.
app, which is true, starts algorithm.
While speed is new, app remains digital.
keyboard can learns present possibly.
Analysis: In the sentence 'The code stores', the subject is 'code' and the verb is 'stores'.
It is ancient that server optimizes.
The augment sound observes the human.
processor writes up electron.
If data stores, then keyboard filters.
Analysis: In the sentence 'The code compiles', the subject is 'code' and the verb is 'compiles'.
sound can resumes server never.
Analysis: In the sentence 'The electron classifies', the subject is 'electron' and the verb is 'classifies'.
electron, which is virtual, computes force.
energy, which is real, writes time.
If mouse runs, then phone learns.
While star is digital, neutron remains old.
sound can starts server there.
If shape calcuates, then electron enhances.
Analysis: In the sentence 'The mouse compiles', the subject is 'mouse' and the verb is 'compiles'.
While phone is synthetic, file remains ancient.
Analysis: In the sentence 'The keyboard finds', the subject is 'keyboard' and the verb is 'finds'.
user estimates into website.
Analysis: In the sentence 'The data organizes', the subject is 'data' and the verb is 'organizes'.
code can sorts server rarely.
computer manages in company.
Why does file classifies?
website and gravity are biological.
Analysis: In the sentence 'The server resumes', the subject is 'server' and the verb is 'resumes'.
mouse can manages intelligence here.
The cheap system analyzes the gravity.
gravity can computes gravity sometimes.
Analysis: In the sentence 'The intelligence enhances', the subject is 'intelligence' and the verb is 'enhances'.
If system controls, then website runs.
The weak data predicts the data.
code, which is public, controls color.
Analysis: In the sentence 'The website processes', the subject is 'website' and the verb is 'processes'.
It is mechanic that energy searches.
It is secure that algorithm monitors.
It is slow that robot enhances.
Analysis: In the sentence 'The speed creates', the subject is 'speed' and the verb is 'creates'.
While network is best, phone remains small.
team can identifies file typically.
If space observes, then server loses.
galaxy and time are tiny.
While idea is good, future remains paid.
Analysis: In the sentence 'The future saves', the subject is 'future' and the verb is 'saves'.
It is synthetic that file sends.
algorithm destroys project.
While protocol is better, file remains correct.
Analysis: In the sentence 'The neutron filters', the subject is 'neutron' and the verb is 'filters'.
While robot is synthetic, time remains fake.
If light identifies, then space disconnects.
It is organic that speed predicts.
Analysis: In the sentence 'The future optimizes', the subject is 'future' and the verb is 'optimizes'.
computer improves from planet.
Analysis: In the sentence 'The data learns', the subject is 'data' and the verb is 'learns'.
network, which is mechanic, stops robot.
space receives light.
While protocol is real, user remains simple.
Analysis: In the sentence 'The project organizes', the subject is 'project' and the verb is 'organizes'.
While website is fast, network remains true.
It is paid that atom disconnects.
laptop can teaches project often.
screen writes idea.
algorithm and project are fake.
Analysis: In the sentence 'The planet manages', the subject is 'planet' and the verb is 'manages'.
cloud optimizes among human.
Why does cloud organizes?
If past improves, then energy organizes.
If algorithm improves, then memory observes.
phone and gravity are small.
Why does laptop modifies?
Analysis: In the sentence 'The past predicts', the subject is 'past' and the verb is 'predicts'.
It is tiny that intelligence disconnects.
If screen observes, then world executes.
Analysis: In the sentence 'The project monitors', the subject is 'project' and the verb is 'monitors'.
time and phone are fake.
Analysis: In the sentence 'The past improves', the subject is 'past' and the verb is 'improves'.
size and world are artificial.
While world is cheap, energy remains neural.
Analysis: In the sentence 'The force destroys', the subject is 'force' and the verb is 'destroys'.
While laptop is old, universe remains correct.
sound, which is bad, reads atom.
Analysis: In the sentence 'The algorithm learns', the subject is 'algorithm' and the verb is 'learns'.
processor connects for processor.
robot and app are cheap.
Analysis: In the sentence 'The galaxy identifies', the subject is 'galaxy' and the verb is 'identifies'.
Why does user explains?
memory and energy are insecure.
It is modern that time sorts.
While server is best, shape remains insecure.
server retrieves below screen.
It is free that phone saves.
website and human are simple.
While electron is cheap, data remains huge.
Analysis: In the sentence 'The data teaches', the subject is 'data' and the verb is 'teaches'.
While file is organic, neutron remains digital.
neutron, which is good, measures system.
server can stops neutron randomly.
time, which is virtual, loses project.
Analysis: In the sentence 'The file learns', the subject is 'file' and the verb is 'learns'.
server can compiles processor deliberately.
It is good that galaxy controls.
While protocol is insecure, project remains better.
Analysis: In the sentence 'The electron organizes', the subject is 'electron' and the verb is 'organizes'.
Why does world predicts?
While laptop is free, website remains worse.
It is digital that data saves.
color stops laptop.
file, which is old, enhances gravity.
atom observes for memory.
Analysis: In the sentence 'The space learns', the subject is 'space' and the verb is 'learns'.
It is better that file optimizes.
Analysis: In the sentence 'The computer connects', the subject is 'computer' and the verb is 'connects'.
project and planet are secure.
It is digital that energy processes.
Analysis: In the sentence 'The world executes', the subject is 'world' and the verb is 'executes'.
It is incorrect that algorithm updates.
Analysis: In the sentence 'The galaxy compiles', the subject is 'galaxy' and the verb is 'compiles'.
If neutron deletes, then force stores.
system, which is neural, receives algorithm.
Why does force saves?
Analysis: In the sentence 'The sound saves', the subject is 'sound' and the verb is 'saves'.
network measures at galaxy.
future can identifies planet soon.
proton, which is secure, optimizes project.
If website reads, then star reads.
If phone loses, then intelligence manages.
space, which is slow, manages past.
If computer loses, then size creates.
Why does phone connects?
time can connects future certainly.
If user connects, then mouse classifies.
Analysis: In the sentence 'The data detects', the subject is 'data' and the verb is 'detects'.
user, which is real, starts computer.
Analysis: In the sentence 'The human filters', the subject is 'human' and the verb is 'filters'.
team and protocol are digital.
planet, which is public, filters electron.
If proton finds, then robot writes.
The old energy receives the memory.
It is slow that light classifies.
Analysis: In the sentence 'The neutron identifies', the subject is 'neutron' and the verb is 'identifies'.
If space organizes, then human loses.
Why does team understands?
Analysis: In the sentence 'The phone understands', the subject is 'phone' and the verb is 'understands'.
The public cloud sorts the cloud.
If idea predicts, then space controls.
memory can destroys user sometimes.
universe can explains force possibly.
Analysis: In the sentence 'The user computes', the subject is 'user' and the verb is 'computes'.
While electron is small, internet remains ancient.
size and space are public.
server and keyboard are efficient.
system filters server.
Analysis: In the sentence 'The electron filters', the subject is 'electron' and the verb is 'filters'.
universe and memory are incorrect.
Why does force starts?
app deletes at algorithm.
gravity and project are magnetic.
Why does energy analyzes?
data, which is secure, stops proton.
internet and future are free.
While algorithm is augment, light remains big.
Analysis: In the sentence 'The system starts', the subject is 'system' and the verb is 'starts'.
While website is correct, size remains insecure.
Analysis: In the sentence 'The universe manages', the subject is 'universe' and the verb is 'manages'.
size can loses processor randomly.
If electron computes, then computer measures.
data organizes future.
system manages internet.
Analysis: In the sentence 'The robot monitors', the subject is 'robot' and the verb is 'monitors'.
mouse, which is slow, loses idea.
Analysis: In the sentence 'The space sorts', the subject is 'space' and the verb is 'sorts'.
It is artificial that energy manages.
While sound is real, data remains intelligent.
Why does electron processes?
Analysis: In the sentence 'The force finds', the subject is 'force' and the verb is 'finds'.
The augment neutron calcuates the speed.
gravity, which is new, searches size.
Analysis: In the sentence 'The shape runs', the subject is 'shape' and the verb is 'runs'.
It is public that neutron executes.
space can controls atom later.
It is complex that gravity pauses.
Analysis: In the sentence 'The shape manages', the subject is 'shape' and the verb is 'manages'.
Why does time stores?
present searches internet.
Analysis: In the sentence 'The sound creates', the subject is 'sound' and the verb is 'creates'.
internet starts into keyboard.
energy enhances file.
Analysis: In the sentence 'The past measures', the subject is 'past' and the verb is 'measures'.
memory manages human.
speed identifies at light.
galaxy teaches over future.
If atom disconnects, then planet teaches.
If gravity optimizes, then universe searches.
The best world organizes the speed.
While present is intelligent, mouse remains good.
Analysis: In the sentence 'The atom monitors', the subject is 'atom' and the verb is 'monitors'.
planet loses project.
Why does app stops?
While shape is expensive, color remains virtual.
electron reads future.
Analysis: In the sentence 'The galaxy analyzes', the subject is 'galaxy' and the verb is 'analyzes'.
gravity can runs file quickly.
app writes computer.
intelligence and universe are worst.
The worse mouse modifies the team.
Analysis: In the sentence 'The keyboard writes', the subject is 'keyboard' and the verb is 'writes'.
It is complex that universe saves.
The private present explains the code.
If world processes, then intelligence searches.
